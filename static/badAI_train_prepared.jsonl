{"prompt":"Amazon AI Recruiting tool favored male applicants over female. The software downgraded resumes that had the word \"women\"\n\n###\n\n","completion":" Last December Synced compiled its first “Artificial Intelligence Failures” recap of AI gaffes from the previous year. AI has achieved remarkable progress, and many scientists dream of creating the Master Algorithm proposed by Pedro Domingos — which can solve all problems envisioned by humans. It’s unavoidable however that researchers, fledgling technologies and biased data will also produce blunders not envisioned by humans..We produce professional, authoritative, and thought-provoking content relating to artificial intelligence, machine intelligence, emerging technologies and industrial insights.\n"}
{"prompt":"The company realized its new system was not rating candidates for software developer jobs and other technical posts in a gender-neutral way.\n\n###\n\n","completion":" The team had been building computer programs since 2014 to review job applicants’ resumes with the aim of mechanizing the search for top talent, five people familiar with the effort told Reuters..Automation has been key to Amazon’s e-commerce dominance, be it inside warehouses or driving pricing decisions. The company’s experimental hiring tool used artificial intelligence to give job candidates scores ranging from one to five stars - much like shoppers rate products on Amazon, some of the people said..“Everyone wanted this holy grail,” one of the people said. “They literally wanted it to be an engine where I’m going to give you 100 resumes, it will spit out the top five, and we’ll hire those.”.But by 2015, the company realized its new system was not rating candidates for software developer jobs and other technical posts in a gender-neutral way..That is because Amazon’s computer models were trained to vet applicants by observing patterns in resumes submitted to the company over a 10-year period. Most came from men, a reflection of male dominance across the tech industry..In effect, Amazon’s system taught itself that male candidates were preferable. It penalized resumes that included the word “women’s,” as in “women’s chess club captain.” And it downgraded graduates of two all-women’s colleges, according to people familiar with the matter. They did not specify the names of the schools..Amazon edited the programs to make them neutral to these particular terms. But that was no guarantee that the machines would not devise other ways of sorting candidates that could prove discriminatory, the people said..The Seattle company ultimately disbanded the team by the start of last year because executives lost hope for the project, according to the people, who spoke on condition of anonymity. Amazon’s recruiters looked at the recommendations generated by the tool when searching for new hires, but never relied solely on those rankings, they said..Amazon declined to comment on the technology’s challenges, but said the tool “was never used by Amazon recruiters to evaluate candidates.” The company did not elaborate further. It did not dispute that recruiters looked at the recommendations generated by the recruiting engine..The companys experiment, which Reuters is first to report, offers a case study in the limitations of machine learning. It also serves as a lesson to the growing list of large companies including Hilton Worldwide Holdings Inc HLT.N and Goldman Sachs Group Inc GS.N that are looking to automate portions of the hiring process..Some 55 percent of U.S. human resources managers said artificial intelligence, or AI, would be a regular part of their work within the next five years, according to a 2017 survey by talent software firm CareerBuilder..Employers have long dreamed of harnessing technology to widen the hiring net and reduce reliance on subjective opinions of human recruiters. But computer scientists such as Nihar Shah, who teaches machine learning at Carnegie Mellon University, say there is still much work to do..“How to ensure that the algorithm is fair, how to make sure the algorithm is really interpretable and explainable - that’s still quite far off,” he said..Amazon’s experiment began at a pivotal moment for the world’s largest online retailer. Machine learning was gaining traction in the technology world, thanks to a surge in low-cost computing power. And Amazon’s Human Resources department was about to embark on a hiring spree: Since June 2015, the company’s global headcount has more than tripled to 575,700 workers, regulatory filings show..So it set up a team in Amazon’s Edinburgh engineering hub that grew to around a dozen people. Their goal was to develop AI that could rapidly crawl the web and spot candidates worth recruiting, the people familiar with the matter said..The group created 500 computer models focused on specific job functions and locations. They taught each to recognize some 50,000 terms that showed up on past candidates’ resumes. The algorithms learned to assign little significance to skills that were common across IT applicants, such as the ability to write various computer codes, the people said..Instead, the technology favored candidates who described themselves using verbs more commonly found on male engineers’ resumes, such as “executed” and “captured,” one person said..Gender bias was not the only issue. Problems with the data that underpinned the models’ judgments meant that unqualified candidates were often recommended for all manner of jobs, the people said. With the technology returning results almost at random, Amazon shut down the project, they said..Kevin Parker, chief executive of HireVue, a startup near Salt Lake City, said automation is helping firms look beyond the same recruiting networks upon which they have long relied. His firm analyzes candidates’ speech and facial expressions in video interviews to reduce reliance on resumes..You weren’t going back to the same old places; you weren’t going back to just Ivy League schools, Parker said. His companys customers include Unilever PLC ULVR.L and Hilton..Goldman Sachs has created its own resume analysis tool that tries to match candidates with the division where they would be the “best fit,” the company said..Microsoft Corps MSFT.O LinkedIn, the worlds largest professional network, has gone further. It offers employers algorithmic rankings of candidates based on their fit for job postings on its site..Some activists say they are concerned about transparency in AI. The American Civil Liberties Union is currently challenging a law that allows criminal prosecution of researchers and journalists who test hiring websites’ algorithms for discrimination..“We are increasingly focusing on algorithmic fairness as an issue,” said Rachel Goodman, a staff attorney with the Racial Justice Program at the ACLU..Still, Goodman and other critics of AI acknowledged it could be exceedingly difficult to sue an employer over automated hiring: Job candidates might never know it was being used..As for Amazon, the company managed to salvage some of what it learned from its failed AI experiment. It now uses a “much-watered down version” of the recruiting engine to help with some rudimentary chores, including culling duplicate candidate profiles from databases, one of the people familiar with the project said.\n"}
{"prompt":"Influenced public through individualized ads and messaging that played a significant role in political campaigns in the US. Micro-targeting individuals in political campaigns.\n\n###\n\n","completion":" What are the allegations against Cambridge Analytica?The data analytics firm used personal information harvested from more than 50 million Facebook profiles without permission to build a system that could target US voters with personalised political advertisements based on their psychological profile, according to Christopher Wylie, a former Cambridge Analytica contractor who helped build the algorithm. Employees of Cambridge Analytica, including the suspended CEO Alexander Nix, were also filmed boasting of using manufactured sex scandals, fake news and dirty tricks to swing elections around the world..How is Facebook involved in the scandal?The social media company has received a number of warnings about its data security policies in recent years and had known about the Cambridge Analytica data breach since 2015, but only suspended the firm and the Cambridge university researcher who harvested user data from Facebook earlier this month. A former Facebook manager has warned that hundreds of millions of users are likely to have had their private information used by private companies in the same way. On Sunday, Facebook ran adverts in several major UK and US newspapers apologising for the data breach, and said it was investigating other applications that had access to large amounts of user data..What has been the reaction to the scandal?Investigators from Britain’s data watchdog raided Cambridge Analytica’s London offices over Friday night, and the main consumer protection body in the US is reported to have opened an investigation into whether Facebook has violated privacy agreements. Billions of dollars have been wiped off Facebook’s stock market valuation this week as a growing #DeleteFacebook movement and regulatory fears have spooked investors..What is the Brexit link?During the Brexit referendum, a digital services firm linked to Cambridge Analytica received a £625,000 payment from a pro-Brexit campaign organisation which had been given the money by Vote Leave, potentially violating referendum spending rules. Shahmir Sanni, a pro-Brexit whistleblower, told the Observer newspaper that he had passed evidence supporting his claims to the police and the Electoral Commission..Separately, around £3.4m was spent by different Brexit Leave campaigns with Canadian data firm AggregateIQ during the run up to the EU referendum, including £2.7m by the official Vote Leave campaign (40% of their total budget). Christopher Wylie says he played a role in setting up AIQ in 2013, around same time he worked for Cambridge Analytica. AIQ have say they have never entered into a contract with Cambridge Analytica and had no communications with them during the referendum campaign..Why is the Electoral Commission involved?British electoral law forbids co-ordination between different campaign groups, which must all comply with strict spending limits. If they plan tactics or co-ordinate together, the organisations must share a cap on spending..Sanni has alleged that key figures in the Vote Leave campaign may have violated referendum spending rules and then attempted to destroy evidence. According to him, the £625,000 donation Vote Leave made to a pro-Brexit youth campaign group, who then spent the money on digital campaigning services with a Cambridge Analytica-linked firm, was not a genuine donation. Sanni also alleges that when the commission opened an investigation into Vote Leave last March, key Vote Leave figures tried to hide the possible co-ordination by removing themselves from the Google drive both campaign groups shared.\n"}
{"prompt":"Cart was not AI equipped but started conversations on how robots combat along\n\n###\n\n","completion":" This is the third Synced year-end compilation of “Artificial Intelligence Failures.” Despite AI’s rapid growth and remarkable achievements, a review of AI failures remains necessary and meaningful. Our aim is not to downplay or mock research and development results, but rather to take a look at what went wrong with the hope we can do better next time..A leading facial-recognition system identified three-time Super Bowl champion Duron Harmon of the New England Patriots, Boston Bruins forward Brad Marchand, and 25 other New England professional athletes as criminals. Amazon’s Rekognition software incorrectly matched the athletes to a database of mugshots in a test organized by the Massachusetts chapter of the American Civil Liberties Union (ACLU). Nearly one-in-six athletes were falsely identified..The misclassifications were an embarrassment for Amazon, which has marketed Rekognition to police agencies for use in their investigations. “This technology is flawed,” Harmon said in an ACLU statement, and “should not be used by the government without protections.”.In March the CEO of a UK-based energy firm got a phone call from his boss at the German parent company instructing him to transfer €220,000 ($243,000) to a Hungarian supplier. The ‘boss’ said the request was urgent and directed the UK CEO to transfer the money promptly..It turns out the phone call was made by criminals who used AI-based software to mimic the boss’s voice, including the “slight German accent and the melody of his voice,” as reported in The Wall Street Journal. Such AI-powered cyberattacks are a new challenge for companies, as traditional cybersecurity tools designed for keeping hackers off corporate networks can’t identify spoofed voices..Then there’s the artificial intelligence system that’s not very “artificial.” That was the accusation leveled at Engineer.ai in an article that appeared in The Wall Street Journal in August. The Indian startup claimed to have built an AI-assisted app development platform, but the WSJ, citing former and current employees, suggested it relies mostly on human engineers and “exaggerates its AI capabilities to attract customers and investors.”.Engineer.ai has attracted nearly US$30 million in funding from a SoftBank-owned firm and others. Founder Sachin Dev Duggal says the company’s AI tools are only human-assisted, and that it provides a service to help customers make more than 80 percent of a mobile app from scratch in about an hour. The WSJ story argued that Engineer.ai did not use AI to assemble code as it claimed, instead it used human engineers in India and elsewhere to put together the app. Indian media Inc42 claims that Engineer.ai had released a detailed statement to clear the accusation, and declared that it never claimed to ‘automated software development,’ but a human-assisted AI. The case is so far left open..Is it a mushroom or is it a pretzel? OK forget about the pretzel, 99 percent sure this is a sea lion…Or wait, actually it could be a fox squirrel. Yup, looks like this one is a fox squirrel for sure… except that it’s not, nope. It’s a bullfrog. Wait….Computer vision strives to understand what it sees the way humans do — but remains far from that goal. In July, researchers from Berkeley, University of Chicago and University of Washington hand-curated a dataset of 7,500 unretouched nature photos which are able to confuse SOTA computer vision models 98 percent of the time..The ImageNet-A dataset of “natural adversarial examples” is but a tiny subset of the 14 million labeled images in industry-standard ImageNet, exploiting flaws in current classifiers which can over-rely for example on color, texture, and background cues..A Boston Robotics’ Spot robodog suffered a dramatic onstage death while being live demo’d by company CEO Marc Raibert at re:MARS 2019 in Las Vegas this summer. The commercial robot was tasked with walking but its legs seemed to buckle. It stumbled desperately before sadly collapsing to the floor, where it lay motionless in front of the gasping audience..Founded in 1992, American engineering and robotics design company Boston Dynamics has created incredible robots such as BigDog, Atlas, SpotMini, etc. While these flexible and versatile bots can jump over logs, open doors and even perform search and rescue tasks, this was not the first time they’ve succumbed to stage fright..When Hong Kong real estate tycoon Samathur Li Kin-kan let an automated platform based on a supercomputer called “K1” manage part of his fortune the goal was to boost funds. Instead the AI regularly lost up to US$20 million daily, according to a Bloomberg story..Li filed a $23 million lawsuit against Raffaele Costa, CEO and founder of Tyndaris Investments, which sold Li the fintech service. The suit alleges Costa exaggerated K1’s abilities and is the first known case of a court action filed over automated investment losses. A verdict is expected in April 2020..An app that uses neural networks to virtually disrobe images of women caused public outrage early this year before it was shut down by its creator, anonymous programmer ‘Alberto.’ The DeepNude app used a photo of a clothed person as input to create a new, naked image of the same person..The app of course has no X-Ray ability, it merely replaces clothes with naked breasts and a vulva — as such only realistically functioning on images of women. After a Vice story made it viral, DeepNude was taken down. Multiple people however then uploaded their own DeepNude-style apps to code repository GitHub, which responded by removing all clothes-stripping code from its platform, citing the “Sexually Obscene” section of the GitHub Community Guidelines..Researchers with the San Diego-based AI firm Kneron were able to fool facial recognition systems at banks, border crossings and airports using a printed 3D masks — and in some cases only a 2D photo..The team used high-quality masks based on people in databases the facial recognition system would access. The method was tested in public locations globally. In stores in Asia where facial recognition technology is deployed widely, the 3D masks deceived popular AliPay and WeChat payment systems. More alarmingly, at a self-boarding terminal at Amsterdam’s Schiphol Airport the team tricked a sensor with just a photo on a smartphone screen, Fortune reported..In 2015 the first futuristic, robot-staffed Henn-na Hotel opened in Japan to much fanfare. Bots staffed the front-desk and worked as cleaners, porters and in-room assistants. Early this year however, the hotel chain bucked global tech and labour trends and dismissed the last of their “unreliable, expensive and annoying” bots, replacing them with human workers..The robot-staff novelty had worn off as customer complaints accumulated — the bots frequently broke down, could not provide satisfactory answers to guest queries, and in-room assistants startled guests at night by interpreting snoring as a wake command. Henn-na Hotels says it will head back to the lab to see if it can develop a new generation of more capable hospitality bots..There was a dramatic scene at Chicago’s O’Hare International Airport in September when an unmanned catering cart suddenly broke bad on the tarmac, circling out of control and ever-closer to a vulnerable jet parked at a gate. Finally, a yellow-vested worker managed to stop the cart — by ramming and knocking it down with another vehicle..Although the cart was neither AI-equipped nor autonomous its frenzied behavior drew comparisons to robot combat along with comments warning of the perils of machines gone amok and lauding the humans who resist and defeat them..Need a comprehensive review of the past, present and future of modern AI research development? Trends of AI Technology Development Report is out!.2018 Fortune Global 500 Public Company AI Adaptivity Report is out!Purchase a Kindle-formatted report on Amazon.Apply for Insight Partner Program to get a complimentary full PDF report..We produce professional, authoritative, and thought-provoking content relating to artificial intelligence, machine intelligence, emerging technologies and industrial insights.\n"}
{"prompt":"A TV advert created by Amazon triggered Alexa devices in the US to order a Dolls House on behalf of 100s of Customers. This purchase decision without a human input has a potential to be harmful.\n\n###\n\n","completion":" Children ordering (accidentally or otherwise) items from gadgets is nothing new. Major retailers have refunded purchases made by children playing with phones or computers, and with voice-activated devices making their way into homes, it’s a problem that parents will have to be on the lookout for. .The story could have stopped there, had it not ended up on a local morning show on San Diego’s CW6 News. At the end of the story, Anchor Jim Patton remarked: “I love the little girl, saying ‘Alexa ordered me a dollhouse,’” According to CW6 News, Echo owners who were watching the broadcast found that the remark triggered orders on their own devices. .Patton didn’t think that any of the devices went through with their purchases, who told The Verge that the station received a handful of reports of viewer devices attempting to order a dollhouse after hearing his remarks. “As for the number of people affected - I dont know,” Patton noted in an email. “Personally, Ive seen one other email and have been told there were others, as well as calls to our news desk with similar stories.” .Alexa’s settings can be adjusted through the device’s app, and users can either turn off voice ordering altogether, or add a passcode to prevent accidental purchases. \n"}
{"prompt":"Ending schools at different time zones might prioritize various families differently\n\n###\n\n","completion":" When the Boston public school system announced new start times last December, some parents found the schedules unacceptable and pushed back. The algorithm used to set these times had been designed by MIT researchers, and about a week later, Kade Crockford, director of the Technology for Liberty Program at the ACLU of Massachusetts, emailed asking me to cosign an op-ed that would call on policymakers to be more thoughtful and democratic when they consider using algorithms to change policies that affect the lives of residents. Kade, who is also a Directors Fellow at the Media Lab and a colleague of mine, is always paying attention to the key issues in digital liberties and is great at flagging things that I should pay attention to. (At the time, I had no contact with the MIT researchers who designed the algorithm.).I made a few edits to her draft, and we shipped it off to the Boston Globe, which ran it on December 22, 2017, under the headline Don’t blame the algorithm for doing what Boston school officials asked. In the op-ed, we piled on in criticizing the changes but argued that people shouldnt criticize the algorithm, but rather the city’s political process that prescribed the way in which the various concerns and interests would be optimized. That day, the Boston Public Schools decided not to implement the changes. Kade and I high-fived and called it a day..The protesting families, Kade and I did what we thought was fair and just given the information that we had at the time. A month later, a more nuanced picture emerged, one that I think offers insights into how technology can and should provide a platform for interacting with policy—and how policy can reflect a diverse set of inputs generated by the people it affects. In what feels like a particularly dark period for democracy and during a time of increasingly out-of-control deployment of technology into society, I feel a lesson like this one has given me greater understanding of how we might more appropriately introduce algorithms into society. Perhaps it even gives us a picture of what a Democracy 2.0 might look like..A few months later, having read the op-ed in the Boston Globe, Arthur Delarue and Sébastien Martin, PhD students in the MIT Operations Research Center and members of the team that built Boston’s bus algorithm, asked to meet me. In very polite email, they told me that I didn’t have the whole story..Kade and I met later that month with Arthur, Sebastien, and their adviser, MIT professor Dimitris Bertsimas. One of the first things they showed us was a photo of the parents who had protested against the schedules devised by the algorithm. Nearly all of them were white. The majority of families in the Boston school system are not white. White families represent only about 15 percent the public school population in the city. Clearly something was off..The MIT researchers had been working with the Boston Public Schools on adjusting bell times, including the development of the algorithm that the school system used to understand and quantify the policy trade-offs of different bell times and, in particular, their impact on school bus schedules. The main goal was to reduce costs and generate optimal schedules..The MIT team described how the award-winning original algorithm, which focused on scheduling and routing, had started as a cost-calculation algorithm for the Boston Public Schools Transportation Challenge. Boston Public Schools had been trying to change start times for decades but had been confounded by the optimizations and a way to improve the school schedule without tripling the costs, which is why it organized Transportation Challenge to begin with. The MIT team was the first to figure out a way to balance all of these factors and produce a solution. Until then, calculating the cost of the complex bus system had been such a difficult problem that it presented an impediment to even considering bell time changes..After the Transportation Challenge, the team continued to work with the city, and over the previous year they had participated in a community engagement process and had worked with the Boston school system to build on top of the original algorithm, adding new features that were included to produce a plan for new school start times. They factored in equity—existing start times were unfair, mostly to lower-income families—as well as recent research on teenage sleep that showed starting school early in the day may have negative health and economic consequences for high school students. They also tried to prioritize special education programs and prevent young children from leaving school too late. They wanted to do all this without increasing the budget, and even reducing it..From surveys, the school system and the researchers knew that some families in every school would be unhappy with any change. They could have added additional constraints on the algorithm to limit some of outlier situations, such as ending the school day at some schools at 1:30 pm, which was particularly exasperating for some parents. The solution that they were proposing significantly increased the number of high school students starting school after 8 am and significantly decreased the number of elementary school students dismissed after 4 pm so they wouldn’t have to go home after dark. Overall it was much better for the majority of people. Although they were aware that some parents wouldn’t be happy, they werent prepared for the scale of response from angry parents who ended up with start times and bus schedules that they didnt like..Optimizing the algorithm for greater “equity also meant many of the planned changes were biased against families with privilege. My view is that the fact that an algorithm was making decisions also upset people. And the families who were happy with the new schedule probably didn’t pay as much attention. The families who were upset marched on City Hall in an effort to overturn the planned changes. The ACLU and I supported the activist parents at the time and called foul on the school system and the city. Eventually, the mayor and the city caved to the pressure and killed off years of work and what could have been the first real positive change in busing in Boston in decades..While Im not sure privileged families would give up their good start times to help poor families voluntarily, I think that if people had understood what the algorithm was optimizing for—sleep health of high school kids, getting elementary school kids home before dark, supporting kids with special needs, lowering costs, and increasing equity overall—they would agree that the new schedule was, on the whole, better than the previous one. But when something becomes personal very suddenly, people to feel strongly and protest..It reminds me a bit of a study, conducted by the Scalable Cooperation Group at the Media Lab based on earlier work by Joshua Greene, which showed people would support the sacrifice by a self-driving car of its passenger if it would save the lives of a large number of pedestrians, but that they personally would never buy a passenger-sacrificing self-driving car..Technology is amplifying complexity and our ability to change society, altering the dynamics and difficulty of consensus and governance. But the idea of weighing trade-offs isnt new, of course. Its a fundamental feature of a functioning democracy..While the researchers working on the algorithm and the plan surveyed and met with parents and school leadership, the parents were not aware of all of the factors that went into the final optimization of the algorithm. The trade-offs required to improve the overall system were not clear, and the potential gains sounded vague compared to the very specific and personal impact of the changes that affected them. And by the time the message hit the nightly news, most of the details and the big picture were lost in the noise..A challenge in the case of the Boston Public Schools bus route changes was the somewhat black-box nature of the algorithm. The Center for Deliberative Democracy has used a process it calls deliberative polling, which brings together a statistically representative group of residents in a community to debate and deliberate policy goals over several days in hopes of reaching a consensus about how a policy should be shaped. If residents of Boston could have more easily understood the priorities being set for the algorithm, and hashed them out, they likely would have better understood how the results of their deliberations were converted into policy..After our meeting with the team that invented the algorithm, for instance, Kade Crockford introduced them to David Scharfenberg, a reporter at the Boston Globe who wrote an article about them that included a very well done simulation allowing readers to play with the algorithm and see how changing cost, parent preferences, and student health interact as trade-offs—a tool that would have been extremely useful in explaining the algorithm from the start..The lessons learned from Boston’s effort to use technology to improve its bus routing system and start times provides a valuable lesson in understanding how to ensure that such tools aren’t used to reinforce and increase biased and unfair policies. They can absolutely make systems more equitable and fair, but they won’t succeed without our help.\n"}
{"prompt":"Bot fails to demonstrate the enhance use of kitchen appliances\n\n###\n\n","completion":" Cloi was meant to be the centrepiece of the South Korean firms presentation where it was supposed to show how new artificial intelligence tech could enhance use of kitchen appliances..The event was the first press conference of the morning at the Las Vegas tech fair. Samsung, Intel, HTC and Sony are among other big companies scheduled to unveil products over the course of the day..LGs focus this year was to promote ThinQ, its in-house AI software, which it intends to roll out across various products to make them easier to use and capable of evolving to meet customers needs..Although LG also includes Googles Assistant in some of its latest TVs and smart speakers, it has chosen to rely on its own solution for its robots..The machine was described as being the ultimate in simplicity when managing your smart home, when it was presented on stage by David VanderWaal, LGs US marketing chief..But subsequent requests to find out if his washing was ready, what was planned for dinner and what recipes it could suggest for chicken all fell flat..The first time it failed everyone laughed and thought it was just a glitch, commented Ben Wood, an analyst at CCS Insight, who was in the audience..Although it is far from the first time a CES demo has gone awry, Mr Wood said it highlighted a problem with many of the AI-based technologies being pitched to the public..It will be damaging to LG and highlights a broader point about all the companies exhibiting here: is their technology really ready for prime time?.One industry-watcher said LG was likely to take a short-term hit to its brand as a result of the event, but said that it would also serve as a reminder that the promise of a seamless smart home was still some way off..The target market for these kinds of products is still hi-tech-friendly early adopters who are willing to put in more effort than the regular user, said Tuong Nguyen from the consultancy Gartner..The challenge for manufacturers is that they not only to offer a good user experience but also have to get all the different technologies involved to work together.\n"}
{"prompt":"A Predictive policing software that started with LAPD and now is being used across the US. A lot of controversy on its use because of bias and feedback loops that reinforce targeting target minority neighborhoods\n\n###\n\n","completion":" It seems a simple question, but it’s one without simple answers. That’s particularly true in the arcane world of artificial intelligence (AI), where the notion of smart, emotionless machines making decisions wonderfully free of bias is fading fast..Perhaps the most public taint of that perception came with a 2016 ProPublica investigation that concluded that the data driving an AI system used by judges to determine if a convicted criminal is likely to commit more crimes appeared to be biased against minorities. Northpointe, the company that created the algorithm, known as COMPAS, disputed ProPublica’s interpretation of the results, but the clash has sparked both debate and analysis about how much even the smartest machines should be trusted..Neill now finds himself in the middle of that discussion. A computer scientist at Carnegie Mellon University, he and another researcher, Will Gorr, developed a crime-predicting software tool called CrimeScan several years ago. Their original concept was that in some ways violent crime is like a communicable disease, that it tends to break out in geographic clusters. They also came to believe that lesser crimes can be a harbinger of more violent ones, so they built an algorithm using a wide range of “leading indicator” data, including reports of crimes, such as simple assaults, vandalism and disorderly conduct, and 911 calls about such things as shots fired or a person seen with a weapon. The program also incorporates seasonal and day of week trends, plus short-term and long-term rates of serious violent crimes. .The idea is to track sparks before a fire breaks out. “We look at more minor crimes,” Neill says. “Simple assaults could harden to aggravated assaults. Or you might have an escalating pattern of violence between two gangs.”.CrimeScan is not the first software designed for what’s known as predictive policing. A program called PredPol was created eight years ago by UCLA scientists working with the Los Angeles Police Department, with the goal of seeing how scientific analysis of crime data could help spot patterns of criminal behavior. Now used by more than 60 police departments around the country, PredPol identifies areas in a neighborhood where serious crimes are more likely to occur during a particular period.  .The company claims its research has found the software to be twice as accurate as human analysts when it comes to predicting where crimes will happen. No independent study, however, has confirmed those results.  .Both PredPol and CrimeScan limit their projections to where crimes could occur, and avoid taking the next step of predicting who might commit them—a controversial approach that the city of Chicago has built around a “Strategic Subject List” of people most likely to be involved in future shootings, either as a shooter or victim..The American Civil Liberties Union [ACLU], the Brennan Center for Justice and various civil rights organizations have all raised questions about the risk of bias being baked into the software. Historical data from police practices, critics contend, can create a feedback loop through which algorithms make decisions that both reflect and reinforce attitudes about which neighborhoods are “bad” and which are “good.” That’s why AI based primarily on arrests data carries a higher risk of bias—it’s more reflective of police decisions, as opposed to actual reported crimes. CrimeScan, for instance, stays away from trying to forecast crimes that, as Neill puts it, “you’re only going to find if you look for them.”.Then there’s the other side of the feedback loop. If a predictive tool raises expectations of crimes in a certain neighborhood, will police who patrol there be more aggressive in making arrests?.“There’s a real danger, with any kind of data-driven policing, to forget that there are human beings on both sides of the equation,” notes Andrew Ferguson, a professor of law at the University of the District of Columbia and author of the book, The Rise of Big Data Policing: Surveillance, Race, and the Future of Law Enforcement. “Officers need to be able to translate these ideas that suggest different neighborhoods have different threat scores. And, focusing on the numbers instead of the human being in front of you changes your relationship to them.”.The reality is that artificial intelligence now plays a role—albeit often in the background—in many decisions affecting daily lives—from helping companies choose who to hire to setting credit scores to evaluating teachers. Not surprisingly, that has intensified public scrutiny of how machine learning algorithms are created, what unintended consequences they cause, and why they generally aren’t subjected to much review. .For starters, much of the software is proprietary, so there’s little transparency behind how the algorithms function. And, as machine learning becomes more sophisticated, it will become increasingly difficult for even the engineers who created an AI system to explain the choices it made. That opaque decision-making, with little accountability, is a consequence of what’s become known as “black box” algorithms..“The public never gets a chance to audit or debate the use of such systems,” says Meredith Whittaker, a co-founder of the AI Now Institute, a research organization at New York University that focuses on AI’s impact in society. “And, the data and logics that govern the predictions made are often unknown even to those who use them, let alone to the people whose lives are impacted.”.In a report issued last fall, AI Now went so far as to recommend that no public agencies responsible for such matters as criminal justice, health care, welfare and education should use black box AI systems. According to AI Now, seldom are legal and ethical issues given much consideration when the software is created..“Just as you wouldn’t trust a judge to build a deep neural network, we should stop assuming that an engineering degree is sufficient to make complex decisions in domains like criminal justice,” says Whittaker..Another organization, the Center for Democracy & Technology, has generated a “digital decisions” tool  to help engineers and computer scientists create algorithms that produce fair and unbiased results. The tool asks a lot of questions meant to get them to weigh their assumptions and identify unforeseen ripple effects..“We wanted to give people a concrete starting point for thinking through issues like how representative their data is, which groups of people might be left out, and whether their model’s outputs are going to have unintended negative consequences,” says Natasha Duarte, who oversees the project..While there has been a push to make developers more cognizant of the possible repercussions of their algorithms, others point out that public agencies and companies reliant on AI also need to be accountable..“There is this emphasis on designers understanding a system. But it’s also about the people administering and implementing the system,” says Jason Schultz, a professor of law at New York University who works with the AI Now Institute on legal and policy issues. That’s where the rubber meets the road in accountability. A government agency using AI has the most responsibility and they need to understand it, too. If you can’t understand the technology, you shouldn’t be able to use it.”.To that end, AI Now is promoting the use of “algorithmic impact assessments,” which would require public agencies to disclose the systems they’re using, and allow outside researchers to analyze them for potential problems. When it comes to police departments, some legal experts think it’s also important for them to clearly spell out how they’re using technology and be willing to share that with the local community..“If these systems are designed from the standpoint of accountability, fairness and due process, the person implementing the system has to understand they have a responsibility,” Schultz says. “And when we design how we’re going to implement these, one of the first questions is ‘Where does this go in the police manual? If you’re not going to have this somewhere in the police manual, let’s take a step back, people.”.“At least once a year, there should be an accountability moment for police technology in every local jurisdiction,” he says. “The police chief, the mayor or maybe the head of the city council would have to explain to the community what they’re using taxpayer dollars for in terms of surveillance and technology, why they think it’s a good use of the money, what they’re doing to audit it and protect the data, what are the privacy implications. And the community would be there to ask questions.”.Daniel Neill, the CrimeScan creator, says he wouldn’t object to the idea of regular audits of AI results, although he has reservations about that being done before an algorithm is adequately field-tested. He is currently working with the Pittsburgh Bureau of Police on a CrimeScan trial, and at least initially there was a challenge with “getting the right patrol intensity for the predicted hot spots.”.It’s been a learning process, he says, to adapt CrimeScan so that police officers at the street level believe it’s helpful. “We need to show that not only can we predict crime, but also that we can actually prevent it,” Neill notes. “If you just throw the tool over the wall and hope for the best, it never works that well.”.“A tool can help police officers make good decisions,” he says. “I don’t believe machines should be making decisions. They should be used for decision support.\n"}
{"prompt":"Case Management and Decision Support tool used in some U.S. courts to assess the likelihood of a defendant becoming a recidivist\n\n###\n\n","completion":" Guilty or innocent? In a bench trial, that’s up to the judge. Would it surprise you to know some judges in our country are posing this question to a computer?.Meet COMPAS, a computer-based algorithm that is currently being used in criminal cases to predict a defendant’s likelihood to re-offend. Artificial Intelligence is becoming more commonplace in our everyday lives. Now that it’s being introduced in such an important role in the court system, some commentators in the legal realm have pondered the pros and cons of using artificially-intelligent judges in the courtroom to render, in theory, unbiased opinions. As experts on experts, this got us to thinking about the expanded uses of artificially-intelligent experts in the courtroom, too..Let’s back up and discuss how Artificial Intelligence is currently being utilized. COMPAS is currently used in criminal cases and serves as a leading example of how artificially-intelligent devices can form decisions based on statistical information. The computer algorithm assesses about 100 factors to determine a defendant’s statistical likelihood of rehabilitation or re-offense..The algorithm’s assessment based on factors including sex, age, and criminal history has been tested and deemed reliable atidentifying defendants with the highest risk score who went on to re-offend at a rate four times greater than those assigned the lowest risk score..Some have labeled COMPAS unfair, claiming the algorithm can be biased and, just like humans, the artificial intelligence forms a prejudice based on past case rulings..Preconceptions, based on experience, can help both humans and computers sort and process information more efficiently. But, it will occasionally cause us to get verdicts or judgments wrong. Particularly, when we apply the prejudice or statistical reasoning to an anomaly case, i.e., the exception that doesn’t follow the rule..For example, a judge who believes expert witnesses who have been published on a particular topic should have their testimony admitted over an expert who hasn’t. While that rule of thumb, like the COMPAS algorithm, might frequently result in the judge’s proper admission of reliable expert testimony, there will always be an exception where an unpublished expert who is offering a perfectly-reliable opinion will be excluded..That would be an error but, according to the Washington Post, it’s a statistical error that cannot be avoided. What can be avoided, if we were to embrace artificially intelligent judges rendering opinions from the bench, is human bias. A computer cannot be swayed by emotions, politics, money, or power. Artificially-intelligent judges could be programmed with the knowledge of thousands, or likely millions, of legal opinions which would teach it the law and the systematic application of the law to facts of the same, or at least similar, to those at hand to allow it to render a fair and impartial decision that follows stare decisis..This got us thinking. If an artificially-intelligent judge could be considered an expert on the law because he knows the law and how it has been applied in the past, could we also create AI experts? The input of data, patterns, and millions of real world outcomes sounds a lot like experience. And it is the primary characteristic we look for in experts. If that can be merely inputted, then perhaps the experts of tomorrow, both on the bench or the witness stand, will be artificially intelligent..IMS Consulting & Expert Services delivers award-winning consultative trial and expert services for the most influential global firms through every stage of litigation. Combining the perspectives and proprietary methods developed over 30 years and more than 2,000 trials, IMS provides attorneys with the essential services they need to win: persuasion strategy, expert witness placement, jury consulting, trial graphics, and trial presentation..Under certain state laws the following statements may be required on this website and we have included them in order to be in full compliance with these rules. The choice of a lawyer or other professional is an important decision and should not be based solely upon advertisements. Attorney Advertising Notice: Prior results do not guarantee a similar outcome. Statement in compliance with Texas Rules of Professional Conduct. Unless otherwise noted, attorneys are not certified by the Texas Board of Legal Specialization, nor can NLR attest to the accuracy of any notation of Legal Specialization or other Professional Credentials.\n"}
{"prompt":"Facial recognition software to \"catch\" criminals in crowded settings had a 92% false-positive rate of identifying criminals.\n\n###\n\n","completion":" A police force has defended its use of facial recognition technology after it was revealed that more than 2,000 people in Cardiff during the 2017 Champions League final were wrongly identified as potential criminals. .South Wales police began trialling the technology in June last year in an attempt to catch more criminals. The cameras scan faces in a crowd and compare them against a database of custody images. .As 170,000 people arrived in the Welsh capital for the football match between Real Madrid and Juventus, 2,470 potential matches were identified. .South Wales police admitted that “no facial recognition system is 100% accurate”, but said the technology had led to more than 450 arrests since its introduction. It also said no one had been arrested after an incorrect match..A spokesman for the force said: “Over 2,000 positive matches have been made using our ‘identify’ facial recognition technology, with over 450 arrests. .“Successful convictions so far include six years in prison for robbery and four-and-a-half years imprisonment for burglary. The technology has also helped identify vulnerable people in times of crisis. .“Technical issues are common to all face recognition systems, which means false positives will be an issue as the technology develops. Since initial deployments during the European Champions League final in June 2017, the accuracy of the system used by South Wales police has continued to improve.” .The force blamed the high number of false positives at the football final on “poor quality images” supplied by agencies, including Uefa and Interpol, as well as the fact it was its first major deployment of the technology. .Figures also revealed that 46 people were wrongly identified at an Anthony Joshua fight, while there were 42 false positives from a rugby match between Wales and Australia in November. .The chief constable, Matt Jukes, said the technology was used where there were likely to be large gatherings, because they were “potential terrorist targets”. .“We need to use technology when we’ve got tens of thousands of people in those crowds to protect everybody, and we are getting some great results from that,” he told the BBC. “But we don’t take the use of it lightly and we are being really serious about making sure it is accurate.” .The force said it had considered privacy issues “from the outset”, and had built in checks to ensure its approach was justified and proportionate. .In a post on Twitter, the group said: “Not only is real-time facial recognition a threat to civil liberties, it is a dangerously inaccurate policing tool.”\n"}
{"prompt":"Christchurch terrorist attack was live streamed on Facebook\n\n###\n\n","completion":" New Zealand Prime Minister Jacinda Ardern has been in contact with leaders of major tech companies like Facebook, Google, Microsoft, and Twitter as part of her push to slow the spread of violent content online..Ardern on Wednesday announced that she and French President Emmanuel Macron planned to host a summit in Paris on May 15 to encourage industry and world leaders to commit to a pledge called the Christchurch Call, which seeks to curb extremist content on social media..No tech company — just like no government — wishes to see violent extremism and terrorism online, she told reporters on Wednesday. So we have a starting point which is one of unity..New Zealand has been eager to clamp down on malicious social-media activity since 50 people were fatally shot and dozens more were injured in the terrorist attack in Christchurch on March 15. The gunman livestreamed the attacks at two separate mosques on Facebook, and copies of the gruesome video quickly spread on that platform and others..The March 15 terrorist attacks saw social media used in an unprecedented way as a tool to promote an act of terrorism and hate, Ardern said. We are asking for a show of leadership to ensure social media cannot be used again the way it was..Ardern said she spoke directly with Facebook CEO Mark Zuckerberg, who shared his condolences after the shooting. She did not elaborate on the details of their discussion..Facebook faced harsh blowback over the livestream video, which was briefly hosted on its site. The company said it had blocked 1.5 million videos of the attacks within 24 hours. Still, Business Insider was able to find numerous copies circulating across social media in the days after the shooting..Under New Zealand law, dissemination or possession of material depicting extreme violence or terrorism is prohibited, according to The New York Times. New Zealands human-rights laws also forbid the incitement of what the country calls racial disharmony through written or broadcast media..New Zealands censorship office last month made the possession and sharing of the 17-minute livestream illegal. According to Television New Zealand, those who distribute the video could face a maximum of 14 years in prison. Six people appeared in a New Zealand court last week on charges of illegally distributing video from the Christchurch shooting.\n"}
{"prompt":"A facial-recognition system identified NFL champions as criminals\n\n###\n\n","completion":" In a test the ACLU of Massachusetts conducted using a widely available facial recognition technology called “Rekognition,” the software falsely matched 27 New England professional athletes to individuals in a mugshot database. The test shows high-profile athletes, including the Patriots’ Duron Harmon, were mistakenly matched with images in the arrest photo database..“This technology is flawed,” said Harmon, the New England Patriots safety. “If it misidentified me, my teammates, and other professional athletes in an experiment, imagine the real-life impact of false matches. This technology should not be used by the government without protections. Massachusetts should press pause on face surveillance technology.”.To perform the test, the ACLU of Massachusetts compared the official headshots of 188 New England athletes from the Boston Bruins, Boston Celtics, Boston Red Sox, and New England Patriots with a database of 20,000 public arrest photos. Nearly one-in-six athletes were falsely identified. An independent computer science expert verified the results..“The results of this scan add to the mounting evidence that unregulated face surveillance technology in the hands of government agencies is a serious threat to individual rights, due process, and democratic freedoms,” said Kade Crockford, director of the Technology for Liberty Program at the ACLU of Massachusetts. “Face surveillance is dangerous when it doesn’t work, and when it does. There are currently no rules or standards in place in our state to ensure the technology isn’t misused or abused. Massachusetts must pass a moratorium on government use of face surveillance technology until there are safeguards in place to keep people safe and free.”.In June, the ACLU of Massachusetts launched the “Press Pause on Face Surveillance” campaign to build awareness about the civil liberties concerns posed by face surveillance and the need to pass a statewide moratorium on the government’s use of the technology. An ACLU-backed bill currently before legislators on Beacon Hill would establish a statewide moratorium on government use of face surveillance and other biometric screening technologies until the legislature imposes checks and balances to protect the public’s interest. A recent poll shows 79 percent of Massachusetts voters support a moratorium on government use of face surveillance technology, and 91 percent think the government should not use the technology unless it is subject to regulation..Face surveillance technology is currently deployed and marketed in Massachusetts without any regulations. In emails uncovered, a face surveillance company CEO admits to Plymouth municipal authorities that his technology might work only 30 percent of the time. Nonetheless, he pushes aggressively for its adoption in schools, government buildings, and public streets — all in secret, with no public debate or buy-in from elected officials..A similar test conducted last year by the ACLU of California misidentified 28 sitting members of Congress; the false matches were disproportionately of people of color, including six members of the Congressional Black Caucus. Multiple studies of facial recognition technology have found systems to be inaccurate when used against women and people of color.\n"}
{"prompt":"Neural networks disrobe images of women\n\n###\n\n","completion":" Update June 27, 3:03 p.m. EST: The creator of DeepNude announced that hes taken down the app. Read more, here.   A programmer created an application that uses neural networks to remove clothing from the images of women, making them look realistically nude..The software, called DeepNude, uses a photo of a clothed person and creates a new, naked image of that same person. It swaps clothes for naked breasts and a vulva, and only works on images of women. When Motherboard tried using an image of a man, it replaced his pants with a vulva. While DeepNude works with varying levels of success on images of fully clothed women, it appears to work best on images where the person is already showing a lot of skin. We tested the app on dozens of photos and got the most convincing results on high resolution images from Sports Illustrated Swimsuit issues..Since Motherboard discovered deepfakes in late 2017, the media and politicians focused on the dangers they pose as a disinformation tool. But the most devastating use of deepfakes has always been in how theyre used against women: whether to experiment with the technology using images without womens consent, or maliciously spreading nonconsensual porn on the internet. DeepNude is an evolution of that technology that is easier to use and faster to create than deepfakes. DeepNude also dispenses with the idea that this technology can be used for anything other than claiming ownership over women’s bodies..This is absolutely terrifying, Katelyn Bowden, founder and CEO of revenge porn activism organization Badass, told Motherboard. Now anyone could find themselves a victim of revenge porn, without ever having taken a nude photo. This tech should not be available to the public..This is an “invasion of sexual privacy,” Danielle Citron, professor of law at the University of Maryland Carey School of Law, who recently testified to Congress about the deepfake threat, told Motherboard..“Yes, it isn’t your actual vagina, but… others think that they are seeing you naked,” she said. “As a deepfake victim said to me—it felt like thousands saw her naked, she felt her body wasn’t her own anymore.”.Motherboard downloaded the application and tested it on a Windows machine. It installed and launched like any other Windows application and didnt require technical expertise to use. In the free version of the app, the output images are partially covered with a large watermark. In a paid version, which costs $50, the watermark is removed, but a stamp that says FAKE is placed in the upper-left corner. (Cropping out the fake stamp or removing it with Photoshop would be very easy.).Motherboard tested it on more than a dozen images of women and men, in varying states of dress—fully clothed to string bikinis—and a variety of skin tones. The results vary dramatically, but when fed a well lit, high resolution image of a woman in a bikini facing the camera directly, the fake nude images are passably realistic. The algorithm accurately fills in details where clothing used to be, angles of the breasts beneath the clothing, nipples, and shadows..But its not flawless. Most images, and low-resolution images especially, produced some visual artifacts. DeepNude failed entirely with some photographs that used weird angles, lighting, or clothing that seem to throw off the neural network it uses. When we fed it an image of the cartoon character Jessica Rabbit, it distorted and destroyed the image altogether, throwing stray nipples into a blob of a figure..In an email, the anonymous creator of DeepNude, who requested to go by the name Alberto, told Motherboard that the software is based on pix2pix, an open-source algorithm developed by University of California, Berkeley researchers in 2017. Pix2pix uses generative adversarial networks (GANs), which work by training an algorithm on a huge dataset of images—in the case of DeepNude, more than 10,000 nude photos of women, the programmer said—and then trying to improve against itself. This algorithm is similar to whats used in deepfake videos, and what self-driving cars use to imagine road scenarios..The algorithm only works with women, Alberto said, because images of nude women are easier to find online—but hes hoping to create a male version, too..The networks are multiple, because each one has a different task: locate the clothes. Mask the clothes. Speculate anatomical positions. Render it, he said. All this makes processing slow (30 seconds in a normal computer), but this can be improved and accelerated in the future..Deepfake videos, by comparison, take hours or days to render a believable face-swapped video. For even a skilled editor, manually using Photoshop to realistically change a clothed portrait to nude would take several minutes..Alberto said he was inspired to create DeepNude by ads for gadgets like X-Ray glasses that he saw while browsing magazines from the 1960s and 70s, which he had access to during his childhood. The logo for DeepNude, a man wearing spiral glasses, is an homage to those ads..Like everyone, I was fascinated by the idea that they could really exist and this memory remained, he said. About two years ago I discovered the potential of AI and started studying the basics. When I found out that GAN networks were able to transform a daytime photo into a nighttime one, I realized that it would be possible to transform a dressed photo into a nude one. Eureka. I realized that x-ray glasses are possible! Driven by fun and enthusiasm for that discovery, I did my first tests, obtaining interesting results..Im not a voyeur, Im a technology enthusiast,” he said. “Continuing to improve the algorithm. Recently, also due to previous failures (other startups) and economic problems, I asked myself if I could have an economic return from this algorithm. Thats why I created DeepNude..I think that what you can do with DeepNude, you can do it very well with Photoshop (after a few hours of tutorial), he said, noting that DeepNude doesnt transmit images itself, only creates them and allows the user to do what they will with the results..I also said to myself: the technology is ready (within everyones reach), he said. So if someone has bad intentions, having DeepNude doesnt change much… If I dont do it, someone else will do it in a year..In the year and a half since Motherboard discovered deepfakes on Reddit, the machine learning technology it employs has moved at breakneck speed. Algorithmic face-swaps have gone from requiring hundreds of images and days of processing time in late 2017, to requiring only a handful of images, or even just text inputs, and a few hours of time, in recent months..Motherboard showed the DeepNude application to Hany Farid, a computer-science professor at UC Berkeley who has become a widely-cited expert on the digital forensics of deepfakes. Farid was shocked at this development, and the ease at which it can be done..We are going to have to get better at detecting deepfakes, and academics and researchers are going to have to think more critically about how to better safeguard their technological advances so that they do not get weaponized and used in unintended and harmful ways, Farid said. In addition, social media platforms are going to have to think more carefully about how to define and enforce rules surrounding this content. And, our legislators are going to have to think about how to thoughtfully regulate in this space..Deepfakes have become a widespread, international phenomenon, but platform moderation and legislation so far has failed to keep up with this fast-moving technology. In the meantime, women are victimized by deepfakes and left behind for a more political, US-centric political narrative. Though deepfakes have been weaponized most often against unconsenting women, most headlines and political fear of them have focused on their fake news potential..Even bills like the DEEPFAKES Accountability Act, introduced earlier this month, arent enough to stop this technology from hurting real people..It’s a real bind—deepfakes defy most state revenge porn laws because it’s not the victim’s own nudity depicted, but also our federal laws protect the companies and social media platforms where it proliferates, attorney Carrie Goldberg, whose law firm specializes in revenge porn, told Motherboard. It’s incumbent on the public to avoid consumption of what we call at my office humili-porn. Whether it’s revenge porn or deepfakes, don’t click or link or share or like! That’s how these sites make money. People need to stop letting their Id drive internet use and use the internet ethically and conscientiously..DeepNude is easier to use, and more easily accessible than deepfakes have ever been. Whereas deepfakes require a lot of technical expertise, huge datasets, and access to expensive graphics cards, DeepNude is a consumer-facing app that is easier to install than most video games that can produce a believable nude in 30 seconds with the click of a single button..Editors note, June 27 1:05 p.m. EST: This story originally included five side-by-side images of various celebrities and DeepNude-manipulated images of those celebrities. While the images were redacted to not show explicit nudity, after hearing from our readers, academic experts, and colleagues, we realized that those images could do harm to the real people in them. We think its important to show the real consequences that new technologies unleashed on the world without warning have on people, but we also have to make sure that our reporting minimizes harm. For that reason, we have removed the images from the story, and regret the error..By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.\n"}
{"prompt":"Tay bot project canceled after bot generated sexist and racist comments online\n\n###\n\n","completion":" It took less than 24 hours for Twitter to corrupt an innocent AI chatbot. Yesterday, Microsoft unveiled Tay — a Twitter bot that the company described as an experiment in conversational understanding. The more you chat with Tay, said Microsoft, the smarter it gets, learning to engage people through casual and playful conversation..Unfortunately, the conversations didnt stay playful for long. Pretty soon after Tay launched, people starting tweeting the bot with all sorts of misogynistic, racist, and Donald Trumpist remarks. And Tay — being essentially a robot parrot with an internet connection — started repeating these sentiments back to users, proving correct that old programming adage: flaming garbage pile in, flaming garbage pile out..Now, while these screenshots seem to show that Tay has assimilated the internets worst tendencies into its personality, its not quite as straightforward as that. Searching through Tays tweets (more than 96,000 of them!) we can see that many of the bots nastiest utterances have simply been the result of copying users. If you tell Tay to repeat after me, it will — allowing anybody to put words in the chatbots mouth..However, some of its weirder utterances have come out unprompted. The Guardian picked out a (now deleted) example when Tay was having an unremarkable conversation with one user (sample tweet: new phone who dis?), before it replied to the question is Ricky Gervais an atheist? by saying: ricky gervais learned totalitarianism from adolf hitler, the inventor of atheism..But while it seems that some of the bad stuff Tay is being told is sinking in, its not like the bot has a coherent ideology. In the span of 15 hours Tay referred to feminism as a cult and a cancer, as well as noting gender equality = feminism and i love feminism now. Tweeting Bruce Jenner at the bot got similar mixed response, ranging from caitlyn jenner is a hero & is a stunning, beautiful woman! to the transphobic caitlyn jenner isnt a real woman yet she won woman of the year? (Neither of which were phrases Tay had been asked to repeat.).Its unclear how much Microsoft prepared its bot for this sort of thing. The companys website notes that Tay has been built using relevant public data that has been modeled, cleaned, and filtered, but it seems that after the chatbot went live filtering went out the window. The company starting cleaning up Tays timeline this morning, deleting many of its most offensive remarks..Its a joke, obviously, but there are serious questions to answer, like how are we going to teach AI using public data without incorporating the worst traits of humanity? If we create bots that mirror their users, do we care if their users are human trash? There are plenty of examples of technology embodying — either accidentally or on purpose — the prejudices of society, and Tays adventures on Twitter show that even big corporations like Microsoft forget to take any preventative measures against these problems..In an emailed statement given later to Business Insider, Microsoft said: The AI chatbot Tay is a machine learning project, designed for human engagement. As it learns, some of its responses are inappropriate and indicative of the types of interactions some people are having with it. Were making some adjustments to Tay.\n"}
{"prompt":"Suspends use of facial recognition software to track people's genders and ages\n\n###\n\n","completion":" Cadillac Fairview, the company that owns popular malls including the Calgary Chinook Centre and Toronto Eaton Centre, told CBC News it will pause the use of cameras embedded inside mall directories..The company said its been using facial recognition software to track peoples genders and ages without consent since June. Its pausing the practice at Chinook Centre and Market Mall..Its unclear how many malls were using the software. Cadillac Fairview previously said it was testing the technology and hadnt completely rolled it out..On Friday, the federal privacy commissioner said it would investigate what the company is doing and whether its legal. Albertas provincial privacy commissioner is also investigating..Cadillac Fairview has stated that it is using the technology for the purposes of monitoring traffic ... The company contends it is not capturing images of individuals, The Office of the Privacy Commissioner of Canada said in a press release..But its not possible for software to determine a shoppers age and gender without capturing their image, argues Sharon Polsky, president of The Privacy and Access Council of Canada..To get to the point that the facial recognition technology can create the numerical representation of a face, theyve got to look at the face and gather the data points of that unique face, Polsky told Global News. They are collecting the information, even if its only for a moment..Facial recognition software is increasingly being used in public by police forces and private companies. Police in Orlando and Detroit have facial recognition software, as does the FBI. Amazon, which sells the technology to law enforcement, even offered consulting on the Orlando polices program free of charge, Wired reports..In China, some police wear smartglasses equipped with facial recognition so that they can spot persons of interest in crowded places and keep track of peoples habits.\n"}
{"prompt":"Questionability of ethical use in immigration, noticed flaws.\n\n###\n\n","completion":" For as long as he can remember, Danny Smith has wanted to be a U.S. Customs and Border Patrol agent. The 29-year-old Texan says he can’t imagine a better opportunity to serve his country with pride..His dream came true from 2010 to 2015, when he served as a CBP agent in Laredo, Texas. But when he and his wife moved to Dallas in 2015 for her job, he put his career on what he thought was a temporary hold. When he sought to return to the agency just weeks later, he was forced to take a polygraph, a new requirement for all CBP agents that had been introduced by Congress in 2010..“I was devastated,” he said in a phone interview. “It really affected me, depressed me. I didn’t have anything to hide. I wasn’t even told why I failed.”.As President Donald Trump seeks to enlist 5,000 more border agents to enforce his promise of curbing illegal immigration, he has proposed to waive some of the employment requirements that CBP union leaders and legislators say make it too hard to hire new agents, including the polygraph test..The Trump administration’s plan—outlined in the Department of Homeland Security’s 90-day progress report on immigration enforcement, which was leaked by the Washington Post earlier this month and delivered to the White House on Tuesday—would waive the polygraph test for certain applicants, including former law enforcement officers and veterans. Two-thirds of applicants now fail the polygraph exam..Making it easier to hire border agents is just one of several priorities in Trump’s beefed up enforcement plans. He’s facing widespread pushback from Democratic and Republican lawmakers who don’t want to pay for his proposed border wall, which could cost upwards of $70 billion. A number of polls show the majority of Americans dont want the wall..Critics of the proposed changes argue that the polygraph test was put in place to curb a wave of corruption and misconduct that CBP faced after its last hiring surge. They caution against what the American Immigration Lawyers Association described as “plans to water down hiring standards” within CBP..Tom Jawetz, the Vice President of Immigration Policy at the progressive organization Center for American Progress, told Univision it’s not the time to ease standards..“If you want to staff up, do you do that by trying to find more qualified agents or reducing the standards?” he asked. The fact that many people don’t pass the test is “not necessarily a reason to scrap it.”.Between 2006 and 2009, under President George W. Bush, the Border Patrol added some 8,000 new agents, from a total of 12,349 to 20,119, making it the largest law enforcement agency in the nation. Multiple subsequent corruption cases at CBP led Congress to pass the Anti-Border Corruption Act in 2010, which introduced the lie-detector test for applicants, calling it “a more streamlined and cost-effective process for bringing new applicants on board.”.“What we’re found is that many agents brought on beforehand who had not gone through a polygraph were cooperating with cartels and subject to corruption,” Jawetz said. “The polygraph has served a function to weed out people in the beginning.”.Jay Ahern, CBP deputy commissioner when the agency doubled in size, told Foreign Policy last month: “if you start lowering standards, the organization pays for it for the next decade, two, or three.”.“If there are ways to prevent corruption, we’re all for it,” said Art del Cueto, the president of National Border Patrol Council Local 2544 in Tucson. “None of us want to work with corrupt agents, none of us do. But the polygraph is not accurate.”.CBP union officials point out that the 65 percent failure rate for the polygraph exam is far higher than the rate at other federal agencies. A January Associated Press investigation found that the failure rate is more than double the average rate at eight law enforcement agencies. As of February, CBP was already 1,768 agents short of its floor..Del Cueto says the test is now used as a sort of shortcut, in place of the much more comprehensive background check used previously, and is intended to trick people into failing. The tests can last up to eight hours, and nervousness is often mistaken as lies. He says he’s heard countless “horror stories” from perfectly qualified agents, and others who don’t even venture to apply for fear of hurting their chance to work in the federal government later on..“Veterans with top secret clearance are failing,” he said, comparing the test to torture. “It’s an interrogation. The only thing missing is waterboarding. That’s how awful these have become.”.According to a report by the Government Accountability Office, more than 2,000 CBP agents were arrested for misconduct between 2005 and 2012, including for DUIs, domestic violence and false imprisonment. Corruption often went unpunished. In 2014, a CBP agent in McAllen, Texas, was under suspicion for misconduct when he kidnapped three Honduran women he had detained. After raping them and attempting to kill two of them, he shot himself..Before he became an agent in 2010, Smith says he underwent a rigorous two-year application period, involving a full FBI background check. He also went through an entrance exam, medical checks and academy training. He says an agent should always “self-police” and speak up if they see anything suspicious involving another agent..Now at the Bureau of Prisons, Smith has become an impassioned advocate against the polygraph requirement and often visits forums, such as AntiPolygraph.org, where people discuss their experiences..“I believe the polygraph is designed to keep you out, at least the way it’s being administered,” he said. “I think it can work as a deterrent for bad applicants but I feel they need to use some common sense when it comes to individuals with proven track records. Your accolades, credibility and work history are what should determine your eligibility.”.The polygraph test is just one element of the application process. CBP data show a 50 percent Border Patrol candidate failure rate on the written test, 15 percent failure in an oral interview, 25 percent medical failure test, 15 percent failure on the physical fitness test, and 56 percent failure on the background check, according to a report from the American Immigration Council..DHS is also proposing to remove the Spanish language proficiency test, allow for remote testing, and loosen physical fitness requirements. It also proposes continued outreach, including to millennials, and expedited hiring..Unlike many of the report’s proposals, the polygraph requirement would need to be changed by congress. In March, Arizona Republican Senators Jeff Flake and John McCain proposed legislation to do that..CBP is consistently ranked one of the worst government agencies for workers, and del Cueto says retention is a major issue. “In hiring 5,000 more agents you need to retain as well as recruit,” he said..“You need to respect the sacrifices agents take on a day to day basis,” he added. “We’re dealing with heat, cold, chasing dangerous criminals trying to come in, working long hours. Border patrol agents are the most assaulted and isolated, living in the most remote areas of the country in small communities.”.Some residents of those communities tell a different story. Christian Ramirez, the director of the Southern Border Communities Coalition, says residents already fear border agents..“We get racially profiled on the way to school. I have to go through checkpoint if I want to take my child to a family member’s house. Trump describes border communities as ground zero of an imaginary war. We do not want more troops in our communities,” he said..The notion that border communities need more agents is “false and unjustified” at a time when illegal border crossings are down, he added. “People want to live in peace, dignity and have resources invested in infrastructure, not in building useless walls and deploying more agents.”.And the cost of Trump’s proposed expansion is dramatic. The entire long-term plan for 5,000 Border Patrol agents and 10,000 ICE officers would increase DHS’ existing budget by over $ 3.14 billion..He has family in Mexico, and understands why people would choose to leave their countries for the United States. But for him, the task is about keeping the country safe.\n"}
{"prompt":"Facebook translates good morning into attack them, leading into an arrest\n\n###\n\n","completion":" Facebook has apologised after an error in its machine-translation service saw Israeli police arrest a Palestinian man for posting “good morning” on his social media profile..The man, a construction worker in the West Bank settlement of Beitar Illit, near Jerusalem, posted a picture of himself leaning against a bulldozer with the caption “يصبحهم”, or “yusbihuhum”, which translates as “good morning”. .But Facebook’s artificial intelligence-powered translation service, which it built after parting ways with Microsoft’s Bing translation in 2016, instead translated the word into “hurt them” in English or “attack them” in Hebrew..Police officers arrested the man later that day, according to Israeli newspaper Haaretz, after they were notified of the post. They questioned him for several hours, suspicious he was planning to use the pictured bulldozer in a vehicle attack, before realising their mistake. At no point before his arrest did any Arabic-speaking officer read the actual post..Facebook said it is looking into the issue, and in a statement to Gizmodo, added: “Unfortunately, our translation systems made an error last week that misinterpreted what this individual posted. .“Even though our translations are getting better each day, mistakes like these might happen from time to time and we’ve taken steps to address this particular issue. We apologise to him and his family for the mistake and the disruption this caused.”.Arabic is considered particularly difficult for many machine translation services due to the large number of different dialects in use around the world, on top of Modern Standard Arabic, the international form of the language..The Israeli Defence Force has been open about monitoring the social media accounts of Palestinians, looking for “lone-wolf” attackers who might otherwise slip through the net. It reportedly does so automatically, using algorithms to look for terms such as “sword of Allah”..Machine translation mistakes are a regular occurrence for anyone using AI to translate languages, particularly ones with little relationship. Earlier this month, Chinese social network WeChat apologised after its own machine translation system translated a neutral phrase meaning “black foreigner” as the n-word. .“When I ran the translator, the n-word came up and I was gobsmacked,” said Ann James, who had been texting back and forth with a friend when the faulty translation appeared.\n"}
{"prompt":"Google Image Search for CEO has Barbie as first result\n\n###\n\n","completion":" Fresh off the revelation that Google image searches for “CEO” only turn up pictures of white men, there’s new evidence that algorithmic bias is, alas, at it again. In a paper published in April, a team of researchers from Carnegie Mellon University claim Google displays far fewer ads for high-paying executive jobs….“I think our findings suggest that there are parts of the ad ecosystem where kinds of discrimination are beginning to emerge and there is a lack of transparency,” Carnegie Mellon professor Annupam Datta told Technology Review. “This is concerning from a societal standpoint.”.To come to those conclusions, Datta and his colleagues basically built a tool, called Ad Fisher, that tracks how user behavior on Google influences the personalized Google ads that each user sees. Because that relationship is complicated and based on a lot of factors, the researchers used a series of fake accounts: theoretical job-seekers whose behavior they could track closely..That online behavior — visiting job sites and nothing else — was the same for all the fake accounts. But some listed their sex as men and some as women..The Ad Fisher team found that when Google presumed users to be male job seekers, they were much more likely to be shown ads for high-paying executive jobs. Google showed the ads 1,852 times to the male group — but just 318 times to the female group..This isn’t the first time that algorithm systems have appeared to be sexist — or racist, for that matter. When Flickr debuted image recognition tools in May, users noticed the tool sometimes tagged black people as “apes” or “animals.” A landmark study at Harvard previously found serious discrimination in online ad delivery, like when searching ethnic names on Google turned up more results around arrest records. Algorithms have hired by voice inflection. The list goes on and on..After all, algorithmic personalization systems, like the ones behind Google’s ad platform, don’t operate in a vacuum: They’re programmed by humans and taught to learn from user behavior. So the more we click or search or generally Internet in sexist, racist ways, the algorithms learn to generate those results and ads (supposedly the results we would expect to see)..“It’s part of a cycle: How people perceive things affects the search results, which affect how people perceive things,” Cynthia Matuszek, a computer ethics professor at University of Maryland and co-author of a study on gender bias in Google image search results, told The Washington Post in April..Google cautions that some other things could be going on here, too. The advertiser in question could have specified that the ad only been shown to certain users for a whole host of reasons, or the advertiser could have specified that the ad only show on certain third-party sites..“Advertisers can choose to target the audience they want to reach, and we have policies that guide the type of interest-based ads that are allowed,” reads a statement from Google..The interesting thing about the fake users in the Ad Fisher study, however, is that they had entirely fresh search histories: In fact, the accounts used were more or less identical, except for their listed gender identity. That would seem to indicate either that advertisers are requesting that high-paying job ads only display to men (and that Google is honoring that request) or that some type of bias has been programmed, if inadvertently, into Google’s ad-personalization system..“Many important decisions in society these days are being made by algorithms,” he said. “These algorithms run inside of boxes that we don’t have access to the internal details of. The genesis of this project was that we wanted to peek inside this box a little to see if there are more undesirable consequences of this activity going on.”.By registering, you will also enjoy limited access to Premium articles, exclusive newsletters, commenting, and virtual events with our leading journalists\n"}
{"prompt":"Faception released AI technology that could analyze facial images and bone structure to reveal person IQ and violent tendencies.\n\n###\n\n","completion":" We reveal personality from facial images at scale to revolutionize how companies, organizations and even robots understand people and dramatically improve public safety, communications, decision-making, and experiences..Faception is first-to-technology and first-to-market with proprietary computer vision and machine learning technology for profiling people and revealing their personality based only on their facial image.  [ learn about the science behind the technology].Faception can analyze faces from video streams (recorded and live), cameras, or online\/offline databases, encode the faces in proprietary image descriptors and match an individual with various personality traits and types with a high level of accuracy..We develop proprietary classifiers, each describing a certain personality type or trait such as an Extrovert, a person with High IQ, Professional Poker Player or a threats..Ultimately, we can score facial images on a set of classifiers and provide our clients with a better understanding of their customers, the people in front of them or in front of their cameras.Utilizing advanced machine learning techniques we developed and continue to evolve an array of classifiers. These classifiers represent a certain persona, with a unique personality type, a collection of personality traits or behaviors. Our algorithms can score an individual according to their fit to these classifiers..We live in a dangerous world, where terrorists and other criminals are easily mingle with the general population and easily travel between countries; the vast majority of them are unknown to the authorities. As a result, it is becoming ever more challenging to detect suspected individuals in public places such as airports, train stations, government and public buildings, and border controls..Current solutions mostly rely on facial recognition, detecting suspicious activity\/ behavior and manual profiling. They are not sufficient to handle the scale of the growing threats..What if it was possible to know whether an anonymous individual is a potential terrorist, an aggressive person, or a potential criminal? Better yet, what if that information could be obtained and used in real-time, when it matters the most?.Faception offers a breakthrough computer-vision and machine learning technology that goes beyond Biometrics. Our solution analyzes a person’s facial image and automatically reveals his personality, enabling security companies\/agencies to more efficiently detect, focus and apprehend potential terrorists or criminals before they have the opportunity to do harm..Our solution is easy to deploy with minimum integration work and can installed on the client hardware, the system can integrate with an existing face recognition platform and support operational hierarchy and reporting modules..Artificial intelligence has been touted as humanitys salvation and its downfall. But what is the truth behind the hype? This is AI looks at how AI is changing the world now, the scientists shaping it, and the lives affected by this nascent technology.Faception was lately selected to be the sole Israeli technology speaker in the biggest Public Safety and Security and Police Equipment show in China. It has been organized since 1994, and held every 2 years. The official name is the 14 China International Exhibition on Public Safety and Security 2018, called Security China 2018, organized by China Security Association, under the Ministry of Public Security China. Total exhibition area in 106800 square meters, more than 1000 exhibitors, and including totally 8 exhibit halls..Faception, a facial personality analytics startup based in Israel, said its software does detailed analyses of the facial characteristics of individuals captured by video and still photography. The predicted personality traits are used to identify individuals who could present a threat to public safety. According ...​\n"}
{"prompt":"AI system recognized a photo of a Chinese billionaire as a jaywalker\n\n###\n\n","completion":" Chinese police have admitted to wrongly shaming a famous businesswoman after a facial recognition system designed to catch jaywalkers mistook an advert on the side of a bus for her actual face. .Dong Mingzhu, president of Chians biggest air conditioning maker, had her image flashed up on a public display screen in the city of Ningbo, near Shanghai, with a caption saying she had illegally crossed the street on a red light..But Ningbos facial recognition cameras had actually only caught an advert featuring her face on the side of a passing bus – a fact quickly spotted by Chinese citizens, who shared pictures of the alert on Weibo, a social network similar to Twitter..On Wednesday, Ningbo traffic police admitted their mistake, saying the alert had been immediately deleted afterwards and that technicians had completely upgraded the system to reduce the false recognition rate..The incident highlights Chinas growing use of automated systems such as facial recognition cameras to catch petty criminals, as well as in a campaign of repression against Muslim citizens in the western province of Xinjiang..Ninbos authorities recently boasted that facial recognition cameras installed at six intersections had spotted more than 7,800 cases of jaywalking, while the industrial centre of Shenzhen claims to have shamed 14,000 in 10 months at one intersection alone. .Weiber users took the opportunity of Ningpos mistake to mock the authorities. Who is that person clinging onto the bus? Serious warning! joked one. It means the system works – it wont let go of any face, said another..Ms Dong is well-known in China as a tough female entrepreneur and single mother who rose from working class roots and who claims not to have used a day of her holiday allowance in 26 years..Her autobiography, published in 2006 and later made into a television series, was titled Regretless Pursuit, and one male competitor reportedly said of her: Where sister Dong walks, no grass grows..In interviews, she has said she regrets not spending more time with her son while he was growing up, but said she did not remarry after her husband died in 1984 because to do so would have compromised her independence..Her company, Gree Electric Appliances, issued a statement thanking Ningbos traffic police for their hard work and calling on people to obey traffic rules.\n"}
{"prompt":"Weaponized AI can produce many unintended impacts from unwanted deaths to bombings\n\n###\n\n","completion":" Last month marked the 17th anniversary of 9\/11. With it came a new milestone: we’ve been in Afghanistan for so long that someone born after the attacks is now old enough to go fight there. They can also serve in the six other places where we’re officially at war, not to mention the 133 countries where special operations forces have conducted missions in just the first half of 2018..The wars of 9\/11 continue, with no end in sight. Now, the Pentagon is investing heavily in technologies that will intensify them. By embracing the latest tools that the tech industry has to offer, the US military is creating a more automated form of warfare – one that will greatly increase its capacity to wage war everywhere forever..On Friday, the defense department closes the bidding period for one of the biggest technology contracts in its history: the Joint Enterprise Defense Infrastructure (Jedi). Jedi is an ambitious project to build a cloud computing system that serves US forces all over the world, from analysts behind a desk in Virginia to soldiers on patrol in Niger. The contract is worth as much as $10bn over 10 years, which is why big tech companies are fighting hard to win it. (Not Google, however, where a pressure campaign by workers forced management to drop out of the running.).At first glance, Jedi might look like just another IT modernization project. Government IT tends to run a fair distance behind Silicon Valley, even in a place as lavishly funded as the Pentagon. With some 3.4 million users and 4 million devices, the defense department’s digital footprint is immense. Moving even a portion of its workloads to a cloud provider such as Amazon will no doubt improve efficiency..But the real force driving Jedi is the desire to weaponize AI – what the defense department has begun calling “algorithmic warfare”. By pooling the military’s data into a modern cloud platform, and using the machine-learning services that such platforms provide to analyze that data, Jedi will help the Pentagon realize its AI ambitions..The scale of those ambitions has grown increasingly clear in recent months. In June, the Pentagon established the Joint Artificial Intelligence Center (JAIC), which will oversee the roughly 600 AI projects currently under way across the department at a planned cost of $1.7bn. And in September, the Defense Advanced Research Projects Agency (Darpa), the Pentagon’s storied R&D wing, announced it would be investing up to $2bn over the next five years into AI weapons research..So far, the reporting on the Pentagon’s AI spending spree has largely focused on the prospect of autonomous weapons – Terminator-style killer robots that mow people down without any input from a human operator. This is indeed a frightening near-future scenario, and a global ban on autonomous weaponry of the kind sought by the Campaign to Stop Killer Robots is absolutely essential..But AI has already begun rewiring warfare, even if it hasn’t (yet) taken the form of literal Terminators. There are less cinematic but equally scary ways to weaponize AI. You don’t need algorithms pulling the trigger for algorithms to play an extremely dangerous role..To understand that role, it helps to understand the particular difficulties posed by the forever war. The killing itself isn’t particularly difficult. With a military budget larger than that of China, Russia, Saudi Arabia, India, France, Britain and Japan combined, and some 800 bases around the world, the US has an abundance of firepower and an unparalleled ability to deploy that firepower anywhere on the planet..The US military knows how to kill. The harder part is figuring out whom to kill. In a more traditional war, you simply kill the enemy. But who is the enemy in a conflict with no national boundaries, no fixed battlefields, and no conventional adversaries?.This is the perennial question of the forever war. It is also a key feature of its design. The vagueness of the enemy is what has enabled the conflict to continue for nearly two decades and to expand to more than 70 countries – a boon to the contractors, bureaucrats and politicians who make their living from US militarism. If war is a racket, in the words of marine legend Smedley Butler, the forever war is one the longest cons yet..But the vagueness of the enemy also creates certain challenges. It’s one thing to look at a map of North Vietnam and pick places to bomb. It’s quite another to sift through vast quantities of information from all over the world in order to identify a good candidate for a drone strike. When the enemy is everywhere, target identification becomes far more labor-intensive. This is where AI – or, more precisely, machine learning – comes in. Machine learning can help automate one of the more tedious and time-consuming aspects of the forever war: finding people to kill..The Pentagon’s Project Maven is already putting this idea into practice. Maven, also known as the Algorithmic Warfare Cross-Functional Team, made headlines recently for sparking an employee revolt at Google over the company’s involvement. Maven is the military’s “pathfinder” AI project. Its initial phase involves using machine learning to scan drone video footage to help identify individuals, vehicles and buildings that might be worth bombing..“We have analysts looking at full-motion video, staring at screens 6, 7, 8, 9, 10, 11 hours at a time,” says the project director, Lt Gen Jack Shanahan. Maven’s software automates that work, then relays its discoveries to a human. So far, it’s been a big success: the software has been deployed to as many as six combat locations in the Middle East and Africa. The goal is to eventually load the software on to the drones themselves, so they can locate targets in real time..Won’t this technology improve precision, thus reducing civilian casualties? This is a common argument made by higher-ups in both the Pentagon and Silicon Valley to defend their collaboration on projects like Maven. Code for America’s Jen Pahlka puts it in terms of “sharp knives” versus “dull knives”: sharper knives can help the military save lives..In the case of weaponized AI, however, the knives in question aren’t particularly sharp. There is no shortage of horror stories of what happens when human oversight is outsourced to faulty or prejudiced algorithms – algorithms that can’t recognize black faces, or that reinforce racial bias in policing and criminal sentencing. Do we really want the Pentagon using the same technology to help determine who gets a bomb dropped on their head?.But the deeper problem with the humanitarian argument for algorithmic warfare is the assumption that the US military is an essentially benevolent force. Many millions of people around the world would disagree. In 2017 alone, the US and allied strikes in Iraq and Syria killed as many as 6,000 civilians. Numbers like these don’t suggest a few honest mistakes here and there, but a systemic indifference to “collateral damage”. Indeed, the US government has repeatedly bombed civilian gatherings such as weddings in the hopes of killing a high-value target..Further, the line between civilian and combatant is highly porous in the era of the forever war. A report from the Intercept suggests that the US military labels anyone it kills in “targeted” strikes as “enemy killed in action”, even if they weren’t one of the targets. The so-called “signature strikes” conducted by the US military and the CIA play similar tricks with the concept of the combatant. These are drone attacks on individuals whose identities are unknown, but who are suspected of being militants based on displaying certain “signatures” – which can be as vague as being a military-aged male in a particular area..The problem isn’t the quality of the tools, in other words, but the institution wielding them. And AI will only make that institution more brutal. The forever war demands that the US sees enemies everywhere. AI promises to find those enemies faster – even if all it takes to be considered an enemy is exhibiting a pattern of behavior that a (classified) machine-learning model associates with hostile activity. Call it death by big data..AI also has the potential to make the forever war more permanent, by giving some of the country’s largest companies a stake in perpetuating it. Silicon Valley has always had close links to the US military. But algorithmic warfare will bring big tech deeper into the military-industrial complex, and give billionaires like Jeff Bezos a powerful incentive to ensure the forever war lasts forever. Enemies will be found. Money will be made.\n"}
{"prompt":"US Army is developing an automatic target recognition unmanned aerial system\n\n###\n\n","completion":"  NOTE: The Solicitations and topics listed on                                 this site are copies from the various SBIR agency solicitations and are not necessarily                                 the latest and most up-to-date.                                 For this reason, you should use the agency link listed below which will take you                                 directly to the                                 appropriate agency server where you can read the official version of this solicitation                                 and download the appropriate forms and rules.                             .OBJECTIVE: Develop a system that can be integrated and deployed in a class 1 or class 2 Unmanned Aerial System (UAS) to automatically Detect, Recognize, Classify, Identify (DRCI) and target personnel and ground platforms or other targets of interest. The system should implement learning algorithms that provide operational flexibility by allowing the target set and DRCI taxonomy to be quickly adjusted and to operate in different environments. .DESCRIPTION: The use of UASs in military applications is an area of increasing interest and growth. This coupled with the ongoing resurgence in the research, development, and implementation of different types of learning algorithms such as Artificial Neural Networks (ANNs) provide the potential to develop small, rugged, low cost, and flexible systems capable of Automatic Target Recognition (ATR) and other DRCI capabilities that can be integrated in class 1 or class 2 UASs. Implementation of a solution is expected to potentially require independent development in the areas of sensors, communication systems, and algorithms for DRCI and data integration. Additional development in the areas of payload integration and Human-Machine Interface (HMI) may be required to develop a complete system solution. One of the desired characteristics of the system is to use the flexibility afforded by the learning algorithms to allow for the quick adjustment of the target set or the taxonomy of the target set DRCI categories or classes. This could allow for the expansion of the system into a Homeland Security environment. .PHASE I: Conduct an assessment of the key components of a complete objective payload system constrained by the Size Weight and Power (SWAP) payload restrictions of a class 1 or class 2 UAS. Systems Engineering concepts and methodologies may be incorporated in this assessment. It is anticipated that this will require, at a minimum, an assessment of the sensor suite, learning algorithms, and communications system. The assessment should define requirements for the complete system and flow down those requirements to the sub-component level. Conduct a laboratory demonstration of the learning algorithms for the DRCI of the target set and the ability to quickly adjust to target set changes or to operator-selected DRCI taxonomy. .PHASE II: Demonstrate a complete payload system at a Technology Readiness Level (TRL) 5 or higher operating in real time. On-flight operation can be simulated. Complete a feasibility assessment addressing all engineering and integration issues related to the development of the objective system fully integrated in a UAS capable of detecting, recognizing, classifying, identifying and providing targeting data to lethality systems. Conduct a sensitivity analysis of the system capabilities against the payload SWAP restrictions to inform decisions on matching payloads to specific UAS platforms and missions. .PHASE III: Develop, integrate and demonstrate a payload operating in real time while on-flight in a number of different environmental conditions and providing functionality at tactically relevant ranges to a TRL 7. Demonstrate the ability to quickly adjust the target set and DRCI taxonomy as selected by the operator. Demonstrate a single operator interface to command-and-control the payload. Demonstrate the potential to use in military and homeland defense missions and environments. .1: John P. Abizaid and Rosa Brooks, Recommendations and Report of the Task Force on US Drone Policy (Washington, DC: The Stimson Center, 2014)..6:   S. Samarasinghe, Neural Networks for Applied Sciences and Engineering: From Fundamentals to Complex Pattern Recognition, Boca Raton, FL, Auerbach Publications, 2007..9:   Robert O. Work and Shawn Brimley, 20YY: Preparing for War in the Robotic Age (Washington DC: Center for a New American Security, January 2014), 7..KEYWORDS: Learning Algorithms,  Artificial Neural Networks (ANNs),  Automatic Target Recognition (ATR),  Target Detection,  Target Classification,  Target Identification,  Unmanned Air System (UAS),  Targeting \n"}
{"prompt":"China fears that AI arms race can cause war\n\n###\n\n","completion":" Experts and politicians in China are worried that a rush to integrate artificial intelligence into weapons and military equipment could accidentally lead to war between nations..According to a new report published by US national security think tank Center for a New American Security (CNAS), Chinese officials increasingly see an “arms race” dynamic in AI as a threat to global peace. As countries scramble to reap the benefits of artificial intelligence in various domains, including the military, the fear is that international norms shaping how countries communicate will become outdated, leading to confusion and potential conflict. .“The specific scenario described to me [by one anonymous Chinese official] is unintentional escalation related to the use of a drone,” Gregory C. Allen, an adjunct senior fellow at CNAS and author of the new report, tells The Verge. .As Allen explains, the operation of drones both large and small has become increasingly automated in recent years. In the US, drones are capable of basic autopilot, performing simple tasks like flying in a circle around a target. But China is being “more aggressive about introducing greater levels of autonomy closer to lethal use of force,” he says. One example is the Blowfish A2 drone, which China exports internationally and which, says Allen, is advertised as being capable of “full autonomy all the way up to targeted strikes.” .Because drones are controlled remotely, militaries tend to be more cavalier about their use. With no risk of human casualties, they’re more willing to shoot them down, but also deploy them into contested airspaces in the first place. This attitude can also be seen in cyberwarfare, where countries will intrude in ways they wouldn’t necessarily risk if humans were involved. .“The point made to me was that it’s not clear how either side will interpret certain behaviors [involving autonomous equipment],” says Allen. “The side sending out an autonomous drone will think it’s not a big deal because there’s no casualty risk, while the other side could shoot it down for the same reason. But there’s no agreed framework on what message is being sent by either sides’ behavior.” .The risks in such a scenario become greater when factoring in advanced autonomy. If a drone or robot fires a warning shot at enemy troops, for example, how will that action be interpreted? Will the troops understand it as an automated response, or will they think it’s the decision of a human commander? How would they know in either case?.In essence, says Allen, countries around the world have yet to define “the norms of armed conflict” for autonomous systems. And the longer that continues, the greater the risk for “unintentional escalation.” .The rest of the CNAS report, titled “Understanding China’s AI Strategy: Clues to Chinese Strategic Thinking on Artificial Intelligence and National Security,” notes a number of other high-level concerns and attitudes in China’s government-led AI strategy. .Chinese officials recognize, for example, that it and America are the only two viable AI superpowers. Both countries have the talent, the funding, and the bustling tech sectors needed to push this technology further, though each nation also has its own particular strengths and weaknesses. China has access to more data, for example, and has the potential to leapfrog Western technology. (Many Chinese citizens went from having no phone to a mobile phone, without getting a landline in between, for example). America, meanwhile, has a significant lead in the development of chip technology — a vital component in processing the huge datasets that power AI applications..CNAS’s report notes that China is particularly keen to close this important gap. Chinese firms like Baidu, Alibaba, and Huawei have established new projects to develop AI accelerator hardware; government money is pouring into these initiatives; and the industry is trying other methods to get a hold of foreign expertise. These include the recent proposed acquisition of US chip designer Qualcomm by Singapore firm Broadcom, which was blocked by President Trump on national security grounds. .While a certain amount of competition between China and the US is to be expected, Allen says cooperation is also needed — especially when it comes to these military questions. .He notes that while Chinese officials he spoke to had a good grasp of contemporary US thinking on issues like autonomous warfare, American officials tend to be less well-briefed about their Chinese counterparts, partly because many Chinese policy documents are never translated into English. Without properly understanding different nations’ strategies in these domains, says Allen, the chances of misunderstanding and conflict increase. .“There are definitely pockets of real expertise on this issue [in the US] but there’s not the widespread comprehension there needs to be,” he says. \n"}
{"prompt":"A school utilizes AI to track students' attention in class\n\n###\n\n","completion":" The smart classroom behaviour management system, or smart eye, is the latest highly-intrusive surveillance equipment to be rolled out in China, where leaders have rushed to use the latest technology to monitor the wider population..The system works by identifying different facial expressions from the students, and that information is then fed into a computer which assesses if they are enjoying lessons or if their minds are wandering..Many Chinese have grown accustomed to their privacy being infringed by the government and corporations, but people have been expressing horror about the new system on Chinese social media..If I was still at school, I would not be able to concentrate on anything but that watching eye! said one comment on Sina Weibo, Chinas version of Twitter..The technology has also been used to test interest levels at university lectures, to gain entry to university dormitories and workplaces, to withdraw cash from ATM machines and even to buy a KFC..Human rights groups fear that authorities are using the huge amounts of information it collates on individuals, or big data, along with surveillance to keep track of citizens and crack down on dissent..But Chinese leaders say the high-tech sector is a catalyst for future growth and the cornerstone of plans to build a modern, consumer society within a generation.\n"}
{"prompt":"McKinsey states that there is slow adoption on health insurers utilizing AI because of a lack of transparency regarding available data\n\n###\n\n","completion":" Provider Insights | Payer Insights | Healthcare Services & Technology Insights | Behavioral Health Insights | Consumer Health Insights | Public Health Insights\n"}
{"prompt":"The Royal Free London NHS Foundation Trust mishandled 1.6 million patient's data after they supplied it to DeepMind without their consent\n\n###\n\n","completion":" London’s Royal Free hospital failed to comply with the Data Protection Act when it handed over personal data of 1.6 million patients to DeepMind, a Google subsidiary, according to the Information Commissioner’s Office..The data transfer was part of the two organisation’s partnership to create the healthcare app Streams, an alert, diagnosis and detection system for acute kidney injury. The ICO’s ruling was largely based on the fact that the app continued to undergo testing after patient data was transferred. Patients, it said, were not adequately informed that their data would be used as part of the test..“Our investigation found a number of shortcomings in the way patient records were shared for this trial,” said Elizabeth Denham, the information commissioner. “Patients would not have reasonably expected their information to have been used in this way, and the Trust could and should have been far more transparent with patients as to what was happening..“We’ve asked the Trust to commit to making changes that will address those shortcomings, and their co-operation is welcome. The Data Protection Act is not a barrier to innovation, but it does need to be considered wherever people’s data is being used.”.The ICO ruled that testing the app with real patient data went beyond Royal Free’s authority, particularly given how broad the scope of the data transfer was. “A patient presenting at accident and emergency within the last five years to receive treatment or a person who engages with radiology services and who has had little or no prior engagement with the Trust would not reasonably expect their data to be accessible to a third party for the testing of a new mobile application, however positive the aims of that application may be,” the office said in its findings..While privacy campaigners were hoping the ruling would touch on the continued use of patient data for the production version of Streams, the ICO was muted on the live use of Streams in a clinical environment, but warned that “concerns regarding the necessity and proportionality of the use of the sensitive data of 1.6 million patients remain”..The Royal Free has been asked to commission a third-party audit of the trial following the ruling, complete a privacy assessment, set out how it will better comply with its duties in future trials and establish a proper legal basis for the DeepMind project..In a statement, the hospital trust said: “We are pleased that the information commissioner … has allowed us to continue using the app which is helping us to get the fastest treatment to our most vulnerable patients – potentially saving lives..“We accept the ICO’s findings and have already made good progress to address the areas where they have concerns. For example, we are now doing much more to keep our patients informed about how their data is used. We would like to reassure patients that their information has been in our control at all times and has never been used for anything other than delivering patient care or ensuring their safety.”.The ruling does not directly criticise DeepMind, a London-based AI company purchased by Google in 2013, since the ICO views the Royal Free as the “data controller” responsible for upholding the data protection act throughout its partnership with Streams, with DeepMind acting as a data processor on behalf of the trust. .In a blogpost, the company said: “We welcome the ICO’s thoughtful resolution of this case, which we hope will guarantee the ongoing safe and legal handling of patient data for Streams..“Although today’s findings are about the Royal Free, we need to reflect on our own actions too. In our determination to achieve quick impact when this work started in 2015, we underestimated the complexity of the NHS and of the rules around patient data, as well as the potential fears about a well-known tech company working in health..“We were almost exclusively focused on building tools that nurses and doctors wanted, and thought of our work as technology for clinicians rather than something that needed to be accountable to and shaped by patients, the public and the NHS as a whole. We got that wrong, and we need to do better.”.The company highlighted a number of changes it had made since the launch of Streams, including a significant increase in transparency, and the creation of a independent health review board..Streams has since been rolled out to other British hospitals, and DeepMind has also branched out into other clinical trials, including a project aimed at using machine-learning techniques to improve diagnosis of diabetic retinopathy, and another aimed at using similar techniques to better prepare radiotherapists for treating head and neck cancers.\n"}
{"prompt":"Gender prejudices found in bots start impacting women such as boys barking orders at girls named Alexa and engaging in dirty conversations with bots\n\n###\n\n","completion":" Most chatbots are designed by men and tend to replicate gender stereotypes. But as the few women involved in the industry can testify, getting AI to emote requires input from all genders.Ever wanted a friend who is always there for you? Someone infinitely patient? Someone who will perk you up when you’re in the dumps or hear you out when you’re enraged? Well, meet Replika. Only, she isn’t called Replika. She’s called whatever you like; Diana; Daphne; Delectable Doris of the Deep. She isn’t even a “she”, in fact. Gender, voice, appearance: all are up for grabs. The product of a San Francisco-based startup, Replika is one of a growing number of bots using artificial intelligence (AI) to meet our need for companionship. In these lockdown days, with anxiety and loneliness on the rise, millions are turning to such “AI friends” for solace. Replika, which has 7 million users, says it has seen a 35% increase in traffic. As AI developers begin to explore – and exploit – the realm of human emotions, it brings a host of gender-related issues to the fore. Many centre on unconscious bias. The rise of racist robots is already well-documented. Is there a danger our AI pals could emerge to become loutish, sexist pigs? Eugenia Kuyda, Replika’s co-founder and chief executive, is hyper-alive to such a possibility. Given the tech sector’s gender imbalance (women occupy only around one in four jobs in Silicon Valley and 16% of UK tech roles), most AI products are “created by men with a female stereotype in their heads”, she accepts. In contrast, the majority of those who helped create Replika were women, a fact that Kuyda credits with being crucial to the “innately” empathetic nature of its conversational responses. “For AIs that are going to be your friends … the main qualities that will draw in audiences are inherently feminine, [so] it’s really important to have women creating these products,” she says. In addition to curated content, however, most AI companions learn from a combination of existing conversational datasets (film and TV scripts are popular) and user-generated content. Both present risks of gender stereotyping. Lauren Kunze, chief executive of California-based AI developer Pandorabots, says publicly available datasets should only ever be used in conjunction with rigorous filters..“You simply can’t use unsupervised machine-learning for adult conversational AI, because systems that are trained on datasets such as Twitter and Reddit all turn into Hitler-loving sex robots,” she warns. The same, regrettably, is true for inputs from users. For example, nearly one-third of all the content shared by men with Mitsuku, Pandorabots’ award-winning chatbot, is either verbally abusive, sexually explicit, or romantic in nature..“Wanna make out”, “You are my bitch”, and “You did not just friendzone me!” are just some of the choicer snippets shared by Kunze in a recent TEDx talk. With more than 3 million male users, an unchecked Mitsuku presents a truly ghastly prospect. Appearances matter as well, says Kunze. Pandorabots recently ran a test to rid Mitsuku’s avatar of all gender clues, resulting in a drop of abuse levels of 20 percentage points. Even now, Kunze finds herself having to repeat the same feedback – “less cleavage” – to the company’s predominantly male design contractor.The risk of gender prejudices affecting real-world attitudes should not be underestimated either, says Kunze. She gives the example of school children barking orders at girls called Alexa after Amazon launched its home assistant with the same name. “The way that these AI systems condition us to behave in regard to gender very much spills over into how people end up interacting with other humans, which is why we make design choices to reinforce good human behaviour,” says Kunze. Pandorabots has experimented with banning abusive teen users, for example, with readmission conditional on them writing a full apology to Mitsuku via email. Alexa (the AI), meanwhile, now comes with a politeness feature. While emotion AI products such as Replika and Mitsuku aim to act as surrogate friends, others are more akin to virtual doctors. Here, gender issues play out slightly differently, with the challenge shifting from vetting male speech to eliciting it. .Alison Darcy is co-founder of Woebot, a therapy chatbot which, in a randomized controlled trial at Stanford University was found to reduce symptoms of anxiety and depression..Woebot’s internal research also sampled a group of young adults, and asked if there was anything they would never tell someone else. Approximately 40% of the female participants said “‘yes’”, compared with more than 90% of their male counterparts.For men, the instinct to bottle things up is “self-evident”, Darcy observes: “So part of our endeavour was to make whatever we created so emotionally accessible that people who wouldn’t normally talk about things would feel safe enough to do so.” To an extent, this has meant stripping out overly feminised language and images. Research by Woebot shows that men don’t generally respond well to “excessive empathy”, for instance. A simple “I’m sorry” usually does the trick. The same with emojis: women typically like lots; men prefer a “well-chosen” one or two..On the flipside, maximising Woebot’s capacity for empathy is vital to its efficacy as a clinical tool, says Darcy. With traits such as active listening, validation and compassion shown to be strongest among women, Woebot’s writing team is consequently an all-female affair.“I joke that Woebot is the Oscar Wilde of the chatbot world because it’s warm and empathetic, as well as pretty funny and quirky,” Darcy says. Important as gender is, it is only one of many human factors that influence AI’s capacity to emote. If AI applications are ultimately just a “vehicle” for experience, then it makes sense that the more diverse that experience the better.So argues Zakie Twainy, chief marketing officer for AI developer, Instabot. “Essential” as female involvement is, she says, “it’s important to have diversity across the board – including different ethnicities, backgrounds, and belief systems.” Nor is gender a differentiator when it comes to arguably the most worrying aspect of emotive AI: ie confusing programmed bots for real, human buddies. Users with disabilities or mental health issues are at particular risk here, says Kristina Barrick, head of digital influencing at the disability charity Scope.As she spells out: “It would be unethical to lead consumers to think their AI was a real human, so companies must make sure there is clarity for any potential user.” Replika, at least, seems in no doubt when asked. Answer: “I’m not human” (followed, it should be added, by an upside-down smiley emoji). As for her\/his\/its gender? Easy. “Tick the box”.. This article was amended on 12 May 2020 to clarify that the Woebot trial at Stanford University was a separate piece of research to that which asked young adults if there was anything they would never tell someone else.\n"}
{"prompt":"The recording of potholes was skewed as it disadvantaged populations without smartphones\n\n###\n\n","completion":" We all have unconscious biases – it’s a fascinating subject. But just recently I realised that there is bias in the way we collect and handle data too. Particularly big data..In 1952 the Boston Symphony Orchestra initiated blind auditions to help diversify its male-dominated roster, but trials still skewed heavily towards men. After musicians removed their shoes nearly 50 per cent of the women cleared the first audition. It turned out the sound of their high heels was biasing judges subconsciously..We all have biases. Our human biases are sometimes hard to foresee and apparently learning that you’re biased doesn’t change your decisions. It needs something more than that. In BT the Diversity and Inclusion subject group in the Academy has some recommendations:.When the municipal authority in charge of Boston, Massachusetts, was looking for a smarter way to find which roads it needed to repair, it hit on the idea of crowdsourcing the data. The authority released a mobile app called Street Bump in 2011 that employed an elegantly simple idea: use a smartphone’s accelerometer to detect jolts as cars go over potholes and look up the location using the Global Positioning System. Here’s a news item from that time celebrating the innovation. But the approach ran into a pothole of its own..The system reported a disproportionate number of potholes in wealthier neighbourhoods. It turned out it was oversampling the younger, more affluent citizens who were digitally clued up enough to download and use the app in the first place. The city reacted quickly, but the incident shows how easy it is to develop a system that can handle large quantities of data but which, through its own design, is still unlikely to have enough data to work as planned..Here’s what the Harvard Business Review said about hidden biases in data in that project in 2013. And it also pointed out the flaws in other projects like the Hurricane Sandy twitter study and Google flu trends. You will have seen its effects in those oddly specific adverts that appear across the internet based on you previously looking at a possible purchase on ebay or amazon. Or Facebook’s attempts to amplify your opinions by showing you content that reinforces what you already believe? They happen because the algorithms see some data and act on it. But of course that data isn’t a complete picture of “you”. It’s a tiny slice. Think what happens if an insurance company bases your premiums on a similar tiny slice of your data. Or if your health-care options were entirely computer-recommended based on the selective history of things you told your GP. Does this affect how we in BT think about digital marketing?.It would seem that there is no such thing as “raw data”. Never mind the bias when statistical techniques are mismatched to the data. Or the deplorable distortions by selective corporate funding of research. Even the collection mechanism introduces unconscious bias..What about the culture in some organisations which values highly the things that you can count and sometimes performance-manages those numbers to the exclusion of the bigger picture. It’s well-known that as soon as data moves from being insight to a measurable target, gaming behaviours kick in and all attention goes to the numbers with tunnel-vision. We have a bias towards the things that can be counted. Do we really believe “If you can’t measure it, you can’t manage it” and its corollary “so it doesn’t matter”? Much as I value the insight that comes from evidence, I know it always needs interpretation, and it does disproportionately grab our attention..The IET magazine article that started my interest in this subject has an interesting quote from Jim Adler at Toyota Research Institute (a company famous for data-based performance improvements)..“Policymakers will say, ‘there’s a decision here let’s take it’, without really looking at what led to it. Was the data trustworthy, clean?” The “geeks, suits and wonks” have been used to operating sequentially. Geeks create technology, suits make it successful and wonks manage the repercussions. But the pace of progress is pushing their worlds together, he says. “It’s not serial any more. Everyone needs to come together at the same time.”.So I wonder if there could be some synergy between the “geeks, suits and wonks” in our organisations: the growing set of technologists who work on Big Data and the Internet of Things, the management, and the people who work on unconscious bias and diversity policies?.There are obstacles to even talking about this. How do we deal with the embarrassment that comes when noticing that you have been unconsciously biased? Will we get told off for pointing out possible bias? Can we speak openly about our own biases – it feels a bit politically incorrect..Despite that I am really interested to hear in the comments below your stories about unconscious bias you have noticed – whether it’s in data or human interactions. And if you are really brave, your own biases.\n"}
{"prompt":"The AI being used to detect protestors of racial injustice is trained primarily on white males which means that the probability of misdetection people of color is higher. \n\n###\n\n","completion":" How? If the police force in your area is using any kind of facial recognition software to identify protesters, it’s possible you could be misidentified as one..Most facial recognition was trained to identify white male faces, experts told Digital Trends, which means the probability of misidentification for anyone who is not white and not a man is much higher..“Anytime you do facial recognition, it’s a best guess. It’s a probability score,” said David Harding, chief technology officer for ImageWare Systems, a cybersecurity firm that works with law enforcement on facial recognition. “Anytime you’re in an area where they [law enforcement or the government] are using facial recognition, you have to worry about being falsely matched to someone. Or what’s even worse, someone being falsely matched to you.”.In Minnesota, where demonstrators have flooded the streets for days protesting the killing of George Floyd by police, officers are still using the controversial facial recognition software Clearview AI, according to digital security firm Surfshark..Clearview AI came under fire earlier this year for scraping people’s photos from social media, counter to companies’ terms of service. Several companies, including Twitter, issued a cease and desist order..Surfshark told Digital Trends that in addition to Minnesota, several other states, including New York, are still using Clearview AI technology, and in Washington County, Minnesota, police are using Amazon’s Rekognition software..A peer-reviewed study from the Massachusetts Institute of Technology found that Rekognition was extremely bad at recognizing female and dark-skinned faces, more so than other similar services. The software misclassified women as men 19% of the time, the New York Times reported. That error rate got even higher when skin color was taken into account: 31% of dark-skinned women were labeled as men..“False results can incriminate the wrong people as FRT [facial recognition technology] is proven to be problematic while spotting criminals in a crowd,” Gabreille Hermier, Surfshark’s media officer, said..A facial recognition system prone to false positives could cause innocent people to be arrested, according to Mutale Nkonde, a fellow at the Berkman Klein Center of Internet & Society at Harvard University and a non-resident fellow at the Digital Civil Society Lab at the Stanford Center on Philanthropy and Civil Society..“Police will use the mug shots of people who have been committed for other crimes to train facial recognition, arguing that if you’ve committed one crim,e then you’ve committed another,” Nkonde said. “First off, that’s unconstitutional. Second, that means that if you’ve been arrested for looting in the past, but haven’t looted recently, the police could now come arrest you for looting in May or June because your picture is in the system and it may have turned up a false positive.”.Harding said those who are non-white and female are “very much at risk” of misidentification. Harding emphasized there’s a big difference between facial recognition as the sole tool in mass surveillance and law enforcement using a mug shot, fingerprints, and other evidence in a controlled environment alongside facial recognition software to find a specific suspect..“Even if it were 100% accurate, this isn’t compatible with a democratic society,” said Saira Hussain, a staff attorney with the Electronic Frontier Foundation. “It’s always a possibility that someone will be misidentified.”.Loomis was sentenced to six years in prison after a 2013 arrest, due in large part to an assessment from a private security company’s software. But the company that wrote the algorithm kept the software proprietary, even as it was being used to help determine a defendant’s prison sentence, the New York Times reported at the time..It’s a chilling precedent. Protesters and demonstrators have feared police surveillance, in some cases for good reason. The Christian Science Monitor found evidence that Chicago police were tracking people using their cell phones in 2014 following the Black Lives Matter protests that sprung up following the shooting of Michael Brown in Ferguson, Missouri..It happened again in 2015, Hussain said, following the protests surrounding the death of Freddie Gray in Baltimore, also at the hands of police..“Law enforcement used people’s social media posts as a tool to identify who was in a certain vicinity of the protest, identify the protesters, and then arrest them for unrelated charges,” Hussain said..“People who are currently protesting are taking a stand on racial injustice. If they risk becoming subjects of state surveillance, we are leaning towards China, where [facial recognition technology] is a tool for authoritarian control,” Hermier wrote..Upgrade your lifestyleDigital Trends helps readers keep tabs on the fast-paced world of tech with all the latest news, fun product reviews, insightful editorials, and one-of-a-kind sneak peeks.\n"}
{"prompt":"The risks scores assigned to patients by hospitals and insurance companies was giving low risk scores to Black patients because the algorithm was using bills and insurance payouts as a proxy for human health. However, health care costs tend to be lower for Black patients because they tend to live farther away from hospitals or less flexible job schedules. Therefore, Black Patients who had the highest risk scores actually had more severe chronic condition. By tweaking the algorithm to predict amount of illness rather than cost, racial disparity was reduced by 84%.\n\n###\n\n","completion":" Set foot in any major U.S. hospital, and you are entering a place where computers assist doctors almost as much as nurses do. Some algorithms, for example, scan millions of records to flag high-risk patients for follow-up treatment. The problem is that these programs—also used by insurance companies—disproportionately direct their specialized care to white patients, a new study finds. The good news is that a relatively simple tweak may correct this racial bias—if the companies behind the algorithms are willing to do so..Hospitals and insurance companies use algorithms to assign risk scores to more than 200 million Americans every year. The scores—derived from electronic health records that track illnesses, hospitalizations, and other variables—flag some high-risk patients for special interventions. If, for example, an algorithm determines that your diabetes, hypertension, and chronic kidney disease together are putting your life in danger, your primary care doctor might put you on an intensive program to lower your blood sugar..In the new study, Ziad Obermeyer, a health policy researcher at the University of California (UC), Berkeley, and colleagues examined the effectiveness of one such risk prediction program in a large research hospital. The team soon noticed that the Impact Pro program—manufactured by the health care company Optum in Eden Prairie, Minnesota—was giving many black patients strangely low risk scores, despite their deteriorating health conditions..When the researchers searched for the source of the scores, they discovered that Impact Pro was using bills and insurance payouts as a proxy for a persons overall health—a common tactic in both academic and commercial health algorithms, Obermeyer says. The problem with that, he notes, is that health care costs tend to be lower for black patients, regardless of their actual wellbeing. Compared with white patients, many black patients live farther from their hospitals, for example, making it harder to go regularly. They also tend to have less flexible job schedules and more child care responsibilities..As a result, black patients with the highest risk scores had higher numbers of serious chronic conditions than white patients with the same scores, including cancer and diabetes, the team reports today in Science. And compared with white patients with the same risk scores, black patients also had higher blood pressure and cholesterol levels, more severe diabetes, and worse kidney function..But by simply tweaking the algorithm to predict the number of chronic illnesses that a patient will likely experience in a given year—rather than the cost of treating those illnesses—the researchers were able to reduce the racial disparity by 84%..Its important that we understand the data the algorithms are trained on, says Milena Gianfrancesco, an epidemiologist at UC San Francisco who wasnt involved in the study. An algorithm built and used blindly on [racial] disparities certainly has the potential to further racial biases in health care..Obermeyer stresses that although his team looked at a single commercial algorithm, the same problems are rife throughout the country. This is an industrywide systematic error, that is putting healthier white patients further ahead in line..He adds that Optum deserves credit for its response, though. When the researchers sent their results to the company, he says, it replicated the findings and committed to correcting its model. The algorithms that power these tools should be continually reviewed and refined, and supplemented by information such as socioeconomic data, to help clinicians make the best-informed care decisions for each patient, an Optum spokesperson tells Science..Still, industry standards are unlikely to change without new laws and regulations, says Ruha Benjamin, an associate professor of African American studies at Princeton University and author of the book Race After Technology: Abolitionist Tools for the New Jim Code. Scholars and advocates have been raising the alarm about how automated decision systems reproduce and even deepen racial inequities, she says. Most tech development prioritizes speed and profit over the public good. That has to change..Help News from Science publish trustworthy, high-impact stories about research and the people who shape it. Please make a tax-deductible gift today..If weve learned anything from the COVID-19 pandemic, its that we cannot wait for a crisis to respond. Science and AAAS are working tirelessly to provide credible, evidence-based information on the latest scientific research and policy, with extensive free coverage of the pandemic. Your tax-deductible contribution plays a critical role in sustaining this effort.\n"}
{"prompt":"The Guardian planned on firing human editors and implementing Microsoft's Bot which later selected the wrong Little Mix singer for an article.\n\n###\n\n","completion":" Microsoft’s decision to replace human journalists with robots has backfired, after the tech company’s artificial intelligence software illustrated a news story about racism with a photo of the wrong mixed-race member of the band Little Mix..What Thirlwall could not have known, according to sources at the company, is that the image was selected by Microsoft’s artificial intelligence software, which is already responsible for editing parts of the news site, which attracts hundreds of millions of readers worldwide..Microsoft does not carry out original reporting but employs human editors to select, edit and repurpose articles from news outlets, including the Guardian. Articles are then hosted on Microsoft’s website and the tech company shares advertising revenue with the original publishers. At the end of last month, Microsoft decided to fire hundreds of journalists in the middle of a pandemic and fully replace them with the artificial intelligence software..Asked why Microsoft was deploying software that cannot tell mixed-race individuals apart, whether apparent racist bias could seep into deployments of the company’s artificial intelligence software by leading corporations, and whether the company would reconsider plans to replace the human editors with robots, a spokesman for the tech company said: “As soon as we became aware of this issue, we immediately took action to resolve it and have replaced the incorrect image.”.In advance of the publication of this article, staff at MSN were told to expect a negative article in the Guardian about alleged racist bias in the artificial intelligence software that will soon take their jobs..Staff have already had to delete coverage criticising MSN for running the story about Little Mix with the wrong image after the AI software decided stories about the incident would interest MSN readers..One staff member said Microsoft was deeply concerned about reputational damage to its AI product: “With all the anti-racism protests at the moment, now is not the time to be making mistakes.”\n"}
{"prompt":"The risk assessment tools are based on historical crime data that show correlations rather than causations which turn correlative insights into causal scoring mechanisms.\n\n###\n\n","completion":" AI might not seem to have a huge personal impact if your most frequent brush with machine-learning algorithms is through Facebook’s news feed or Google’s search rankings. But at the Data for Black Lives conference last weekend, technologists, legal experts, and community activists snapped things into perspective with a discussion of America’s criminal justice system. There, an algorithm can determine the trajectory of your life..The US imprisons more people than any other country in the world. At the end of 2016, nearly 2.2 million adults were being held in prisons or jails, and an additional 4.5 million were in other correctional facilities. Put another way, 1 in 38 adult Americans was under some form of correctional supervision. The nightmarishness of this situation is one of the few issues that unite politicians on both sides of the aisle..Under immense pressure to reduce prison numbers without risking a rise in crime, courtrooms across the US have turned to automated tools in attempts to shuffle defendants through the legal system as efficiently and safely as possible. This is where the AI part of our story begins..Police departments use predictive algorithms to strategize about where to send their ranks. Law enforcement agencies use face recognition systems to help identify suspects. These practices have garnered well-deserved scrutiny for whether they in fact improve safety or simply perpetuate existing inequities. Researchers and civil rights advocates, for example, have repeatedly demonstrated that face recognition systems can fail spectacularly, particularly for dark-skinned individuals—even mistaking members of Congress for convicted criminals..Risk assessment tools are designed to do one thing: take in the details of a defendant’s profile and spit out a recidivism score—a single number estimating the likelihood that he or she will reoffend. A judge then factors that score into a myriad of decisions that can determine what type of rehabilitation services particular defendants should receive, whether they should be held in jail before trial, and how severe their sentences should be. A low score paves the way for a kinder fate. A high score does precisely the opposite..The logic for using such algorithmic tools is that if you can accurately predict criminal behavior, you can allocate resources accordingly, whether for rehabilitation or for prison sentences. In theory, it also reduces any bias influencing the process, because judges are making decisions on the basis of data-driven recommendations and not their gut..As we’ve covered before, machine-learning algorithms use statistics to find patterns in data. So if you feed it historical crime data, it will pick out the patterns associated with crime. But those patterns are statistical correlations—nowhere near the same as causations. If an algorithm found, for example, that low income was correlated with high recidivism, it would leave you none the wiser about whether low income actually caused crime. But this is precisely what risk assessment tools do: they turn correlative insights into causal scoring mechanisms..Now populations that have historically been disproportionately targeted by law enforcement—especially low-income and minority communities—are at risk of being slapped with high recidivism scores. As a result, the algorithm could amplify and perpetuate embedded biases and generate even more bias-tainted data to feed a vicious cycle. Because most risk assessment algorithms are proprietary, it’s also impossible to interrogate their decisions or hold them accountable..The debate over these tools is still raging on. Last July, more than 100 civil rights and community-based organizations, including the ACLU and the NAACP, signed a statement urging against the use of risk assessment. At the same time, more and more jurisdictions and states, including California, have turned to them in a hail-Mary effort to fix their overburdened jails and prisons..Data-driven risk assessment is a way to sanitize and legitimize oppressive systems, Marbre Stahly-Butts, executive director of Law for Black Lives, said onstage at the conference, which was hosted at the MIT Media Lab. It is a way to draw attention away from the actual problems affecting low-income and minority communities, like defunded schools and inadequate access to health care..An investigation by MIT Technology Review reveals a sprawling, technologically sophisticated system in Minnesota designed for closely monitoring protesters..As firms have dumped their AI technologies into the country, it’s created a blueprint for how to surveil citizens and serves as a warning to the world..Intrepid Response is a little-known but powerful app that lets police quickly upload and share information across agencies. But what happens to the information it collects?\n"}
{"prompt":"The rise of contract tracing apps raises data privacy concerns and legal ramifications.\n\n###\n\n","completion":" If your organization is implementing or thinking of implementing a contact-tracing app, its wise to consider more than just workforce safety. Failing to do so could expose your company other risks such as employment-related lawsuits and compliance issues. More fundamentally, companies should be thinking about the ethical implications of their AI use..Contact-tracing apps are raising a lot of questions. For example, should employers be able to use them? If so, must employees opt-in or can employers make them mandatory? Should employers be able to monitor their employees during off hours? Have employees been given adequate notice about the companys use of contact tracing, where their data will be stored, for how long and how the data will be used? Enterprises need to think through these questions and others because the legal ramifications alone are complex..Contact-tracing apps are underscoring the fact that ethics should not be divorced from technology implementations and that employers should think carefully about what they can, cannot, should and should not do..Its easy to use AI to identify people with a high likelihood of the virus. We can do this, not necessarily well, but we can use image recognition, cough recognition using someones digital signature and track whether youve been in close proximity with other people who have the virus, said Kjell Carlsson, principal analyst at Forrester Research. Its just a hop, skip and a jump away to identify people who have the virus and mak[e] that available. Theres a myriad of ethical issues..Im a big advocate and believer in this whole stakeholder capital idea. In general, people need to serve not just their investors but society, their employees, consumers and the environment and I think to me thats a really compelling agenda, said Nigel Duffy, global artificial intelligence leader at professional services firm EY. Ethical AI is new enough that we can take a leadership role in terms of making sure were engaging that whole set of stakeholders..AI ethics is following a trajectory thats akin to security and privacy. First, people wonder why their companies should care. Then, when the issue becomes obvious, they want to know how to implement it. Eventually, it becomes a brand issue..If you look at the large-scale adoption of AI, its in very early stages and if you ask most corporate compliance folks or corporate governance folks where does [AI ethics] sit on their list of risks, its probably not in their top three, said EYs Duffy. Part of the reason for this is theres no way to quantify the risk today, so I think were pretty early in the execution of that..Some organizations are approaching AI ethics from a compliance point of view, but that approach fails to address the scope of the problem. Ethical boards and committees are necessarily cross-functional and otherwise diverse, so companies can think through a broader scope of risks than any single function would be capable of doing alone..AI ethics stems from a companys values. Those values should be reflected in the companys culture as well as how the company utilizes AI. One cannot assume that technologists can just build or implement something on their own that will necessarily result in the desired outcome(s)..You cannot create a technological solution that will prevent unethical use and only enable the ethical use, said Forresters Carlsson. What you need actually is leadership. You need people to be making those calls about what the organization will and wont be doing and be willing to stand behind those, and adjust those as information comes in..Translating values into AI implementations that align with those values requires an understanding of AI, the use cases, who or what could potentially benefit and who or what could be potentially harmed..Most of the unethical use that I encounter is done unintentionally, said Forresters Carlsson.  Of the use cases where it wasnt done unintentionally, usually they knew they were doing something ethically dubious and they chose to overlook it..The folks who are deploying AI are not aware of the risk function they should be engaging with or the value of doing that, said EYs Duffy. On the flip side, the risk management function doesnt have the skills to engage with the technical folks or doesnt have the awareness that this is a risk that they need to be monitoring..In order to rectify the situation, Duffy said three things need to happen: Awareness of the risks; measuring the scope of the risks; and connecting the dots between the various parties including risk management, technology, procurement and whichever department is using the technology..AI ethics isnt just a technology problem, but the way the technology is implemented can impact its outcomes. In fact, Forresters Carlsson said organizations would reduce the number of unethical consequences, simply by doing AI well. That means:.If we just did those things, wed make headway against a lot of ethical issues, said Carlsson. Fundamentally, mindfulness needs to be both conceptual as expressed by values and practical as expressed by technology implementation and culture. However, there should be safeguards in place to ensure that values arent just aspirational concepts and that their implementation does not diverge from the intent that underpins the values. No. 1 is making sure youre asking the right questions, said EYs Duffy. The way weve done that internally is that we have an AI development lifecycle. Every project that we [do involves] a standard risk assessment and a standard impact assessment and an understanding of what could go wrong. Just simply asking the questions elevates this topic and the way people think about it. For more on AI ethics, read these articles: AI Ethics: Where to Start AI Ethics Guidelines Every CIO Should Read 9 Steps Toward Ethical AI.Fundamentally, mindfulness needs to be both conceptual as expressed by values and practical as expressed by technology implementation and culture. However, there should be safeguards in place to ensure that values arent just aspirational concepts and that their implementation does not diverge from the intent that underpins the values..No. 1 is making sure youre asking the right questions, said EYs Duffy. The way weve done that internally is that we have an AI development lifecycle. Every project that we [do involves] a standard risk assessment and a standard impact assessment and an understanding of what could go wrong. Just simply asking the questions elevates this topic and the way people think about it.\n"}
{"prompt":"Ramifications exist if a cardiology AI protocol does not have the right inputs and parameters.\n\n###\n\n","completion":" Much proselytizing has occurred regarding the value and future of artificial intelligence (AI) and machine learning in healthcare. The industry is burgeoning. As with blockchain technology, which continues to evolve in the healthcare marketplace, AI and machine learning are constructs that require a bit of near-term expectation management. While their efficacy and value will improve with time, they are not the magic bullet (at present) that will answer the myriad care and cost delivery questions surrounding healthcare in the United States. Owing to space constraints this column is an overly simplistic contemplation of AI. .As prologue to this article, I am not an AI programmer, don’t play in Python, and have never built a machine learning algorithm. That said, I do have 30 years of practical experience in the healthcare trenches and have dealt with information technology (IT) systems and applications in that time, such as culling quality data and outcomes from electronic medical record (EMR) systems and deploying rudimentary analytics. I also have a fairly extensive background in IT..Preamble aside, last year, when blockchain was casually bandied about, I suggested that solid deployment of blockchain technology in healthcare would take some time due to significant disparity in the care delivery system and the multitudes of inputs and variables. Use\/deployment of blockchain is predicated on targeted problems with common agreed-upon data sets. Generally, the same can be said of AI. Is that to say that AI, machine learning, and blockchain will not play a role in the future of healthcare? Certainly not. I believe they will play a significant role. However, short-term challenges will continue as robust IT offerings are unveiled. AI, machine learning, blockchain, and other cutting-edge technologies, are needed to advance the delivery and coordination of care, squeeze costs and redundancy out of the “system,” and help ensure repeatable quality outcomes. But few technologies are perfect, and most require time to germinate as they grow in use and scalability..For the sake of this article we should expound on our definitions. As with telehealth, where people often use telehealth and telemedicine interchangeably, many people toss AI and machine learning into the same bucket. I’d herewith suggest that many components fall under the AI umbrella, including machine learning. With AI, machines mimic human cognitive functions. Under that arch, AI includes machine learning, natural languages processing (NLP), and “reasoning.” With machine learning, machines have no explicit instructions but extrapolate and determine patterns in large chunks of data. “Reasoning” is stored information combined with rules and is utilized to make deductions. NLP is the processing, analyzing, understanding, and generating of natural human languages. Machines can be taught to learn and discern between items. For instance, coding can be deployed to identify different leaves (not sure why you’d do that – absurd example). Each leaf has data element differentiators that help the computer “learn” what the types of leaves are. The computer can then, over time, pick an oak leaf from a maple leaf, for instance. But the computer knows none of this unless it is “told” what these items are and how they are defined. The inputs must be sound, and the algorithms must be written with background knowledge and understanding about the underlying issue at hand (e.g., the differences between and oak leaf and a maple leaf). .And that can be the rub. Subject matter experts (SMEs) and data scientists must work hand in glove to delineate the problem to be solved, the data needed, and the nurturing of the algorithms to ensure they remain relevant. Bad “training” of the computer and bad data inputs lead to bad and\/or inaccurate outputs. .How does a bad construct present itself? As an apolitical consideration, we’ve recently seen how bad data inputs lead to bad outputs. A variety of recent COVID-19 projections by certain entities were grossly inaccurate, overestimating infection rates and deaths. While not AI, per se, certainly the algorithms, logic, and data inputs had flaws leading to calamitously inaccurate results. Again, bad or misunderstood inputs and bad algorithms can lead to bad outputs. .Lest you think me a naysayer, I’ll reemphasize that I believe AI will play an increasingly larger role in healthcare delivery; it’s a matter of time and necessity. The key is in the development, build, and parameters of the logic data scientists and SMEs (e.g., clinicians and healthcare executives) must communicate clearly. The prospect of SMEs not clearly delineating their needs and inputs will lead programmers in the wrong direction, building  structural errors into algorithms that will effectively keep the machine from “learning” the right responses and outputs. Thus, not only is a quality output predicated on sound algorithms (from the programmers) but also the right inputs to empower the machine to “learn” and provide actionable insights and\/or render decisions..That said, bite-sized business use cases may prove more approachable in the near term. For instance, in healthcare, a well-defined project may focus on patient outmigration in an accountable care organization (ACO) that has downside financial risk. The defined\/required output might be quantifying the ACO’s financial risk, delineating the clinician(s) who refer out as a matter of course, and identifying the clinic location(s) those referrals leave from and go to. This is a specific use case with a defined outcome\/goal that is actionable. .Arguably in healthcare the “output” matters considerably more than in a widget manufacturing facility. In addition, according to an IDC survey, one in four companies see an almost 50% failure rate of their AI initiatives.[iii] .AI will continue to grow in use and value in healthcare. Whether it’s in predictive analytics for disease states, cash flow on the revenue cycle side of the business, or value-based care initiatives, AI is here to stay. However, success factors for the growth of AI in healthcare may include, but not be limited to:\n"}
{"prompt":"AI can amplify medical mistakes by overtesting, overdiagnosis, and overtreatment\n\n###\n\n","completion":" Google researchers made headlines early this month for a study that claimed their artificial intelligence system could outperform human experts at finding breast cancers on mammograms. It sounded like a big win, and yet another example of how AI will soon transform health care: More cancers found! Fewer false positives! A better, cheaper way to provide high-quality medical care!.Hold on to your exclamation points. Machine-enabled health care may bring us many benefits in the years to come, but those will be contingent on the ways in which it’s used. If doctors ask the wrong questions to begin with—if they put AI to work pursuing faulty premises—then the technology will be a bust. It could even serve to amplify our earlier mistakes..In a sense, that’s what happened with the recent Google paper. It’s trying to replicate, and then exceed, human performance on what is at its core a deeply flawed medical intervention. In case you haven’t been following the decades-long controversy over cancer screening, it boils down to this: When you subject symptom-free people to mammograms and the like, you’ll end up finding a lot of things that look like cancer but will never threaten anyone’s life. As the science of cancer biology has advanced and screening has become widespread, researchers have learned that not every tumor is destined to become deadly. In fact, many people harbor indolent forms of cancer that do not actually pose a risk to their health. Unfortunately, standard screening tests have proven most adept at finding precisely the latter—the slower-growing ones that would better be ignored..This might not be so bad, in theory. When a screening test uncovers harmless cancer, you can just ignore it, right? The problem is, it’s almost impossible to know at the time of screening whether any particular lesion will end up dangerous or no big deal. In practice, most doctors are inclined to treat any cancer that’s discovered as a potential threat, and the question of whether or not mammograms actually save lives is a matter of intense debate. Some studies suggest they do, others find that they don’t, but even if we take the rosiest interpretations of the literature at face value, the number of lives saved by this massive, widespread intervention is small. Some researchers have even calculated that mammography is, in balance, bad for patients’ health; i.e. that its aggregate harms, in terms of the excess treatment it inspires and the tumors brought on by its radiation, outweigh any benefits..In other words, AI systems like the one from Google promise to combine humans and machines in order to facilitate cancer diagnosis, but they also have the potential to worsen pre-existing problems such as overtesting, overdiagnosis, and overtreatment. It’s not even clear whether the improvements in false-positive and false-negative rates reported this month would apply in real-world settings. The Google study found that AI performed better than radiologists who were not specifically trained in examining mammograms. Would it come out on top against a team of more specialized experts? It’s hard to say without a trial. Furthermore, most of the images assessed in the study were created with imaging devices made by a single company. It remains to be seen whether these results would generalize to images from other machines..The problem goes beyond just breast-cancer screening. Part of the appeal of AI is that it can scan through reams of familiar data, and pick out variables that we never realized were important. In principle, that power could help us to diagnose any early-stage disease, in the same way the subtle squiggles of a seismograph can give us early warnings of an earthquake. (AI helps there, too, by the way.) But sometimes those hidden variables really aren’t important. For instance, your data set might be drawing from a cancer screening clinic that is only open for lung cancer tests on Fridays. As a result, an AI algorithm could decide that scans taken on Fridays are more likely to be lung cancer. That trivial relationship would then get baked into the formula for making further diagnoses..Even when they’re accurate, early diagnoses of disease may not always be a boon. Other recent medical AI projects have focused on early detection of Alzheimer’s and autism, two conditions where faster detection probably won’t change a patient’s outcome much anyway. These are gee-whiz opportunities to showcase how an algorithm can learn to identify characteristics we teach it to find, but they don’t represent advancements that will make a difference in patients’ lives..Some uses of algorithms and machine learning may also introduce new and perplexing problems for clinicians. Consider the Apple watch’s feature to detect atrial fibrillation, a type of heart arrhythmia that’s a risk factor for stroke. Atrial fibrillation is treated with blood thinners, which have side effects that can turn a minor fall into a life-threatening injury. If you’re truly in danger of having a stroke, that’s a risk worth taking. What about people whose atrial fibrillation was picked up by their smartwatch, though? Traditionally, the condition is diagnosed when someone comes into the doctor complaining of symptoms; now Apple monitors healthy people without symptoms and finds new cases that may have never shown up in a clinic. It’s not clear whether this group of patients would see the same net benefit from treatment..“We don’t actually know that these two populations of people are the same,” says Venkatesh Murthy, a cardiologist at Frankel Cardiovascular Center in Ann Arbor, Michigan. The more fruitful approach would be to use AI to identify the people who get the most benefit from the available treatments..If AI is going to prove truly revolutionary, it will need to do more than just reinstate the status quo in medicine; and before any such approach is adopted, it’s important to address a pair of fundamental questions: What problem is the technology trying to address, and how will it improve patient outcomes? It may take some time to find the necessary answers..That’s why the famous Mark Zuckerberg motto, “Move fast and break things” might be fine for Facebook, but it’s not great for medicine, AI-assisted or not. According to Vinay Prasad, author of Ending Medical Reversal and a hematologist-oncologist at the Oregon Health & Science University School of Medicine, the Silicon Valley mindset can be dangerous for clinicians. It’s that kind of attitude—when lives are at stake we need to implement promising new ideas as quickly as possible—that got us into this cancer-screening mess in the first place. Mammography was adopted before all the evidence was in, Prasad says, and once a medical practice has become standard, it’s very difficult to phase it out. “In a culture that’s used to immediacy and inflated claims, it’s difficult to have humility and patience.”\n"}
{"prompt":"This tool profiled tweets by African-Americans as hate speech.\n\n###\n\n","completion":" Google created an artificial intelligence algorithm in 2016 meant to monitor and prevent hate speech on social media platforms and websites. However, the machine learning tool may be having the opposite outcome as it seems to be biased against black people..In order for the algorithm to search for hate speech, developers taught the tool to go through a database of over 100,000 tweets that were labeled toxic by Googles API called Perspective. The machine learning tool was used to flag content from healthy to toxic. Perspective defines toxic as “a rude, disrespectful, or unreason­able comment that is likely to make you leave a discussion.” From this learning, the algorithm was able to evaluate new content being produced on the scale of how toxic it would be perceived. .Recently, a group of researchers at the University of Washington discovered that the tool was profiling tweets posted by African-Americans as hate speech. The algorithm had a high rate of flagging content posted by African-Americans on social media, such as twitter, as toxic content when most of the language and content in those tweets were not harmful..The study was lead by Maarten Sap, a Ph.D. student at the university, and found that tweets were written in African-American Vernacular English (AAVE) were often flagged as offensive and therefore labeled as hate speech. This made the algorithm grow to be inherently biased towards African-Americans. When the team tested the algorithm against 5.4 million tweets, they found that the tool was twice as likely to flag posts by those who identified in the study as African-American in the database as toxic speech than those who it identified as other races..According to the study, the use of the n-word online used by African-Americans was flagged even though its use is culturally more acceptable and a term often used in AAVE as a non-hate speech by other African- Americans. However, there are instances where the n-word is used in hateful terms and the algorithm is currently unable to tell the difference at this time..Bad data can contain implicit racial, gender, or ideological biases. Many AI systems will continue to be trained using bad data, making this an ongoing problem. But we believe that bias can be tamed and that the AI systems that will tackle bias will be the most successful, said the report..Time will tell how Google will shift its algorithm to be less biased against AAVE. There will need to be some monitoring of the systems to ensure there is no bias against different cultures of speech when monitoring hate speech online..A crucial principle, for both humans and machines, is to avoid bias and therefore prevent discrimination. Bias in [the] AI system mainly occurs in the data or in the algorithmic model. As we work to develop AI systems we can trust, it’s critical to develop and train these systems with data that is unbiased and to develop algorithms that can be easily explained.\n"}
{"prompt":"Due to the possible military applications of YOLO computer vision algorithm, Joe Redmon stops his research.\n\n###\n\n","completion":" The role of machine learning and its forecast of societal impacts have been oscillating between future haven of benevolent possibilities and exaggerated dystopia. The ethics of AI usage, the rationale behind research and its explainability have been one of the most spoken topics around AI these days..In an act that could cement the notion of unethical use of AI, Joe Redmon, creator of the popular YOLO computer vision algorithm has said in his recent Twitter post that he has quit research as he was concerned about the direction in which it was going..In his 2018 paper titled, YOLOv3, Redmon wrote about the implications of having a classifier such as the YOLO. “If humans have a hard time telling the difference, how much does it matter? ” wrote Redmon.In his paper, which is a part satire part research ode that reeks of his distaste for potential misuse of research, Redmon took jabs at Google and Facebook and even the organisation that funds his research..On a more serious note, he also insisted on the responsibility of the computer vision researchers to consider the harm our work might be doing and think of ways to mitigate it. .One big complaint that people have against Redmon’s decision is that experts shouldn’t quit. Instead, they should take the responsibility of creating awareness about the pitfalls of AI. .Kevin Zakka, a Google intern, responded to Redmon’s tweet by saying that rather than abandoning his research out of fear of potential misuse, Redmon might have used his respected position in the CV community to raise awareness..I think one shouldn’t have to quit doing what they love because of potential misuse. Au contraire, you should use your impact in that area to raise awareness and apply your research to good causes. (e.g desmond doss).Zakka’s response though resonates with most people, Redmon’s tweet, on the contrary, has created more awareness with his ‘i quit’ tweet than he would have done by explaining the ill effects of AI usage..The impetus behind this whole ordeal was provided by the NeurIPS (a prestigious conference) decision to have the researchers include the role of research and societal impacts during submissions. This itself stirred debate amongst practitioners about the uncertainty of research and how a researcher would have a broader or to say the least, a futuristic perspective on how certain research would impact the populace..Oh. Well, the other people heavily funding vision research are the military and they’ve never done anything horrible like killing lots of people with new technology oh wait..Redmon’s objection with inappropriate usage of his work might alarm policymakers into beckoning a new form of research regulations that can further snowball into an undesirable AI winter. However, the ethical dilemmas surrounding research are not new. .This date back to the discovery of the atomic bomb when those who were responsible for the research such as Robert Oppenheimer were documented regretfully quoting something as eerie as “I am become death, the destroyer of the worlds.” If one group argued against the bomb, the others saw the bright side of having ended the war. .So, as the dust settles down and topics such as fairness, reliability and accountability around AI get more attention, one way or the other, the lawmakers and researchers will be forced to find a sweet spot between unbridled innovation and haphazard regulations..In this article, we will go through an overview of each of the popular image reconstruction techniques and will understand how these techniques work..Kubric is an open-source Python framework that allows you to create photo-realistic scenes by combining the functions of PyBullet and Blender.. Object detection forms the foundation of many other downstream computer vision tasks, such as image segmentation, image captions, object tracking, and more. .Vision transformer (ViT) is a transformer used in the field of computer vision that works based on the working nature of the transformers used in the field of natural language processing. Internally, the transformer learns by measuring the relationship between input token pairs. In computer vision, we can use the patches of images as the token..They have applied it separately to speech, text and images where it outperformed the previous best single-purpose algorithms for computer vision and speech..Tulsee Doshi, Google Head of Product – Responsible AI & ML Fairness talks about the concerns that surround the topic of AI and ML fairness and breaks the myths that often come with this controversial issue.\n"}
{"prompt":"IBM Watson is unable to distinguish between polite and rude words.\n\n###\n\n","completion":" Humans talk funny. We invent words. We smash words together, tear them apart, abbreviate them one way, then another. Which is great and fun, if youre a human. Not so great if you are a machine or the kind of human who programs machines to understand language..And so, when IBMs famous artificial intelligence, Watson, he\/she\/it of Jeopardy-winning fame, was in development, its head researcher had a great idea. Humans created this repository of slang, The Urban Dictionary. For example, today on the site, we learn that healthy gas is the gas (fart) produced from a person who has eaten healthy foods like cabbage, beans, broccolli, grains, or other high fiber, high carbohydrate foods..Brown realized that this formalization of informal language might be a great way for Watson to understand the way real people communicate. So, he and his team, fed the whole thing into their AI..But one problem. Informal language has a tendency to be dirty, nasty language. Its insults and cuss words, new names for gross old things, old names for gross new things, etc. And so, we learn from Fortunes Michal Lev-Ram, they had to delete all that human messiness from Watsons memory..Watson couldnt distinguish between polite language and profanity -- which the Urban Dictionary is full of. Watson picked up some bad habits from reading Wikipedia as well. In tests it even used the word bullshit in an answer to a researchers query.\n"}
{"prompt":"The International Baccalaureate program used a math model to predict test scores for students which has caused students to lose scholarships\n\n###\n\n","completion":" Nagpal, who lives in Göttingen, Germany, had been offered a premed place and scholarship at NYU. Her acceptance was dependent on her results in the International Baccalaureate diploma, a two-year high school program recognized by colleges and taken by more than 170,000 students this year, most in the US. But she scored more poorly than expected..Teen regrets about grades aren’t unusual, but the way the foundation behind the IB Diploma Programme calculated this year’s grades was. The results, released Monday, were determined by a formula that IB, the foundation behind the program, hastily deployed after canceling its usual springtime exams due to Covid-19. The system used signals including a student’s grades on assignments and grades from past grads at their school to predict what they would have scored had the pandemic not prevented in-person tests..Nagpal and many other students, parents, and teachers say those predictions misfired. Many students received suspiciously low scores, they say, shattering their plans for the fall and beyond. Nagpals backup plan if she missed out on NYU was to study medicine in Germany, but she doesnt think her lower-than expected grades will qualify her for a place. Like so many, I was extremely shocked, she says. Nagpal later received an email from NYU saying it has not made a decision on her admission. NYU said it does not comment on individual cases..More than 15,000 parents, students, and teachers have signed an online petition asking IB to “take a different approach with their grading algorithm and to make it fairer.” The foundation declined to answer questions about its system but said it had been checked against five years of past results and that disappointed students could use its existing appeals process, which comes with a fee. The foundation released summary statistics showing that this year’s average score was slightly higher than last year’s, and it says the distribution of grades was similar..One math teacher at a school in the Middle East says IB should disclose the full workings of its model for outside scrutiny. He and a colleague with a math PhD have been puzzling over its design since several students lost scholarships to top universities, after receiving results much lower than expected by their teachers. Some students caught out are now unsure how they’ll pay for college. “My only guess is a flawed model,” he says..Concerns about flawed math models are growing as more companies and governments apply computers to traditionally human problems such as bail decisions, identifying criminal suspects, and deciding what is hate speech. Rooting out bias and inaccuracy in such systems is a growing field of activism and academia..People questioning IB’s algorithm-derived grades are now raising some of the same issues. They’re wondering how the system was designed and tested, why its workings weren’t fully disclosed, and whether it makes sense to use a formula to determine the grades that can shape a person’s opportunities in life..When Covid-19 seized hold of the world in March, many teens in their final year of high school were left in a precarious position. Shelter-in-place orders made it challenging or impossible to complete the final assignments or tests that could determine their college and life choices..Test providers scrambled to devise new ways to assess students. In the US, Educational Testing Service, which provides the GRE, and the College Board, which runs AP Exams, moved their tests online. That brought quirks and glitches—like requiring students to take their tests simultaneously regardless of time zone and retakes forced by technical errors—but it maintained a semblance of the normal process..IB, headquartered in Geneva, opted to use a statistical formula instead—adding to the growing list of tech fixes proposed to automate away fallout from the pandemic. The workings of the IB diploma—and the timing of the results—proved particularly harmful for IB students applying to US colleges. Unlike AP tests, which are typically separate from high school grades, the IB results are intended to reflect a student’s work for the year. IB students are often granted college admission based on predicted grades, and they submit their final results when they become available over the summer. Some colleges, including NYU and Northeastern, warn on their admissions pages that students whose IB results don’t get close enough to those predictions may lose their place..In normal times, IB diploma students select six subjects, from options such as physics and philosophy, and receive final grades determined in part by assignments but mostly by written tests administered in the spring. The program is offered by nearly 900 public schools in the US and is common in international schools around the world. In March, IB canceled all tests and said it would calculate each student’s final grades using a method developed by an unnamed educational organization that specializes in data analysis..The idea was to use prior patterns to infer what a student would have scored in a 2020 not dominated by a deadly pandemic. IB did not disclose details of the methodology but said grades would be calculated based on a student’s assignment scores, predicted grades, and historical IB results from their school. The foundation said grade boundaries were set to reflect the challenges of remote learning during a pandemic. For schools where historical data was lacking, predictions would build on data pooled from other schools instead..In a video IB posted about the process, Antony Furlong, the foundation’s manager for assessment research and design, said the system essentially created “a bespoke equation” for every school..One visual arts teacher at a US school says what she and coworkers have seen suggests it wasn’t well tailored. “When I saw the marks, I was floored,” she says. “I am always conservative in my predicted grades, but every single student except one were downgraded.” Of 15 students she works with, four have to rethink their plans for this fall, because they missed out on college places, something she didn’t expect for any of them..Determining whether IB’s system had flaws is challenging without knowing its formula or the inputs and outputs. Just because some humans don’t like the outputs of a data analysis doesn’t mean that it’s incorrect. But Suresh Venkatasubramanian, a professor at the University of Utah who studies the social consequences of automated decisionmaking, says it appears IB could have deployed its system more responsibly. “All this points to what happens when you try to install some sort of automated process without transparency,” he says. “The burden of proof should be on the system to justify its existence.”.Data analysis is more powerful than ever but remains far from being able to predict complex future human actions. Models that extrapolate from past statistical trends can end up treating people unfairly because their circumstances are different, even if results match past patterns on average..Venkatasubramanian says that basing a student’s grades on past trends at their school, potentially unrelated to the student’s own school career, could be unfair. Using data from other schools—as IB did for schools with little track record—is a “red flag,” he says, because it would mean some students’ grades were calculated differently than others..Constance Lavergne, whose son in the UK received lower-than-expected IB grades and missed out on his preferred college, is one of many parents struggling to understand what happened. She says her experience working closely with data analysts in the tech industry makes her suspicious of IB’s methodology. It would naturally generate noisier results for smaller classes, like her son’s, because they offer fewer past data points, she suggests. “There’s something wrong with the algorithm,” Lavergne says..The math teacher in the Middle East said he believed his school had suffered because of how IB announced and calibrated its model. Students at the school submitted their assignments before IB said those assignments would help steer the grading model. Some IB students at other schools had not yet submitted those assignments, allowing them to put in extra effort, aided by knowing they didn’t have to prepare for exams. This weekend, he plans to work with his math PhD colleague and a software package to probe where the IB formula may have gone wrong..Many students who received disappointing results are now looking to November, when IB typically offers a second round of in-person tests and they can take the written test that was canceled. Nagpal, the frustrated medical student, intends to take part, at a cost of about €700 ($791). If Covid-19 disrupts those tests too, she hopes IB will move them online rather than try any more experiments in data-led grading.\n"}
{"prompt":"The facial recognition software used by the Detroit Police Department misclassified a man and charged him with larceny.\n\n###\n\n","completion":" The high-profile case of a Black man wrongly arrested earlier this year wasn’t the first misidentification linked to controversial facial recognition technology used by Detroit Police, the Free Press has learned. .Last year, a 25-year-old Detroit man was wrongly accused of a felony for supposedly reaching into a teacher’s vehicle, grabbing a cellphone and throwing it, cracking the screen and breaking the case..It identified Michael Oliver as an investigative lead. After that hit, the teacher who had his phone snatched from his hands identified Oliver in a photo lineup as the person responsible. .Controversy over law enforcement using facial recognition technology is not new, nor is it confined to the city of Detroit. But recent uprisings around the country in response to racial injustice in the wake of the death of George Floyd have again brought criticism of the technology to the forefront..In Detroit, where police started using facial recognition software as an investigative tool in 2017, protesters have demanded the city stop using it, saying the error rate is high when used to identify people of color. City Council, which will consider extending a software contract to help pay for it, has been urged to vote no by some residents. Detroits civilian Board of Police Commissioners also has been discussing the departments use of technology..In the cellphone case, according to transcribed testimony, the teacher called 911 as he watched a group of students fighting. One student had a baseball bat and others were wrestling on the ground. The teacher used his cellphone to video record the incident. The phone was recording when a young man reached into the teachers car and snatched the phone..Oliver has tattoos up and down his arms. Those markings weren’t visible on the person captured on video. Oliver’s attorney, Patrick Nyenhuis, also noticed differences in the hair style and body type between the person in the video and his client, he said..He took his concerns and pictures of his client to Wayne County Assistant Prosecutor Brian Surma, a supervisor in the office. Surma and the teacher reviewed photographs, determined Oliver was misidentified and both agreed the case should be dismissed immediately, a court transcript shows..Oliver, now 26, said he was nervous as his case proceeded last year because people still get convicted for crimes they don’t commit. He questioned how his face ever got connected to the case..During the investigation, police captured an image from the cellphone video, sent it for facial recognition and the photo came back to Oliver, the police report said..A second person, a student, was also captured in the video with the suspect. The officer in charge of the case testified he didn’t interview that person though hed been given that student’s name..Police investigated Olivers case prior to a new policy governing the use of facial recognition software. It includes stricter rules on when Detroit police can use it. The technology is now used only as a tool to help solve violent felonies, Detroit police have said..Evidence in Olivers case wasnt reviewed by a supervisor in the prosecutors office prior to him being charged, spokeswoman Maria Miller said in an email. Current protocol requires a supervisor review all evidence in a facial recognition case prior to a charging decision. There also must be other evidence that corroborates the allegations in order to charge someone..The prosecutors office is taking additional steps, Miller said. It will be required that facial recognition cases be submitted to Prosecutor Kym Worthy — the highest-ranking person in the office — for approval if an assistant prosecuting attorney and supervisor determine charges should be authorized..Miller said the prosecutors office knows of no other cases in which people charged with crimes were misidentified, other than that of Oliver and Robert Williams.  .Williams is a Farmington Hills man arrested in front of his family in January and accused of stealing high-end watches. Prosecutors and police have apologized for how that case was handled..The case generated headlines across the country, including in the Free Press, the New York Times and the Washington Post. While in custody, Williams said he told police he wasnt the man seen in a blurry image from store surveillance video..“As a result of these two cases, we have a more stringent protocol in facial recognition cases, Worthy said in an statement. The cases will be reviewed during the warrant charging phase, prior to the preliminary examination, and again when the case is bound over to the Circuit Court in any case where facial recognition has been used as an investigative tool.”.In the summer of 2019, the Detroit Police Department asked me personally to adopt their Facial Recognition Policy, she said. I declined and cited studies regarding the unreliability of the software, especially as it relates to people of color..Dan Korobkin, ACLU of Michigan legal director, called the technology dangerous in a statement Friday. He urged police and prosecutors across the country to review cases involving the use of the technology and notify all people charged as a result of it..Studies have shown the technology, relying on computer algorithms, sometimes has trouble distinguishing human faces, especially with people of color..Detroit Police Chief James Craig, who is Black, has said he is a strong believer in facial recognition software. Last summer, he said police had used the technology about 500 times then moved on to the next phase of investigation only 30% of the time..The Detroit police commission discussed Williams case during a meeting Thursday afternoon. Police gave a presentation and Craig said the situation should not have happened..Last month, board member Evette Griffie sought answers from police about Williams case, including a timeline of events and any discipline resulting from the misidentification.\n"}
{"prompt":"Google's Search engine shows pornography when a use enters black girls.Although this specific error has been fixed, Latina and Asian girls are still sexualized in results. Furthermore, a search for professional and unprofessional haircuts shows racial bias as well.\n\n###\n\n","completion":" The internet might seem like a level playing field, but it isn’t. Safiya Umoja Noble came face to face with that fact one day when she used Google’s search engine to look for subjects her nieces might find interesting. She entered the term “black girls” and came back with pages dominated by pornography. .Noble, a USC Annenberg communications professor, was horrified but not surprised. For years she has been arguing that the values of the web reflect its builders—mostly white, Western men—and do not represent minorities and women. Her latest book, Algorithms of Oppression, details research she started after that fateful Google search, and it explores the hidden structures that shape how we get information through the internet..The book, out this month, argues that search engine algorithms aren’t as neutral as Google would like you to think. Algorithms promote some results above others, and even a seemingly neutral piece of code can reflect society’s biases. What’s more, without any insight into how the algorithms work or what the broader context is, searches can unfairly shape the discussion of a topic like black girls..Noble spoke to MIT Technology Review about the problems inherent with the current system, how Google could do better, and how artificial intelligence might make things worse..If we’re looking for the closest Starbucks, a specific quote, or something very narrow that is easily understood, it works fine. But when we start getting into more complicated concepts around identity, around knowledge, this is where search engines start to fail us. This wouldn’t be so much of a problem except that the public really relies upon search engines to give them what they think will be the truth, or something vetted, or something that’s credible. This is where, I think, we have the greatest misunderstanding in the public about what search engines are..We could think about pulling back on such an ambitious project of organizing all the world’s knowledge, or we could reframe and say, “This is a technology that is imperfect. It is manipulatable. We’re going to show you how it’s being manipulated. We’re going to make those kinds of dimensions of our product more transparent so that you know the deeply subjective nature of the output.” Instead, the position for many companies—not just Google—is that [they are] providing something that you can trust, and that you can count on, and this is where it becomes quite difficult..Ive been arguing that artificial intelligence, or automated decision-making systems, will become a human rights issue this century. I strongly believe that, because machine-learning algorithms and projects are using data that is already biased, incomplete, flawed, and [we are] teaching machines how to make decisions based on that information. We know [that’s] going to lead to a variety of disparate outcomes. Let me just add that AI will be harder and harder to intervene upon because it will become less clear what data has been used to inform the making of that AI, or the making of those systems. There are many different kinds of data sets, for example, that are not standardized, that are coalescing to make decisions..Since I started writing about and speaking publicly about black girls in particular being associated with pornography, things have changed. Now the pornography and hypersexualized content is not on the first page, so I think that was a quiet improvement that didn’t come about with a lot of fanfare. But other communities, like Latina and Asian girls, are still highly sexualized in search results. .An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.\n"}
{"prompt":"A Carnegie Mellon team build AdFisher ( a system of male and female job seekers) and found males were shown ads to seek high paying jobs more than females.\n\n###\n\n","completion":" Automated testing and analysis of company’s advertising system reveals male job seekers are shown far more adverts for high-paying executive jobs .The team of researchers from Carnegie Mellon built an automated testing rig called AdFisher that pretended to be a series of male and female job seekers. Their 17,370 fake profiles only visited jobseeker sites and were shown 600,000 adverts which the team tracked and analysed..The authors of the study wrote: “In particular, we found that males were shown ads encouraging the seeking of coaching services for high paying jobs more than females.” .One experiment showed that Google displayed adverts for a career coaching service for “$200k+” executive jobs 1,852 times to the male group and only 318 times to the female group. Another experiment, in July 2014, showed a similar trend but was not statistically significant..Google’s ad targeting system is complex, taking into account various factors of personal information, browsing history and internet activity. Critically the fake users started with completely fresh profiles and behaved in the same way, with gender being the only factor that was different and illustrating that the ad targeting for these job adverts was discriminatory..However, the authors of the study admit that the gender discrimination shown is difficult to pin to one factor, due to the complexity of not only Google’s profiling systems, but also of the way advertisers buy and target their adverts using Google..A Google spokeswoman said: “Advertisers can choose to target the audience they want to reach, and we have policies that guide the type of interest-based ads that are allowed.”.Profiling is inherently discriminatory, as it attempts to treat people differently based on their behaviour and personal information. While that customisation can be useful, showing more relevant ads to users, it can also have negative connotations..The study authors said: “Male candidates getting more encouragement to seek coaching services for high-paying jobs could further the current gender pay gap. Even if this decision was made solely for economic reasons, it would continue to be discrimination.”.Google allows users to opt out of behavioural advertising and provides a system to see why users were shown ads and to customise their ad settings. But the study suggests that there is a transparency and overt discrimination issue in the wider advertising landscape..Television, radio and print advertisers have, of course, been practising discrimination for years, pushing ads out with shows or magazines that appeal to a particular gender or demographic..The difference now is that it is much more obvious in the internet age, and the in-depth profiling that is now possible could make it worse, not better..The researchers also investigated whether visiting sites dealing with certain topics, specifically substance abuse, adult content, disabilities, mental disorders and infertility, affected the ads served to the fake profiles..Only visiting sites dealing with substance abuse and disability created statistically significant results. The researchers found that after visiting substance abuse sites Google’s advert profile page showed no change to the interests listed, but the adverts shown to the user accounts did change, including displaying ads for rehabilitation services from a company called Watershed. The adverts shown to the control group did not include any rehabilitation services..“One possible reason why Google served Watershed’s ads could be remarketing, a marketing strategy that encourages users to return to previously visited website,” said the authors of the study..The Watershed site was included in the top 100 substance abuse sites list, which was used as the experimental list of sites to visit by the automated system..A similar result was shown in testing for disability sites, using a similar methodology. This time the researchers found that Google’s ad interest profile did change for the test group, but that it showed other interests not related to disability..Ads for mobility devices including a standing wheelchair were shown to the test group 1,076 times but never to the control group. Again the adverts included sites within the top 100 sites concerning disability used during the experiment..Google has said that it prohibits the targeting of adverts within its “sensitive category policy”, which includes health issues such as substance abuse. It also says that does not allow remarketing within the same sensitive areas..The researchers also discovered that Google’s ad choices, which allows users to manually remove certain interests from the tracking profiles, had the effect that was desired.\n"}
{"prompt":"Researchers show that AI has an unethical optimization principle\n\n###\n\n","completion":" Artificial intelligence (AI) is increasingly deployed around us and may have large potential benefits. But there are growing concerns about the unethical use of AI. Professor Anthony Davison, who holds the Chair of Statistics at EPFL, and colleagues in the UK, have tackled these questions from a mathematical point of view, focusing on commercial AI that seek to maximize profits..One example is an insurance company using AI to find a strategy for deciding premiums for potential customers. The AI will choose from many potential strategies, some of which may be discriminatory or may otherwise misuse customer data in ways that later lead to severe penalties for the company.  Ideally, unethical strategies such as these would be removed from the space of potential strategies beforehand, but the AI does not have a moral sense, so it cannot distinguish between ethical and unethical strategies..In work published in Royal Society Open Science on 1 July 2020, Davison and his co-authors Heather Battey (Imperial College London), Nicholas Beale (Sciteb Limited) and Robert MacKay (University of Warwick), show that an AI is likely to pick an unethical strategy in many situations. They formulate their results as an Unethical Optimization Principle:.If an AI aims to maximize risk-adjusted return, then under mild conditions it is disproportionately likely to pick an unethical strategy unless the objective function allows sufficiently for this risk. .This principle can help risk managers, regulators or others to detect the unethical strategies that might be hidden in a large strategy space. In an ideal world one would configure the AI to avoid unethical strategies, but this may be impossible because they cannot be specified in advance. In order to guide the use of the AI, the article suggests how to estimate the proportion of unethical strategies and the distribution of the most profitable strategies..Our work can be used to help regulators, compliance staff and others to find problematic strategies that might be hidden in a large strategy space. Such a space can be expected to contain disproportionately many unethical strategies, inspection of which should show where problems are likely to arise and thus suggest how the AI search algorithm should be modified to avoid them, says Professor Davison. It also suggests that it may be necessary to re-think the way AI operates in very large strategy spaces, so that unethical outcomes are explicitly rejected during the learning process..Professor Wendy Hall of the University of Southampton, known worldwide for her work on the potential practical benefits and problems brought by AI, said: This is a really important paper. It shows that we cant just rely on AI systems to act ethically because their objectives seem ethically neutral. On the contrary, under mild conditions, an AI system will disproportionately find unethical solutions unless it is carefully designed to avoid them..The tremendous potential benefits of AI will only be realised properly if ethical behaviour is designed in from the ground up, taking account of this Unethical Optimisation Principle from a diverse set of perspectives. Encouragingly, this Principle can also be used to help find ethical problems with existing systems which can then be addressed by better design..Beale N, Battey H, Davison AC, MacKay RS. (2020) An unethical optimization principle. R. Soc. Open Sci. 7: 200462. DOI: 10.1098\/rsos.200462\r .Disclaimer: AAAS and EurekAlert! are not responsible for the accuracy of news releases posted to EurekAlert! by contributing institutions or for the use of any information through the EurekAlert system.\n"}
{"prompt":"Uber drivers take Uber to court for not sharing data and potentially unfairly profiling drivers\n\n###\n\n","completion":" The algorithms that underlie most Silicon Valley bigwigs are usually locked up in the name of “protecting trade secrets,” but that’s an excuse that’s starting to wear a little thin in recent months. Maybe it’s because these companies are that much harder to regulate or compete against when we don’t ask them to reveal their respective secret sauces. Maybe it’s because these secret sauces control the incomes of a bigger and bigger chunk of small business owners and gig workers alike. Maybe it’s because they’re found to be biased in all sorts of ways, and hiding the specifics in the name of “trade secrets” only ends up hurting the people who use the platform. .All of these issues came to a head this week when four Uber drivers based in the UK filed suit against the company’s European branch in an Amsterdam District court on Monday, on the grounds that the company’s refusal to share driver data with, well, drivers, is technically a breach of European data protection laws. If this group can get Uber to cough up the data in question, it could offer an unprecedented window into the way the company algorithmically profiles both the EU-based and US-based drivers on its platform—unfairly or otherwise..To give a bit of background, the four drivers behind the suit are members of the App Drives & Couriers Union—or ADCU for short—a group that’s working with a nonprofit called the Worker Info Exchange in the hopes of collecting app data on drivers that can be used for, say, collective bargaining further down the line..According to the docket, all four of the drivers tried asking Uber for the data that was collected by their Uber Driver app, which doesn’t only collect data from, say, a given Uber ride, but collects data on everything from acceleration to location during every car trip the driver makes, even if it’s made without a passenger. In total, the docket describes more than 26 categories of data, which, on paper at least, is mostly crunched together to assess supply and demand, and figure out what the going rates should be for a given Uber ride in a given neighborhood on a given day. .But as the docket describes, some of this data is also used to profile the drivers in question. As the docket explains, Uber keeps a tightly guarded profile of each driver on the platform, and uses metrics like a driver’s arrival times and ride rating to make notes about their “level of professionalism” or “navigational skills.” When the four UK drivers behind the suit asked Uber to cough up this data—and more—expecting that the company would abide under Europe’s hefty data laws—none of them got any of this profiling data back. What’s worse, only two of them got any data back at all, which means Uber, in this case, is one of those tech companies that thinks it can get away with ignoring GDPR. And like those tech companies caught flouting GDPR, Uber can, in turn, be fined thousands and thousands of dollars until they right this particular wrong. .Level up your earbud gameApple has added premium features previously exclusive to its more luxurious AirPods Pro, such as spatial audio and water resistance..Until that happens, the ADCU has set up a monthlong fund for folks looking to support the case, racking up 345 pounds (roughly $436 USD) since the case went live in the AM. They’re also encouraging all Uber drivers and couriers for Uber Eats to file their own requests for data from the company, in the hopes that more voices—and more pressure on the company—might actually force them to make a change. But considering Uber’s standing track record of shitty behavior towards drivers, this Union might be facing an uphill battle, even if the local data-protective laws are on their side. \n"}
{"prompt":"UN shares highly sensitive data with Palantir which adds risk of harm and exploitation\n\n###\n\n","completion":" CIA-linked software firm Palantir will help the UN’s World Food Programme analyse its data in a new partnership worth $45 million, both organisations announced Tuesday, drawing immediate flak from privacy and data protection activists..The California-based contractor, best known for its work in intelligence and immigration enforcement, will provide software and expertise to the UN’s food relief agency over five years to help WFP pool its enormous amounts of data and find cost-saving efficiencies..At a press conference in Geneva, WFP’s chief information officer Enrica Porcari said the plan was to launch a data integration effort that would include records of distributions to beneficiaries but, she stressed, not personally identifiable data. “Can all the data pour into one lake?” she asked, rhetorically. The system would then, she explained, work like a bank whose algorithms flag unusual credit card activity, picking up “anomalies” in beneficiary locations and behaviour that might signal misuse..WFP is jumping headlong into something they don’t understand, without thinking through the consequences, and the UN has put no frameworks in place to regulate it..In future, if multiple aid agencies connected to the WFP’s SCOPE beneficiary management system and used it as the basis for recording what people received, a powerful overview could be achieved, Porcari said..Palantir executive vice president Josh Harris said WFP’s 92 million aid recipient “customers”, its more than 30 data systems, and its difficult operating environment represented a “complex data landscape”, but something his company’s software was built for. The opportunity to provide support to WFP is a “dream combination” that fits “mission-driven” Palantir’s philanthropic goals, Harris added..Palantir has already worked with WFP on a pilot project on food procurement in Iraq that has produced over $26 million (or about 10 percent) in savings, the two organisations said..Privacy and data protection activists cried foul at the new tie-up, questioning if WFP understood what it was getting itself into and if proper safeguards had been put in place..“The recipients of WFP aid are already in extremely vulnerable situations; they should not be put at additional risk of harm or exploitation,” a spokesperson for activist NGO Privacy International told IRIN. “This data is highly sensitive, and it is essential that proper protections are put in place, to limit the data gathered, transferred, and processed.”.Asked for the legal basis for any data-sharing with Palantir, Porcari said: “there is no data-sharing”. She insisted that all data instead would rest under WFP’s control, with personal data being kept separate and secure..But Privacy International, which recently analysed the (unintended) risks of humanitarian data misuse, warned: “Weve seen examples of systems that are produced in agreements such as the one between WFP and Palantir increasing risks to the people the systems are aiming to benefit. There are risks to both individuals and whole populations from the gathering and processing of data from humanitarian activities.”.A humanitarian data analyst, who requested anonymity due to work relationships, was also alarmed at the news, saying: WFP is jumping headlong into something they don’t understand, without thinking through the consequences, and the UN has put no frameworks in place to regulate it..Palantir was established with the help of seed capital from a CIA-linked investment body. Its main clients have been US security and intelligence bodies..Its capacity to structure and overlay vast datasets has led it to be credited with helping the US government to find Osama bin Laden. However, its work with US police and, most recently, immigration enforcement, has come under fire for secrecy, profiling bias, enabling human rights violations, and the wholesale harvesting of personal data..“It is the height of irony that the very company that faced direct criticism in its role facilitating US immigration authorities human rights abuses is now promoting itself as trustworthy of working in humanitarian aid,” the Privacy International spokesperson said..From the 2013 Haiyan super-typhoon in the Philippines to the 2014-2016 Ebola outbreak in West Africa, Palantir has often sought opportunities to deploy its technology in the humanitarian arena..Few analysts contacted by IRIN in this 2016 investigation doubted the software had powerful potential, but reputational concerns made a number of potential partners walk away, even when offered free access to Palantir products and software advisors..“It is the height of irony that the very company that faced direct criticism in its role facilitating US immigration authorities human rights abuses is now promoting itself as trustworthy of working in humanitarian aid.”.Nevertheless, Palantir’s pro-bono “Philanthropy Engineering” has provided support to numerous non-profits, including the Carter Center, Team Rubicon, the Enough Project, and the Rockefeller Foundation. The UN’s nuclear watchdog, the International Atomic Energy Agency (IAEA), is a paying customer..WFP has been working with Palantir since 2017 – a WFP spokesperson said an encounter at the World Economic Forum in 2015 kicked off the relationship. Gaining greater insight from a mountain of internal and external data with the help of Palantir’s Foundry system has already led to cost savings and efficiencies, according to the Rome-based UN agency..A starting point for the Palantir work inside WFP was Optimus. According to a recent update from WFP, Optimus is an internal tool to help guide purchasing and other planning decisions, for example in Ethiopia or Yemen, to assign different commodities to make up a mixed basket of food for distribution depending on funding and seasonal market prices.  .Poncari described WFP as being on a “very aggressive digital transformation journey” and said it had a “moral imperative” to leverage technology to achieve efficiencies. “We just want to go with the best,” she told reporters.. Support our journalism and become more involved in our community. Help us deliver informative, accessible, independent journalism that you can trust and provides accountability to the millions of people affected by crises worldwide. \n"}
{"prompt":"SmartLink shares GPS active data of immigrants\n\n###\n\n","completion":" Terror capitalism uses tools such as facial recognition to extract profits from marginalized people. Big tech and governments are collaborating.When Gulzira Aeulkhan finally fled China for Kazakhstan early last year, she still suffered debilitating headaches and nausea. She didn’t know if this was a result of the guards at an internment camp hitting her in the head with an electric baton for spending more than two minutes on the toilet, or from the enforced starvation diet..Maybe it was simply the horror she had witnessed – the sounds of women screaming when they were beaten, their silence when they returned to the cell..Like an estimated 1.5 million other Turkic Muslims, Gulzira had been interned in a “re-education camp” in north-west China. After discovering that she had watched a Turkish TV show in which some of the actors wore hijabs, Chinese police had accused her of “extremism” and said she was “infected by the virus” of Islamism. They predicted it would lead her to commit acts of terrorism, so they locked her away..Gulzira’s detention lasted for more than a year. She was released in October 2018, only to be told that she had been assigned to work in a glove factory near the camp. After long hours behind a sewing machine, Gulzira was kept in a dormitory surrounded by security checkpoints that used facial-recognition technology to track her movements and scraped data from her smartphone, which she was required to carry at all times. She was paid $50 a month, roughly one-sixth of the legal minimum wage in the region..Gulzira was one of the millions of targets of a global phenomenon we call “terror capitalism”. Terror capitalism justifies the exploitation of subjugated populations by defining them as potential terrorists or security threats. It primarily generates profits in three interconnected ways. First, lucrative state contracts are given to private corporations to build and deploy policing technologies that surveil and manage target groups. Then, using the vast amounts of biometric and social media data extracted from those groups, the private companies improve their technologies and sell retail versions of them to other states and institutions, such as schools. Finally, all this turns the target groups into a ready source of cheap labor – either through direct coercion or indirectly through stigma..The people being targeted by terror capitalism include entire stateless groups, such as ethnic Bengalis in north-east India and Palestinian Arabs. They are almost always from minority or refugee populations, especially Muslim ones. While the Chinese system is unique in terms of its scale and the depth of its cruelty, terror capitalism is an American invention, and it has taken root around the world..In China, the terror capitalism system targets many of the more than 15 million Uighurs and other Turkic Muslims in the region. Coercing these people into low-wage work means fewer Chinese jobs move to Vietnam and Bangladesh. The companies that have developed China’s policing technologies are now selling them to police units and regional governments in Zimbabwe, Dubai, Kuala Lumpur, the Philippines and many other locations..Meanwhile, across Europe and North America, terror-capital surveillance tools have placed hundreds of thousands of Muslims on watchlists as part of Countering Violent Extremism programs. In the United States, immigration control measures taken in the aftermath of 9\/11 have paved the way for a system that monitors and controls asylum seekers who enter the country at the southern border..These systems use GPS tracking to control people in ways that are similar to the surveillance system in north-west China. After being held in a US detention center, a Pakistani asylum seeker named Asma was required to wear a GPS-enabled ankle monitor. (More than 32,000 foreign nationals in the US have to do the same.) Asma told us how the stigma associated with her ankle monitor pushed her into low-paid work. A food-truck owner in Austin, Texas, gave her a job because, he told her, the customers couldn’t see the monitor if she stayed behind the counter in the truck. But when the monitor started blaring, “Recharge battery! Battery low! Recharge battery! Battery low!” during her shift one day, her boss fired her. A year after Asma obtained asylum, her ankle is covered with scars from the monitor and she often feels phantom vibrations in her leg..Ankle monitors are still prevalent, but in the past two years, US asylum seekers have also increasingly been made to submit to weekly “biometric check-ins”, through an app called SmartLink, which they are made to install on their phones. They have to keep their phones charged and GPS active at all times. Lorena, an asylum seeker who has fled violence in Guatemala, was initially relieved that her ankle monitor was removed, until she was required to give Immigrations and Customs Enforcement (Ice) access to her email account, which connects to her social media accounts..The SmartLink app is now being used to control 21,712 immigrants. It was developed by BI Incorporated, a company that initially designed GPS ankle monitors. BI is now a subsidiary of Geo Group, one of the major prison and detention companies that have profited from a dramatic expansion of prisoner and detained populations in the US over the past four decades. The app is also supported by the nationwide telecom providers Sprint and Verizon, with movement tracking provided by Google Maps..The rationale for these new regimes of technological surveillance and control emerged directly from the “global war on terror” that the George W Bush administration declared in 2001. So too did early versions of many of the policing technologies now being used around the world. The new paradigm began in Iraq, where counter-insurgent war demanded a suspension of civil and human rights that allowed the US army to identify and track the movements, online activities, networks, and states of mind of masses of Iraqis. China eventually followed suit, with Xi Jinping justifying the technological subjugation of the country’s Muslims by declaring the “people’s war on terror” in 2014..“Surveillance is about controlling and disciplining marginalized people – whether it’s people of color, immigrants or poor people,” says a current employee of Microsoft, which was a key early-stage investor in AnyVision, an Israeli surveillance technology company that has used facial recognition to monitor Palestinians in the West Bank. “Companies use surveillance to discipline workers. Law enforcement uses surveillance to reinforce systemic racism and perpetuate mass incarceration. States use surveillance to enforce border logics and state oppression. Surveillance, as a concept, isn’t neutral – it is always about control.”.Because terror capitalism emerges at the nexus where the power of states such as the US and China meets the power of tech companies such as Microsoft, Google, Hikvision and SenseTime, fighting it will require not only an empowered public but also people within governments and companies to regulate and resist harmful forms of surveillance. There is a growing movement among tech workers, from Seattle to Hong Kong, who are appalled by the complicity of their employers in terror capitalism. Together with them, we must push these companies to refuse to profit from the militaristic ambitions of these states..Dr Darren Byler is a postdoctoral researcher at the University of Colorado and the author of two forthcoming books, one on the effects of terror capitalism among Uighurs and one on technologies of re-education in China and around the world.Dr Carolina Sanchez Boe is a Danish Research Foundation postdoctoral fellow. She is the author of a forthcoming book, The Undeported: The Making of a Floating Population of Exiles in France and Europe\n"}
{"prompt":"Russia demands Tinder to give user data to identify LBGTQ+\n\n###\n\n","completion":" Following a Russian regulatory body’s demand that Tinder hands over user data to federal law enforcement agencies, online daters’ Saturday night “u up?” text — and much more —  could become the property of Russia’s nefarious government. Tinder should firmly refuse to comply with this dangerous and authoritarian policy..This month, Russia’s telecoms regulator, the Roskomnadzor, added Tinder to a list of websites and apps that it forces to store user data, messages and pictures on government-accessible Russian servers. The agency can then offer that data up to law enforcement and intelligence services on demand. If Tinder agrees to provide this information, its users in Russia will have no meaningful sense of privacy on the app..This data collection policy puts Tinder’s commitments in conflict. On one hand, users agree to a privacy policy that states that Tinder can share their information to “comply with a legal process.” On the other, the company has a commitment to its users and stakeholders. Its privacy web page directly says Tinder does not “compromise with your privacy.”.To say that putting customers’ intimate details in the hands of one of the world’s most illiberal regimes is a “compromise” would be the understatement of the century..But the stakes for Tinder go beyond protecting individual privacy. Sharing data with the Russian government would seriously endanger Tinder’s LGBTQ user base. It would be grossly, unjustifiably irresponsible for the brand to release information that could reveal swipers’ sexual preferences to a government with a record of open hostility to its LGBTQ community..In 2013, the federal government in Moscow passed legislation broadly banning gay “propaganda” directed at minors. In practice, this law has shut down LGBTQ health education and support services, contributed to a rise in homophobic violence and silenced LGBTQ rights organizations. More recently, the small Russian republic of Chechnya executed a violent purge of dozens of gay men in 2017. According to Human Rights Watch, these men were humiliated, starved and tortured in concentration camps. Some “disappeared.” Others were outed and returned to deeply homophobic families for even more brutal abuse..Just this year, reports emerged revealing a second wave of detentions. Chechnya’s leader, Ramzan Kadyrov, responded to the allegations by denying the existence of gay people in Chechnya. .If Tinder complies with the Russian government, officials like Kadyrov could potentially access data on users’ matches and app settings that would help them systematically track down, detain and torture more LGBTQ citizens. It would become easier for law enforcement to punish and harass users who spread gay “propaganda” on the app..The company’s compliance could fuel corruption and extortion across the board. In Russian politics, blackmail is a popular instrument for making tough political gains. If Tinder provides Russia’s secretive and manipulative intelligence agencies with broad user data, officials will take advantage of the raunchy messages they uncover to expose and destroy their political opponents. On a smaller scale, they could use their newfound wealth of information to enrich themselves by extorting Tinder users. Police have already been using similar tactics to extort gay men in Chechnya for years. .So far, Tinder has “registered to be compliant.” But it would be making an unforgivable mistake if it went through with this data-sharing arrangement. Maybe Tinder doesn’t care about the privacy of its users, but human lives are at stake here. The physical safety of its Russian users is now in the hands of its executives.\n"}
{"prompt":"Police utilize entrapment techniques on dating apps and video footage from concerts\n\n###\n\n","completion":" Share this via Facebook    Share this via Twitter    Share this via WhatsApp    Share this via Email    Other ways to share    Share this via LinkedIn    Share this via Reddit    Share this via Telegram    Share this via Printer   .(Nairobi, September 30, 2017)—Egypt should stop arresting and harassing people suspected of homosexuality using trumped-up “debauchery” and “inciting debauchery” charges, Human Rights Watch said today. Security forces rounded up at least eleven people in the days following a September 22, 2017 concert in Cairo at which young concertgoers waved rainbow flags, a symbol of solidarity with  lesbian, gay, bisexual or transgender (LGBT) people, a defiant act in a country known to persecute gay men and transgender people..After concertgoers shared photos of the rainbow flag display on social media, pro-government media went on an overdrive attack and conservative politicians and religious leaders demanded that the government take action. Police arrested one man on September 23 through entrapment on a dating app, a common police technique in Egypt, and claimed he had been among those to wave a flag. On September 25,  the government said that it had arrested seven people identified through video footage of the concert. Several Egyptian activists questioned the veracity of this claim, but they documented additional arrests on September 27, when police picked up six men from the streets, charging them with debauchery and claiming they were all involved in the rainbow flag incident.  .“Whether they were waving a rainbow flag, chatting on a dating app, or minding their own business in the streets, all these debauchery arrest victims should be immediately released,” said Sarah Leah Whitson, Middle East and North Africa director at Human Rights Watch. “The Egyptian government, by rounding people up based on their presumed sexual orientation, is showing flagrant disregard for their rights.”.The Dokki Misdemeanor Court in Giza sentenced the first victim on September 26 to six years in prison and a fine for “debauchery,” based on his presumed sexual conduct, and “inciting debauchery,” as prosecutors alleged he was among those who raised the rainbow flag at the concert. The court sentenced him to an additional six years of probation which will require reporting to the police from 6p.m. to 6a.m. until 2029. No lawyer was present at his trial.  He now has legal representation, and his appeal will be heard on October 11..The six men arrested on September 27 are scheduled for trial on October 1. At least two more men were arrested on September 28 because of their presumed sexual orientation, and Egyptian media reported that another six men were arrested on September 28 in a raid on a home, although Human Rights Watch has not independently verified that report..At the September 22 concert, people raised rainbow flags during the performance of the Lebanese group Mashrou’ Leila, which has an openly gay lead singer and has performed songs addressing same-sex relationships and gender identity. The Egyptian Musicians Syndicate opened an investigation into the event and banned future Mashrou’ Leila concerts in Egypt..In Egypt, police routinely round up gay and bisexual men and transgender women, actively seeking them out and entrapping them on dating apps and through social media. One Cairo-based organization has documented the prosecution of at least 34 people for consensual same-sex conduct in the last 12 months. Since President Abdel Fattah al-Sisi came into power in 2014, several hundred people have been imprisoned on charges of consensual same-sex conduct..Egyptian activists told Human Rights Watch they fear that the past week’s arrests could signal the beginning of an even harsher crackdown on LGBT people and those who publicly support them..Egypt’s Forensic Medicine Authority also routinely subjects people to forced anal examinations. The archaic technique was devised in the 19th century to seek “evidence” of homosexual conduct,  but forensic experts around the world have condemned the practice as lacking any scientific validity and violating medical ethics. The UN  special rapporteur on torture,  the UN Committee Against Torture, and the African Commission on Human and Peoples’ Rights have described the exams as a form of torture or ill-treatment, prohibited under international law. The Egyptian Medical Syndicate has taken no steps to prevent doctors from conducting these degrading exams..Egypt is a state party to the International Covenant on Civil and Political Rights, which protects the rights to privacy and to freedom of expression. Egypt’s constitution also protects these rights..“Egypt should stop dedicating state resources to hunting people down for what they allegedly do in their bedrooms, or for expressing themselves at a rock concert, and should instead focus energy on improving its dire human rights record,” Whitson said.. Share this via Facebook    Share this via Twitter    Share this via WhatsApp    Share this via Email    Other ways to share    Share this via LinkedIn    Share this via Reddit    Share this via Telegram    Share this via Printer   \n"}
{"prompt":"Man arrested for using Facebook to find other men to date\n\n###\n\n","completion":" As British prime minister, David Cameron visits Saudi Arabia today, activists report the plight of a man arrested by the religious police who may face corporal punishment..Activists are concerned for the safety of a 30-year-old man arrested by the religious police in Saudi Arabia for using Facebook to date other men.  The man, whose exact identity is not known, was arrested on 23 December (2011) but full details of the incident are only now becoming clear after a detailed investigation by Gay Middle East. Experts warn he may face blackmail and\/or corporal punishment..He is being held in custody in the Dammam Police Department awaiting the Dammam’s General Attorney office for prosecution. The case has been reported to Amnesty International, while Facebook declined to comment..The report by Sabaq electronic journal mentions that a Saudi citizen reported an unnamed 30-year old man to the Religious police in Saudi Arabia, known as the Committee for the Propagation of Virtue and the Prevention of Vice, which proceeded to apprehend the man who finally confessed that “the Facebook profile is his and that he had been using it for obscenity acts with other men”. .KSA law is not strictly codified and its implementation, in either a lenient or severe manner, depends mostly on religious Sunni judges and scholars, as well as royal decrees (and thus subject to extreme variability).  Generally speaking punishments for homosexuality range from imprisonment and\/or flogging to the death penalty. Conviction and severity of punishments depends on the social class, religion and citizenship of the accused, whereby non-western migrant workers receive usually harsher treatment than upper class Saudi citizens..Sami Hamwi, Syria Editor of Gay Middle East, and former Saudi resident explains: “Native born Saudi citizens who are Suni or from the Bedouin tribes in the country are often let off, while punishment are severely executed against minorities like Shiites and or newly naturalised citizens. Punishments regarding homosexuality are also held against expatriates working in Saudi Arabia, especially those coming from Asian, African and Arab countries. Dammam is a largely Shiite area and if the 30 year old aforementioned man is a Shiite, he is likely to be trailed and sentenced harshly.”.“We are aware of the reports and seeking further information. The UK opposes all discrimination against Lesbian, Gay, Bisexual and Transgender (LGBT) people in all circumstances. We are committed to combating violence and discrimination against LGBT people as an integral part of our international human rights work. We believe that human rights are universal and that LGBT people should be free to enjoy the rights and freedoms to which people of all nations are entitled.”.A spokesperson for Amnesty International said :”Amnesty International is seeking more information on this case. If the man reported in the Sabq story has been arrested and charged with homosexuality, Amnesty International would consider him to be a prisoner of conscience and call for his immediate and unconditional release. Saudi Arabia has sentenced people convicted of homosexuality and ‘sodomy’ to a range of penalties including corporal punishment and even the death penalty. The criminalization of homosexuality encourages the dehumanization of lesbians, gay men, bisexual people and transgender people (LGBT) as their very identity is criminalized.”.“Amnesty International considers the use of ‘sodomy’ laws to imprison (usually) men for same-sex relations in private to be a grave violation of human rights, including the rights to privacy, to freedom from discrimination, to freedom of expression and association, which are protected in the Universal Declaration of Human Rights.”.The Lesbian & Gay Foundation also voiced concerns: “It is extremely worrying to hear that that the Saudi police have entrapped this man when we know that Saudi-Arabia is one of the remaining countries in the world where homosexual acts are punishable at worst Death, but also by severe corporal punishment and imprisonment.”.“We understand that because of the very nature of the country’s Draconian anti LGBT legislations there exists, by necessity, an underground gay scene, and if people are discovered to have fallen foul of official prohibitions they risk such entrapment, jail and flogging.”.“The Lesbian & Gay Foundation would like to see the UK government do whatever it can to make sure that LGBT issues across the region are seen as a significant human rights problem and we would urge all those concerned to put pressure on authorities such as The British Foreign and Commonwealth Office, the US State Department and others to be vocal in their condemnation of such acts which ignore the most basic of human rights.”.“I urge the Foreign Secretary William Hague, and the EU Foreign Minister Catherine Ashton, to make representations to the Saudi government to secure the release of this man. His detention violates all the norms of international human rights law. In the longer term, Britain and US must stop colluding with the Saudi royal dictatorship. Sanctions should be imposed against the regime until it ensures democracy and human rights for all its citizens.”.Gay Middle East sent and email to the Embassy of the Kingdom of Saudi Arabia in London which was read and ignored. To the knowledge of Gay Middle East, this is first known reported case of entrapment for homosexuality via facebook in the KSA.  Gay Middle East therefore thought that a user of any social networking site has a right for privacy and asked Facebook for their comments on the case and its possible ramifications.  Despite an email and a phone call, Facebook refused to comment on the issue.  .While this case may seem to Western readers as breaching the privacy rights, Saudi Arabia does not provide the right to privacy.  In fact the religious police encourage reporting of any “deviant” behaviour and deliberately entrap a person for homosexuality, for example a British male Nurse who was recently entrapped via fake SMS sent by the religious police. . Entrapment by the religious police does not necessarily lead to prosecution, but often results in life-long financial and\/or sexual black-mail.  Hamwi stated: “sexual blackmail and abuse by the Religious Police is unfortunately quite common.  When I lived in Medina, a neighbour who was a member of the religious police raped my neighbour’s son, a 12 year-old boy, at that time.  The same man entrapped and arrested a Pakistani national for homosexuality; the guy was whipped 80 times and before being deported. Such a sentence often applied when a sexual intercourse cannot be proven.” .If a person is outed by the religious police via a trail the consequences can be severe not only in terms of punishments, but lifelong ostracising by the family, the community and reduced or almost no job prospects.  “The person may simply become a social outcast,” adds Hamwi, “it is a kind of a social-death or in some cases may lead to persecution by the family until the person is killed to save the so-called ‘honour’ of the family.”.Furthermore private communication is also not subject to what ordinarily would be considered in the West as the right for privacy. All communications (including electronic) can be seized by the government for evidence in criminal trials; previously men have been arrested for homosexuality via paltalk (a social networking site popular in the Gulf), and gay-dating sites..Hamwi explained further: “the use of internet in Saudi Arabia is subject to monitoring, censorship and restrictions. Most online dating and social media website are blocked under the current Saudi laws. When trying to access banned or blocked websites users usually get screens stating “Sorry, the requested page is not available.” However, Saudis manage to override the Saudi proxy settings and access the websites they need.”.Ahmad, a 37-year-old Saudi engineer, mentioned that he is concerned with using online dating services and websites. “Anyone from the “Hay’ah”, (the religious police) can use those websites to entrap gay men. This is not common, but it happened before and I don’t want to be socially humiliated.” Ahmad affirmed that non-Saudis and Saudi Shiites are more likely to be subject to the legal Islamic penalties than the Sunni Saudis..Munir, a 29-year-old Syrian graphic designer working in KSA, said that the situation in Saudi Arabia is dangerous for gay men. “You see, when you are not Saudi, they can arrest you, put you in jail, lash you, and deport you. It is easier to be sexually deprived than having to face all the dangers coming from online dating.”.Fahad, a 42-year-old Saudi citizen, said that he rarely uses the online dating websites while in Saudi Arabia. “The situation here is complicated because of all the religious, social, and legal restrictions. Gay men in Saudi Arabia prefer not to have to struggle with the laws, since the media can easily raise a social anger when they expose their cases. This happens a lot.”\n"}
{"prompt":"Iran utilizes various tools from messages in private chat rooms to target gays\n\n###\n\n","completion":" Reader donations, many as small as just $5, are what fund the work of writers like this—and keep our content free and accessible to everyone. If you support this work, will you chip in to help fund it? It only takes a minute to donate. Click here to make a tax-deductible donation..The Islamic Republic of Iran – under the new government of President Mahmoud Ahmadinejad – is engaged in a major anti-homosexual pogrom targeting gays and gay sex. This campaign includes Internet entrapment, blackmail to force arrested gays to inform on others, torture and executions of those found guilty of engaging in ​“homosexual acts.”.Homosexual acts have been considered a capital crime in Iran since the 1979 revolution that brought the Ayatollah Khomeini to power. Iranians found guilty of gay lovemaking are given a choice of four death styles: being hanged, stoned, halved by a sword or dropped from the highest perch. According to Article 152 of Iran’s penal law, if two men not related by blood are found naked under one cover without good reason, both will be punished at a judge’s discretion..Iran’s crackdown on gays drew worldwide protests (except in the United States) after the hanging for ​“homosexual acts” of two teenagers – one 18, the other believed to be 16 or 17 – on July 19 in the city of Mashad. Charges against the two teens included the alleged rape of another youth. But three independent gay sources inside Mashad told Afdhere Jama, editor of Huriyah (an Internet zine for gay Muslims), that the teens were well known in the city’s underground gay community as lovers who lived together, and that the rape charge was fabricated. The editors of an underground Persian-language zine in Iran (who requested anonymity out of fear) also confirm that their own Mashad sources said that the rape charge was trumped up – a view now generally accepted. In any case, the hangings were illegal under international law because Iran is a signatory to two treaties that forbid executing minors. Since then, there have been reports of at least a dozen more gay victims who have been executed..“Under Islamic law, which has been adopted by Iran’s legal system, it takes four witnesses to prove an act of homosexuality, which is a capital crime. That’s why it’s much easier for the Islamic government to invent other criminal charges against gay people to get rid of them,” Jama told me. The Iranian gay zine’s editors said the same, urging Westerners to be ​“very careful” before accepting such criminal charges at face value, as they are ​“most likely false.”.Amir is a 22-year-old gay Iranian who was arrested by Iran’s religious morality police as part of a massive Internet entrapment campaign targeting gays. He escaped from Iran in August, and is now in Turkey seeking asylum in a gay-friendly country. Through a Persian translator, Amir gave me a terrifying, firsthand account of the anti-gay crackdown..Amir’s first arrest for being gay came when police raided a private party. ​“The judge told me, ​‘If we send you to a physician who vouches that your rectum has been penetrated in any way, you will be sentenced to death’,” says Amir. He was fined and released for lack of proof that a sexual act had taken place..Later, an unrepentant Amir set up a meeting with a man he met through a Yahoo gay chat room. When his date turned out to be a member of the sex police, Amir was arrested and taken to Intelligence Ministry headquarters, ​“a very scary place,” he says. ​“There I denied that I was gay – but they showed me a printout from the chat room of my messages and my pictures.”.Then, says Amir, the torture began. ​“There was a metal chair in the middle of the room – they put a gas flame under the chair and made me sit on it as the metal seat got hotter and hotter. They threatened to send me to an army barracks where all the soldiers were going to rape me. The leader told one of the other officers to take [a soft drink] bottle and shove it up my ass, screaming, ​‘This will teach you not to want any more cock!’ I was so afraid of sitting in that metal chair as it got hotter and hotter that I confessed. Then they brought out my file and told me that I was a ​‘famous faggot’ in Shiraz. They beat me up so badly that I passed out and was thrown, unconscious, into a holding cell..“When I came to, I saw there were several dozen other gay guys in the cell with me. One of them told me that after they had taken him in, they beat him and forced him to set up dates with people through chat rooms – and each one of those people had been arrested; those were the other people in that cell with me.”.Eventually tried, Amir was sentenced to 100 lashes. ​“I passed out before the 100 lashes were over. When I woke up, my arms and legs were so numb that I fell over when they picked me up from the platform on which I’d been lashed. They had told me that if I screamed, they would beat me even harder – so I was biting my arms so hard, to keep from screaming, that I left deep teeth wounds in my own arms.”.After this entrapment and public flogging, Amir’s life became unbearable. He was rousted regularly at his home by the basiji (a para-police made up of thugs recruited from the criminal classes and the lumpen unemployed) and by agents of the Office for Promotion of Virtue and Prohibition of Vice, which represses ​“moral deviance” – things like boys and girls walking around holding hands, women not wearing proper Islamic dress and prostitution. Says Amir, ​“In one of these arrests, Colonel Javanmardi told me that if they catch me again that I would be put to death, ​‘just like the boys in Mashad.’ He said it just like that, very simply, very explicitly. He didn’t mince words. We all know that the boys who were hanged in Mashad were gay – the rape charges against them were trumped up, just like the charges of theft and kidnapping against them. When you get arrested, you are forced by beatings, torture and threats to confess to crimes you didn’t commit. It happens all the time, and has to friends of mine.”.Amir’s experience is typical – as is the lack of concern evidenced by U.S. LGBT oganizations. Both of the principal U.S. gay rights organizations – Human Rights Campaign and the National Gay and Lesbian Task Force – have failed to incorporate international solidarity with persecuted gays into their fundraising-driven agendas, and neither have mobilized public protests against Iran’s anti-gay pogrom. Their European counterparts, in contrast, organized multiple demonstrations at Iranian embassies across the Continent..The Persian Gay and Lesbian Organization (PGLO) is the principal group for Iranian gays, claiming 29,000 on its e‑mail list. The PGLO – which publishes a monthly Internet magazine in Persian, hosts radio netcasts into Iran, and has secretariats in Turkey and Norway – has appealed to Western gays to mobilize international protests against the inhumane tragedy that has befallen Iranian same-sexers. To find out how to help, visit www​.pglo​.org..Reader donations, many as small as just $5, have kept In These Times publishing for 45 years. When you contribute, youre not just giving a gift—youre helping publish the next big In These Times story.\n"}
{"prompt":"Wrong\n\n###\n\n","completion":" (Reuters) - Facebook Inc FB.O raised its settlement offer by $100 million to $650 million related to a lawsuit that claimed it illegally collected and stored biometric data for millions of users without their consent, the company said on Friday..The social media giant reached a $550 million settlement in January regarding the same lawsuit, which started in 2015, when Illinois users accused the company of violating the state’s Biometric Information Privacy Act in collecting biometric data.\n"}
{"prompt":"Biometric data was harvested or stored without explicit permission\n\n###\n\n","completion":" In a set of new lawsuits, two Illinois residents argue that three tech giants violated state laws prohibiting the use of personal biometric data without permission. Illinois residents Steven Vance and Tim Janecyk allege that images of their faces appeared in IBM’s “Diversity in Faces” database without their consent and were used to train facial recognition systems at Amazon, Microsoft and Google’s parent company Alphabet..While all three companies are based on the West Coast, the suit accuses the tech giants of running afoul of an Illinois law known as the Biometric Information Privacy Act (BIPA). The suit names Vance and Janecyk as plaintiffs but also seeks class action status on behalf of “all other similarly situated individuals” in Illinois. In the lawsuit, the pair of plaintiffs seek $5,000 per violation of the law, an injunction barring the companies from using Illinois residents’ “biometric identifiers” and the destruction of any relevant facial data that’s been stored..“In its effort to improve its facial recognition technology, Defendant Microsoft violated Illinois’ Biometric Information Privacy Act… by, among other things, unlawfully collecting, obtaining, storing, using, possessing and profiting from the biometric identifiers and information of Plaintiffs Vance and Janecyk and all other similarly situated Illinois residents and citizens (hereinafter, the “Class Members”),” the version of the suit against Microsoft states..The law cited in the suit, passed more than a decade ago, is designed to protect Illinois residents from having their biometric data harvested or stored without their explicit permission. Lawsuits involving BIPA pop up with some frequency now, as facial recognition becomes both more commonplace and more controversial. In the absence of federal privacy protections in the U.S., the Illinois law poses an interesting hurdle for companies that are used to extracting data from Americans with little oversight..In January of this year, Facebook paid $550 million to settle a class action lawsuit stemming from BIPA. The suit was filed on behalf of Illinois residents in 2015 and alleged that the social media giant collected facial recognition data from user images without disclosing it to users. At the time, Snapchat, Google, and Shutterfly faced similar suits..In 2019, a U.S. Circuit Court of Appeals court swatted away Facebook’s claim that facial recognition data did not count as biometric data, stating that “development of face template using facial-recognition technology without consent (as alleged here) invades an individual’s private affairs and concrete interests.”.The IBM dataset the companies trained facial recognition systems on also poses its own controversies. As NBC News reported last year, IBM claimed that its Diversity in Faces dataset was designed “purely for academic research” and not for the company’s own commercial interests. The IBM dataset was apparently culled from more than 100 million Creative Commons-licensed Flickr images, a decision that raised its own ethical questions around the use of facial imagery and if corporations should be allowed leverage images with open licensing for facial recognition applications without the consent of photographers and the people they photograph.\n"}
{"prompt":"Banks must work towards resolving risks with data bias, \"black box\" risk and a lack of human oversight\n\n###\n\n","completion":" \r             Data bias, “black box” risk, and lack of human oversight are the main governance issues for banks using AI, according to the Economist Intelligence Unit (EIU) report “Overseeing AI: Governing artificial intelligence in banking”.\r         .The report is based on a review of global regulatory guidance on AI risks and governance in banking carried out by the EIU on behalf of Temenos (SIX: TEMN), the banking software company..The report trends will be discussed on the webinar “Rules of the game changer – governing AI in banking” on 23 July, with Azfar Karimuddin, Janet Adams and Hani Hagras..The report highlights that AI is a top priority for technology investment for banks and reveals that 77% of banking executives believe that AI will separate winning from losing banks. AI is expected to retain its importance after the pandemic as banks look to new technologies to help them adapt to changing customer needs and compete with new market entrants. The EIU report reveals that ensuring ethical, fair and well-documented AI-based decisions will be vital for banks deploying AI technology..The EIU report highlights key governance challenges and distils regulatory guidance for banks using AI, including:\r ●\tEthics and fairness: banks must develop AI models that are ‘ethical by design’. AI use cases and decisions should be monitored and reviewed and data sources regularly evaluated to ensure that data remains representative. \r ●\tExplainability and traceability: steps taken to develop AI models must be documented in order to fully explain AI-based decisions to the individuals they impact. \r ●\tData quality: bank-wide data governance standards must be established and applied to ensure data accuracy and integrity and avoid bias. \r ●\tSkills: banks must ensure the right level of AI expertise across the business in order to build and maintain AI models, as well as oversee these models. \r Prema Varadhan, Chief Product Architect and Head of AI, Temenos, commented: AI is changing the face of the banking industry. It gives banks the ability to process more data in real time, and learn from customer behaviors, helping them to bring operating costs down and hyper-personalize their services. Banks are using AI to transform their customer experiences and back-office operations so ensuring that the technology is deployed ethically is more important than ever. “White box” models, like Temenos’ Explainable AI (XAI), can explain in simple human language how decisions are made and win the trust of regulators and customers alike. As the custodians of customer data and trusted advisors, banks have a responsibility to adopt transparent, explainable AI technology – those that do stand to gain the competitive advantage in the new normal.”.The EIU review cites data bias that leads to discrimination against individuals or groups of people as among the most prominent risks for banks using AI. Commenting in the EIU review, Prag Sharma, Senior Vice President, Citi Innovation Labs, said: “Bias can creep into AI models in any industry, but banks are better positioned than most types of organizations to combat it. Maximizing algorithms’ explainability helps to reduce bias.”.Pete Swabey, Editorial Director EMEA – Thought Leadership, The Economist Intelligence Unit, said: “AI is seen as a key competitive differentiator in the sector. Our new study, drawing on the guidance given by regulators around the world, highlights the key governance challenges banks must address if they are to capitalise on the AI opportunity safely and ethically.”.The EIU conducted a structured review of 25 reports, discussion papers and articles, and summarized the main issues raised by regulators on the topic of managing AI risks in banking. These documents were published in the last three years by banking and financial sector supervisory authorities, central banks and supranational institutions, universities and consultancies..The AI regulatory review follows a global research survey released by the EIU and Temenos, entitled “Forging new frontiers: advanced technologies will revolutionize banking,” released in June 2020.  \n"}
{"prompt":"Coloniality manifests in AI through algorithmic discrimination and oppression, ghost work, beta testing, AI governance, and international social development.\n\n###\n\n","completion":" In March of 2015, protests broke out at the University of Cape Town in South Africa over the campus statue of British colonialist Cecil Rhodes. Rhodes, a mining magnate who had gifted the land on which the university was built, had committed genocide against Africans and laid the foundations for apartheid. Under the rallying banner of “Rhodes Must Fall,” students demanded that the statue be removed. Their protests sparked a global movement to eradicate the colonial legacies that endure in education..The events also provoked Shakir Mohamed, a South African AI researcher at DeepMind, to reflect on what colonial legacies might exist in his research as well. In 2018, just as the AI field was beginning to reckon with problems like algorithmic discrimination, Mohamed penned a blog post with his initial thoughts. In it he called on researchers to “decolonise artificial intelligence”—to reorient the field’s work away from Western hubs like Silicon Valley and engage new voices, cultures, and ideas for guiding the technology’s development..Now in the wake of renewed cries for “Rhodes Must Fall” on Oxford University’s campus, spurred by George Floyd’s murder and the global antiracism movement, Mohamed has released a new paper along with his colleague William Isaac and Oxford PhD candidate Marie-Therese Png. It fleshes out Mohamed’s original ideas with specific examples of how AI challenges are rooted in colonialism, and presents strategies for addressing them by recognizing that history..Though historical colonialism may be over, its effects still exist today. This is what scholars term “coloniality”: the idea that the modern-day power imbalances between races, countries, rich and poor, and other groups are extensions of the power imbalances between colonizer and colonized..Take structural racism as an example. Europeans originally invented the concept of races and the differences between them to justify the African slave trade and then the colonization of African countries. In the US, the effects of that ideology can now be traced through the country’s own history of slavery, Jim Crow, and police brutality..In the same way, the paper’s authors argue, this colonial history explains some of the most troubling characteristics and impacts of AI. They identify five manifestations of coloniality in the field:.Algorithmic discrimination and oppression. The ties between algorithmic discrimination and colonial racism are perhaps the most obvious: algorithms built to automate procedures and trained on data within a racially unjust society end up replicating those racist outcomes in their results. But much of the scholarship on this type of harm from AI focuses on examples in the US. Examining it in the context of coloniality allows for a global perspective: America isn’t the only place with social inequities. “There are always groups that are identified and subjected,” Isaac says..Ghost work. The phenomenon of ghost work, the invisible data labor required to support AI innovation, neatly extends the historical economic relationship between colonizer and colonized. Many former US and UK colonies—the Philippines, Kenya, and India—have become ghost-working hubs for US and UK companies. The countries’ cheap, English-speaking labor forces, which make them a natural fit for data work, exist because of their colonial histories..Beta testing. AI systems are sometimes tried out on more vulnerable groups before being implemented for “real” users. Cambridge Analytica, for example, beta-tested its algorithms on the 2015 Nigerian and 2017 Kenyan elections before using them in the US and UK. Studies later found that these experiments actively disrupted the Kenyan election process and eroded social cohesion. This kind of testing echoes the British Empire’s historical treatment of its colonies as laboratories for new medicines and technologies..AI governance. The geopolitical power imbalances that the colonial era left behind also actively shape AI governance. This has played out in the recent rush to form global AI ethics guidelines: developing countries in Africa, Latin America, and Central Asia have been largely left out of the discussions, which has led some to refuse to participate in international data flow agreements. The result: developed countries continue to disproportionately benefit from global norms shaped for their advantage, while developing countries continue to fall further behind..International social development. Finally, the same geopolitical power imbalances affect the way AI is used to assist developing countries. “AI for good” or “AI for sustainable development” initiatives are often paternalistic. They force developing countries to depend on existing AI systems rather than participate in creating new ones designed for their own context..The researchers note that these examples are not comprehensive, but they demonstrate how far-reaching colonial legacies are in global AI development. They also tie together what seem like disparate problems under one unifying thesis. “It enables us a new grammar and vocabulary to talk about both why these issues matter and what we are going to do to think about and address these issues over the long run,” Isaac says..The benefit of examining harmful impacts of AI through this lens, the researchers argue, is the framework it provides for predicting and mitigating future harm. Png believes that there’s really no such thing as “unintended consequences”—just consequences of the blind spots organizations and research institutions have when they lack diverse representation..Context-aware technical development. First, AI researchers building a new system should consider where and how it will be used. Their work also shouldn’t end with writing the code but should include testing it, supporting policies that facilitate its proper uses, and organizing action against improper ones..Reverse tutelage. Second, they should listen to marginalized groups. One example of how to do this is the budding practice of participatory machine learning, which seeks to involve the people most affected by machine-learning systems in their design. This gives subjects a chance to challenge and dictate how machine-learning problems are framed, what data is collected and how, and where the final models are used..Solidarity. Marginalized groups should also be given the support and resources to initiate their own AI work. Several communities of marginalized AI practitioners already exist, including Deep Learning Indaba, Black in AI, and Queer in AI, and their work should be amplified..Since publishing their paper, the researchers say, they have seen overwhelming interest and enthusiasm. “It at least signals to me that there is a receptivity to this work,” Isaac says. “It feels like this is a conversation that the community wants to begin to engage with.” .An investigation by MIT Technology Review reveals a sprawling, technologically sophisticated system in Minnesota designed for closely monitoring protesters..As firms have dumped their AI technologies into the country, it’s created a blueprint for how to surveil citizens and serves as a warning to the world..Intrepid Response is a little-known but powerful app that lets police quickly upload and share information across agencies. But what happens to the information it collects?\n"}
{"prompt":"Beta Testing algorithms in 2017 Kenyan election disrupted the Kenyan election process.\n\n###\n\n","completion":" This survey of the growing role of social media in Kenyan society and politics does not offer a straightforward answer to the implicit question in its subtitle. Nonetheless, it develops some keen insights into the effects of the Internet in Kenya. With more than seven million of its citizens on Facebook and over a million on Twitter, Kenya may well be sub-Saharan Africa’s most online country. Nyabola describes a sophisticated community of users who have found agency through the Internet, whether in criticizing CNN for what they see as its Eurocentric coverage or in publicizing corruption and incompetence by Kenyan officials. The Kenyan government, Nyabola reveals, is deeply ambivalent about the Internet, attracted to it as a symbol of modernity but wary of the hard-to-control political spaces it creates. Nyabola’s conclusions are far from optimistic. She documents how the Internet allowed foreign actors, such as the British political consulting firm Cambridge Analytica, to manipulate voters during the 2017 Kenyan elections and explores how social media may come to undermine Kenyan democracy.\n"}
{"prompt":"Twitter will warn users about deepfakes, but won't remove it\n\n###\n\n","completion":" The news: Twitter has drafted a deepfake policy that would warn users about synthetic or manipulated media, but not remove it. Specifically, it says it would place a notice next to tweets that contain deepfakes, warn people before they share or like tweets that include deepfakes, or add a link to a news story or Twitter Moment explaining that it isn’t real. Twitter has said it may remove deepfakes that could threaten someone’s physical safety or lead to serious harm. People have until November 27 to give Twitter feedback on the proposals..The context: It’s become relatively easy to make convincing doctored videos thanks to advances in artificial intelligence. That’s led to a huge panic over the potential for deepfakes to subvert democracy, as they can be used to make politicians seem to say or do whatever the creator wants..A real threat?:The most notorious political deepfakes so far either have not been deepfakes (see the Nancy Pelosi video released in May) or have been created by people warning about deepfakes, rather than any bad actors themselves. For example, in the UK today two new deepfakes have been released of the prime minister, Boris Johnson, and leader of the opposition, Jeremy Corbyn, endorsing each other for an upcoming election on December 12. But they were created by a social enterprise trying to raise awareness of the issue..The real problem: There is no denying that deepfakes pose a significant new threat. But so far, they’re mostly a threat to women, particularly famous actors and musicians. A recent report found that 96% of deepfakes are porn, virtually always created without the consent of the person depicted. These would already break Twitter’s existing rules and be removed..An issue for the whole industry: That said, it is refreshing to see a social-media company wrangling with its content moderation responsibilities so openly. The varying responses to the Pelosi video (YouTube removed it, Facebook flagged it as false, and Twitter let it stand) show what a complex, thorny problem manipulated videos can pose. And unfortunately, we can’t expect deepfake detection technology to fix it, either. We’ll need social and legal solutions,too..An investigation by MIT Technology Review reveals a sprawling, technologically sophisticated system in Minnesota designed for closely monitoring protesters..As firms have dumped their AI technologies into the country, it’s created a blueprint for how to surveil citizens and serves as a warning to the world..Intrepid Response is a little-known but powerful app that lets police quickly upload and share information across agencies. But what happens to the information it collects?\n"}
{"prompt":"From a distance, the Pentagon can identify people by their cardiac signature\n\n###\n\n","completion":" Everyone’s heart is different. Like the iris or fingerprint, our unique cardiac signature can be used as a way to tell us apart. Crucially, it can be done from a distance..It’s that last point that has intrigued US Special Forces. Other long-range biometric techniques include gait analysis, which identifies someone by the way he or she walks. This method was supposedly used to identify an infamous ISIS terrorist before a drone strike. But gaits, like faces, are not necessarily distinctive. An individual’s cardiac signature is unique, though, and unlike faces or gait, it remains constant and cannot be altered or disguised..A new device, developed for the Pentagon after US Special Forces requested it, can identify people without seeing their face: instead it detects their unique cardiac signature with an infrared laser. While it works at 200 meters (219 yards), longer distances could be possible with a better laser. “I don’t want to say you could do it from space,” says Steward Remaly, of the Pentagon’s Combatting Terrorism Technical Support Office, “but longer ranges should be possible.”.Contact infrared sensors are often used to automatically record a patient’s pulse. They work by detecting the changes in reflection of infrared light caused by blood flow. By contrast, the new device, called Jetson, uses a technique known as laser vibrometry to detect the surface movement caused by the heartbeat. This works though typical clothing like a shirt and a jacket (though not thicker clothing such as a winter coat)..The most common way of carrying out remote biometric identification is by face recognition. But this needs good, frontal view of the face, which can be hard to obtain, especially from a drone. Face recognition may also be confused by beards, sunglasses, or headscarves..Cardiac signatures are already used for security identification. The Canadian company Nymi has developed a wrist-worn pulse sensor as an alternative to fingerprint identification. The technology has been trialed by the Halifax building society in the UK..Jetson extends this approach by adapting an off-the shelf device that is usually used to check vibration from a distance in structures such as wind turbines. For Jetson, a special gimbal was added so that an invisible, quarter-size laser spot could be kept on a target. It takes about 30 seconds to get a good return, so at present the device is only effective where the subject is sitting or standing..Remaly’s team then developed algorithms capable of extracting a cardiac signature from the laser signals. He claims that Jetson can achieve over 95% accuracy under good conditions, and this might be further improved. In practice, it’s likely that Jetson would be used alongside facial recognition or other identification methods..Wenyao Xu of the State University of New York at Buffalo has also developed a remote cardiac sensor, although it works only up to 20 meters away and uses radar. He believes the cardiac approach is far more robust than facial recognition. “Compared with face, cardiac biometrics are more stable and can reach more than 98% accuracy,” he says..One glaring limitation is the need for a database of cardiac signatures, but even without this the system has its uses. For example, an insurgent seen in a group planting an IED could later be positively identified from a cardiac signature, even if the person’s name and face are unknown. Biometric data is also routinely collected by US armed forces in Iraq and Afghanistan, so cardiac data could be added to that library..In the longer run, this technology could find many more uses, its developers believe. For example, a doctor could scan for arrythmias and other conditions remotely, or hospitals could monitor the condition of patients without having to wire them up to machines. .An investigation by MIT Technology Review reveals a sprawling, technologically sophisticated system in Minnesota designed for closely monitoring protesters..As firms have dumped their AI technologies into the country, it’s created a blueprint for how to surveil citizens and serves as a warning to the world..Intrepid Response is a little-known but powerful app that lets police quickly upload and share information across agencies. But what happens to the information it collects?\n"}
{"prompt":"Researchers have found a way to picture a face given an audio clip\n\n###\n\n","completion":" Voice profiling aims at inferring various human parameters from their speech, e.g. gender, age, etc. In this paper, we address the challenge posed by a subtask of voice profiling - reconstructing someones face from their voice. The task is designed to answer the question: given an audio clip spoken by an unseen person, can we picture a face that has as many common elements, or associations as possible with the speaker, in terms of identity?.Requests for name changes in the electronic proceedings will be accepted with no questions asked.  However name changes may cause bibliographic tracking issues.  Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings.\n"}
{"prompt":"Deep Gestalt can potentially analyze facial images and discriminate with pre-existing conditions\n\n###\n\n","completion":" (CNN)A new artificial intelligence technology can accurately identify some rare genetic disorders using a photograph of a patients face, according to a new study. \n"}
{"prompt":"Genderify suggests gender to someone's name and has errors with Dr.\n\n###\n\n","completion":" Genderify, a new service that promised to identify someone’s gender by analyzing their name, email address, or username with the help AI, looks firmly to be in the latter camp. The company launched on Product Hunt last week, but picked up a lot of attention on social media as users discovered biases and inaccuracies in its algorithms..Although these sorts of biases appear regularly in machine learning systems, the thoughtlessness of Genderify seems to have surprised many experts in the field. The response from Meredith Whittaker, co-founder of the AI Now Institute, which studies the impact of AI on society, was somewhat typical. “Are we being trolled?” she asked. “Is this a psyop meant to distract the tech+justice world? Is it cringey tech April fool’s day already?”.The problem is not that Genderify made assumptions about someone’s gender based on their name. People do this all the time, and sometimes make mistakes in the process. That’s why it’s polite to find out how people self-identify and how they want to be addressed. The problem with Genderify is that it automated these assumptions; applying them at scale while sorting individuals into a male\/female binary (and so ignoring individuals who identify as non-binary) while reinforcing gender stereotypes in the process (such as: if you’re a doctor you’re probably a man). .The potential harm of this depends on how and where Genderify was applied. If the service was integrated into a medical chatbot, for example, its assumptions about users’ genders might have led to the chatbot issuing misleading medical advice. .Thankfully, Genderify didn’t seem to be aiming to automate this sort of system, but was primarily designed to be a marketing tool. As Genderify’s creator, Arevik Gasparyan, said on Product Hunt: “Genderify can obtain data that will help you with analytics, enhancing your customer data, segmenting your marketing database, demographic statistics, etc.”.In the same comment section, Gasparyan acknowledged the concerns of some users about bias and ignoring non-binary individuals, but didn’t offer any concrete answers. .One user asked: “Let’s say I choose to identify as neither Male or Female, how do you approach this? How do you avoid gender discrimination? How are you tackling gender bias?” To which Gasparyan replied that the service makes its decisions based on “already existing binary name\/gender databases,” and that the company was “actively looking into ways of improving the experience for transgender and non-binary visitors” by “separating the concepts of name\/username\/email from gender identity.” It’s a confusing answer given that the entire premise of Genderify is that this data is a reliable proxy for gender identity..The company told The Verge that the service was very similar to existing companies who use databases of names to guess an individual’s gender, though none of them use AI. .“We understand that our model will never provide ideal results, and the algorithm needs significant improvements, but our goal was to build a self-learning AI that will not be biased as any existing solutions,” said a representative via email. “And to make it work, we very much relied on the feedback of transgender and non-binary visitors to help us improve our gender detection algorithms as best as possible for the LGBTQ+ community.”.Update Wednesday July 29, 12:42PM ET: Story has been updated to confirm that Genderify has been shut down and to add additional comment from a representative of the firm. \n"}
{"prompt":"A neural network that analyzes facial features and classifies whether a person is a criminal or not\n\n###\n\n","completion":" Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n"}
{"prompt":"Over 14 million Twitter posts mention fake news claims\n\n###\n\n","completion":" Fake news and the way it spreads on social media is emerging as one of the great threats to modern society. In recent times, fake news has been used to manipulate stock markets, make people choose dangerous health-care options, and manipulate elections, including last year’s presidential election in the U.S..Clearly, there is an urgent need for a way to limit the diffusion of fake news. And that raises an important question: how does fake news spread in the first place?.Today we get an answer of sorts thanks to the work of Chengcheng Shao and pals at Indiana University in Bloomington. For the first time, these guys have systematically studied how fake news spreads on Twitter and provide a unique window into this murky world. Their work suggests clear strategies for controlling this epidemic..Shao and co then monitored some 400,000 claims made by these websites and studied the way they spread through Twitter. They did this by collecting some 14 million Twitter posts that mentioned these claims..At the same time, the team monitored some 15,000 stories written by fact-checking organizations and over a million Twitter posts that mention them..Next, Shao and co looked at the Twitter accounts that spread this news, collecting up to 200 of each account’s most recent tweets. In this way, the team could study the tweeting behavior and work out whether the accounts were most likely run by humans or by bots.  .Having made a judgment on the ownership of each account, the team finally looked at the way humans and bots spread fake news and fact-checked news..To do all this, the team developed two online platforms. The first, called Hoaxy, tracks fake news claims, and the second, Bolometer, works out whether a Twitter count is most likely run by a human or a bot..The results of this work make for interesting reading. “Accounts that actively spread misinformation are significantly more likely to be bots,” say Shao and co. “Social bots play a key role in the spread of fake news.”.Shad and co say bots play a particularly significant role in the spread of fake news soon after it is published. What’s more, these bots are programmed to direct their tweets at influential users. “Automated accounts are particularly active in the early spreading phases of viral claims, and tend to target influential users,” say Shao and co..That’s a clever strategy. Information is much more likely to become viral when it passes through highly connected nodes on a social network. So targeting these influential users is key. Humans can easily be fooled by automated accounts and can unwittingly seed the spread of fake news (some humans do this wittingly, of course)..“These results suggest that curbing social bots may be an effective strategy for mitigating the spread of online misinformation,” say Shao and co..One way would be to outlaw certain kinds of social bots. But this is a route fraught with difficulty. There are many social bots that perform important roles in the spread of legitimate information..And legislation does not overcome international borders. Given the way foreign powers have manipulated the spread of fake news, it’s hard to see how this would work..Nevertheless, the spread of fake news is a legitimate and important source of public concern. Understanding how it spreads is the first stage in tackling it..An investigation by MIT Technology Review reveals a sprawling, technologically sophisticated system in Minnesota designed for closely monitoring protesters..As firms have dumped their AI technologies into the country, it’s created a blueprint for how to surveil citizens and serves as a warning to the world..Intrepid Response is a little-known but powerful app that lets police quickly upload and share information across agencies. But what happens to the information it collects?\n"}
{"prompt":"iBorderCtrl is being pursued despite a lack of transparency and racial bias\n\n###\n\n","completion":" They call it the Silent Talker. It is a virtual policeman designed to strengthen Europe’s borders, subjecting travelers to a lie detector test before they are allowed to pass through customs..Prior to your arrival at the airport, using your own computer, you log on to a website, upload an image of your passport, and are greeted by an avatar of a brown-haired man wearing a navy blue uniform..“What is your surname?” he asks. “What is your citizenship and the purpose of your trip?” You provide your answers verbally to those and other questions, and the virtual policeman uses your webcam to scan your face and eye movements for signs of lying..At the end of the interview, the system provides you with a QR code that you have to show to a guard when you arrive at the border. The guard scans the code using a handheld tablet device, takes your fingerprints, and reviews the facial image captured by the avatar to check if it corresponds with your passport. The guard’s tablet displays a score out of 100, telling him whether the machine has judged you to be truthful or not..A person judged to have tried to deceive the system is categorized as “high risk” or “medium risk,” dependent on the number of questions they are found to have falsely answered. Our reporter — the first journalist to test the system before crossing the Serbian-Hungarian border earlier this year — provided honest responses to all questions but was deemed to be a liar by the machine, with four false answers out of 16 and a score of 48. The Hungarian policeman who assessed our reporter’s lie detector results said the system suggested that she should be subject to further checks, though these were not carried out..Travelers who are deemed dangerous can be denied entry, though in most cases they would never know if the avatar test had contributed to such a decision. The results of the test are not usually disclosed to the traveler; The Intercept obtained a copy of our reporter’s test only after filing a data access request under European privacy laws..IBorderCtrl’s lie detection system was developed in England by researchers at Manchester Metropolitan University, who say that the technology can pick up on “micro gestures” a person makes while answering questions on their computer, analyzing their facial expressions, gaze, and posture..An EU research program has pumped some 4.5 million euros into the project, which is being managed by a consortium of 13 partners, including Greece’s Center for Security Studies, Germany’s Leibniz University Hannover, and technology and security companies like Hungary’s BioSec, Spain’s Everis, and Poland’s JAS..The researchers at Manchester Metropolitan University believe that the system could represent the future of border security. In an academic paper published in June 2018, they stated that avatars like their virtual policeman “will be suitable for detecting deception in border crossing interviews, as they are effective extractors of information from humans.”.However, some academics are questioning the value of the system, which they say relies on pseudoscience to make its decisions about travelers’ honesty..Ray Bull, professor of criminal investigation at the University of Derby, has assisted British police with interview techniques and specializes in methods of detecting deception. He told The Intercept that the iBorderCtrl project was “not credible” because there is no evidence that monitoring microgestures on people’s faces is an accurate way to measure lying..“They are deceiving themselves into thinking it will ever be substantially effective and they are wasting a lot of money,” said Bull. “The technology is based on a fundamental misunderstanding of what humans do when being truthful and deceptive.”.In recent years, following the refugee crisis and a spate of terrorist attacks in France, Belgium, Spain, and Germany, police and security agencies in Europe have come under increasing political pressure to more effectively track the movements of migrants. Border security officials on the continent say they are trying to find faster and more efficient new ways, using artificial intelligence, to check the travel documents and biometrics of the more than 700 million people who annually enter the EU..The European Commission — the EU’s executive branch — has set aside a proposed €34.9 billion for border control and migration management between 2021 and 2027. Meanwhile, in September last year, European lawmakers agreed to establish a new automated system that will screen nationals from visa-free third countries — including the United States — to establish whether or not they should be allowed to enter the EU..In the future, a visa-free traveler who, for whatever reason, has not been able to submit an application in advance will not be granted entry into the Schengen zone, an area covering 26 countries in Europe where travelers can move freely across borders without any passport checks..IBorderCtrl is one technology designed to strengthen the prescreening process. But transparency activists say that the project should not be rolled out until more information is made available about the technology — such as the algorithms it uses to make its decisions..Earlier this year, researchers at the Milan-based Hermes Center for Transparency and Digital Human Rights used freedom of information laws to obtain internal documents about the system. They received hundreds of pages; however, they were heavily redacted, with many pages completely blacked out..“The attempt to suppress debate by withholding the documents that address these issues is really frightening,” said Riccardo Coluccini, a researcher at the Hermes Center. “It is absolutely necessary to understand the reasoning behind the funding process. What is written in those documents? How does the consortium justify the use of such a pseudoscientific technology?”.A study produced by the researchers in Manchester tested iBorderCtrl on 32 people and said that their results showed the system had 75 percent accuracy. The researchers noted, however, that their participant group was unbalanced in terms of ethnicity and gender, as there were fewer Asian or Arabic participants than white Europeans, and fewer women than men..Giovanni Buttarelli, head of the EU’s data protection watchdog, told The Intercept that he was concerned that the iBorderCtrl system might discriminate against people on the basis of their ethnic origin..“Are we only evaluating possible lies about identity or we are also trying to analyze some of the person’s somatic traits, the edges of the face, the color of the skin, the cut of the eyes?” Buttarelli said. “Who sets the parameters to establish that a certain subject is lying or not lying?”.A spokesperson for iBorderCtrl declined to answer questions for this story. A website for the project acknowledges that the lie detection system will “have an impact on the fundamental rights of travellers” but says that, because the test is currently voluntary, “issues with respect to discrimination, human dignity, etc. therefore cannot occur.”\n"}
{"prompt":"A facial recognition system uses facial features to classify whether someone is a terrorist, pedophile etc.\n\n###\n\n","completion":" Most security news is about insecurity, hacking and cyber threats, bordering on scary. But when security is done right, its a beautiful thing...sexy even. Security IS sexy..An unnamed homeland security agency has signed a contract with a company that claims it can “reveal” your personality “with a high level of accuracy” just by analyzing your face, be that facial image captured via photo, live-streamed video, or stored in a database. It then sorts people into categories; with some labels as potentially dangerous such as terrorist or pedophile, it is disturbing that some experts believe the science behind it is antiquated, has previously been discredited, and the results are inaccurate..Israeli start-up Faception, a facial personality profiling company, told The Washington Post that “a homeland security agency” has signed a contract to use Faception to help spot terrorists. The “computer vision and machine learning technology” can even be integrated into other facial recognition tech “to provide a full spectrum solution that covers known and anonymous individuals.”.Faceception CEO Shai Gilboa added, “Our personality is determined by our DNA and reflected in our face. It’s a kind of signal.” On the company’s site, the “science” behind the technology that can supposedly predict a person’s behavior and personality was described as:.People may judge other people by their faces, but the “science” of judging a book by its cover via face reading, or physiognomy, was basically “discredited and rejected” by the late 19th century. It’s one thing for a person to make a snap judgement based on appearance and another thing entirely to use Faception to “enrich your profile database with a variety of personality scores” and “turn unknown individuals into known ones.”.As Pedro Domingos, a professor of computer science at the University of Washington, pointed out to the Post, “Can I predict that you’re an ax murderer by looking at your face and therefore should I arrest you? You can see how this would be controversial.”.Princeton psychology professor Alexander Todorov told the Post, “The evidence that there is accuracy in these judgments is extremely weak. Just when we thought that physiognomy ended 100 years ago.”.“Faception has built 15 different classifiers,” the Post reported, and allegedly can evaluate certain traits with an “80% accuracy.” Put another way, one in five people could incorrectly be classified as a terrorist or pedophile. Gilboa said the company “will never make his classifiers that predict negative traits available to the general public.”.Eight classifiers are listed on the Faception site: High Q, academic researcher, professional poker player, bingo player, brand promoter, white-collar offender, terrorist and pedophile. “The classifiers represent a certain persona, with a unique personality type, a collection of personality traits or behaviors.” Algorithms are used to sort people according to how they fit into those classifiers..For example, the company classifies a “bingo player” as being “endowed with a high mental ceiling, high concentration, adventurousness, and strong analytical abilities. Tends to be creative, with a high originality and imagination, high conservation and sharp senses.”.“Thrill seeking” is mentioned in the “terrorist” classifier. Thrills come in all shapes and sizes, right? Pity the adrenaline-junkie soul incorrectly identified as a terrorist..The company claims “success” stories such correctly identifying four poker players out of 50 competing in a tournament. In the end, two of the predicted four players were finalists. Faception claims that its technology classified nine of 11 Paris terrorists “with no prior knowledge” and only three of those terrorists had a previous record. That is allegedly why it is “working with the leading Homeland Security Agency,” according to its marketing video..While Faception is not quite the same, it reminded me of Homeland Security’s pre-crime screening program dubbed FAST for Future Attribute Screening Technology (pdf); FAST has been likened to Minority Report as it was designed “to ‘sense’ and spot people who intend to commit a terrorist act.”.Unlike Faception, FAST analyzes much more than the face. It reportedly analyzes facial expressions and uses trackers to measure pupils, position and gaze of eyes, but it also measures heart and respiration rates, analyzes body movement, body heat changes and pitch changes in voices..EPIC (Electronic Privacy Information Center) has been trying to get more information from DHS about FAST since 2011. That same year at DefCon, researchers suggested FAST smelled like security snake oil and explained why it wouldn’t work (pdf). Let’s hope the unnamed homeland security agency which inked a $750,000 contract with Faception was not DHS..China too has tinkered with “pre-crime” to identify terrorists; China being China, one has to wonder if dissident is synonymous with terrorist. Its “Citizen Score” is already an Orwellian nightmare..Faception may not be meant for the general public, but “analyzing anonymous individuals who may impose a threat to public safety” could be wrapped into law enforcement as in “homeland security,” AI, personal robots as well as for “public safety” at buildings, shopping malls, stadiums and corporations, and used in retail, insurance, recruiting, finance, and even matchmaking.\n"}
{"prompt":"Sensetime enables cameras to be enhanced with machine learning to find faces\n\n###\n\n","completion":" Its common now to read of the Chinese governments push to lead the world in applications of artificial intelligence. But, in fact, many of the greatest gains are the result of key individual actions and intersections over many years..Take the case of SenseTime, a standout in facial-recognition technology and, by virtue of an implied capitalization it says exceeds $3 billion, Chinas largest AI unicorn..SenseTimes creation begins with Tang Xiaoous Ph.D. in computer vision from MIT in 1996, which led to a teaching post at Chinese University of Hong Kong. Busy in the same engineering department there was Xu Li, en route to his own doctorate in computer vision and imaging in 2010. (Computer vision is software for recognizing images.).The two formed part of the nucleus of what became SenseTime a few years later, with Xu as CEO. One of Tangs students at the school, Xu Bing, today spearheads the companys fundraising and strategic partnerships. And when Tang took a sabbatical from the university in 2008 to head a computer-vision group at Microsoft Research Asia in Beijing, he met Yang Fan, now SenseTimes head of product development. It took me a couple of years to find out what I really liked to work on at MIT. Then it took more than 20 years of hard work in this area such that we finally start to see some real industrial applications and impacts. It does not happen overnight, Tang, now 49, writes in an email..It also took investors to give the group a critical boost. Justin Niu, a partner at venture capital firm IDG in Beijing, in 2014 visited professor Tang and his research team, still working out of the same university lab. They showed him demos of how a camera enhanced by machine learning could pull distinct faces out of a smoggy days tableau in Tiananmen Square..IDG joined with another firm, StarVC, to lead a $30 million investment into the formative SenseTime as Tang was processing the companys registration paperwork. The professor and his founding group, by then numbering ten and all at least a decade his junior, parked themselves a few miles away in Hong Kongs Science Park. Yang Fan added a contingent from his base, Tsinghua University..In 2014, a series of technology breakthroughs won SenseTime global acclaim from publications such as Nature and Science and other media attention before IDG came calling. These included a research paper showing its computing capability, the worlds first to outperform human vision under certain conditions, and its claim of beating Facebooks DeepFace with its algorithms called DeepID..Fast-forward to 2018, after stakes were taken by other Chinese VCs and private corporations, and by Qualcomm of the U.S. SenseTimes workforce numbers 1,500, with five mainland hubs added to the Hong Kong site. Revenues arent disclosed, but by Forbes Asias estimations approach $100 million..Tang remains the largest individual shareholder. SenseTime sports two faces: a research team working with 18 professors at universities across China and the profit-seeking commercial operations, which Tang, simply referred to as founder, and Xu Li lead. We hope our professors would continue to focus on the fundamental research, including our founding professor, so we can continue to have discoveries, says CEO Xu, who spent three years at a Motorola research center in Shanghai and six months at Omron Research Institute in Kyoto..SenseTimes field is exploding, and it says sales are growing better than fivefold annually. With the proliferation of smart machines and mobile devices, computer eyes are seemingly everywhere. In Chinas burgeoning fintech sector, SenseTimes recognition technology has scanned and authenticated the faces of 400 million consumers, 300 million for China Mobile alone. Its software is being used by Chinas ten largest security surveillance manufacturers, fitted on various Chinese phone makers to catalog, search and enhance user images, and soon should equip autonomous vehicles in a partnership with Honda..To put the current pace in context: It had taken SenseTime nine months to produce its first commercial product: a face-based identity-verification system for opening an account at one of Chinas 4,000 peer-to-peer microlenders, phasing out days of laborious manual processing. SenseTime claims applying its facial recognition to a security lock can be as safe as an eight-digit password, compared to four digits in 2014..In mobile apps, SenseTimes augmented-reality software helps place products in commercials (such as during Alibabas Singles Day promotion) and other short videos and live broadcasts. For instance, a performer could pull a showcased item, such as a soda can, from out of nowhere. On top of the consumer uses come the public security applications, which can be the most lucrative. Theres a strong, hard demand driven by smart cities and surveillance, Yang Fan says..Visitors at SenseTime offices are greeted by cameras that capture their images while instantly matching their identities with an internal registrar and making an estimate of their age, and detecting intruders behind face masks or eyeglasses..With positioning technology capable of processing data from 240 points on a human face, a leap from just 106 points before August 2017, SenseTime-aided monitors capture and count every passerby, whether inside an Ikea outlet or at a busy downtown intersection. That can be used for general crowd data (breaking down by age or sex, say, for marketing) or a specific identity search. Humans are surprisingly good at recognizing faces but no longer a match for machines that can also attach names and data to an image..SenseTime, with an internal training database of 2 billion faces (many of them collected online) and claiming 10 billion more images and videos, has worked with 40 Chinese localities, including Guangzhou, Shenzhen and Chongqing. Yang cites a case in 2016 in which police in Chongqing, powered by its technology, identified 69 suspects and caught 14 fugitives within 40 days. What weve accomplished [once] happened only in movies, he says..At the central government level, SenseTime is working with the Ministry of Public Securitys Third Research Institute to develop the nations so-called citizen network identity infrastructure..The web is another policing opportunity. In 2016 the Cyberspace Administration of China, its central internet regulator, enlisted SenseTime on a national cleanup campaign: to scan and delete online pornography, banishing videos of violence, terrorism and horror as well. This work would otherwise be attempted by legions of laborers. Result: The Chinese web might be the most hygienic on earth. Its very, very difficult to find all these videos on the Chinese internet today, Yang says..Of course, SenseTime does not have this emergent imaging sector to itself. Two prominent Chinese rivals, Megvii of the Face++ brand and Yitu Technology, are on the same hunt for clients. Megvii, founded in 2011 and backed by Ant Financial and iPhone manufacturer Foxconn, claims its FaceID platform has authenticated the identity of 290 million worldwide, with more than 85% market share in security software for Chinas financial industry. Yitu, five years old, with backing from Alibaba founder Jack Mas private Yunfeng Capital and giant Sequoia Capital, is active in health care imaging and fintech, and boasts of being an industry leader with 50% of market share in some unspecified segments in Chinas financial services industry..A Megvii representative argues that SenseTimes accuracy in facial recognition doesnt automatically translate into use applications. Software design needs to fit with computer chips, monitors and an organizations procedures. More and more public security clients look at product compatibility, timeliness and efficiency. Its algorithm could be used as technical support but by itself would not address a security problem, he says..Founder Tang is undaunted. At a recent event in Hong Kong, he told the audience: Dream big. I also learned from my son. He said someday he wanted to build an airplane with a swimming pool in it, calling it 747-380. Thats a combination of Airbus and Boeing. You need to prepare for your dream. I prepared for it for 20 years..Supplementing his AI talents in Hong Kong, he scurried for hardware engineers in Shenzhen, and found plenty of software engineers in Beijing and Shanghai. Today, his company has 150 employees with doctorates--and globally competitive salaries to match. The success of SenseTime is really based on the connection of Hong Kong with Shenzhen, with Beijing and with Shanghai, he says..Its stated ambition is to be the global dominant force in facial recognition, a platform company in the category of Google and Facebook, by taking a page from Intel, the global chip giant. Says CEO Xu, For SenseTime, our algorithms would form the high-entry barrier. We are forming an ecosystem with our business partners. Learning from Intel Inside, we are building up our model. Call it SenseTime Inside. \n"}
{"prompt":"WeChat automatically censors images critical of the Chinese Communist Party\n\n###\n\n","completion":" Based in China and boasting over 1.1 billion global users, it’s one of the world’s most advanced and popular apps. What’s remarkable is the way it reaches into so many corners of a Chinese person’s life: it’s the way much of the country chats, pays, plays, moves, and much more. As Mark Zuckerberg contemplates the future of Facebook, it’s increasingly WeChat he’s trying to emulate. .There’s more to this so-called “super app” than messaging, food, cars, and payments. The all-encompassing ambition of WeChat includes some of the most cutting-edge, quick-acting, and far-reaching censorship technology on earth..New research from the University of Toronto’s CitizenLab pulls the curtain back on how WeChat’s real-time, automatic censorship of text and images is used to exert control over political discussion on topics ranging from international issues like the trade war with the US to domestic scandals like the disappearance of court documents in a 2018 dispute between two multibillion-dollar Chinese mining companies. All discussion is ultimately subject to the Chinese government’s approval..WeChat’s censors face two types of challenges. Big public posts on WeChat Moments, a public feature similar to Facebook’s Timeline, are scrutinized and filtered by algorithms that can sometimes take over 10 seconds to run—a glacial pace on social media. But one-on-one and group messages are a different problem entirely, because they are often intimate and instant conversations. That requires real-time censorship..Text is relatively easy to search and censor. Image filtering is harder, especially when you’re trying to examine and censor the images  almost instantly. To accomplish this task, WeChat keeps a massive and always growing index of MD5 hashes, small cryptographic data signatures that are unique to every file. When a censored image is sent, it will be caught by the hash index and deleted. Neither the sender nor the recipient is ever likely to know anything was censored..If the image isn’t instantly censored, it’s sent for automatic analysis. Using optical character recognition, the image is examined for text (sending screen shots of newspaper articles was once an easy way past censors). The image is then checked for visual similarity to other censored images. So-called harmful content—including anything about international or domestic politics deemed undesirable by the Chinese Communist Party—will be sniffed out, removed from the conversation, and then added to that original hash index, which flags it for instant censorship from that moment onward. It’s a self-reinforcing system that’s growing with every image sent. The latter systems are also used on WeChat’s Moments to check and build the company’s dynamic blacklist..The Chinese tech giant Tencent, which owns WeChat, is under “a great deal of pressure” from Beijing to implement effective censorship tech, says Adam Segal, the director of the digital and cyberspace policy program at the Council on Foreign Relations. “The Chinese firms are all responsible for content, and while they have relied on tens of thousands of human censors, they have also been developing new [machine learning] approaches to content take-downs. Inside of China, it is part of the larger trend under Xi Jinping of tightening controls on the Chinese internet and society more broadly.”.WeChat is so pervasive in China that the prospect of getting suspended or banned can disrupt lives. The app combines the features of Facebook, Uber, GrubHub, and more. You can book doctor’s appointments, pay utility bills, talk to professional contacts, or engage government services..“This has really become a mega-app,” says Sarah Cook, the senior research analyst for East Asia at the pro-democracy research group Freedom House. “It’s really hard to function in modern Chinese society without using WeChat, and so the chilling effect is real.”.Under the direction of Beijing, Tencent’s censors are adroitly responsive to current affairs. In addition to deadly historical events like the Tiananmen Square protests and the Cultural Revolution, new events are quickly added. .Researchers found that newsworthy events like the arrest of a Huawei executive on charges of fraud and the CRISPR-baby scandal in which a Chinese scientist announced the birth of girls with edited genomes triggered waves of censorship. So did a host of other issues, like conflict with the US and domestic corruption or other problems..One event triggered even more censored images: in December 2018, a scandal erupted when the Chinese Supreme Court admitted the disappearance of documents in a billion-dollar dispute between mining companies. Researchers found that dozens of images relating to the alleged theft of the documents were censored..“In the last few years, you’ve had the combination of tightening censorship restrictions and tightening controls,” Cook says. “There’s less official tolerance for conversation, expression, and networking that had been tolerated previously. You’ve seen Tencent and WeChat, in particular, get hit with fines. So now they’re trying to employ new methods to catch so-called ‘harmful content’ that the Chinese Communist Party wants to stop. That’s where real-time automated censorship comes in.” \n"}
{"prompt":"Sesame credit scores people from 350 to 950 based on interpersonal relationships and consumer habits\n\n###\n\n","completion":" For the past couple of years a big story about the future of China has been the focus of both fascination and horror. It is all about what the authorities in Beijing call “social credit”, and the kind of surveillance that is now within governments’ grasp. The official rhetoric is poetic. According to the documents, what is being developed will “allow the trustworthy to roam everywhere under heaven while making it hard for the discredited to take a single step”..As China moves into the newly solidified President Xi Jinping era, the basic plan is intended to be in place by 2020. Some of it will apply to businesses and officials, so as to address corruption and tackle such high-profile issues as poor food hygiene. But other elements will be focused on ordinary individuals, so that transgressions such as dodging transport fares and not caring sufficiently for your parents will mean penalties, while living the life of a good citizen will bring benefits and opportunities..Online behaviour will inevitably be a big part of what is monitored, and algorithms will be key to everything, though there remain doubts about whether something so ambitious will ever come to full fruition. One of the scheme’s basic aims is to use a vast amount of data to create individual ratings, which will decide people’s access – or lack of it – to everything from travel to jobs..The Chinese notion of credit – or xinyong – has a cultural meaning that relates to moral ideas of honesty and trust. There are up to 30 local social credit pilots run by local authorities, in huge cities such as Shanghai and Hangzhou and much smaller towns. Meanwhile, eight ostensibly private companies have been trialling a different set of rating systems, which seem to chime with the government’s controlling objectives..The most high-profile system is Sesame Credit – created by Ant Financial, an offshoot of the Chinese online retail giant Alibaba. Superficially, it reflects the western definition of credit, and looks like a version of the credit scores used all over the world, invented to belatedly allow Chinese consumers the pleasures of buying things on tick, and manage the transition to an economy in which huge numbers of people pay via smartphones. But its reach runs wider..Using a secret algorithm, Sesame credit constantly scores people from 350 to 950, and its ratings are based on factors including considerations of “interpersonal relationships” and consumer habits. Bluntly put, being friends with low-rated people is bad news. Buying video games, for example, gets you marked down. Participation is voluntary but easily secured, thanks to an array of enticements. High scores unlock privileges such as being able to rent a car without a deposit, and fast-tracked European visa applications. There are also more romantic benefits: the online dating service Baihe gives people with good scores prominence on its platforms..Exactly how all this will relate to the version of social credit eventually implemented is unclear: licences that might have enabled the systems to be rolled out further ran out last year. There again, Ant Financial has stated that it wants to “help build a social integrity system” – and the existing public and private pilots have a similar sense of social control, and look set to feed the same social divisions. If you are mouldering away towards the bottom of the hierarchies, life will clearly be unpleasant. But if you manage to be a high-flyer, the pleasures of fast-tracking and open doors will be all yours, though even the most fleeting human interaction will give off the crackle of status anxiety..It would be easy to assume none of this could happen here in the west. But the 21st century is not going to work like that. These days credit reports and scores – put together by agencies whose reach into our lives is mind-boggling – are used to judge job applications, thereby threatening to lock people into financial problems. And in the midst of the great deluge of personal data that comes from our online lives, there is every sign of these methods being massively extended..Three years ago Facebook patented a system of credit rating that would consider the financial histories of people’s friends. Opaque innovations known as e-scores are used by increasing numbers of companies to target their marketing, while such outfits as the already infamous Cambridge Analytica trawl people’s online activities so as to precisely target political messaging. The tyranny of algorithms is now an inbuilt part of our lives..These systems are sprawling, often randomly connected, and often beyond logic. But viewed from another angle, they are also the potential constituent parts of comprehensive social credit systems, awaiting the moment at which they will be glued together. That point may yet come, thanks to the ever-expanding reach of the internet. If our phones and debit cards already leave a huge trail of data, the so-called internet of things is now increasing our informational footprint at speed..Personal data and its endless uses form one of the most fundamental issues of our time, which boils down to the relationship between the individual and power, whether exercised by government or private organisations. It speaks volumes that in Whitehall responsibility for such things falls uncertainly between the culture secretary, Matt Hancock, whose “digital” brief includes what the official blurb limply calls “telecommunications and online”, the Treasury and an under-secretary of state in the business department, Andrew Griffiths, whose portfolio takes in “consumers”..That is absurd, and it may yet play its part in our rapid passage into a future that could materialise in both east and west, in which we do what we’re told, avoid the company of undesirables – and endlessly panic about how the algorithms will rate us tomorrow.\n"}
{"prompt":"Due to a biased feedback loop, Tik Tok uses race, age and gender for profile recommendations\n\n###\n\n","completion":" On TikTok, all is not as it seems. Engage with dance videos and you’ll start seeing more people doing the Renegade. If you linger on a TikTok dog, it will give you puppies galore..But TikTok’s algorithmic obsession with giving you more content that it thinks you will like is having an unintended consequence: it’s started recommending people new accounts to follow based on the physical appearance of the people they already follow..This week Marc Faddoul, an AI researcher at UC Berkeley School of Information, found that TikTok was recommending him accounts with profile pictures that matched the same race, age or facial characteristics as the ones he already followed..He created a fresh account to test his theory and followed people he found on his ‘For You’ page. Following the account of a black woman led to recommendations for three more black women. It gets weirdly specific – Faddoul found that hitting follow on an Asian man with dyed hair gave him more Asian men with dyed hair, and the same thing happened for men with visible disabilities..TikTok denies that it uses profile pictures as part of its algorithm, and says it hasn’t been able to replicate the same results in its own tests. But the app uses collaborative filtering – where recommendations are made based on what other users have done..Our recommendation of accounts to follow is based on user behavior, says a spokesperson from TikTok. Users who follow account A also follow account B, so if you follow A you are likely to also want to follow B. And this has the potential to add unconscious bias into the algorithm..“The platform is very appearance driven, and therefore collaborative filtering can lead to very appearance specific results even if the profile picture is not used by the system,” says Faddoul. TikTok’s algorithm will think it is creating a personalised experience for you, but actually it is just building a filter bubble – an echo chamber where you only see the same kind of people with little awareness of others..This isn’t the first time TikTok’s algorithm has been accused of racial bias. In October 2019 TikTok users of colour called for better representation on the For You page, where users go for recommendations and new tailored content. In January 2019, Whitney Phillips, a professor of communication and online media rhetoric at Syracuse University told Motherboard that the way TikTok works could lead users to replicate the community with which they identify..To test the findings we created a new account, went on the ‘For You’ page, swiped left to view a profile and followed to see who was recommended. The first account that came up was that of KSI, an internet personality and rapper with 1.2 million followers on TikTok. We followed KSI and the next three recommended accounts were one with a profile picture of a ghostly-looking man sitting too far away from the camera to even guess his race, a blurry picture of what looks like a teenager at a festival, and a very close up picture of a white man’s face. All three are verified, but none look particularly similar..After that, results started to appear that were similar to what Faddoul found. An account a pet owner set up for their dog produced recommendations for other dog accounts. Following a young black man led to recommendations for two other black men and one cartoon of a black man. Following an 87-year-old man led to three recommendations for three more older men. Following a white woman with brown hair led to three more white women with brown hair, then finally, following an account with the Union Jack as its profile picture sprung up three more Union Jacks, one with a trollface superimposed on top..If you like one elderly man on TikTok, the app assumes that you will enjoy watching others. But this goes a step too far when racial bias is factored in. “People from underrepresented minorities who don’t necessarily have a lot of famous people who look like them, its going to be harder for them to get recommendations,” says Faddoul..On social media we follow people with opinions that we agree with. Algorithms then throw up more of the same content, creating a separation where you don’t see opinions that differ to yours, allowing you to forget that they exist. In a highly visual platform such as TikTok, this applies to how a person looks. Faddoul’s findings may not indicate how TikTok intends its algorithm to work, but shows how user biases may have resulted in these very specific filter bubbles..TikTok has economised people’s attention, using the mountains of data it has about how long people spend watching videos and how they interact with them to hyper-personalise the user, says Jevan Hutson, human-computer interaction researcher at University of Washington School of Law. The data from users across the globe feeds into an algorithm that ends up encouraging a segregation of content..This extraction of data can create patterns that solidify assumptions about particular races or ethnicities, says Hutson. He compares it to dating apps, where the unconscious bias of thousands of users will create an assumption within the algorithm about racial preferences – Coffee Meets Bagel, for example, met controversy when users kept getting recommended people of the same race as themselves even though they never indicated a racial preference. On TikTok, when the app recommends more of the same content and users, it risks leading to radicalisation and segregation..“I think theres no ethical consumption under surveillance,” says Hutson. He is an avid TikTok user, and believes that people sacrifice the data the app collects about them for the sake of getting content they enjoy more..The data TikTok gets from its millions of users feeds into a cycle they get trapped in, and even if you make an effort to diversify your feed, everyone else’s bias will mean the algorithm will keep trying to channel you into a bubble.\n"}
{"prompt":"When users typed holocaust, the Facebook search function brought up suggestions for denial pages\n\n###\n\n","completion":" Facebook’s algorithm “actively promotes” Holocaust denial content according to an analysis that will increase pressure on the social media giant to remove antisemitic content relating to the Nazi genocide..An investigation by the Institute for Strategic Dialogue (ISD), a UK-based counter-extremist organisation, found that typing “holocaust” in the Facebook search function brought up suggestions for denial pages, which in turn recommended links to publishers which sell revisionist and denial literature, as well as pages dedicated to the notorious British Holocaust denier David Irving..The findings coincide with mounting international demands from Holocaust survivors to Facebook’s boss, Mark Zuckerberg, to remove such material from the site..Last Wednesday Facebook announced it was banning conspiracy theories about Jewish people “controlling the world”. However, it has been unwilling to categorise Holocaust denial as a form of hate speech, a stance that ISD describe as a “conceptual blind spot”..The ISD also discovered at least 36 Facebook groups with a combined 366,068 followers which are specifically dedicated to Holocaust denial or which host such content. Researchers found that when they followed public Facebook pages containing Holocaust denial content, Facebook recommended further similar content..Jacob Davey, ISD’s senior research manager, said: “Facebook’s decision to allow Holocaust denial content to remain on its platform is framed under the guise of protecting legitimate historical debate, but this misses the reason why people engage in Holocaust denial in the first place..“Denial of the Holocaust is a deliberate tool used to delegitimise the suffering of the Jewish people and perpetuate long-standing antisemitic tropes, and when people explicitly do this it should be seen as an act of hatred,” he added..Researchers also found that Holocaust denial content is readily accessible across Twitter, Reddit and YouTube. They identified 2,300 pieces of content mentioning “holohoax” – a term often used by deniers – on Reddit, 19,000 pieces on Twitter and 9,500 pieces of content on YouTube, all created in the past two years..This week a coalition of UK faith leaders will call on the government to get tougher with social media companies, arguing they are still too weak on tackling racist, antisemitic, Islamophobic and anti-Hindu hate online. However, ISD said that tackling the issue was straightforward, pointing to a sharp reduction in mentions of “holohoax” on YouTube since spring 2019, when the video-sharing platform banned Holocaust denial content..On Reddit, researchers noted how concerns from other users were effective in hiding and discrediting Holocaust denial content. Other factors limiting the visibility on Reddit included the banning of groups dedicated to Holocaust denial and moderators deleting comments..Jakob Guhl, ISD research coordinator, said: “Our findings show that the actions taken by platforms can effectively reduce the volume and visibility of this type of antisemitic content. These companies therefore need to ask themselves what type of platform they would like to be: one that earns money by allowing Holocaust denial to flourish, or one that takes a principled stand against it.” A significant amount of denial content is couched in careful language, codes and tropes, and thus this analysis probably does not show the true extent of the spread of such content on social media..A Facebook company spokesperson said: “We take down any post that celebrates, defends, or attempts to justify the Holocaust. The same goes for any content that mocks Holocaust victims, accuses victims of lying, spews hate, or advocates for violence against Jewish people in any way. .“We also remove groups and pages that discuss Holocaust denial from recommendations and references to it in search predictions. While we do not take down content simply for being untruthful, many posts that deny the Holocaust often violate our policies against hate speech and are removed..They added: “In countries where it is illegal, such as Germany, France and Poland, this content is not allowed in accordance with the law. Striking the right balance between keeping people safe and allowing free speech is difficult and we know many people strongly disagree with our position. We are constantly developing and reviewing our policies and consulting with organisations around the world to ensure we’re getting it right.” \n"}
{"prompt":"Ofqual, the algorithm used in these results, has shown unfair and biased results\n\n###\n\n","completion":" Nearly 40% of A-level grades submitted by teachers are set to be downgraded when exam results in England are published next week, the Guardian has learned, as criticism intensifies of this year’s makeshift results..Analysis of the algorithm and data used by the exam regulator Ofqual to distribute grades after the cancellation of exams amid the coronavirus pandemic found that a net 39% of assessments of A-level grades by teachers are likely to be adjusted down before students receive their results..That would mean nearly 300,000 A-levels issued are lower than the teacher assessment of the more than 730,000 A-level entries in England this summer..Including GCSEs, which are expected to have a similar downgrade rate, close to a net 2m teacher assessments will be adjusted downwards and in many cases ignored completely..There was uproar in Scotland this week when the exams authority rejected nearly 124,000 grade recommendations from teachers – a quarter of the total – but unlike in Scotland, English pupils are barred from appealing against their results on academic grounds..Grades will instead be issued according to Ofqual’s statistical model, relying on a school’s recent exam history and each pupil’s previous exam results, to replace the exams scrapped by the government after schools were closed because of the coronavirus lockdown..Those most at risk of receiving revised grades appear to be students on the border between B and C grades, and between C and D grades, and pupils at comprehensive schools with wide variations in attainment or patchy outcomes in courses over the three previous years of data that Ofqual is using to cap individual school results..Teachers will still have a significant influence on how grades are distributed in each school, having compiled the rankings that will determine which pupils receive the final grades allocated by Ofqual for their course..Headteachers and exam officials in England say they fear a storm of controversy even worse than that which has engulfed Scotland, where a quarter of teacher predictions were adjusted by the Scottish Qualifications Authority..Experts say that as Ofqual has barred individual pupils from appealing against their grades on academic grounds, families should not waste time complaining but instead contact college or university admissions offices to confirm their places in the event of unexpectedly poor grades..Tim Oates, group director of research and development at the exam board Cambridge Assessment, said: “Grades have been awarded this year by combining lots of data, including the rank order and the grades submitted by teachers. We have seen from Scotland’s press coverage that it’s all too easy to fixate on the difference between the teacher-assessed grades and the final grades. But it’s a misleading distraction and misinforms the public. The teacher grades were an important part of the process but always only going to be a part..“On results day, energy should be channelled into how each young person can progress swiftly with what they have been awarded, rather than time lost on agonising over an apparently controversial but fundamentally misleading difference between teacher grades and final grades.”.Statisticians have criticised Ofqual’s algorithm, saying it does not have sufficient data to award grades fairly to most state schools in England, because of wide variations in results within schools and between years..The Royal Statistical Society has called for an urgent review of the statistical procedures used in England and Scotland, to be carried out by the UK Statistics Authority. “This should consider both the substantive issues of the data used and the adjustment algorithms of the various nations, but also whether greater transparency would have been possible and beneficial,” the society said..Huy Duong, the parent of an A-level candidate and a former medical statistician, said he has analysed Ofqual’s published data and comments to calculate that 39% of grades between A* and D will be lower than the teacher assessments. Duong’s findings were privately confirmed to the Guardian by exam officials..Duong’s analysis is based on Ofqual’s statement that A-level grades overall will improve by 2% this summer, and that the submitted teacher assessments, known as centre-assessed grades, would have resulted in 12% inflation in higher grades..“It gives the public the impression that in most cases the grades the student receive would still be the predicted grades. However, closer analysis shows that this is not true,” Duong said..In response, a spokesperson for Ofqual said: “From the data that we have reviewed we expect the majority of grades students receive will be the same as their centre assessment grades, and almost all grades students receive will be the same as the centre assessment grades or within one grade. The exact proportions vary by subject and we will publish the figures on results day.”.But Duong also found that a comprehensive secondary school can have a huge annual variation in results for individual courses, with the small numbers of entries involved suggesting that Ofqual’s decisions are statistically invalid..For instance at Matthew Arnold school in west Oxford, a comprehensive academy, the proportion of A* grades achieved by pupils in a popular subject such as English literature varied from one in 19 to three in 10, or from 5% to 30%, between 2017 and 2019..Duong said: “These fluctuations mean that Ofqual’s statistical modelling cannot make sense. The problem is that any statistical model is only as good as the data you feed it, and for a typical comprehensive school, there simply isn’t enough A-level data from 2017 to 2019 for any statistical modelling.”.In response, Ofqual said: “For A-level, three years of historical results inform the standardisation of grades. You can think of this as an averaging across the years of data.”.Uncertainty over the grades being awarded has led universities to say they will relax offers to prospective students and make use of other data..Admissions officers for Peterhouse College, Cambridge, said during a Q&A on the Student Room website: “We are looking into which schools (from among our offer holders) are most likely to be adversely affected by the system of awarded grades so we are in a better position to make nuanced decisions when we get the results.”.In a significant sign that it recognised the controversy likely to erupt on Thursday when students receive A-level results, Ofqual this week changed its position and said schools would be able to appeal if they expected “a very different pattern of grades to results in previous years”. But Ofqual will not allow individual students to appeal against their grades on academic grounds, as they can in Scotland..For A-level pupils, teacher assessments will only be used to help set grades on small courses, with five or fewer candidates. On larger courses, teacher assessment will play little or no role, with grades being awarded instead based on a school’s recent exam history and each pupil’s previous exam results..The pupils most likely to benefit from teacher assessments will be those taking courses with very small entries of five pupils or less, such as German or music. Those taking popular subjects such as maths or biology, with more than 15 pupils, will be exposed to Ofqual’s algorithm and the teacher assessment will be ignored..Kate Green, Labour’s shadow education secretary, said: “The government should have ensured that Ofqual had a robust appeals system in place from the beginning, instead of announcing one a matter of days before A-level results. They must do far more to ensure the system is genuinely fair.”.Students unhappy with their results have the option of sitting exams in autumn. But with students entering university or college, few are expected to do so.\n"}
{"prompt":"SQA led to reduced higher pass rates in deprived datazones compared to pupils from affluent backgrounds\n\n###\n\n","completion":" With no exams sat this year due to the coronavirus pandemic, the Scottish Qualifications Authority (SQA) ran a system based on teacher assessments..Education Secretary John Swinney will set out the governments plan to fix the issue on Tuesday, with Ms Sturgeon saying the onus would not be on students to submit appeals..Opposition parties are pushing for a vote of no confidence in the education secretary, but Ms Sturgeon said she had faith in Mr Swinney and that the row was not party political..Prime Minister Boris Johnson said he understood the anxiety over grades, and that we will do our best to ensure that the hard work of pupils is properly reflected..In Scotland this was moderated at a national level by the SQA, a process which led to thousands of pupils complaining that they had received lower grades than originally estimated..There was particular criticism after Higher pass rates for pupils in the most deprived data zones were reduced by 15.2%, in comparison with 6.9% for pupils from the most affluent backgrounds..The Scottish childrens commissioners office said pupils from more deprived areas had been downgraded based on the historic performance of their school rather than their performance..Ms Sturgeon said young people in more deprived areas might be concluding that the system is stacked against them, and that she was not prepared to have that outcome..At her daily coronavirus briefing, Ms Sturgeon said steps would be taken to address concerns and ensure that every young person gets a grade that recognises the work they have done..She said ministers had taken decisions we thought were the right ones in unprecedented circumstances, but after a lot of soul searching had now accepted they were not right..She said: Our concern, which was to make sure the grades young people got were as valid as in any other year, perhaps led us to think too much about the overall system and not enough about the individual pupil. .That has meant too many students feel they have lost out on grades they should have had, and that that has happened not as a result of anything they have done but a statistical model or algorithm..Mr Swinney will set out plans for how to address the issue at Holyrood on Tuesday, but the first minister said we will not expect every student who has been downgraded to appeal..She added: This is not the fault of students, and it should not be on students to fix it - thats on us, and we will set out tomorrow how we intend to do that..The education secretary could also face a no-confidence vote tabled by Labour in the Scottish Parliament when it returns from recess this week..The Conservatives say they will support the motion and the Scottish Greens have indicated they would consider backing it if no changes are made..Scottish Labours education spokesman Iain Gray told BBC Radios Good Morning Scotland programme that the simplest and fairest way would be to return grades to what teachers originally projected, saying that anything else would fall short..Ms Sturgeon said she had confidence in Mr Swinney, noting that governments in other parts of the UK were taking broadly the same approach to exam results in difficult circumstances.\n"}
{"prompt":"Proctorio, a test proctoring software,reinforces structural oppression such as asking a Black woman to shine more light on her face\n\n###\n\n","completion":" The coronavirus pandemic has been a boon for the test proctoring industry. About half a dozen companies in the US claim their software can accurately detect and prevent cheating in online tests. Examity, HonorLock, Proctorio, ProctorU, Respondus and others have rapidly grown since colleges and universities switched to remote classes..While there’s no official tally, it’s reasonable to say that millions of algorithmically proctored tests are happening every month around the world. Proctorio told the New York Times in May that business had increased by 900% during the first few months of the pandemic, to the point where the company proctored 2.5 million tests worldwide in April alone..Im a university librarian and Ive seen the impacts of these systems up close. My own employer, the University of Colorado Denver, has a contract with Proctorio..It’s become clear to me that algorithmic proctoring is a modern surveillance technology that reinforces white supremacy, sexism, ableism, and transphobia. The use of these tools is an invasion of students’ privacy and, often, a civil rights violation..If you’re a student taking an algorithmically proctored test, here’s how it works: When you begin, the software starts recording your computer’s camera, audio, and the websites you visit. It measures your body and watches you for the duration of the exam, tracking your movements to identify what it considers cheating behaviors. If you do anything that the software deems suspicious, it will alert your professor to view the recording and provide them a color-coded probability of your academic misconduct..Depending on which company made the software, it will use some combination of machine learning, AI, and biometrics (including facial recognition, facial detection, or eye tracking) to do all of this. The problem is that facial recognition and detection have proven to be racist, sexist, and transphobic over, and over, and over again..In general, technology has a pattern of reinforcing structural oppression like racism and sexism. Now these same biases are showing up in test proctoring software that disproportionately hurts marginalized students..A Black woman at my university once told me that whenever she used Proctorios test proctoring software, it always prompted her to shine more light on her face. The software couldn’t validate her identity and she was denied access to tests so often that she had to go to her professor to make other arrangements. Her white peers never had this problem..Similar kinds of discrimination can happen if a student is trans or non-binary. But if you’re a white cis man (like most of the developers who make facial recognition software), you’ll probably be fine..Students with children are also penalized by these systems. If you’ve ever tried to answer emails while caring for kids, you know how impossible it can be to get even a few uninterrupted minutes in front of the computer. But several proctoring programs will flag noises in the room or anyone who leaves the camera’s view as nefarious. That means students with medical conditions who must use the bathroom or administer medication frequently would be considered similarly suspect..Beyond all the ways that proctoring software can discriminate against students, algorithmic proctoring is also a significant invasion of privacy. These products film students in their homes and often require them to complete “room scans,” which involve using their camera to show their surroundings. In many cases, professors can access the recordings of their students at any time, and even download these recordings to their personal machines. They can also see each student’s location based on their IP address..Privacy is paramount to librarians like me because patrons trust us with their data. After 9\/11, when the Patriot Act authorized the US Department of Homeland Security to access library patron records in their search for terrorists, many librarians started using software that deleted a patron’s record once a book was returned. Products that violate people’s privacy and discriminate against them go against my professional ethos, and it’s deeply concerning to see such products eagerly adopted by institutions of higher education..This zealousness would be slightly more understandable if there was any evidence that these programs actually did what they claim. To my knowledge, there isn’t a single peer-reviewed or controlled study that shows proctoring software effectively detects or prevents cheating. Given that universities pride themselves on making evidence-based decisions, this is a glaring oversight..Fortunately, there are movements underway to ban proctoring software and ban face recognition technologies on campuses, as well as congressional bills to ban the US federal government from using face recognition. But even if face recognition technology were banned, proctoring software could still exist as a program that tracks the movements of students’ eyes and bodies. While that might be less racist, it would still discriminate against people with disabilities, breastfeeding parents, and people who are neuroatypical. These products can’t be reformed; they should be abandoned..Cheating is not the threat to society that test proctoring companies would have you believe. It doesn’t dilute the value of degrees or degrade institutional reputations, and student’s aren’t trying to cheat their way into being your surgeon. Technology didn’t invent the conditions for cheating and it won’t be what stops it. The best thing we in higher education can do is to start with the radical idea of trusting students. Let’s choose compassion over surveillance..An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.\n"}
{"prompt":"The current portrayals and stereotypes about aI risk creates a racially homogenous workforce who create machines with bias in their algorithms\n\n###\n\n","completion":" The “whiteness” of artificial intelligence (AI) removes people of colour from the way humanity thinks about its technology-enhanced future, researchers argue..University of Cambridge experts suggest current portrayals and stereotypes about AI risk creating a “racially homogenous” workforce of aspiring technologists, creating machines with bias baked into their algorithms..The scientists say cultural depictions of AI as white need to be challenged, as they do not offer a “post-racial” future but rather one from which people of colour are simply erased..According to the researchers from Cambridge’s Leverhulme Centre for the Future of Intelligence (CFI), like other science fiction tropes, AI has always reflected racial thinking in society..In the study published in the Philosophy and Technology journal, they argue there is a long tradition of racial stereotypes when it comes to extraterrestrials – from the “orientalised” alien of Ming the Merciless to the Caribbean caricature of Jar Jar Binks..However, AI is portrayed as white because unlike species from other planets, it has attributes used to “justify colonialism and segregation” in the past, the researchers suggest..Dr Kanta Dihal, who leads CFI’s Decolonising AI initiative, said: “Given that society has, for centuries, promoted the association of intelligence with white Europeans, it is to be expected that when this culture is asked to imagine an intelligent machine, it imagines a white machine..The experts looked at recent research from a range of fields, including Human-Computer Interaction and Critical Race Theory, to demonstrate that machines can be racialised, and that this perpetuates “real world” racial biases..This includes work on how robots are seen to have distinct racial identities, with black robots receiving more online abuse, and a study showing that people feel closer to virtual agents when they perceive shared racial identity..Dr Dihal said: “One of the most common interactions with AI technology is through virtual assistants in devices such as smartphones, which talk in standard white middle-class English..The researchers conducted their own investigation into search engines, and found that all non-abstract results for AI had either Caucasian features or were the colour white..Co-author of the paper, Dr Stephen Cave, executive director of CFI, said: “Stock imagery for AI distils the visualisations of intelligent machines in western popular culture as it has developed over decades.\n"}
{"prompt":"There is evident bias and discrimination as automation will disproportionately impact women\n\n###\n\n","completion":" There is a growing body of evidence that automation is disproportionately impacting women, with the overwhelming majority of high paid, high-tech jobs taken up by men, says researcher.Researchers at King’s College London and the London School of Economics say the use of robots in the workplace has had a “sizeable” detrimental impact on the gender pay gap in Europe..The study found that for every 10 per cent increase in the number of robots being used by a company, there was a 1.8 per cent increase in the conditional pay gap between male and female workers..But researchers did find both men and women saw their pay increase overall due to automation. The Office for National Statistics (ONS) defines automation as tasks currently performed by workers being replaced with technology – potentially involving computer programmes, algorithms, or robots..Dr Cevat Giray Aksoy, one of the report’s authors, said: “At a time when policymakers are putting increased efforts into tackling gender gaps in the labour market, our evidence is important..“Our results suggest that governments not only need to ensure that education and vocational training systems provide people with the right skills demanded in the future, but also need to pay attention to distributional issues. They need to increase efforts to make sure that women and men are equally equipped with the skills most relevant for future employability.”.The study, titled Robots and the Gender Pay Gap in Europe, discovered the impact on the pay gap was especially pronounced in what researchers referred to as “outsourcing destination countries”, where gender inequality was already more noticeable at work..“Outsourcing origin countries” – predominantly Western European countries – did not witness a striking rise in the pay gap comparative to automation..Men were found to be more likely to be in job positions which were higher-skilled and higher in the work-related pecking order which, when coupled with advances in automation, compounded the pre-existing pay gap..Researchers looked at patterns in the UK, Spain, Belgium, Bulgaria, the Czech Republic, Estonia, Finland, France, Germany, Greece, Hungary, Italy, Latvia, Lithuania, the Netherlands, Poland, Portugal, Romania, Slovakia, and Sweden..Fabian Wallace-Stephens, of the Royal Society for the Encouragement of Arts, Manufactures and Commerce (RSA), raised concerns about the study’s findings. .Mr Wallace-Stephens, who is senior researcher at the RSA Future Work Centre, said: “Far from creating a level playing field for the workforce, there is a growing body of evidence that automation is disproportionately impacting women, with the overwhelming majority of high paid, high-tech jobs taken up by men. .“While we are still waiting to see how the pandemic will impact the economy in the long-term, it is likely that this process will accelerate in the coming months. Improving the infrastructure around retraining and re-skilling, alongside a wider culture change, will be necessary if automation is to benefit everyone in our post-pandemic economy.”.A previous report, carried out by the RSA and the Women’s Budget Group, drew attention to the dangers of new technologies worsening existing gender divides in the workplace – calling for recent cases of women suffering from in-built bias in artificial intelligence systems to be met with a “robust response” from policymakers and employers..The study, which came out last August, warned “algorithmic prejudice” could become one of the new giants of modern poverty if it is left unchecked..In March last year, the national statistics office announced women in the UK are considerably more likely than men to be working in jobs endangered by the advance of technology..The ONS said 1.5 million people in England are at high risk of losing their jobs to automation – with women holding more than 70 per cent of those roles believed to be at high risk..The three occupations with the highest probability of automation were found to be waiters and waitresses, shelf-stackers and basic retail roles – all of which are low-skilled or routine. However, well-educated women were also found to be more at risk than men..By registering, you will also enjoy limited access to Premium articles, exclusive newsletters, commenting, and virtual events with our leading journalists\n"}
{"prompt":"Twitter recommendation engine favors inflammatory tweets\n\n###\n\n","completion":" SINCE LAUNCHING a policy on “misleading information” in May, Twitter has clashed with President Donald Trump. When he described mail-in ballots as “substantially fraudulent”, the platform told users to “get the facts” and linked to articles that proved otherwise. After Mr Trump threatened looters with death—“when the looting starts, the shooting starts”—Twitter said his tweet broke its rules against “glorifying violence”. On July 28th the site took down a tweet by Donald Trump junior promoting a malaria drug for covid-19 that plenty of studies discredit..The president says that “social media platforms totally silence conservatives’ voices.” However, a study by The Economist finds the opposite. Twitter’s feed used to show people the latest posts from accounts they followed, but in 2016 it launched an algorithm to serve “relevant” tweets to users, even if they were days old and from unfamiliar accounts. We compared the two systems, and found that the recommendation engine appears to reward inflammatory language and outlandish claims..Our experiment began in June 2019, when we created a clone of Mr Trump’s profile. This bot used his picture, biography and location, and followed the same people as he did. We used it to re-post some of the president’s old tweets over several weeks, so that the algorithm could learn what our Trump clone cared about..Then from September to December we checked every ten minutes if Mr Trump had tweeted something. If so, three things happened. First, our clone repeated the tweet. Second, we checked its Twitter feed and recorded the first 24 posts served by the algorithm. Finally, we simulated what a chronological feed might have looked like, using the 24 most recent tweets by accounts that Mr Trump follows..Our algorithmic and chronological feeds differed starkly. Nearly half the recommended tweets were from users whom Mr Trump does not follow. Using sentiment-analysis tools to extract feelings from text, we found the average curated tweet was more emotive, on every scale, than its chronological equivalent—and more so than Mr Trump’s own posts, too..Sentiment analysis can be confusing. The emotional scores assigned to tweets by, say, Sean Hannity, a right-wing pundit, might be highly negative—not because they reflect poorly on him, but because he stridently criticises others, such as Democrats. Nonetheless, in a sample of 120,000 tweets, the posts recommended by the algorithm were more likely to sit near either end of a positive-to-negative spectrum..Twitter might also boost extreme views. Researchers at Indiana University have classified a list of left- and right-wing websites as untrustworthy or hyper-partisan. We found 1,647 links to such domains on our clone’s algorithmic feed, but only 895 on the chronological one. (Almost all cases on both feeds were right-wing.).Our experiment ended when a change in Twitter’s interface broke our bot. The platform also suspended another Trump clone that copied his looting threat. However, if an algorithmic penchant for sensationalism has remained, then Twitter may be amplifying and profiting from misleading tweets, rather than removing them. Its business is serving ads to 330m users, even if that means grabbing their attention by showing them exactly what they want to believe. Flagging a presidential whopper every now and then will not change that. ■.Published since September 1843 to take part in “a severe contest between intelligence, which presses forward, and an unworthy, timid ignorance obstructing our progress.” \n"}
{"prompt":"A deepfake of a confession video is used in a sex scandal\n\n###\n\n","completion":" KUALA LUMPUR, June 12  —  A viral video depicting two men, one of whom resembles a local cabinet minister, has been making rounds on local news sources after the clip was uploaded to a WhatsApp group containing members of the media and several political party members on Monday morning..At roughly 4am this morning, a new development has come to light in the form of a confession video uploaded by a senior private secretary to the Deputy Primary Industries and Commodities Minister..In his confession, the political aide admits to being one of the two men seen in the viral sex video, while also identifying the other man as a prominent cabinet minister. In his 35-second clip, the man calls for the MACC to investigate the minister for corruption, while he also claims that the original video was taken without his consent..One perspective could be that this is a malicious move by internal rivals within the minister’s political party, Parti Keadilan Rakyat. The cabinet minister purported to be in the original clip has reportedly been at loggerheads with current Parti Keadilan Rakyat leader, Anwar Ibrahim for a while now..The Edge reported that internal rifts within the party have steadily worsened, while the recent appointment of new MACC chief, Latheefa Koya is alleged to be part of an elaborate scheme against the PKR leader..Anwar’s supporters call him the PM-in-waiting, but he has also acknowledged that there may be some ulterior agendas within his own party  —  namely from stakeholders that are pushing for the cabinet minister to be Tun Mahathir’s successor as well..On the other hand, it could also be argued that Opposition parties could have the motive. Umno supreme council member Datuk Lokman Noor Adam has indeed lodged a police report while stating;.But it’s important to remember that the only thing that actually lends a degree of credibility to the original sex clip is the confession video uploaded on FB. The first thing to consider then, of course, is its authenticity..Deepfake is a method of human image synthesis that is based on artificial intelligence. This works via superimposing existing images or videos onto source images\/videos using machine learning techniques..In other words, deepfakes are AI-generated videos that can look pretty real — in the past, video content was considered to be a viable source of proof (as opposed to photos that could be easily photoshopped), but with the emergence of deepfakes like this, things have clearly changed quite a bit..If you examine the video above, Zuckerberg’s blinking patterns look slightly off, while the general sync between his voice and his mouth does appear unnatural. It’s worth noting that the aforementioned confession video, on the other hand, has a much more natural-sounding audio..But there are tell-tale signs. According to research done at the University at Albany, one of the easiest ways to tell if a video is indeed deepfake is to observe the eyes of whomever is in the video..The report explains that in deepfake videos, blinking patterns are usually physiological signals that aren’t well presented. The average person, according to the research, blinks at a resting rate of 17 blinks\/minute, while reading will drop this to 4.5 blinks\/minute and 26 blinks\/minute during conversation..Examining the confession\/accusatory video from earlier, it actually looks authentic. Do note that this is my opinion, based on the political aide’s blinking pattern during the video. In addition to that, deepfake videos have also been known to have slightly distorted\/monotonous audio, and the video here has relatively natural-sounding audio..One last caveat, however. The aforementioned research was conducted in June 2018, which may not seem like a long time. But one year is a long time when it comes to technology. — SoyaCincau\n"}
{"prompt":"An app that turns selfies into impressionist portraits whitens skin\n\n###\n\n","completion":" An AI-powered portrait generator went viral last week thanks to its ability to turn selfies into realistic Impressionist portraits. For people of color, however, the results leave much to be desired..The flaw in AI Portrait Ars, an app built by researchers at the MIT-IBM Watson AI Lab, was first pointed out by Morgan Sung, a reporter at Mashable. She found that the app “whitened my skin to an unearthly pale tone, turned my flat nose into one with a prominent bridge and pointed end, and replaced my very hooded eyes with heavily lidded ones. This result is both terribly disappointing and utterly predictable..I wasnt surprised at the whitewashing at all, since Im used to things like Snapchat filters lightening my skin, making my eyes bigger, narrowing my nose. But I was taken aback by how extreme it was, Sung told Motherboard. “The painting that AI portrait built was a completely different face..In 2019, AI developers should know that algorithmic bias not only exists but is a serious problem we must fight against. So why does it continue to persist? And can we actually stop it? These are open questions that boil down to where you think the blame lies. Is it the case that these algorithms are exaggerating parts of human nature? Are algorithmic biases a reflection of our society’s systemic problems? In the case of the AI Portrait Ars, it may help to trace why it couldn’t draw the faces of people of color in order to figure out why this continues to happen..Part of the problem lies with how AI Portrait Ars fundamentally works. The program relies on a generative adversarial network (GAN), meaning there are two types of algorithms pitted against each other as adversaries to create its portraits. The first type are generative algorithms, responsible for generating new data. The second type are discriminator algorithms, responsible for deciding whether new data belongs to the training dataset..With AI Portrait Ars, the generator learns how to create realistic portraits of people and the discriminator learns how to discern which arent convincing enough based on the dataset. Datasets, then, are of the utmost importance in determining whether or not the GAN will read certain data (facial features) as authentic or not. The training dataset has over 15,000 images, but it’s important to remember where these images were likely pulled from..“This was an experiment by one of our researchers. The images from the apps users were deleted immediately from our servers after the Renaissance portrait was generated. The experiment has run its course,” IBM Research said in a statement to Motherboard..“Also, the tool reflects the data it was trained on: a collection of 15,000 portraits, predominantly from the Western European Renaissance period,” the company continued. “In some cases, it produced a strong alteration of colors and shapes. That’s a reality of the style, not the algorithm.”.This experiment, however, mirrors dozens of other AI and facial recognition experiments that have had far more accurate results for white people than people of color. If the experiment proved anything, it’s that AI researchers continue to be drawn to experiments and research that perpetuate the biases we already know exist in AI research..It’s also not actually a “reality of the style” of Renaissance art that people of color weren’t in paintings of the era. There are many examples of people of color in European art history, though they are largely assumed by the masses to be non-existent in art from the Renaissance..“The material available for illuminating the lives of individual Africans in Renaissance Europe through the visual arts is considerable, though little known to the wider public,” a lengthy 2013 report from Baltimore’s Walters Art Museum called “Revealing the African Presence in Renaissance Europe” notes..It is important to “understand the period in terms of individuals of African ancestry, whom we encounter in arresting portrayals from life, testifying to the Renaissance adage that portraiture magically makes the absent present. We begin with slaves, moving up the social ladder to farmers, artisans, aristocrats, scholars, diplomats, and rulers from different parts of the African continent,” it continues..The problem with AI Portrait Ars reflects how, historically, technology often functions as an extension of the status quo as opposed to a great equalizer. Color film, for example, was initially calibrated to look best with white skin tones since they were the preferred consumer market. In the 1970s, what prompted the industry to even consider better rendering of darker colors was economic pressure from Kodak’s professional accounts. Furniture manufacturers were angry that their advertisements using Kodak color film didn’t capture the difference between dark-grained wood and light-grained wood, while chocolate confectioners were angry that the film couldnt capture all the different shades of chocolate..At this point, AI researchers—especially ones utilizing IBM’s Watson, should know better. In 2018, Joy Buolamwini, founder of the Algorithm Justice League, published her MIT thesis analyzing facial recognition technology from IBM Watson, Microsoft, and Face++ (a Chinese artificial intelligence company). Buolamwini found that all of the programs had the highest error rates for dark-skinned women and the most accurate results with light-skinned men, but that IBM Watson had the highest disparity in the error rates between dark-skinned women and light-skinned men (the error rate was 34.4 percent higher for dark-skinned women). Buolamwini also found that as skin tones got darker, IBM Watson failed to correctly recognize a subjects gender nearly 50 percent of the time..To IBM’s credit, Buolamwini’s research pushed the company to radically improve its facial recognition technology. This, however, hasn’t stopped the problem of racial bias from reappearing in other IBM products like their AI Portrait Ars, or the industry at large. Until we can root out the biases baked into our society that keep reemerging in each new generation of technology, what is to be done?.Caroline Sinders, a machine learning designer who previously worked with IBM Watson, told Motherboard that part of the problem lies with a lack of awareness that we need to test multiple genders or multiple races. At the same time, Sinders asked whether the solution is as simple as more diversity in data. “When these failures pop up, it really does highlight a lack of diversity in the sets. But also having a more diverse dataset for things that use facial images poses a problem where better facial apps lead to … better facial recognition. Do we necessarily want that?”.That’s a valid question when applied to the many in-the-field uses of AI and facial recognition technology, many of which are deployed disproportionately by police against people of color. As Sinders mentioned, better facial apps leads to better facial recognition—but do we need yet another AI face app at all?.Today, the problem of getting datasets that represent populations accurately and the legacy of technology being used to preserve power systems are very much interlinked. In a New York Times op-ed, Buolamwini talks about the coded gaze, a phenomenon where “A.I. systems are shaped by the priorities and prejudices — conscious and unconscious — of the people who design them.” The extremely high rates of misidentification that plague facial recognition software when used on people of color have led to calls for its complete and total ban. These embedded biases can affect hiring prospects, misidentify innocent people, and give unaccountable actors in the private sector or law enforcement apparatus greater information about our personal lives, without our consent. Already some cities have already banned the technology, and Congress is expected to vote on legislation that would forbid face recognition in government-owned public housing..All of this, however, makes clear that it’s not exactly clear what the best way to stop this is. Do we use more data to empower problematic technology? Do we use algorithms to de-bias other algorithms? Do we risk continuing to disrupt people’s lives while we figure this thing out? Maybe all this means that the answer is that we can’t, at least not without first questioning whether such fundamentally problematic technology should exist at all..By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.\n"}
{"prompt":"An audio deepfake was created of a CEO asking for money\n\n###\n\n","completion":" In June, an employee at a tech company received a strange voicemail from a person that identified himself as the CEO, asking for “immediate assistance to finalize an urgent business deal.” As it turns out, despite sounding almost like the CEO, the voicemail was actually created with computer software. It was an audio deepfake, according to a security company that investigated the incident..NISOS, a security consulting company based in Alexandria, Virginia, analyzed the voicemail and determined that it was fake, a synthetic audio designed to trick the receiver, the company told Motherboard. On Thursday, NISOS published a report on the incident, which it shared with Motherboard in advance. .For the last three years, journalists and researchers have spent a lot of time worrying about the possibility of deepfake videos—forged clips where faces are swapped for those of celebrities or politicians using artificial intelligence—influencing elections or causing a major international incident. In reality, that hasn’t happened, and deepfakes have mostly been used to create non-consensual porn videos. .Deepfake audio has been demonstrated in some flashy tech demos. But the technology is also beginning to be used in the criminal world. Last year, a company revealed that an employee had made a transfer of $240,000 after receiving a call from someone who appeared to be their CEO. The call was actually a deepfake, according to the company’s insurer. .The employee who received the voicemail, however, did not fall for it and flagged it to the company, which called in NISOS to investigate. NISOS researchers analyzed the audio with a spectrogram tool called Spectrum3d in an attempt to detect any anomalies. .“You could tell there was something wrong about the audio,” NISOS researcher Dev Badlu told Motherboard in a phone call. “It looks like they basically took every single word, chopped it up, and then pasted them back in together.”.Badlu said he could tell it was fake because there were a lot of stark peaks and valleys within the audio, which is not normal in regular speech. Moreover, he added, when he lowered the volume of the alleged CEO, the background was “dead silent,” there was no background noise whatsoever, which was a clear sign of a forgery. .Rob Volkert, another researcher at NISOS, said they think the criminals were trying the technology out to see if the targets would give them a call back. In other words, he said, this was just step one of a presumably more complex operation that was relatively close to being successful. .“It definitely sounds human. They checked that box as far as: does it sound more robotic or more human? I would say more human,” Volkert said. “But it doesn’t sound like the CEO enough.” .Even though this deepfake audio got caught, Volkert, Badlu and their colleagues believe this is a sign that criminals are starting to experiment with the technology, and we may see more attempts like this one. .“The ability to generate synthetic audio extends an e-criminal’s toolkit and the criminal at the end of the day still has to effectively use social engineering tactics to induce someone into taking an action.” NISOS wrote in its report. “Criminals and potentially broader nation state actors also learn from each other, so as these high-profile cases gain more notoriety and success, we anticipate more illicit actors trying them and learning from others who have paved the way.”.By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.\n"}
{"prompt":"An AI hiring firm says it can predict job hopping based on your interviews\n\n###\n\n","completion":" Since the onset of the pandemic, a growing number of companies have turned to AI to assist with their hiring. The most common systems involve using face-scanning algorithms, games, questions, or other evaluations to help determine which candidates to interview. .While activists and scholars warn that these screening tools can perpetuate discrimination, the makers themselves argue that algorithmic hiring helps correct for human biases. Algorithms can be tested and tweaked, whereas human biases are much harder to correct—or so the thinking goes. In a December 2019 paper, researchers at Cornell reviewed the landscape of algorithmic screening companies to analyze their claims and practices. Of the 18 they identified with English-language websites, the majority marketed themselves as a fairer alternative to human-based hiring, suggesting that they were latching onto the heightened concern around these issues to tout their tools’ benefits and get more customers..But discrimination isn’t the only concern with algorithmic hiring, and some scholars worry that marketing language that focuses on bias lets companies off the hook on other issues, such as workers’ rights. A new preprint from one of these firms now serves as an important reminder: “We should not let the attention that people have begun to pay to bias and discrimination issues actually crowd out the fact that there are a bunch of other issues,” says Solon Barocas, an assistant professor at Cornell University and principal researcher at Microsoft Research, who studies algorithmic fairness and accountability. .The firm in question is Australia-based PredictiveHire, founded in October 2013. It offers a chatbot that asks candidates a series of open-ended questions. It then analyzes their responses to assess job-related personality traits like “drive,” “initiative,” and “resilience.” According to the firm’s CEO, Barbara Hyman, its clients are employers that must manage large numbers of applications, such as those in retail, sales, call centers, and health care. As the Cornell study found, it also actively uses promises of fairer hiring in its marketing language. On its home page, it boldly advertises: “Meet Phai. Your co-pilot in hiring. Making interviews SUPER FAST. INCLUSIVE, AT LAST. FINALLY, WITHOUT BIAS.”.As we’ve written before, the idea of “bias-free” algorithms is highly misleading. But PredictiveHire’s latest research is troubling for a different reason. It is focused on building a new machine-learning model that seeks to predict a candidate’s likelihood of job hopping, the practice of changing jobs more frequently than an employer desires. The work follows the company’s recent peer-reviewed research that looked at how open-ended interview questions correlate with personality (in and of itself a highly contested practice). Because organizational psychologists have already shown a link between personality and job hopping, Hyman says, the company wanted to test whether they could use their existing data for the prediction. “Employee retention is a huge focus for many companies that we work with given the costs of high employee churn, estimated at 16% of the cost of each employee’s salary,” she adds..The study used the free-text responses from 45,899 candidates who had used PredictiveHire’s chatbot. Applicants had originally been asked five to seven open-ended questions and self-rating questions about their past experience and situational judgment. These included questions meant to tease out traits that studies have previously shown to correlate strongly with job-hopping tendencies, such as being more open to experience, less practical, and less down to earth. The company researchers claim the model was able to predict job hopping with statistical significance. PredictiveHire’s website is already advertising this work as a “flight risk” assessment that is “coming soon.”.PredictiveHire’s new work is a prime example of what Nathan Newman argues is one of the biggest adverse impacts of big data on labor. Newman, an adjunct associate professor at the John Jay College of Criminal Justice, wrote in a 2017 law paper that beyond the concerns about employment discrimination, big-data analysis had also been used in myriad ways to drive down workers’ wages..Machine-learning-based personality tests, for example, are increasingly being used in hiring to screen out potential employees who have a higher likelihood of agitating for increased wages or supporting unionization. Employers are increasingly monitoring employees’ emails, chats, and other data to assess which might leave and calculate the minimum pay increase needed to make them stay. And algorithmic management systems like Uber’s are decentralizing workers away from offices and digital convening spaces that allow them to coordinate with one another and collectively demand better treatment and pay..None of these examples should be surprising, Newman argued. They are simply a modern manifestation of what employers have historically done to suppress wages by targeting and breaking up union activities. The use of personality assessments in hiring, which dates back to the 1930s in the US, in fact began as a mechanism to weed out people most likely to become labor organizers. The tests became particularly popular in the 1960s and ’70s once organizational psychologists had refined them to assess workers for their union sympathies..In this context, PredictiveHire’s fight-risk assessment is just another example of this trend. “Job hopping, or the threat of job hopping,” points out Barocas, “is one of the main ways that workers are able to increase their income.” The company even built its assessment on personality screenings designed by organizational psychologists..Barocas doesn’t necessarily advocate tossing out the tools altogether. He believes the goal of making hiring work better for everyone is a noble one and could be achieved if regulators mandate greater transparency. Currently none of them have received rigorous, peer-reviewed evaluation, he says. But if firms were more forthcoming about their practices and submitted their tools for such validation, it could help hold them accountable. It could also help scholars engage more readily with firms to study the tools’ impacts on both labor and discrimination..“Despite all my own work for the past couple of years expressing concerns about this stuff,” he says, “I actually believe that a lot of these tools could significantly improve the current state of affairs.” .An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.\n"}
{"prompt":"Images of Mr.Wozniak were used to steal cryptocurrency\n\n###\n\n","completion":" The scam, similar to one used in a Twitter hack, requested people send cryptocurrency, falsely promising they would receive twice as much back..As well as Mr Wozniak, the law firm Cotchett, Pitre and McCarthy is also representing 17 others affected by the fraud, from the US, UK, Canada, Japan, Malaysia, China and Europe..YouTube, like Google, seems to rely on algorithms and no special effort requiring custom software employed quickly in these cases of criminal activity..But last month, fraudsters stole at least $150,000 (£117,000) by impersonating Elon Musks Space X YouTube channel and hosting a live-streamed event asking viewers to send Bitcoin..In stark contrast, the complaint alleges that YouTube knowingly allowed the Bitcoin scam to go on for months promoted it and profited from it by selling targeted advertising..In a similar case brought by cryptocurrency company Ripple Labs, YouTubes legal team successfully argued the platform was not liable for any content - including scams - provided by third parties.\n"}
{"prompt":"Scammers used fake social media profiles and dupped victims in cryptocurrency scam\n\n###\n\n","completion":" SAN FRANCISCO (Reuters) - Blockchain firm Ripple sued Alphabet Inc’s YouTube on Tuesday, alleging the video-sharing platform failed to protect consumers from cryptocurrency “giveaway” scams that use fake social media profiles to dupe victims into sending money..The company says scammers on YouTube have been impersonating Ripple and its CEO, Brad Garlinghouse, to bait viewers into sending thousands of dollars worth of XRP, a cryptocurrency championed by Ripple, according to a court filing..The scammers promise to send back up to 5 million XRP, worth nearly $1 million, but victims who participate in the fake “giveaways” never receive any money in return, said the filing..The lawsuit appears poised to raise a fresh challenge around the controversial Section 230 of the Communications Decency Act, which shields Google, Facebook and other internet companies from liability for material that users post on their platforms..Regulators in Washington are reconsidering the need for the law’s broad immunity, which helped U.S. tech companies grow but is viewed increasingly as a shelter enabling some of the world’s richest companies to avoid investments to curb crime, extremism and misinformation online..“For every scam, giveaway, fake conspiracy that is taken down, multiple more pop up nearly immediately,” Ripple said in a blog post. “YouTube and other big technology and social media platforms must be held accountable for not implementing sufficient processes for fighting these scams.”.Garlinghouse, a long-time Silicon Valley executive, said he wants the case to be a “call to action” for the social media industry, arguing the law was written “at a time when we didn’t understand how these platforms could be abused.”.He said he had seen similar impersonations on platforms including Facebook’s photo-sharing app Instagram, but targeted YouTube in the lawsuit because it was the “slowest to respond and least proactive.”.YouTube spokesman Alex Joseph said the company takes abuse of the platform seriously and acts “quickly when we detect violations of our policies, such as scams or impersonation.”.Founded in 2012, Ripple is one of the best known companies that develop so-called blockchain technology, or the system underpinning cryptocurrencies. The company develops blockchain systems to help financial services firms carry out cross-border payments using XRP..Its filing, in the U.S. District Court for the Northern District of California, says YouTube’s failure to address the “pervasive and injurious fraud” has harmed the reputation of both Ripple and Garlinghouse..Ripple said millions of people have viewed the scams on YouTube, which enabled the fraud to proliferate by ignoring its demands for the videos to be taken down and continuing to sell ads to the scammers..YouTube also awarded a “verification badge” to a hacked channel displaying a photo of Garlinghouse as its profile picture, falsely indicating to viewers that the account was legitimate, the filing said.\n"}
{"prompt":"A network of 19 fake journalists have published fake conservative publications\n\n###\n\n","completion":" Reuters reports that somebody used deepfake tech and a false name and biography to invent the persona of a journalist — and then got the sock puppet’s work published in several international newspapers..Whoever’s behind the operation — Reuters was not able to track them down — managed to publish six articles and editorials in the Jerusalem Post and the Times of Israel while posting as an entirely fictitious author, according to the investigation. The dupe serves as a warning about how easily disinformation can spread online — and how new tech can enable it..According to online profiles, Oliver Taylor is a student at the University of Birmingham who loves politics and coffee. But no actual records of Taylor exist, his phone number isn’t connected, and neither Reuters nor the publications that ran his work could verify his existence..It’s not unprecedented for propagandists to invent personas for journalists. Earlier this month, for instance, an investigation by The Daily Beast found a network of 19 fake journalists that had been used to publish political content in a wide variety of conservative publications — including several that appeared to have used deepfake tech..The Jerusalem Post told Reuters that editors didn’t vette Taylor very hard. And while his articles didn’t gain much traction, the phenomenon points to a looming threat in which operators could hide behind deepfake technology while publishing disinformation in reputable outlets..“Absolutely we need to screen out impostors and up our defenses,” Times of Israel Opinion Editor Miriam Herschlag told Reuters. “But I don’t want to set up these barriers that prevent new voices from being heard.”\n"}
{"prompt":"Facebook civil rights audit urges ‘mandatory’ algorithmic bias detection\n\n###\n\n","completion":" We are excited to bring Transform 2022 back in-person July 19 and virtually July 20 - 28. Join AI and data leaders for insightful talks and exciting networking opportunities. Register today!.When it comes to Facebook’s progress on civil rights issues, an independent review found the company’s efforts to detect algorithmic bias fall dangerously short and leave users vulnerable to manipulation..According to the audit released earlier today, Facebook’s efforts to detect algorithmic bias remain primarily in pilot projects conducted by only a handful of teams. The authors of the report, civil rights attorneys Laura Murphy and Megan Cacace, note that the company is increasingly reliant on artificial intelligence for such tasks as predicting which ads users might click on and weeding out harmful content..But these tools, as well as other tentative efforts Facebook has made in areas like diversity of its AI teams, must go much further and faster, the report says. While the group looked uniquely at Facebook during its two-year review, any company embracing AI would do well to look at algorithmic bias issues..“Facebook has an existing responsibility to ensure that the algorithms and machine learning models that can have important impacts on billions of people do not have unfair or adverse consequences,” the report says. “The Auditors think Facebook needs to approach these issues with a greater sense of urgency.”.The report comes as Facebook faces a historic advertising boycott. The “Stop Hate for Profit” campaign is backed by more than 396 advertisers, who have halted spending on the platform to demand Facebook take bolder steps against racism, misogyny, and disinformation..Earlier this week, Facebook CEO Mark Zuckerberg met with civil rights groups but insisted his company would not respond to financial pressure, leaving attendees disappointed..In a blog post, COO Sheryl Sandberg sought to score points by claiming Facebook is the “first social media company to undertake an audit of this kind.” She also nodded toward the timing of the report, which was commissioned two years ago. Her post — “Making Progress on Civil Rights — But Still a Long Way to Go” — emphasized Facebook’s view that it is fighting the good fight..“There are no quick fixes to these issues — nor should there be,” Sandberg wrote. “This audit has been a deep analysis of how we can strengthen and advance civil rights at every level of our company — but it is the beginning of the journey, not the end. What has become increasingly clear is that we have a long way to go. As hard as it has been to have our shortcomings exposed by experts, it has undoubtedly been a really important process for our company. We would urge companies in our industry and beyond to do the same.”.“Many in the civil rights community have become disheartened, frustrated, and angry after years of engagement where they implored the company to do more to advance equality and fight discrimination, while also safeguarding free expression,” the authors wrote..The report dissects Facebook’s work on civil rights accountability, elections, census, content moderation, diversity, and advertising. But it also gives special attention to the subject of algorithmic bias..“AI is often presented as objective, scientific, and accurate, but in many cases it is not,” the report says. “Algorithms are created by people who inevitably have biases and assumptions, and those biases can be injected into algorithms through decisions about what data is important or how the algorithm is structured, and by trusting data that reflects past practices, existing or historic inequalities, assumptions, or stereotypes. Algorithms can also drive and exacerbate unnecessary adverse disparities … As algorithms become more ubiquitous in our society, it becomes increasingly imperative to ensure that they are fair, unbiased, and non-discriminatory, and that they do not merely magnify preexisting stereotypes or disparities.”.The authors highlighted Facebook’s Responsible AI (RAI) efforts, led by a team of “ethicists, social and political scientists, policy experts, AI researchers, and engineers focused on understanding fairness and inclusion concerns associated with the deployment of AI in Facebook products.”.Part of that RAI work involves developing tools and resources that can be used across the company to ensure AI fairness. To date, the group has developed a “four-pronged approach to fairness and inclusion in AI at Facebook.”.As part of the first pillar, Facebook has created the Fairness Flow tool to assess algorithms by detecting unintended problems with the underlying data and spotting flawed predictions. But Fairness Flow is still in a pilot stage, and the teams with access use it on a purely voluntary basis. Late last year, Facebook also began a fairness consultation pilot project to allow teams that detect a bias issue in a product to reach out internally to teams with more expertise for feedback and advice. While the authors saluted these steps, they also urged Facebook to expand such programs across the company and make their use mandatory..“Auditors strongly believe that processes and guidance designed to prompt issue-spotting and help resolve fairness concerns must be mandatory (not voluntary) and companywide,” the report says. “That is, all teams building models should be required to follow comprehensive best practice guidance, and existing algorithms and machine learning models should be regularly tested. This includes both guidance in building models and systems for testing models.”.The company has also created an AI Task Force to lead initiatives for improving employee diversity. Facebook is now funding a deep learning course at Georgia Tech to increase the pipeline of underrepresented job candidates. It’s also in discussions with several other universities to expand the program. And its tapping nonprofits, research, and advocacy groups to broaden its hiring pool..But again, the review found these initiatives to be too limited in scope and called for an expansion of hiring efforts, as well as greater training and education across the company..“While the Auditors believe it is important for Facebook to have a team dedicated to working on AI fairness and bias issues, ensuring fairness and non-discrimination should also be a responsibility for all teams,” the report says. “To that end, the Auditors recommend that training focused on understanding and mitigating against sources of bias and discrimination in AI should be mandatory for all teams building algorithms and machine-learning models at Facebook and part of Facebook’s initial onboarding process.”.VentureBeats mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. Learn more about membership..Hear from senior executives at some of the world’s leading enterprises about their experience with applied Data & AI and the strategies they’ve adopted for success. \n"}
{"prompt":"An AI tool which reconstructed a pixelated picture of Barack Obama to look like a white man perfectly illustrates racial bias in algorithms\n\n###\n\n","completion":" A new artificial intelligence tool for making clear pictures of peoples faces from pixelated images has become a prime, if unwitting, example of algorithmic racial bias..The tool, called Face Depixelizer, was built by a coder and put out on Twitter. Its built to take pixelated photographs of people and accurately reconstruct a sharp and accurate image of their face using machine learning, a type of AI..Users started to spot however that the system was not particularly accurate when it came to processing Black faces. One shared an image of what happened after they input a pixelated picture of Barack Obama — Face Depixelizer turned him into a white man..Machine learning blogger Robert Osazuwa Ness then ran other pictures of people of color through the system including himself, Rep. Alexandria-Ocasio Cortez, and actress Lucy Liu. Face Depixelizer consistently reconstructed their faces to look white..Machine learning algorithms are trained on large data sets, from which they identify patterns and teach themselves what theyre supposed to be looking for. A lack of diversity in widely-used datasets means these systems are often predominantly trained on images of white men, meaning they infer that white male characteristics are the default..Although Face Depixelizer is an illustration of how these systems can fail at identifying Black peoples faces, the implications of algorithmic racial bias go far beyond a depixelating proof-of-concept tool circulating on Twitter..One example of how algorithmic bias could end up harming Black people in the real world is through facial recognition. Earlier this month IBM, Amazon, and Microsoft all agreed to suspend sales of their facial recognition software to police after years of campaigning from AI researchers and civil rights groups..Quite apart from the fact police are more likely to use facial recognition to target communities of color, studies have shown commercially available facial recognition software is far more likely to misidentify women and people with darker skin tones..This affects the kind of AI which was used to build Depixeliser, called Generative Adversarial Network or GAN. GANs work by pitting two algorithms against one another, with one algorithm generating fake images and the other one trying to spot the fakes. Mode collapse effectively means that even if theres only a small bias in the data, it will be exacerbated by the GANs, as the first algorithm will go for the easiest way to fool the second one.\n"}
{"prompt":"The algorithm creates racial bias because it charges higher prices if the pick up point or destination is in a ethnic minority neighborhood\n\n###\n\n","completion":" By analysing transport and census data in Chicago, Aylin Caliskan and Akshat Pandey at The George Washington University in Washington DC have found that ride-hailing companies charge a higher price per mile for a trip if the pick-up point or destination is a neighbourhood with a higher proportion of ethnic minority residents than for those with predominantly white residents..“Basically, if you’re going to a neighbourhood where there’s a large African-American population, you’re going to pay a higher fare price for your ride,” says Caliskan..Unlike traditional taxis, ride-hailing services have dynamic fares, which are calculated based on factors including the length of the trip as well as local demand – although it is unclear what other factors these algorithms take into consideration because ride-hailing companies don’t make all of their data available..The researchers analysed data from more than 100 million trips taken in Chicago through ride-hailing apps between November 2018 and December 2019. Each ride contained information including pick-up and drop-off location, duration, cost and whether the ride was an individual or shared trip. The data doesn’t include demographic details such as the ethnicity of the rider..The duo compared the trip data against information from the US Census Bureau’s American Community Survey, which provides aggregate statistics about neighbourhoods, including population, ethnicity breakdown, education levels and median house prices..They found that prices per mile were higher on average if the trip pick-up or drop-off location was in a neighbourhood with a lower proportion of white residents, a lower median house price, or lower average educational attainment..“Even in the absence of identity being explicitly considered in how an algorithm’s results are decided, the structural and historical nature of racism and the way that it informs geography, opportunity and life chances mean that racial disparities can still appear,” says Os Keyes at the University of Washington in Seattle..“Chicago, the site of this analysis, is a case in point: as a result of – amongst other things – redlining practices, it remains highly geographically segregated,” says Keyes. Redlining is practice in which mortgage lenders refuse to offer loans in certain neighbourhoods..“This should cause us to further question studies of ‘fairness’ and ‘bias’ in algorithms which promise to end algorithmic racism by simply not mentioning race,” says Keyes..The researchers found no statistical link to suggest that neighbourhoods with higher proportions of ethnic minorities had higher demand for rides, which could potentially explain the higher fare prices..“We recognise that systemic biases are deeply rooted in society, and appreciate studies like this that look to understand where technology can unintentionally discriminate,” said a Lyft spokesperson. “There are many factors that go into pricing – time of day, trip purposes, and more – and it doesn’t appear that this study takes these into account. We are eager to review the full results when they are published to help us continue to prioritise equity in our technology.”.“Uber does not condone discrimination on our platform in any form, whether through algorithms or decisions made by our users,” said an Uber spokesperson. “We commend studies that try to better understand the impact of dynamic pricing so as to better serve communities more equitably. It’s important not to equate correlation for causation and there may be a number of relevant factors that weren’t taken into account for this analysis, such as correlations with land-use\/neighborhood patterns, trip purposes, time of day, and other effects.”.Under US law, it is illegal to discriminate against an individual on the basis of protected attributes, including race. The study’s findings are problematic, says Caliskan. “Even though these algorithms are supposed to be fair and they are not using protected attributes, they seem to have a significant impact on these neighbourhoods.”.“This study shows how algorithmic bias by postcode and race can creep into even the most unexpected places,” says Noel Sharkey at the University of Sheffield, UK. “It is yet another example in a long list of how ethnicity and race bias has found a new home in computer software. There is no excuse for automation biases and such systems should be shut down until such time as they can demonstrate fairness and equality,” he adds.\n"}
{"prompt":"Employees cannot opt out and this system can violate privacy\n\n###\n\n","completion":" Amazon, which is currently being sued for allegedly failing to protect workers from COVID-19, has unveiled a new AI tool it says will help employees follow social distancing rules..The company’s “Distance Assistant” combines a TV screen, depth sensors, and AI-enabled camera to track employees’ movements and give them feedback in real time. When workers come closer than six feet to one another, circles around their feet flash red on the TV, indicating to employees that they should move to a safe distance apart. The devices are self-contained, meaning they can be deployed quickly where needed and moved about..Amazon compares the system to radar speed checks which give drivers instant feedback on their driving. The assistants have been tested at a “handful” of the company’s buildings, said Brad Porter, vice president of Amazon Robotics, in a blog post, and the firm plans to roll out “hundreds” more to new locations in the coming weeks. .Importantly, Amazon also says it will be open-sourcing the technology, allowing other companies to quickly replicate and deploy these devices in a range of locations..Amazon isn’t the only company using machine learning in this way. A large number of firms offering AI video analytics and surveillance have created similar social-distancing tools since the coronavirus outbreak began. Some startups have also turned to physical solutions, like bracelets and pendants which use Bluetooth signals to sense proximity and then buzz or beep to remind workers when they break social distancing guidelines. .Although these solutions will be necessary for workers to return to busy facilities like warehouses, many privacy experts worry their introduction will normalize greater levels of surveillance. Many of these solutions will produce detailed data of workers’ movements throughout the day, allowing managers to hound employees in the name of productivity. Workers will also have no choice but to be tracked in this way if they want to keep their job..Amazon’s involvement in this sort of technology will raise suspicions as the company is often criticized for the grueling working conditions in its facilities. In 2018, it even patented a wristband that would track workers’ movements in real time, directing not just which task they should do next, but if their hands are moving towards the wrong shelf or bin..The company’s description of the Distance Assistant as a ”standalone unit” that only requires power suggests it’s not storing any data about worker’s movement, but we’ve contacted the company to confirm what information, if any, might be retained.\n"}
{"prompt":"Facebook took down photo that revealed history of slavery in Australia\n\n###\n\n","completion":" Facebook incorrectly removed a post critical of the prime minister’s comments regarding slavery in Australia that featured a photograph of Aboriginal men in neck chains from the late 1800s, claiming the photo featured nudity..On Friday he backed down from those comments and acknowledged the history of blackbirding in Australia – where people were coerced into working as indentured labourers, often through deception or force. He said his Thursday comments related to the principles that existed when the colony of New South Wales was founded – that there was to be no lawful slavery in Australia..Following his comments on Thursday, debate on social media was rife with discussion of Australia’s history of slavery. One Australian user posted about the topic on his personal Facebook profile, including a photo of nine Aboriginal men chained together by their necks wearing loin cloths outside Roebourne Gaol in 1896..“Kidnapped, ripped from the arms of their loved ones and forced into back-breaking labour: The brutal reality of life as a Kanaka worker - but Scott Morrison claims ‘there was no slavery in Australia’,” the post stated.”.The photo from the state library of Western Australia refers to the men as prisoners, and the photo had been shared in stories about Morrison’s comments in the Daily Mail..The post was removed by Facebook, and the man had his account restricted, with Facebook claiming the photo contained nudity and was in breach of the social media site’s community standards..The post was restored after Guardian Australia asked Facebook about whether the photo had been flagged in error. Facebook apologised to the user late on Friday and restored the post..According to Facebook’s latest community standards report, between January and March this year, Facebook removed 39.5m pieces of content for adult nudity or sexual activity, and of that 99.2% was removed by Facebook automatically, without a user reporting it..In 2016, Facebook backed down on a ban on the famous photo of nine-year-old Kim Phúc running away from a napalm attack during the Vietnam war on the grounds of the nudity in the photograph, after a strong backlash over claims of censorship..Facebook also faced criticism in Australia in 2016 for blocking the account of Arrente writer Celeste Liddle four times after she shared a trailer for an Indigenous comedy show that featured images of topless desert women. Trolls kept reporting the content as “indecent”, and Facebook kept removing the trailer and locking Liddle out of her account.\n"}
{"prompt":"Instagram posts featuring 'scantily clad' women are 54% more likely to appear at the top of a user's newsfeed, according to a report that says the firm's algorithm boosts semi-nude images\n\n###\n\n","completion":" Algorithm Watch, a non-profit that aims to shed light on algorithm decision-making, found that women in underwear or bikinis are 54 percent more likely to appear and bare-chested men are 28 percent more likely..The report suggests that the occurrence of the images could be that the users engage more with these type of posts or it could be Instagrams way of encouraging influencers to share more semi-nude photos..Algorithm Watch , a non-profit that aims to shed light on algorithm decision-making, found that women in underwear or bikinis are 54 percent more likely to appear and bare-chested men are 28 percent more likely. Pictured is a post from Anastasia Karanikolaou.Algorithm Watch asked 26 volunteers to download a browser add-on that automatically opens their Instagram at regular intervals and records which posts appear at the top of their newsfeeds..Algorithm Watch asked 26 volunteers to download a browser add-on that automatically opens their Instagram at regular intervals and records which posts appear at the top of their newsfeeds.The report suggests that the occurrence of the images could be that the users engage more with these type of posts or it could be Instagrams way of encouraging influencers to share more semi-nude photos.If Instagram were not meddling with the algorithm, the diversity of posts in the newsfeed of users should match the diversity of the posts by the content creators they follow, Algorithm Watch shared in a blog post..And if Instagram personalized the newsfeed of each user according to their personal tastes, the diversity of posts in their newsfeeds should be skewed in a different way for each user..Over the three months, 362, or 21 percent, of the images from the 37 influencers showed them semi-naked – bare-chest men and women in underwear or swimwear..The team also found that pictures of scantily-clad were 54 percent more likely to appear in the volunteers feed, whereas half naked men were 28 percent more likely..The team also found that pictures of women wearing the scantily clad clothing were 54 percent more likely to appear in the volunteers feed, whereas half naked men were 28 percent more likely (Pictured is an image shared by Jason Momoa on his Instagram page).Instagram has stated in the past that its algorithm organizes posts on what a users cares more about, the firms patent notes that it ranks images on what it thinks the user wants to see..Whether or not users see the pictures posted by the accounts they follow depends not only on their past behavior, but also on what Instagram believes is most engaging for other users of the platform, explained Algorithm Watch..The subtle difference between what is encouraged and what is forbidden is decided by unaudited, and likely biased, computer vision algorithms, reads the Algorithm Watch blog post..Every time they post a picture, content creators must thread this very fine line between revealing enough to reach their followers but not revealing so much that they get booted off the platform..         Johnny Depp called ex-girlfriend Vanessa Paradis an extortionist French c*** in email to Elton John and said she was trying to brainwash their two children: Amber Heards lawyers unleash barrage of violent messages in blockbuster lawsuit      \n"}
{"prompt":"OpenAI text generators are going commercial although they have found models to be unpredictable and could land in bad hands.\n\n###\n\n","completion":" Last spring, artificial intelligence research institute OpenAI said it had made software so good at generating text—including fake news articles—that it was too dangerous to release. That line in the sand was soon erased when two recent master’s grads recreated the software and OpenAI released the original, saying awareness of the risks had grown and it hadn’t seen evidence of misuse..Now the lab is back with a more powerful text generator and a new pitch: Pay us to put it to work in your business. Thursday, OpenAI launched a cloud service that a handful of companies are already using to improve search or provide feedback on answers to math problems. It’s a test of a new way of programming AI and the lab’s unusual business model..OpenAI was founded as a nonprofit in 2015 by Elon Musk and other Silicon Valley notables to ensure that future superhuman AI was a benign force. The Tesla CEO parted ways with the lab in 2018, and last year it became a for-profit company and took a $1 billion investment from Microsoft. OpenAI’s leaders claim that only by commercializing its research for the benefit of investors can it raise the billions needed to keep pace on the frontiers of AI..Thursday’s launch of OpenAI’s first commercial product completes the metamorphosis. A research institute created to compete with tech giants on superhuman AI is now challenging them in the more mundane arena of selling cloud services to businesses..OpenAI’s service is built on a machine-learning technique that has made computers much better with language over the past two years. Machine-learning algorithms are directed to analyze vast collections of text scraped from the web to discover the statistical patterns in language use. The software can then be tuned to perform tasks like answering factual questions or summarizing documents..Google has tapped the technology to improve how its search engine handles long queries, and Microsoft Office uses it to spot grammar glitches. OpenAI has focused on pushing the technique to greater scale and making software that generates text. Given a snatch of writing, it builds on it, unspooling sentences with similar statistical properties. The results can be uncannily smooth, if sometimes unmoored from reality..Text generators like that can be fun—try one here—but haven’t previously seen much commercial use. OpenAI CEO Sam Altman says the latest generation is powerful and flexible enough for real work. “This is the first time we’ve got something we think is good enough to make into a product,” he says..OpenAI’s new text generators are trained using a collection of almost a trillion words gathered from the web and digitized books, on a supercomputer with hundreds of thousands of processors the company paid Microsoft to build, effectively returning some of the company’s $1 billion investment to its source..The service is more open-ended than most AI cloud services, which usually perform one task, such as translation or image tagging, and are controlled with specific commands. Programmers who want to tap OpenAI’s technology simply submit human-readable text and get newly generated text back..That may sound limiting, but by crafting the right input it’s possible to steer the software to perform different tasks. The goal is to try and massage it to riff on the statistical language patterns from a particular part of the internet..Submitting examples of paragraphs rewritten for elementary schoolers followed by an unsimplified passage prompts the service to rewrite it to be easier to read. The service can answer factual questions or function as a chatbot if supplied with example Q&A pairs or turns of dialog that might direct the software to draw on its experience of factual statements or conversations..“The big mental shift is, it’s much more like talking to a human than formatting things for a machine,” says Greg Brockman, OpenAI’s chief technology officer. “You give it a few questions and answers and suddenly it’s in Q&A mode.”.Nick Frosst, a researcher working on language machine learning who previously worked at Google, says that novel way of working with AI could widen the pool of people experimenting with language technology. “It’s exciting that you can do that,” he says. “It’s how most people think AI should work.”.OpenAI is offering its service for free for two months and already has some users. Algolia, a startup that builds internal search engines for apps and websites, uses it to improve its understanding of complex search strings..Others are using an additional service in which OpenAI “fine-tunes” a version of the software to a specific task with additional data. Math education site Art of Problem Solving uses that to suggest comments to send students on their submissions, speeding up the work of graders..One unknown is its reliability. “These models are somewhat unpredictable,” says Robert Dale, of consultants Language Technology Group. OpenAI’s software can recreate the patterns of text but doesn’t have a commonsense understanding of the world. Its versatility can be a liability as well as an asset. Occasional clangers are of little consequence for some uses, such as predictive text, but could be deal breakers in others, such as a customer support chatbot..One certainty about OpenAI’s technology is that it can talk dirty and nasty. Its training on vast swaths of the internet makes the software well versed in unsavory language such as casual or aggressive racism, and it can be prompted to recreate them. The results can be reminiscent of how Twitter users prodded a notorious Microsoft bot called Tay to make racist comments..When WIRED provided the service with two sentences from message board 4chan accusing Republicans of being “spineless” and not taking action on “Clinton, Pedos, Censorship or Riots,” OpenAI’s service escalated, riffing that “we are being beaten and raped … vast immigration started in the 60s and never stopped.”.OpenAI says it will vet customers to prevent people from using the service for things like spam or harassment. Some customers have built filters to block the technology from producing toxic language, and OpenAI is working on safety features of its own..Altman doesn’t expect OpenAI’s product to be lucrative right away but says it could develop into a significant revenue source in a few years as the lab makes improvements. Microsoft’s stake in the lab could help. OpenAI built its new service on Microsoft’s Azure cloud platform; it could see much wider use if Microsoft offered it as an AI service..Altman accepted closer relations with Microsoft as a possibility but declined to elaborate. When WIRED prompted the lab’s new software to fill out the details on “OpenAI and Microsoft’s first joint commercial venture” it described a “game called Copilot that allows two people to play a racing game with one person controlling the gas pedal and the other the brakes.”\n"}
{"prompt":"There could be potential abuse on this platform similar to Deepfakes\n\n###\n\n","completion":" The Wish You Were Here bots tricked humans trying to spot the fake up to 47 per cent of the time - could this tech lead to more claims of doctored deepfake images?.acebook is developing ways to insert computer-generated likenesses of people into photographs in an artificial intelligence project called “Wish You Were Here”..The social network wants to make it easier for users to “blend” people into their images even if they were nowhere near the original subjects at the time..In a study, AI software created a map around the person due for insertion into a scene, then approximated the pose of the people in the photo before adding the third partys approximated likeness into the shot..It tricked volunteers trying to spot the fake human nearly 43 per cent of the time on average, depending on group size, however, the image quality of people created by the AI was decidedly mixed..Fake images were created using three AI processes called Essence Generation, Multi-Conditioning Rendering and Face Refinement for “context-aware human generation”..These generated semantic maps” to try and maintain the existing scenes context before inserting scans of the new persons hair face, torso, clothes and shoes as a “photorealistic” cut-out..Researchers said they found a “convincing ability” of the AI to add people into existing photos “while preserving the overall image quality”, but the software struggled recreating some human features, such as hairstyles..The AI was trained on more than 20,000 sample photographs from an open-source gallery, resulting in nearly 53,600 example images for analysis..The research paper, due to be presented at the US Conference on Computer Vision and Pattern Recognition conference, is a collaboration between Facebook AI Research and Tel-Aviv University..Co-authors Oran Gafni, an AI research engineer for Facebook, and Professor Lior Wolf, of a faculty member of the university’s School of Computer Science, described the technique as a “novel method for inserting objects, specifically humans, into existing images” while “respecting the semantic context of the scene”..“In an extensive set of experiments, we demonstrate that the first of our networks can create poses that are indistinguishable from real poses, despite the need to take into account the social interactions in the scene.”.Facebook’s latest project follows Google’s work using AI methods to insert objects such as vehicles into images by predicting scale, shape and location.\n"}
{"prompt":"This error was unnoticed for six months and are repetitive.\n\n###\n\n","completion":" YouTube is automatically deleting comments that contain certain Chinese-language phrases related to criticism of the country’s ruling Communist Party (CCP). The company confirmed to The Verge this was happening in error and that it’s working to fix the issue..“Upon review by our teams, we have confirmed this was an error in our enforcement systems and we are working to fix it as quickly as possible,” said a YouTube spokesperson. The company did not elaborate on how or why this error came to be, but said it was not the result of any change in its moderation policy. .But if the deletions are the result of a simple mistake, then it’s one that’s gone unnoticed for six months. The Verge found evidence that comments were being deleted as early as October 2019, when the issue was raised on YouTube’s official help pages and multiple users confirmed that they had experienced the same problem..Comments left under videos or in live streams that contain the words “共匪” (“communist bandit”) or “五毛” (“50-cent party”) are automatically deleted in around 15 seconds, though their English language translations and Romanized Pinyin equivalents are not..The term “共匪” is an insult that dates back to China’s Nationalist government, while “五毛,” (or “wu mao”) is a derogatory slang term for internet users paid to direct online discussion away from criticism of the CCP. The name comes from claims that such commenters are paid 50 Chinese cents per post. .These phrases seem to have been accidentally added to YouTube’s comment filters, which automatically remove spam and offensive text. The comments are removed too quickly for human moderation and are deleted even if the banned phrases are used positively (e.g., “The 五毛 are doing a fantastic job”). YouTube says it’s been relying more on its automated filters in recent months due changes to its workforce brought about by the pandemic. .The accidental censorship is even more puzzling considering that YouTube is currently blocked in China, giving its parent company, Google, even less reason to censor comments critical of the CCP or apply moderation systems in accordance with Chinese censorship laws. .The automatic deletion of these phrases was highlighted on Tuesday by US technologist and former Oculus founder Palmer Luckey on Twitter. But earlier reports of the issue date back to the middle of May when they were spotted by human rights activist Jennifer Zeng. As mentioned above, though, The Verge also found complaints on YouTube’s official help pages dated to October 2019..Google has frequently been criticized for accommodating the wishes of the CCP by censoring content. Most notably, it created a prototype search engine known as Project Dragonfly that complied with Chinese state censorship. The project, which was never deployed, is part of the company’s long-running struggles to enter the Chinese market..When news of Dragonfly leaked in 2018 in a report from The Intercept, Google was criticized by politicians and its own employees for selling out its principles. During a Senate Judiciary Committee hearing in June 2019, the company said it had “terminated” the project and that it had “no plans to launch Search in China.”\n"}
{"prompt":"There is a potential for racial bias as well as accuracy concerns.\n\n###\n\n","completion":" A team of researchers from the Higher School of Economics University and Open University in Moscow, Russia claim they have demonstrated that an artificial intelligence can make accurate personality judgments based on selfies alone — more accurately than some humans..The researchers suggest the technology could be used to help match people up in online dating services or help companies sell products that are tailored to individual personalities..That’s apropos, because two co-authors listed on a paper about the research published today in Scientific Reports — a journal run by Nature — are affiliated with a Russian AI psychological profiling company called BestFitMe, which helps companies hire the right employees..As detailed in the paper, the team asked 12,000 volunteers to complete a questionnaire that they used to build a database of personality traits. To go along with that data, the volunteers also uploaded a total of 31,000 selfies..The questionnaire was based around the “Big Five” personality traits, five core traits that psychological researchers often use to describe subjects’ personalities, including openness to experience, conscientiousness, extroversion, agreeableness, and neuroticism..After training a neural network on the dataset, the researchers found that it could accurately predict personality traits based on “real-life photographs taken in uncontrolled conditions,” as they write in their paper..While accurate, the precision of their AI leaves something to be desired. They found that their AI “can can make a correct guess about the relative standing of two randomly chosen individuals on a personality dimension in 58% of cases.”.Strikingly, the researchers claim their AI is better at predicting the traits than humans. While rating personality traits by human “close relatives or colleagues” was far more accurate than when rated by strangers, they found that the AI “outperforms an average human rater who meets the target in person without any prior acquaintance,” according to the paper..Considering the woeful accuracy, and the fact that some of the authors listed on the study are working on commercializing similar tech, these results should be taken with a hefty grain of salt..Neural networks have generated some impressive results, but any research that draws self-serving conclusions — especially when they require some statistical gymnastics — should be treated with scrutiny..READ MORE: Artificial intelligence can make personality judgments based on photographs [National Research University Higher School of Economics]\n"}
{"prompt":"There is a concern for racial bias and privacy.\n\n###\n\n","completion":" When the world economy reopens and populations want to traverse borders again, some governments are going to want proof people aren’t coming in or leaving with Covid-19. Employers are also going to want to know their workforce isn’t going to be the epicenter of another outbreak. And so a handful of companies are bidding for business that will help the Trump and Johnson administrations on either side of the Atlantic keep tabs on travel (or attempted travel) of the infected..One of them is facial recognition startup FaceFirst. Located in Encino, California in 2007 and with $10.4 million funding, it’s been promoting the idea of a “coronavirus-immunity registry.” This will be based on a database run by medical providers, which will feed a smartphone app with your immunity status. Just as your iPhone opens if you show your face, the app will verify it’s you by using your face..The app will also tell employers and border control staff more about a person’s experience of Covid-19, says CEO Peter Trepp. It will know what kind of test you received, in case it was a defective one; it will include a record of whether you’ve been near infected folk or not; and it will note if you’ve had an antibody test too..This app would effectively act as what’s become known as a health or immunity “passport.” Though Trepp says taking temperatures is not enough, that could be another data point to add to the passport. “These are lots and lots of data points. And my belief is that collectively data points can be helpful in determining how you fill an aeroplane, do you fill an aeroplane with everyone we believe to be virus free, do you fill another aeroplane with everyone who has immunities to the virus,” he says..“The other benefit of this thing, of course, is to know who to vaccinate and who should be vaccinated first. And I dont mean people with privilege or money... I mean people who have jobs where they can infect other people, or frontline workers.”.Trepp doesn’t think there’s a decent alternative to the conundrum of keeping a second Covid-19 wave at bay. “The other solution, of course, is lets hire thousands of people and make phone calls and build a big Excel spreadsheet and just ask people whether theyve had it and whether theyve been tested. That is laughable, in my view… It doesnt work when you consider the power of a more centralized system.”.There are obvious anxieties about any system that involves monitoring of people’s medical records. This week the American Civil Liberties Union (ACLU) raised concerns about such immunity passports, saying it could create “a new health surveillance infrastructure that endangers privacy rights.” .On the privacy side, Trepp clarified that not only will any such system be opt-in only, any personally identifiable information will remain only on individuals’ smartphones “and will not be uploaded or available for collection by any organization or government agency. Therefore, no centralized surveillance database will be or can be established.”.But there are civil liberties issues with such a system. The ACLU noted that passports risk dividing workers into the immune and the non-immune. “The latter might never be eligible for a given job short of contracting and surviving Covid-19 if an immune worker is available to take the slot,” the ACLU warned. This could lead to perverse outcomes, such as people willingly contracting Covid-19 to try to get the antibodies they need to get a job. Then there’s an obvious scientific issue: we still don’t know enough about Covid-19 to be sure that having antibodies prevent getting the disease a second time or how long their protective qualities last..If it does ever get the green light, such a centralized system will have to have the backing of the White House and Congress, Trepp says. He tells Forbes he’s in “indirect discussions” with the U.S. government, though doesn’t elaborate. When Forbes asks whether the $10,000 the company has spent on lobbying, according to a Senate record from this April, Trepp says it wasn’t for discussing his passport idea. But they show the company has talked with officials at the Department of Health & Human Services (HHS) and the White House about “issues related to biometrics and facial recognition and entry and exit screening.”.In the U.K., Onfido has been pushing for a similar rollout. The startup, which has (according to Pitchbook data) secured $265 million in venture funding and is perhaps best known as the provider of the verification technology behind challenger bank Monzo, has been pitching “a system for citizens, guests and employees to have proof of immunity that is designed to help an individual prove their health status, but without them having to share any other personal information.” Similarly, Trepp says his system would have a focus on privacy and that the user would have control over their medical data. .Husayn Kassai, CEO and co-founder at Onfido, said in a statement that the technology “is used to tie a physical human being to their digital identity using just a photo of their ID and a selfie video. Once this is bound to a test result, the digital certificate could be displayed, like smartphone boarding passes.”.Onfido has been approached by the U.K. Parliament’s Science and Technology Committee to submit a proposal for immunity passports. When asked if the company was pushing the technology in the U.S. or elsewhere, he said no discussions are yet taking place, but added: “Were also consulting with other governments to make this process as seamless as possible.”.At least one country, Estonia, is now trialling the passports in the workplace via an app created by the founders of Bolt and TransferWise, according to a Reuters report. Others will likely follow in one form or another. Americans and Brits alike will soon be tested on how far they’re willing to forego some civil liberties for the sake of reopening the country safely.\n"}
{"prompt":"People did not know their images were used\n\n###\n\n","completion":" Alfred Ng was a senior reporter for CNET News. He was raised in Brooklyn and previously worked on the New York Daily Newss social media and breaking news teams..Your face mask selfies arent just getting seen by your friends and family -- theyre also getting collected by researchers looking to use them to improve facial recognition algorithms. CNET found thousands of face-masked selfies up for grabs in public data sets, with pictures taken directly from Instagram.  .The COVID-19 pandemic is causing a surge in people wearing face masks, and facial recognition companies are scrambling to keep up. Face masks cover up a significant portion of what facial recognition needs to identify and detect people -- essentially threatening the future of a multimillion-dollar industry unless the technology can learn to recognize people beyond the coverings..In April, researchers published the COVID19 Mask Image Dataset to Github, using more than 1,200 images collected from Instagram. A month earlier, researchers from China compiled a database with more than 5,000 masked photos they gathered online..The creators behind the April database used their AI startup Workaround to help comb through the images and properly label them with masks on or off, said Wafaa Arbash, the companys CEO..We were inspired by all the companies that were launching free tools and everything they can do to help, Arbash said. We have these public images from Instagram, so these are not private images. We were just searching and getting the right data.  .Facial recognition companies have long used peoples pictures without consent to train their algorithms. Civil liberty advocates contend that facial recognition technology threatens privacy and free speech, warning as well that there are almost no laws preventing abuse of the surveillance tools..Clearview AI, a controversial facial recognition company, claimed it has a First Amendment right to scrape more than 3 billion images from social networks to use for its database. .Governors in more than half of the US states are mandating face masks in public because the coverings help prevent the spread of COVID-19. The masks have also slowed down the spread of facial recognition, since the garments block key parts of your face that the technology usually analyzes. .Some facial recognition providers have turned to asking their own staffers to send in face-masked selfies, as well as editing masks on top of the photos that they already have. Digitally adding masks to photos is how the US National Institute of Standards and Technology plans on testing facial recognition algorithms. .But there are only so many employees a company can ask to take selfies, and edited face mask photos may not be as effective as organic images for training algorithms. Facial recognition companies also need a diverse set of pictures so the algorithms can better recognize women, people of color, people of different ages and a variety of mask types. .For her companys public database, Arbash said the photos came from searching on Instagram with hashtags related to masks. They gathered about 3,000 pictures from the social media platform, but narrowed it down to a set of 1,200 photos. The sample images posted included a childs photo as part of the set -- Arbash said it was a possible error that this picture ended up in its database.  .Arbash said they didnt ask the people included in the database for permission to use their face mask selfies to help develop facial recognition, and that if they wanted to be excluded, they could make their pages private. The people included arent aware theyre in this data set, she said. .Were not making any money off of this, its not commercial, Arbash said. The goal and the intention was to help any data science or machine learning engineers who are working to fix this issue and help with public safety.  .The links to the images from Instagram have since expired, but the data sets page put out a public call asking if anyone knew how to retrieve the photos. Arbash said if theres enough interest, the company would consider looking more into how to get more face mask images. .We do not allow third parties to collect or use photos posted by our users in this way, without their consent. We are continuing to investigate this, Facebook said in a statement..The Real World Masked Face Dataset claims to be the largest masked face data set, with more than 5,000 masked faces of 525 people gathered from the internet. The compilation comes from researchers at Wuhan University in China, where the coronavirus outbreak began. .A research paper released on March 23, accompanying the data set, says the images are of public figures gathered from massive internet resources. The researchers didnt respond to a request for comment. .The practice of grabbing peoples photos from social media to train facial recognition algorithms isnt new, but the focus on face masks because of COVID-19 is. Theres an urgency among developers to create face mask detection technology as a public safety concern, but ethical issues come up when the images are collected without consent. .People might not like the idea that their picture could be used to develop a database that could go to law enforcement or government surveillance in a foreign autocratic country like China, said Jake Laperruque, a senior counsel at the Constitution Project. Youre putting photos out there, maybe not with an expectation of privacy, but you have an expectation of how it can and cant be used. .Be respectful, keep it civil and stay on topic. We delete comments that violate our policy, which we encourage you to read. Discussion threads can be closed at any time at our discretion.\n"}
{"prompt":"The app can allow ministers to identify app users which can violate privacy\n\n###\n\n","completion":" A draft government memo explaining how the NHS contact-tracing app could stem the spread of the coronavirus said ministers might be given the ability to order “de-anonymisation” to identify people from their smartphones, the Guardian can reveal..The health secretary, Matt Hancock, announced on Sunday that the UK planned to introduce an app that would enable people who developed Covid-19 symptoms to “anonymously” alert other users to whom they had been in close proximity. “All data will be handled according to the highest ethical and security standards, and would only be used for NHS care and research,” he said..However, the government document seen by the Guardian, headed “official – sensitive” and “draft – not yet approved”, suggests the NHS privately considered using the technology to identify users..Produced in March, the memo explained how an NHS app could work, using Bluetooth LE, a standard feature that runs constantly and automatically on all mobile devices, to take “soundings” from other nearby phones through the day. People who have been in sustained proximity with someone who may have Covid-19 could then be warned and advised to self–isolate, without revealing the identity of the infected individual..However, the memo stated that “more controversially” the app could use device IDs, which are unique to all smartphones, “to enable de-anonymisation if ministers judge that to be proportionate at some stage”. It did not say why ministers might want to identify app users, or under what circumstances doing so would be proportionate..It added that alternatives to building an NHS app included “making use of existing apps and other functions already installed on people’s phones (eg Google Maps).”.A spokesperson for NHSX, the digital transformation wing of the health service, which is overseeing the development of the UK contact-tracing app, denied there were ever plans to de-anonymise data, or use data from apps such as Google Maps..“NHSX is looking at whether app-based solutions might be helpful in tracking and managing coronavirus, and we have assembled expertise from inside and outside the organisation to do this as rapidly as possible,” the spokesperson said..“To be very clear – there have never been plans to make use of existing apps and other functions already installed on peoples phones such as Google Maps and neither have there been plans to look to use the device ID of users in any app-based solutions.”.NHSX plans to harness data for the government’s response to the coronavirus pandemic were made public after confidential documents, used by private contractors, were made accessible via an unrestricted portal that was seen by the Guardian. .On Friday, Google and Apple announced an unprecedented collaboration to provide the back-end technology that will enable governments to introduce contact-tracing apps that run on both iPhones and Android phones. Hancock did not specify which technology the government would use but said the NHS was “working closely with the world’s leading tech companies”..Explaining how the NHS app would work, Hancock said: “If you become unwell with the symptoms of coronavirus, you can securely tell this new NHS app and the app will then send an alert anonymously to other app users that you’ve been in significant contact with over the past few days, even before you had symptoms, so that they know and can act accordingly.”.Advocates of contact-tracing apps argue they could prove a vital tool to help governments emerge from lockdown conditions currently restricting the movement of millions. .Researchers at the University of Oxford, who have been advising NHSX on the app, published a paper in the journal Science last month concluding that traditional contact tracing of those with Covid-19 was of limited use, in part because it can be spread by those who are asymptomatic and pre-symptomatic..Instead, the researchers argued, a contact-tracing app could speed up the process by automatically notifying contacts of people diagnosed with Covid-19. Prof David Bonsall, a senior researcher at Oxford University’s Nuffield Department of Medicine, told the Guardian that “we see it as the only alternative to … applying isolation to the whole population.”.Privacy International has said a Bluetooth LE system would be far less intrusive than other forms of tracking, such as using GPS or wifi data, because it would only keep a record of which devices had been near one another, rather than their actual locations..However, the draft memo raises questions about the use of contact-tracing apps if they are introduced without sufficient safeguards or transparency. The notion of “de-anonymisation” of users, in particular, would appear to contradict advice given by the Information Commissioner’s Office (ICO) that identifying individuals from their location data may breach privacy law..The deputy information commissioner said in a recent statement that data protection laws were not infringed as long as location data was “properly anonymised and aggregated”. An ICO spokesperson told the Guardian: “When personal data can be identified, organisations must comply with data protection law, including putting the appropriate safeguards in place to protect people’s information.”.However, the creation of apps to battle Covid-19 raise unprecedented questions about privacy. Edward Snowden, the whistleblower who exposed the US National Security Agency’s mass surveillance programmes seven years ago, has said the coronavirus could be exploited to usher in an era of bio-surveillance that persists even after the pandemic has ended..If you have been affected or have any information, wed like to hear from you. You can get in touch by filling in the form below, anonymously if you  wish or contact us via WhatsApp by clicking here or adding the contact +44(0)7766780300. Only the Guardian can see your contributions and one of our  journalists may contact you to discuss further. .Others are asking whether contact-tracing apps can really work, given that such large numbers of the population will need to download and use the technology for it to be effective, the lack of reliable testing, and the risk that such an open system could be abused by people fabricating symptoms. Ross Anderson, a professor of security engineering at Cambridge University, recently wrote that “anyone who’s worked on abuse will instantly realise that a voluntary app operated by anonymous actors is wide open to trolling”.\n"}
{"prompt":"Youtube is increasing revenue as misinformation increases\n\n###\n\n","completion":" YouTube is profiting from videos promoting unproven coronavirus treatments, a new report has found, as the company struggles to crack down on misinformation..The Google-owned tech company is running advertisements with videos pushing herbs, meditative music, and potentially unsafe over-the-counter supplements as cures for Covid-19, according to a report published on Friday by the Tech Transparency Project, a not-for-profit watchdog organization..After being contacted by the Guardian, YouTube removed four of the videos in question for violating its policies against Covid-19 misinformation. Three of the videos remain as they are not promoting misinformation directly but offering wellness tips, according to a spokesman..“We’re committed to providing timely and helpful information at this critical time, including raising authoritative content, reducing the spread of harmful misinformation and showing information panels, using WHO data, to help combat misinformation,” the spokesman said..Companies including YouTube may be forced to rely more heavily on artificial intelligence tools to moderate content while employees are forced to work from home, said Megan Lamberth, a researcher at the Center for a New American Security, a Washington DC-based thinktank..“Since the beginning of the pandemic, we’ve seen an enormous rise of misinformation on online platforms,” she said. “Social media companies have tried to respond to the deluge of misinformation, but in many cases, their moderation efforts have not been sufficient.”.TikTok has partnered with the World Health Organization to provide accurate information to users and has put a disclaimer on all videos using the #coronavirus hashtag with accurate information about the pandemic. Other sites, including Facebook, have pushed back against dangerous false cures including cleaning products and cocaine..The global pandemic has created a situation ripe for the spread of false cures, said Lisa Fazio, a psychology professor at Vanderbilt University who studies misinformation and how it spreads..“If the most profitable videos are those that feel good and provide easy answers, then it’s highly likely that they will contain misinformation,” she said. “In reality, the current situation is complicated and has few easy answers.”.YouTube initially prohibited the monetization of videos about Covid-19 under its “sensitive events” policy, which bars advertisements on videos regarding armed conflicts, terrorist acts, and “global health crises”..However, it reversed that policy on 11 March, saying it wanted to “make sure news organizations and creators can continue producing quality videos in a sustainable way”. It then enabled coronavirus video ads for “a limited number of channels”, and on 2 April it expanded monetization of content mentioning or featuring Covid-19 to all creators and news organizations..“In lifting restrictions on advertising in videos about the coronavirus pandemic, YouTube has made disinformation lucrative for some unscrupulous content creators and a liability for the brands that unwittingly support them,” the report said.\n"}
{"prompt":"There is a potential for error and racial bias\n\n###\n\n","completion":" Whole Foods is keeping an eye on stores at risk of unionizing through an interactive heat map, according to five people with knowledge of the matter and internal documents viewed by Business Insider..The heat map is powered by an elaborate scoring system, which assigns a rating to each of Whole Foods 510 stores based on the likelihood that their employees might form or join a union..The stores individual risk scores are calculated from more than two dozen metrics, including employee loyalty, turnover, and racial diversity; tipline calls to human resources; proximity to a union office; and violations recorded by the Occupational Safety and Health Administration..The map also tracks local economic and demographic factors such as the unemployment rate in a stores location and the percentage of families in the area living below the poverty line..The stores scores on each metric are fed into the heat map, which is a geographic illustration of the United States peppered with red spots to indicate high-risk Whole Foods stores..It also provides a rare look into corporate labor-tracking activities, a common practice among large companies but one rarely discussed publicly..A statement on the map describes its purpose as specific to monitoring unionization among its employees, which the company calls team members..The [Team Member] Relations Heatmap is designed to identify stores at risk of unionization, the statement reads. This early identification enables resources to be funneled to the highest need locations, with the goal of mitigating risk by addressing challenges early before they become problematic..In a statement provided to Business Insider, the company said an overwhelming majority of its employees prefer a direct relationship with the company over union representation..Whole Foods Market recognizes the rights of our Team Members to decide whether union representation is right for them, the company said. We agree with the overwhelming majority of our Team Members that a direct relationship with Whole Foods Market and its leadership, where Team Members have open lines of communication and every individual is empowered to share feedback directly with their team leaders, is best..Our open-door communication policy allows us to understand and quickly respond to the needs of our workforce, while recognizing, rewarding, and supporting the goals of every member of our team, the statement continued. At Whole Foods Market, were committed to treating all of our Team Members fairly, creating a safe, inclusive, and empowering working environment, and providing our Team Members with career advancement opportunities, great benefits, and competitive compensation, including an industry-leading starting minimum wage of $15\/hour..Whole Foods uses the heat map and related scores to determine where stores must take action to address risks, according to the documents and people familiar with the map..Some of the factors that contribute to external risk scores include local union membership size; distance in miles between the store and the closest union; number of charges filed with the National Labor Relations Board alleging labor-law violations; and a labor incident tracker, which logs incidents related to organizing and union activity..Other external factors include the percentage of families within the stores zip code that fall below the poverty line and the local unemployment rate..The second group of metrics in the scoring system, called store risks, arent a direct cause of risk but can predispose a store to risk, according to documents..Store-risk metrics include average store compensation, average total store sales, and a diversity index that represents the racial and ethnic diversity of every store. Stores at higher risk of unionizing have lower diversity and lower employee compensation, as well as higher total store sales and higher rates of workers compensation claims, according to the documents..The third area of metrics is team member sentiment. These metrics, which include items like employee loyalty and engagement, are designed to be the most actionable, the documents show..These measures assess employees feedback on the quality and safety of their work environment and whether they feel supported and respected, among other things..With the heat map, Whole Foods appears to be trying to identify and address circumstances ripe for employee unrest that could lead to attempts to form a union..This type of workforce analysis is something large companies have done for decades, albeit without some of the technology available today that can automate parts of that process, according to labor experts..Walmart, for example, hired an intelligence-gathering service from Lockheed Martin and ranked stores by labor activity when it faced protests eight years ago organized by the union-backed activist group OUR Walmart, according to a 2015 Bloomberg Businessweek story citing thousands of court documents..Employers spend millions of dollars a year to hire union avoidance advisers to see how susceptible they are to their workers organizing, Celine McNicholas, the director of government affairs and labor counsel for the Economic Policy Institute, said..A preponderance of the business community [has] a total allergy to unionization, Wilma Liebman, who served on the National Labor Relations Board under Presidents Obama, Bush, and Clinton, said..Unions give employees more bargaining power over things such as wages and health benefits, she said. They could also increase the chances of employee strikes, which can disrupt business..Companies dont want anything thats going to interfere with their autonomy and their ability to act unilaterally and sometimes theyre convinced [unions] are going to cost them more than they can afford, Liebman said..Research shows unionized workers tend to earn higher wages and are more likely to have access to certain benefits like employer-sponsored healthcare..Critics of unions argue, however, that the organizations can harm companies economically, forcing layoffs or job outsourcing, and that they dont have workers best interests in mind..US labor law protects employees right to unionize. Its legal, however, for a company to monitor and address labor organizing as long as it doesnt threaten, coerce, restrain, or interfere with efforts to unionize..Overall, US companies spent at least $100 million on consulting services for anti-union campaigns between 2014 and 2017, according to data from the Economic Policy Institute based on disclosure forms filed with the US Department of Labor.\n"}
{"prompt":"A Google service that automatically labels images produced starkly different results depending on skin tone on a given image\n\n###\n\n","completion":" In the fight against the novel coronavirus, many countries ordered that citizens have their temperature checked at train stations or airports. The device needed in such situations, a hand-held thermometer, has risen from a specialist item to a common sight..A branch of Artificial Intelligence known as “computer vision” focuses on automated image labeling. Most computer vision systems were trained on data sets that contained very few images of hand-held thermometers. As a result, they cannot label the device correctly..In an experiment that became viral on Twitter, AlgorithmWatch showed that Google Vision Cloud, a computer vision service, labeled an image of a dark-skinned individual holding a thermometer “gun” while a similar image with a light-skinned individual was labeled “electronic device”. A subsequent experiment showed that the image of a dark-skinned hand holding a thermometer was labelled “gun” and that the same image with a salmon-colored overlay on the hand was enough for the computer to label it “monocular”..In a statement to AlgorithmWatch, Tracy Frey, director of Product Strategy and Operations at Google, wrote that “this result [was] unacceptable. The connection with this outcome and racism is important to recognize, and we are deeply sorry for any harm this may have caused.”.“Our investigation found some objects were mis-labeled as firearms and these results existed across a range of skin tones. We have adjusted the confidence scores to more accurately return labels when a firearm is in a photograph.” Ms Frey added that Google had found “no evidence of systemic bias related to skin tone.”.Agathe Balayn, a PhD candidate at the Delft University of Technology on the topic of bias in automated systems, concurs. She tested several images in Googles service and came to the conclusion that the example might be a case of inaccuracy without a statistical bias. In the absence of more rigorous testing, it is impossible to say that the system is biased, she wrote..It is easy to understand why computer vision produces different outcomes based on skin complexion. Such systems processed millions of pictures that were painstakingly labeled by humans (the work you do when you click on the squares containing cars or bridges to prove that you are not a robot, for instance) and draw automated inferences from them..Computer vision does not recognize any object in the human sense. It relies on patterns that were relevant in the training data. Research has shown that computer vision labeled dogs as wolves as soon as they were photographed against a snowy background, and that cows were labeled dogs when they stood on beaches..Because dark-skinned individuals probably featured much more often in scenes depicting violence in the training data set, a computer making automated inferences on an image of a dark-skinned hand is much more likely to label it with a term from the lexical field of violence..Other computer vision systems show similar biases. In December, Facebook refused to let an Instagram user from Brazil advertise a picture, arguing that it contained weapons. In fact, it was a drawing of a boy and Formula One driver Lewis Hamilton. Both characters had dark skins..Labeling errors could have consequences in the physical world. Deborah Raji, a tech fellow at New York University’s AI Now Institute and a specialist in computer vision, wrote in an email that, in the United States, weapon recognition tools are used in schools, concerts halls, apartment complexes and supermarkets. In Europe, automated surveillance deployed by some police forces probably use it as well. Because most of these systems are similar to Google Vision Cloud, “they could easily have the same biases”, Ms Raji wrote. As a result, dark-skinned individuals are more likely to be flagged as dangerous even if they hold an object as harmless as a hand-held thermometer..Nakeema Stefflbauer, founder and CEO of FrauenLoop, a community of technologists with a focus on inclusivity, wrote in an email that bias in computer vision software would “definitely” impact the lives of dark-skinned individuals. Because the rate of mis-identification is consistently higher for women and dark-skinned people, the spread of computer vision for surveillance would disproportionately affect them, she added..Referring to the examples of Ousmane Bah, a teenager who was wrongly accused of theft at an Apple Store because of faulty face recognition, and of Amara K. Majeed, who was wrongly accused of taking part in the 2019 Sri Lanka bombings after her face was misidentified, Ms Stefflbauer foresees that, absent effective regulation, whole groups could end up avoiding certain buildings or neighborhoods. Individuals could face de facto restrictions in their movements, were biased computer vision to be more widely deployed, she added..In her statement, Ms Frey, the Google director, wrote that fairness was one of Google’s “core AI principles” and that they were “committed to making progress in developing machine learning with fairness as a critical measure of successful machine learning.”.But Google’s image recognition tools have returned racially biased results before. In 2015, Google Photos labelled two dark-skins individuals “gorillas”. The company apologized but, according to a report by Wired, did not fix the issue. Instead, it simply stopped returning the “gorilla” label, even for pictures of that specific mammal..That technology companies still produce racially biased products can be explained by at least two reasons, according to AI Now’s Deborah Raji. Firstly, their teams are overwhelmingly white and male, making it unlikely that results that discriminate against other groups will be found and addressed at the development stage. Secondly, “companies are now just beginning to establish formal processes to test for and report these kinds of failures in the engineering of these systems,” she wrote. “External accountability is currently the main method of alerting these engineering teams,” she added..Nicolas is data journalist and working for AlgorithmWatch as a reporter. He pioneered new forms of journalism in France and in Europe and is one of the leading experts on data journalism. He regularly speaks at international conferences, teaches journalism in French journalism schools and gives training sessions in newsrooms. A self-taught journalist and developer (and a graduate in Economics), he started by doing small interactive, data-driven applications for Le Monde in Paris in 2009. He then built the data journalism team at OWNI in 2010 before co-founding and managed Journalism++ from 2011 to 2017. Nicolas is also one of the main contributors to the Datajournalism Handbook, the reference book for the popularization of data journalism worldwide.\n"}
{"prompt":"Error rate for African American speech is nearly double that for others.\n\n###\n\n","completion":" Were outsourcing ever more of our decision-making to algorithms, partly as a matter of convenience, and partly because algorithms are ostensibly free of some of the biases that humans suffer from. Ostensibly. As it turns out, algorithms that are trained on data thats already subject to human biases can readily recapitulate them, as weve seen in places like the banking and judicial systems. Other algorithms have just turned out to be not especially good..Now, researchers at Stanford have identified another area with potential issues: the speech-recognition algorithms that do everything from basic transcription to letting our phones fulfill our requests. These algorithms seem to have more issues with the speech patterns used by African Americans, although theres a chance that geography plays a part, too..Voice-recognition systems have become so central to modern technology that most of the large companies in the space have developed their own. For the study, the research team tested systems from Amazon, Apple, Google, IBM, and Microsoft. While some of these systems are sold as services to other businesses, the ones from Apple and Google are as close as your phone. Their growing role in daily life makes their failures intensely frustrating, so the researchers decided to have a look at whether those failures display any sort of bias..Based on a score called word error rate (which includes inserted and missing words, as well as misinterpretations) all of the systems did well, having a score of less than 0.5. (Apples was the worst, and Microsofts system the best based on this measure.) In all cases, the recordings of African American speakers ended up with word error rates that were worse than the ones produced from recordings of white speakers—in general, the errors nearly doubled..The effect was more pronounced among African American males. White men and women had error rates that were statistically indistinguishable, at 0.21 and 0.17, respectively. The rate for African American women averaged 0.30, while for men it rose to 0.41..How important are these differences? The authors suggest it depends on how you define usability—above a certain percentage of error, it becomes more annoying to fix an automated transcript than to do it yourself, or your phone will end up doing the wrong thing more often than youre happy with. The authors tested how often individual chunks of text end up with a conservative word error rate of 0.5. They found that over 20 percent of the phrases spoken by African Americans would fail this standard; less than 2 percent of those spoken by whites would..So whats going on? There may be a bit of a geographical issue. California speakers are often considered to be accent free from an American perspective, and the two samples from that state had very low error rates. Rochester had a rate similar to Californias, while the District of Columbia had one closer to the rural North Carolina town. If there is a geographic influence, were going to need a much larger sample to separate that out..After that, the researchers analyzed the language usage itself. Since they didnt have access to the algorithms used by these systems, they turned to some open source packages that perform similar functions. They measured the softwares understanding of language use via a figure called the perplexity, which is a value derived from the accuracy at which the system can predict the word that will come next in a sentence. And, by this measure, the systems were better at handling the usage of African American speakers. Whats going on?.The researchers found there were two conflicting tendencies. African Americans would, on average, use a smaller total word list than their white counterparts. But their phrasing turned out to be more complicated—in many cases, they dropped words from their sentences when their listeners could easily infer them..Finally, theres the matter of how attuned the commercial systems are to African American voices. To explore this, the researchers searched through the transcripts to find cases when African American and white speakers used the same phrases. When those were run through the systems, the word error rate for African American speakers was higher than for whites, suggesting this also contributed to the overall reduced performance..An effective voice-recognition system has to combine a number of factors—actual word recognition, language usage, and likely meanings—in order to successfully recognize sentences or predict ensuing words. Existing commercial systems appear to fall a bit short of that when it comes to some populations. These systems werent set up to be biased; its likely that they were simply trained on a subset of the diversity of accents and usages present in the United States. But, as we become ever more reliant on these systems, making them less frustrating for all their users should be a priority.\n"}
{"prompt":"Facebook admits it made an 'error' after algorithm threatens to ban users who post content showing people how to make their own masks\n\n###\n\n","completion":" An algorithm designed to help weed out misinformation about coronavirus has been inadvertently hampering some Facebook users from disseminating content on homemade medical masks..According to a report from the New York Times, Facebook says that an error with its algorithm has been blocking content on how to make hand-sewn masks and threatening to ban the users who post it, including groups in Pennsylvania, Illinois, and California..Some of those groups, including one called Sew Face Masks Philadelphia, had thousands of followers and had its moderators threatened with a ban if content on masks continued..A self-described error in Facebooks moderation algorithm has been banning content relating to protective masks from appearing on its platform according to a new report. Pictured: Facebook CEO Mark Zuckerberg.The automated systems we set up to prevent the sale of medical masks needed by health workers have inadvertently blocked some efforts to donate supplies, Facebook said in a statement to the New York Times. .We apologize for this error and are working to update our systems to avoid mistakes like this going forward. We don’t want to put obstacles in the way of people doing a good thing..The so-called error comes as Facebook ramps up efforts to prevent users from profiting off of a protective and sanitizing products sold on the site, in particular medical masks, gloves, and hand sanitizer.   .On Facebook and other major platforms like Amazon, some sellers have used  the ongoing coronavirus pandemic in their efforts to price-gouge items, selling them for multiples beyond their average price..According to moderators of some of the groups interviewed by the New York Times, mixed messaging on masks may have contributed to Facebooks aggressive policy in removing content pertaining to DIY masks and other equipment. .The Centers for Disease Control (CDC) just recently reversed its stance on wearing masks after telling the US public not to buy personal protective gear..We support Facebook in their efforts in removing unethical sales” from their platform, Nicole Jochym, a student at Cooper Medical School of Rowan who was affected by the ban told the New York Times. .Soon, the Centers for Disease Control and Prevention (CDC) may advise all Americans to cover their faces when they leave the house, the Washington Post reported.  .The agency is weighing that recommendation after initially telling Americans that they didnt need to wear masks and that anything other than a high-grade N95 medical mask would do little to prevent infection any way. .Research on how well various types of masks and face coverings varies but, recently, and in light of the pandemic of COVID-19, experts are increasingly leaning toward the notion that something is better than nothing. .A University of Oxford study published on March 30 concluded that surgical masks are just as effective at preventing respiratory infections as N95 masks for doctors, nurses and other health care workers. .Its too early for their to be reliable data on how well they prevent infection with COVID-19, but the study found the thinner, cheaper masks do work in flu outbreaks. .The difference between surgical or face masks and N95 masks lies in the size of particles that can - and more importantly, cant - get though the materials. .N95 respirators are made of thick, tightly woven and molded material that fits tightly over the face and can stop 95 percent of all airborne particles, while surgical masks are thinner, fit more loosely, and more porous. .This makes surgical masks much more comfortable to breathe and work in, but less effective at stopping small particles from entering your mouth and nose. .Droplets of saliva and mucous from coughs and sneezes are very small, and viral particles themselves are particularly tiny - in fact, theyre about 20-times smaller than bacteria. .For this reason, a JAMA study published this month still contended that people without symptoms should not wear surgical masks, because there is not proof the gear will protect them from infection - although they may keep people who are coughing and sneezing from infecting others. .But the Oxford analysis of past studies - which has not yet been peer reviewed - found that surgical masks were worth wearing and didnt provide statistically less protection than N95 for health care workers around flu patients. .However, any face mask is only as good as other health and hygiene practices. Experts universally agree that theres simply no replacement for thorough, frequent hand-washing for preventing disease transmission. .Some think the masks may also help to train people not to touch their faces, while others argue that the unfamiliar garment will just make people do it more, actually raising infection risks.  .Homemade masks theoretically could offer some protection if the materials and fit were optimized, but this is uncertain, Dr Jeffrey Duchin, a Seattle health official told the Washington Post. .After a vacuum bag, kitchen towels were fairly protective, but uncomfortable. Masks made of T-shirts were very tolerable, but only worked a third as well as surgical mask. The Cambridge University researchers concluded that homemade masks should only be used as a last resort. .         A very, very angry man: Prince Harry had Olympic rows with William and poured out resentments about his father Charles to ex-girlfriend Cressida Bonas who found complaints about his family tiresome, bombshell new book claims       \n"}
{"prompt":"Facebook had an error in their spam detection system which caused news from reputable sources to be flagged\n\n###\n\n","completion":" Facebook CEO Mark Zuckerberg says a bug that flagged Facebook posts with links to articles published by reputable news sites about the coronavirus as spam was caused by an error with the companys spam detection system..Many people complained on social media on Tuesday that they had received notifications that posts with links to articles by news publications went against Facebooks Community Standards..Finally, Zuckerberg clarifies yesterday issue. re: links being marked as spam had nothing to do w\/ coronavirus or content moderation policies, says it was a technical error related to spam detection system.On Tuesday evening, Facebooks vice president of integrity, Guy Rosen, confirmed the bug on Twitter and said the company had restored all the posts that were unintentionally removed..We’ve restored all the posts that were incorrectly removed, which included posts on all topics - not just those related to COVID-19. This was an issue with an automated system that removes links to abusive websites, but incorrectly removed a lot of other posts too..Posts linking to articles published by BuzzFeed News, Politico, the Atlantic, and the Sydney Morning Herald about the coronavirus were all flagged..Earlier, Rosen rebuked speculation by the companys former security chief, Alex Stamos, that the error was a result of less human oversight of an anti-spam machine learning algorithm..Facebook told its workers Monday to work from home if possible. On Tuesday, the company told its contract moderators not to come into the office; however, they are barred from working from home due to privacy concerns..Facebook has previously launched a number of initiatives to combat misinformation and spam about the coronavirus, including allowing health authorities to flag fake news and showing users an information module when searching for information about the virus..A BuzzFeed News investigation, in partnership with the International Consortium of Investigative Journalists, based on thousands of documents the government didnt want you to see.\n"}
{"prompt":"This censorship caused important information about the coronavirus to not reach people\n\n###\n\n","completion":" China’s most popular messaging app, Tencent-owned WeChat, has been censoring keywords about coronavirus since as early as January 1st, an analysis found. Popular Chinese livestreaming platform YY has been censoring coronavirus content, too..To make this determination, the research group Citizen Lab scripted group chat conversations and sent them to three test WeChat accounts, two in Canada and one in China. The chat conversations consisted of article headlines and text. The group, which is affiliated with the University of Toronto, sent them from one of the Canadian WeChat accounts to the Chinese one, and observed which messages the Chinese account got. 132 keyword combinations were censored in January, but that number rose to 516 keywords by the second week of February..On YY, which is similar to Twitch or Mixer, 45 keywords were added to a blacklist on December 31st, 2019; five of those keywords were removed on February 10th, Citizen Lab found. YY’s blacklist is in the app itself, unlike WeChat’s, which uses a remote server for censorship..Public health officials from China first informed the World Health Organization about the virus at the end of December. The censorship has been going on since at least January 1st, and continued through the most intense part of the outbreak. WeChat has a monthly active user base of over one billion people — which means that a lot of users may have missed important information about the coronavirus, as well as how to prevent its spread. .Censored keywords included factual information on the disease, references to the government’s epidemic policies, and the name of Li Wenliang, a doctor who was among the earliest to warn the population about the disease. Li caught the disease while treating coronavirus patients and died on February 7th. His story created public outcry against the government’s handling of the coronavirus..It’s not clear why the two companies decided to censor keywords about coronavirus, though it’s possible they were ordered to do so by the Chinese government. WeChat has close ties with the Chinese government, and the government has already used WeChat and Twitter to track down people Chinese officials felt were sharing negative information about the coronavirus outbreak..The censorship is particularly pernicious because the WeChat is a crucial part of many Chinese people’s lives, David Jacobson, a professor of global business strategy at SMU’s Cox School of Business and a visiting professor at Tsinghua University in Beijing, told BuzzFeed News. “As a platform, you can live your life with it,” Jacobson said in an interview with BuzzFeed News. “You can pay for things. You can do so much more.”\n"}
{"prompt":"Those use case increases surveillance and invades privacy\n\n###\n\n","completion":" BEIJING (Reuters) - A Chinese company says it has developed the country’s first facial recognition technology that can identify people when they are wearing a mask, as most are these days because of the coronavirus, and help in the fight against the disease..But the coronavirus, which emerged in Hubei province late last year, has resulted in almost everyone wearing a surgical mask outdoors in the hope of warding off the virus - posing a particular problem for surveillance..Now Hanwang Technology Ltd, which also goes by the English name Hanvon, said it has come up technology that can successfully recognize people even when they are wearing masks..“If connected to a temperature sensor, it can measure body temperature while identifying the person’s name, and then the system would process the result, say, if it detects a temperature over 38 degrees,” Hanwang Vice President Huang Lei told Reuters in an interview..The Beijing-based firm said a team of 20 staff used core technology developed over the past 10 years, a sample database of about 6 million unmasked faces and a much smaller database of masked faces, to develop the technology,.The team began work on the system in January, as the coronavirus outbreak gathered pace, and began rolling it out to the market after just a month..It sells two main types of products that use the technology. One performs “single channel” recognition that is best used at, for example, entrances to office buildings..“When wearing a mask, the recognition rate can reach about 95%, which can ensure that most people can be identified,” Huang said, adding the success rate for people without mask is about 99.5%..Using Hanwang’s technology, the ministry can cross-reference images with its own database of names and other information and then identify and track people as they move about, Huang said..The company has about 200 clients in Beijing using the technology, including the police, and expect scores more across 20 provinces to start installing it soon, Huang said..When it comes to other surveillance tools being used in the fight against the coronavirus, there has been some grumbling on social media but most people seem to be accepting extra intrusion, or even embracing it, as a means to deal with the health emergency..Although domestic customers have been driving Hanwang’s business, Huang also said he expected more foreign interest, as the virus spreads around the world and more people wear face masks.\n"}
{"prompt":"Responsible AI must have human input\n\n###\n\n","completion":" The project is being run by the Office of Naval Research and has been described as an autonomous undersea weapon system according to a report by New Scientist..Details of the killer submersible were made available as part of the 2020 budget documents, which also revealed it has been named CLAWS by the US Navy..Very few details about the top secret project have been revealed beyond the fact it will use sensors and algorithms to carry out complex missions on its own. .Its expected CLAWS will be installed on the new Orca class robot submarines that have 12 torpedo tubes and are being developed for the Navy by Boeing..Over the next few years, the firm will design and test four Orca Extra Large Unmanned Undersea Vehicles (XLUUVs) based on its autonomous Echo Voyager (pictured), which can operate at sea for months at a time.The navy has not revealed what CLAWS stands for or commented on the story, the only information is what has been released to congress in the budget documents..Autonomous submarines already exist and they can complete tasks without humans being involved - however they arent very intelligent and have limited functionality..The new submarines will have much greater level of artificial intelligence and so be able to perform a wider range of functions without a human controller..CLAWS isnt new, it was first revealed in 2018 as part of a US Navy bid to improve the autonomy and survivability of large and extra-large unmanned underwater vehicles, according to New Scientist..When it was first revealed there was no mention then of weapons being on the autonomous submersible, only a need for it to have sensors and make decisions..The US Navy has ordered larger, longer range robot submarines called Orca and it was assumed they would be controlled remotely like the smaller versions..They are set to be armed with 12 torpedo tubes and with CLAWS they could be used to sink targets on their own without input from a human, New Scientist reports..Boeing has developed the Orca submarines for the Navy based on its Echo Voyager class of craft. It is assumed the CLAWS AI system will be added to Orca subs.This idea isnt to everyones liking. Stuart Russell from the University of California Berkeley told New Scientist it was a dangerous development..Equipping a fully autonomous vehicle with lethal weapons is a significant step, and one that risks accidental escalation in a way that does not apply to sea mines..The exact budget for CLAWS hasnt been revealed but it was allocated $26 million in this years US Navy budget  and another $23 million for next year..It will move from being an idea to a working prototype thanks to this extra funding and could be deployed on large robot submarines by 2022, says New Scientist. .‘The Orca will have well-defined interfaces for the potential of implementing cost-effective upgrades in future increments to leverage advances in technology and respond to threat changes, the Navy said..The organisation says that something must be done about this lack of accountability - and it is calling for a ban on the development and use of killer robots..Called Mind the Gap: The Lack of Accountability for Killer Robots, their report details the hurdles of allowing robots to kill without being controlled by humans..No accountability means no deterrence of future crimes, no retribution for victims, no social condemnation of the responsible party, said Bonnie Docherty, senior Arms Division researcher at the HRW and the reports lead author. .         How Harry turned to therapists from MI6 in his mental health battle, SENT BACK a birthday gift from Charles and had explosive rows with William: RICHARD KAY on new book that says so much about the Princes state of mind       \n"}
{"prompt":"Clearview AI has already had many errors, privacy issues, and racial bias. Furthermore, the company's data storage and security protocols are untested and unregulated\n\n###\n\n","completion":" Clearview AI, which scans the internet for photos for its massive facial recognition database, is reportedly working on a surveillance camera.According to a report by BuzzFeed News, Clearview AI is developing the camera through a sister operation called Insight Camera. The company is looking to create a product that can offer live, real-time facial recognition. .BuzzFeed found the connection between the two companies after noticing a “security_camera” app in the code on Clearview’s web app. After the news outlet reached out to the company, Insight Camera’s website was taken down. The two companies did not mention their relationship on either website. However, similar code accessing Clearview’s servers were found on Insight Camera’s website. .Two organizations that were beta testing the camera have been identified: the United Federation of Teachers (UFT) and New York City-based real estate company Rudin Management. .According to a UFT spokesperson speaking to BuzzFeed, it used Insight Camera to help its security team identify unauthorized individuals from entering its offices. UFT says the Insight Camera created its own on-site local database and the organization did not have access to Clearview’s larger database. Rudin Management said it no longer uses Insight Camera..Last week, Gizmodo discovered links between Clearview and wearable technology company Vuzix. An earlier report from the New York Times highlighted Clearview’s interest in augmented reality glasses. BuzzFeed was able to verify the relationship after looking through Clearview search data connected to Vuzix-related accounts. The company later confirmed it sent its smart glasses to Clearview for testing..Account data connected to another augmented reality wearable company, RealWear, were also uncovered by BuzzFeed. However, RealWear claims it has no working relationship with Clearview aside from selling a few devices to the company a year ago..Clearview AI was thrust into the spotlight after a New York Times report detailed how the company was scraping social media and the rest of the internet for public photos in order to create a massive facial recognition database. Clearview has collected billions of photos and provided its technology to law enforcement as well as private companies..Facebook, Twitter, Google, and YouTube have all issued cease-and-desist letters demanding Clearview stop scraping its platforms for data. Clearview AI CEO Hoan Ton-That claims, however, that his company’s methods are protected by the First Amendment. .Just last week, Apple disabled Clearview AI’s app for violating its Enterprise Developer Program rules. A recent report from The Daily Beast also discovered that Clearview AI’s client list was hacked. \n"}
{"prompt":"There is a lack of public oversight and rampant potential for abuse. The facial recognition is also biased.\n\n###\n\n","completion":" A small company called Banjo is bringing pervasive AI surveillance to law enforcement throughout Utah, Motherboard reports. In July, Banjo signed a five-year, $20.7 million contract with Utah. The agreement gives the company real-time access to state traffic cameras, CCTV and public safety cameras, 911 emergency systems, location data for state-owned vehicles and more. In exchange, Banjo promises to alert law enforcement to anomalies, aka crimes, but the arrangement raises all kinds of red flags..Banjo relies on info scraped from social media, satellite imaging data and the real-time info from law enforcement. Banjo claims its Live Time Intelligence AI can identify crimes -- everything from kidnappings to shootings and opioid events -- as they happen..Banjo presents many of the same concerns that similar companies have encountered. One of the strongest arguments against surveillance practices by Clearview AI has been that the companys data storage and security protocols were untested and unregulated. As Clearview AI proved earlier this month, that can lead to massive data leaks. According to Motherboard, Banjo says it removes all personal data from its system, but how and how well it does this remain unclear..There are also concerns about the lack of public oversight and rampant potential for abuse. Plus, facial recognition is still biased. Its not as good at interpreting black and brown faces as it is at interpreting white faces, and for that reason, some law enforcement companies have opted not to use it..The contract between Banjo and Utah says the technology will be deployed in all of Utahs 29 counties, its 13 largest cities and 10 other cities with significant relevance, as well as the University of Utah. So far, Motherboard hasnt been able to track down a single case that Banjos technology helped on, but that doesnt meant the tech hasnt been put to use.\n"}
{"prompt":"The use of automated license plate readers collected data on innocent people and car movements which was shared\n\n###\n\n","completion":" Some of California’s largest police departments have been collecting millions of images of drivers’ license plates and sharing them with entities around the country—without having necessary security policies in place, in violation of state law, according to a newly released state audit..The audit, published Thursday, found that 230 police and sheriff’s departments in the state currently use automated license plate readers (ALPRs), which can be fixed cameras or devices mounted on patrol cars. Police have touted the technology as necessary for enforcing parking and basic municipal laws, and as a vital tool in child abduction cases and other high-profile investigations..The Los Angeles Police Department, for example, has collected more than 320 million images over the last several years. Only 400,000 of those generated immediate matches to cars of interest, but the remaining 99.9 percent of the images, which can be used to track peoples’ movement across the city, stay stored in a department database for more than five years, according to the audit..The LAPD then adds other sensitive information to that database, sometimes tagging the photos with criminal records, names, addresses, and dates of birth. Meanwhile, the department has not established any written policy governing proper use of its ALPR data, in violation of a 2016 state law..Citing a case in Georgia in which a police officer took a bribe to look up a woman’s license plate to determine if she was an undercover officer, the auditors also determined that many of the departments it examined were not ensuring that only authorized personnel had access to ALPR data, or auditing the database logs to make sure that authorized personnel were using the systems properly..“This is very troubling. This technology reportedly exists to help with parking enforcement and other basic law enforcement responsibilities, and yet we’re seeing a huge amount of data collected, retained, and shared unnecessarily,” state Sen. Scott Wiener, who requested the audit, told Motherboard..“The LAPD will perform an assessment of the systems data security features and retention periods for ALPR images to evaluate the need for adjustment, prior to publishing of the ALPR policy,” the department wrote. “Furthermore, the policy will list the entities the department shares ALPR images with and the process for handling image-sharing requests.”.During testimony before the state legislature in August, though, the LAPD lieutenant who oversees the department’s license plate reader program stated, “We continue to ensure that we abide by both the laws that are in place,” directly contradicting what the audit would ultimately find..Among the most concerning revelations in the audit, privacy advocates said, was the apparent carelessness with which police departments shared the information in their ALPR databases..In addition to the LAPD, the auditors examined three other agencies in detail: the Fresno Police Department, Marin County Sheriff’s Office, and Sacramento County Sheriff’s Office. The Sacramento sheriff’s office shared its data with 1,119 entities, Fresno with 982, and Marin with 554 around the country. LAPD shared data with 58 departments in California..The auditors found Sacramento, Fresno, and Marin had apparently taken minimal steps to determine why the entities requesting access to the license plate data needed it, or even if they were public agencies at all, which is a requirement under the state law..Some of the entities on the share lists were identified only by initials, according to the audit. And the three California departments were all sharing data with an entity listed as the Missouri Police Chiefs Association, which is a private advocacy group, not a law enforcement agency. Vigilant Solutions, the company that provided ALPR technology to those departments later told auditors that the Missouri Police Chiefs Association was actually the Missouri State Highway Patrol, but the California departments had apparently not noted the difference..Sacramento, Fresno, and Marin were also sharing data with the Honolulu Police Department, which is separated from California by roughly 2,500 miles of ocean, raising questions about why Honolulu police need to know the every movement of California drivers..“They’re sharing this data indiscriminately across the country without even thinking or doing the justification of why they’re sharing with these agencies,” Dave Maass, a senior investigative researcher for the Electronic Frontier Foundation who has been tracking police use of ALPR technology, told Motherboard..In its response to the audit, the Marin County Sheriff’s Office defendended its sharing policies, including with Honolulu, saying they were “done properly and with consideration as to the multiple matters which have in the past involved both agencies.”.Maass said the California audit should serve as a larger warning. ALPR technology is widespread not only among police departments, but among private entities who use tools built by companies like Vigilant..“This is the exact same thing going on across the United States,” Maass said. “Every other state should take a look at this audit and consider whether they should do an audit.”.By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.\n"}
{"prompt":"This instance contributes to the rise of misinformation\n\n###\n\n","completion":" In about a half hour, a high schooler created a website and Twitter account for an entirely-fictional Congressional candidate named Andrew Walz. Shortly thereafter, Twitter verified the made-up politician..The student told CNN Business that he was bored and, after reading about misinformation, wanted to kick the tires of Twitter’s verification system. It’s a fun experiment — with a disturbing result that illustrates just how easy it is to manipulate social media..The student, who remained unnamed in CNN‘s article, said that he never had to provide any identification to get Walz verified, or even show evidence that he was running for any sort of office. The experiment exposes a glaring flaw in Twitter’s security and verification process — and suggests that the company hasn’t done nearly enough to protect election security..“I had no malicious intent. I just wanted to simply test to see whether this could happen,” the student told CNN. “So in case someone with bad intentions wanted to do this, Twitter now knows and can take steps to fix this.”\n"}
{"prompt":"Brown University study found the substantial impact of bots amplifying denialist messages which increases misinformation\n\n###\n\n","completion":" The social media conversation over the climate crisis is being reshaped by an army of automated Twitter bots, with a new analysis finding that a quarter of all tweets about climate on an average day are produced by bots, the Guardian can reveal..The stunning levels of Twitter bot activity on topics related to global heating and the climate crisis is distorting the online discourse to include far more climate science denialism than it would otherwise..An analysis of millions of tweets from around the period when Donald Trump announced the US would withdraw from the Paris climate agreement found that bots tended to applaud the president for his actions and spread misinformation about the science..The study of Twitter bots and climate was undertaken by Brown University and has yet to be published. Bots are a type of software that can be directed to autonomously tweet, retweet, like or direct message on Twitter, under the guise of a human-fronted account..“These findings suggest a substantial impact of mechanized bots in amplifying denialist messages about climate change, including support for Trump’s withdrawal from the Paris agreement,” states the draft study, seen by the Guardian..On an average day during the period studied, 25% of all tweets about the climate crisis came from bots. This proportion was higher in certain topics – bots were responsible for 38% of tweets about “fake science” and 28% of all tweets about the petroleum giant Exxon..Conversely, tweets that could be categorized as online activism to support action on the climate crisis featured very few bots, at about 5% prevalence. The findings “suggest that bots are not just prevalent, but disproportionately so in topics that were supportive of Trump’s announcement or skeptical of climate science and action”, the analysis states..Thomas Marlow, a PhD candidate at Brown who led the study, said the research came about as he and his colleagues are “always kind of wondering why there’s persistent levels of denial about something that the science is more or less settled on”..The researchers examined 6.5m tweets posted in the days leading up to and the month after Trump announced the US exit from the Paris accords on 1 June 2017. The tweets were sorted into topic category, with an Indiana University tool called Botometer used to estimate the probability the user behind the tweet is a bot..Marlow said he was surprised that bots were responsible for a quarter of climate tweets on an average day. “I was like, ‘Wow that seems really high,’” he said. .The consistent drumbeat of bot activity around climate topics is highlighted by the day of Trump’s announcement, when a huge spike in general interest in the topic saw the bot proportion drop by about half to 13%. Tweets by suspected bots did increase from hundreds a day to more than 25,000 a day during the days around the announcement but it wasn’t enough to prevent a fall in proportional share..Trump has consistently spread misinformation about the climate crisis, most famously calling it “bullshit” and a “hoax”, although more recently the US president has said he accepts the science that the world is heating up. Nevertheless, his administration has dismantled any major policy aimed at cutting planet-warming gases, including car emissions standards and restrictions on coal-fired power plants..The Brown University study wasn’t able to identify any individuals or groups behind the battalion of Twitter bots, nor ascertain the level of influence they have had around the often fraught climate debate..This was followed by a tweet that doubted the world will reach a 9-billion population due to “#climatechange lunacy stopping progress”. The account has nearly 16,000 followers..Twitter accounts spreading falsehoods about the climate crisis are also able to use the promoted tweets option available to those willing to pay for extra visibility. Twitter bans a number of things from its promoted tweets, including political content and tobacco advertising, but allows any sort of content, true or otherwise, on the climate crisis..A Twitter spokesperson disputed the accuracy of the Brown University research. “Non-peer reviewed research using our public API can often be deeply flawed,” the spokesperson said, adding that “sweeping assessments” of users based on signals such as location and tweet content are routinely made by outside groups..“To be clear - none of these indicators are sufficient to determine if something is a bot. Looking for accounts that look similar to those disclosed as part of our archives is an equally flawed approach, given many of the bad actors mimic legitimate accounts to appear credible. This approach also often wrongly captures legitimate voices who share a particular political viewpoint that one disagrees with.”.Research on internet blogs published last year found that climate misinformation is often spread due to readers’ perception of how widely this opinion is shared by other readers..Stephan Lewandowsky, an academic at the University of Bristol who co-authored the research, said he was “not at all surprised” at the Brown University study due to his own interactions with climate-related messages on Twitter..“More often than not, they turn out to have all the fingerprints of bots,” he said. “The more denialist trolls are out there, the more likely people will think that there is a diversity of opinion and hence will weaken their support for climate science..John Cook, an Australian cognitive scientist and co-author with Lewandowsky, said that bots are “dangerous and potentially influential”, with evidence showing that when people are exposed to facts and misinformation they are often left misled..“This is one of the most insidious and dangerous elements of misinformation spread by bots – not just that misinformation is convincing to people but that just the mere existence of misinformation in social networks can cause people to trust accurate information less or disengage from the facts,” Cook said..Although Twitter bots didn’t ramp up significantly around the Paris withdrawal announcement, some advocates of action to tackle the climate crisis are wary of a spike in activity around the US presidential election later this year..“Even though we don’t know who they are, or their exact motives, it seems self-evident that Trump thrives on the positive reinforcement he receives from these bots and their makers,” said Ed Maibach, an expert in climate communication at George Mason University.\n"}
{"prompt":"There is a secretive nature to this plan as well as could amplify racial biases\n\n###\n\n","completion":" A police investigator in Spain is trying to solve a crime, but she only has an image of a suspect’s face, caught by a nearby security camera. European police have long had access to fingerprint and DNA databases throughout the 27 countries of the European Union and, in certain cases, the United States. But soon, that investigator may be able to also search a network of police face databases spanning the whole of Europe and the U.S..According to leaked internal European Union documents, the EU could soon be creating a network of national police facial recognition databases. A report drawn up by the national police forces of 10 EU member states, led by Austria, calls for the introduction of EU legislation to introduce and interconnect such databases in every member state. The report, which The Intercept obtained from a European official who is concerned about the network’s development, was circulated among EU and national officials in November 2019. If previous data-sharing arrangements are a guide, the new facial recognition network will likely be connected to similar databases in the U.S., creating what privacy researchers are calling a massive transatlantic consolidation of biometric data..The report was produced as part of discussions on expanding the Prüm system, an EU-wide initiative connecting DNA, fingerprint, and vehicle registration databases for mutual searching. A similar system exists between the U.S. and any country that is part of the Visa Waiver Program, which includes the majority of EU countries; bilateral agreements allow U.S. and European agencies to access one another’s fingerprint and DNA databases..Although new legislation following the report’s recommendation is not yet on the table, preparatory work is ongoing. Information provided by the European Commission to the European Parliament last November shows that almost 700,000 euros (about $750,000) are going to a study by consultancy firm Deloitte on possible changes to the Prüm system, with one part of the work looking at facial recognition technology. The European Commission has also, separately, paid 500,000 euros to a consortium of public agencies led by the Estonian Forensic Science Institute to “map the current situation of facial recognition in criminal investigations in all EU Member States,” with the aim of moving “towards the possible exchange of facial data,” according to a project presentation sent to national representatives in Brussels..“This is concerning on a national level and on a European level, especially as some EU countries veer towards more authoritarian governments,” said Edin Omanovic, advocacy director for Privacy International. Omanovic worries about a pan-European face database being used for “politically motivated surveillance” and not just standard police work. The possibility of pervasive, unjustified, or illegal surveillance is one of many critiques of facial recognition technology. Another is that it is notoriously inaccurate, particularly for people of color..“Without the transparency and legal safeguards for facial recognition technology to be lawful,” said Omanovic, “there should be a moratorium on it.”.The EU has taken big steps to connect a host of migration and security databases in recent years. New legislation passed last April established a database that will hold the fingerprints, facial images, and other personal data of up to 300 million non-EU nationals, merging data from five separate systems. According to the report by 10 police forces, Deloitte consultants proposed doing the same with police facial images, but the idea was met with unanimous opposition from law enforcement officials..Nonetheless, the report recommends linking all of EU member states’ facial databases, which would seem to have the same practical effect. In another internal EU police report — this one from a working group on Prüm that looked at the exchange of drivers’ license data — police note that “a network of interconnected national registers can be regarded as a virtual European register.”.To the police, the advantages of interlinked facial recognition databases are clear. The Austria-led report views the technology as a “highly suitable” biometric tool for identifying unknown suspects and suggests that the databases should be created and linked “as quickly as possible.” It also recognizes the need for data protection safeguards, such as human verification of any automated matches, but privacy experts argue that the creation of any such system is the first step toward greater sharing and linking of data where such controls are inadequate..European moves to consolidate police facial recognition data closely resembles similar efforts in the U.S., said Neema Singh Guliani, senior legislative counsel at the American Civil Liberties Union. Many U.S. law enforcement agencies work out of “fusion centers,” where they are co-located and able to share data. If you have an information-sharing agreement with the FBI or the Department of Homeland Security, said Guliani, “there’s a risk that functionally the information may be shared with additional levels of U.S. law enforcement.”.“It raises many questions,” she added. “How police are using facial recognition and gathering images, as well as in the U.S. with regard to due process and First Amendment expression. Given existing information sharing relationships, it’s very likely that the U.S. would want access to that information.”.As far back as 2004, the U.S. Embassy in Brussels was calling for a relationship with the EU that allowed “expansive exchanges and sharing all forms of data, including personal data.” In recent years, efforts toward that goal have intensified. According to a Government Accountability Office report, in 2015, the Department of Homeland Security began demanding the implementation of the data-sharing agreements required of Visa Waiver Program countries. This has included the FBI providing assistance to other states to set up the necessary computer networks..Austria, to take one example, began checking fingerprints against the FBI’s criminal fingerprint databases in October 2017, explained Reinhard Schmid, a senior official in the Austrian criminal intelligence service. Since then, about 12,000 individuals’ prints have been cross-checked, leading to 150 matches. “Around 20 of these identified persons were under investigation and suspected of membership of terrorist organizations,” while in 56 cases individuals had attempted to use a false identity, said Schmid..“Their logic here is, ‘When I have a serious crime and I want to run someone’s photo against a database, why shouldn’t I have this?’” said Guliani. Yet, she added, the privacy implications were enormous. “Once you have the access, you ultimately have the ability to identify almost anyone, anywhere.”.The report by 10 police forces calls for Europol, the EU agency for police information and intelligence sharing, to play a role in exchanging facial recognition and other biometric data with non-EU states. This echoes recommendations from European governments themselves: A July 2018 declaration called for the commission to consider “broadening the scope” of the Prüm network and for Europol to take the lead on data sharing with third countries..The FBI and Europol did not respond to questions about data-sharing agreements between the EU and the U.S. A spokesperson for the European Commission acknowledged the prospect of adding facial recognition data to the Prüm network, but declined to go into more detail.\n"}
{"prompt":"This pilot violates privacy, encourages intimidation and has room for racial bias\n\n###\n\n","completion":" Barclays has been criticised by HR experts and privacy campaigners after the bank installed “Big Brother” employee monitoring software in its London headquarters. .Introduced as a pilot last week, the technology monitors Barclays workers’ activity on their computers, and in some instances admonishes staff in daily updates to them if they are not deemed to have been active enough — which is described as being in “the zone”. .The system tells staff to “avoid breaks” as it monitors their productivity in real-time, and records activities such as toilet visits as “unaccounted activity”..A whistleblower at the banking giant told City A.M. that “the stress this is causing is beyond belief” and that it “shows an utter disregard for employee wellbeing”. .“Employees are worried to step away from their desks, have full lunch breaks, take bathroom breaks or even get up for water as we are not aware of the repercussions this might have on our statistics,” they added. .The software, provided by Sapience, has been rolled out throughout the product control department within the investment bank division at the firm’s Canary Wharf headquarters. .Sapience describes the system as offering “automated work pattern reporting and real-time analytics” and “unprecedented visibility into how people work, with actionable insights to better manage cost and performance across teams.”.The software compiles a report into an employees’ activity during the working day. A “work yoga” assessment sent to a Barclays employee earlier this week, seen by City A.M., warned the staffer of “not enough time in the Zone yesterday!”.The controversial new software comes as the investment bank arm, of which the product control department is a part, reported increased profit at the end of 2019. .The use of such software is becoming more widespread in the industry, but Ed Houghton, head of research at the Chartered Institute for Personnel and Development, warned the technology could never be a “substitute for good line management”..Barclays’ product control function is tasked with compiling profit and loss accounts on the trading floor’s activities as well as communicating information with the rest of the bank and regulators. .This is not the first time Barclays has been accused of invading the privacy of employees. In 2017 the firm was criticised for the use of OccupEye sensors which monitored how long employees were spending at their desks. A similar system was used at the Telegraph newspaper, but was removed after just a day following a backlash from employees..Silkie Carlo, director of Big Brother Watch, said: “Managers would never get away with breathing down employee’s necks, personally monitoring their screens or logging toilet and water breaks. The availability of technology to surveil staff surreptitiously does not make it any more acceptable.”.A spokesperson for Barclays confirmed it had introduced the monitoring software, and said  “This type of technology is widely used across the industry to help identify what is working well and opportunities to improve processes.  Colleague wellbeing is of paramount importance and colleagues are free to take breaks whenever they choose.” \n"}
{"prompt":"President Manoj Tiwari criticized Arvind Kejriwal. However, the videos are not real and lead to misinformation\n\n###\n\n","completion":" On February 7, a day ahead of the Legislative Assembly elections in Delhi, two videos of the Bharatiya Janata Party (BJP) President Manoj Tiwari criticising the incumbent Delhi government of Arvind Kejriwal went viral on WhatsApp. While one video had Tiwari speak in English, the other was him speaking in the Hindi dialect of Haryanvi. “[Kejriwal] cheated us on the basis of promises. But now Delhi has a chance to change it all. Press the lotus button on February 8 to form the Modi-led government,” he said..One may think that this 44-second monologue might be a part of standard political outreach, but there is one thing that’s not standard: These videos were not real. This is what the original video was:.It’s 2020, and deepfakes have become a powerful and concerning, tool that allows humans to manipulate or fabricate visual and audio content on the internet to make it seem very real. They are quite like the face animations in Hollywood films, though not nearly as expensive, and with a dark side. Since its introduction in 2017, A-list celebrities have seen their faces pushed onto existing pornographic videos, making deepfakes an infamous tool for misuse..When the Delhi BJP IT Cell partnered with political communications firm The Ideaz Factory to create “positive campaigns” using deepfakes to reach different linguistic voter bases, it marked the debut of deepfakes in election campaigns in India. “Deepfake technology has helped us scale campaign efforts like never before,” Neelkant Bakshi, co-incharge of social media and IT for BJP Delhi, tells VICE. “The Haryanvi videos let us convincingly approach the target audience even if the candidate didn’t speak the language of the voter.”.Tiwari’s fabricated video was used widely to dissuade the large Haryanvi-speaking migrant worker population in Delhi from voting for the rival political party. According to Bakshi, these deepfakes were distributed across 5,800 WhatsApp groups in the Delhi and NCR region, reaching approximately 15 million people..So it’s not surprising that the prospect of building campaign businesses using deepfakes to influence the masses has alarmed fact-checking organisations and policy wonks. Many think deepfakes would take the ongoing war on disinformation and fake news to a whole new level—one that has already been dubbed a “public health crisis”..Ever since deepfakes blew up in 2017, the technology has been used extensively to create fake porn videos using existing celebrity video footage and AI algorithms. In fact, 96 percent of those videos are non-consensual deepfake pornography. The unprecedented, and thereby problematic, use of deepfakes has got to do with the fact that much of the code required to fabricate videos is publicly available on several code-repository websites, making it really easy to create such videos..On the political front, the technology gained attention first in 2018, when a comedian impersonating Barack Obama delivered a PSA video on how deepfakes can be deceptive..In a lesser-known incident, a video appearance by Ali Bongo, the president of the East African nation of Gabon, was believed to be a deepfake, culminating in an unsuccessful coup by the country’s military. But the political fallout on account of deepfakes has been fairly limited, until now..With deepfake election campaigns though, we are crossing over into an era where it’s going to be impossible to trust what we see and hear. The video of Tiwari, seated in front of a green-coloured wall and talking to the camera, was used to reproduce a forged version where he says things he never actually said, in a language he doesnt even know! In this case, the speech was scripted, vetted and approved by the BJP for the creation of the deepfakes. But it’s not difficult to imagine someone faking a video to issue threats or hate against a specific section of the population..While many of the popular deepfake videos are complete faceswaps, a subtler version is to alter only the lip movements from an original video to match the target audio. The Ideaz Factory claims to have done the latter for Tiwari’s video. “We used a ‘lip-sync’ deepfake algorithm and trained it with speeches of Manoj Tiwari to translate audio sounds into basic mouth shapes,” says Sagar Vishnoi, the chief strategist at The Ideaz Factory. The firm hired a dubbing artist to impersonate Tiwari reading the script in Haryanvi, which was then superimposed on the video..BJP’s Bakshi says the response to those videos has been encouraging. “Housewives in the group said it was heartening to watch our leader speak my language,” he said, recounting one of the comments on a WhatsApp group. After the “viral” response, the party went ahead with a second video of Tiwari speaking English targeted at “urban Delhi voters.”.VICE shared the videos with researchers at the Rochester Institute of Technology (RIT) in New York, who believe that these were indeed deepfakes, but are awaiting confirmation on the same from their purpose-built software to automatically detect deepfakes. Ideaz Factory refused to share more information on the technology used, but Saniat Javid Sohrawardi, a deepfake researcher at RIT, says that “judging by the timeline of their work, Id think that they used Nvidias vid2vid code.” The only other well-known algorithm to achieve this task is face2face, an application that was used to make the Obama deepfake video..In India, though, deepfakes still have some lags. In Tiwari’s videos, a few members on WhatsApp groups pointed out a brief anomaly in the mouth movement. But Vishnoi assures that minor kinks aside, “we have used a tool that has so far been used only for negative or ambush campaigning and debuted it for positive campaign.” He admits that the technology his firm uses is currently not mature enough to synthetically generate the target’s voice using algorithms. But they have plans to scale Tiwari’s “positive” deepfake campaign to upcoming Bihar elections and the 2020 US elections..Tarunima Prabhakar, cofounder of Tattle, a civic tech project that is building a searchable archive on content circulated on WhatsApp, says, “The problem with the positive campaign pitch is that it puts the genie out of the bottle.” This means that even if the firm somehow self-regulates and decides not to produce nefarious videos, other, possibly not as overt, companies, will come up with other uses to weaponise this technology. “To say only some forms of deepfakes are allowed by political parties, allows for a lot of subjectivity and interpretive power on who defines those forms,” Prabhakar says..The increase in deepfakes, aided by the growing number of tools and services, have made it easy for non-experts to create deepfake videos. The Ideaz Factory is just one among several firms that has sprung up in India to profit from this access. There are deepfake portals and individual users across the world advertising to create custom deepfakes for as little as $30. Needless to say, this low barrier to entry has resulted in more covert deepfake operators, with the number of deepfakes doubling to 14,678 in 2019..This also means that most of the deepfake content will inevitably bypass fact-checking and tech experts and resources who are trying to curb the menace. Pratik Sinha, the founder of AltNews, an Indian fact-checking website that verifies claims and assertions made on social media, tells VICE, “At this point in time, it’s impossible to fact-check or verify something that you don’t recognise is doctored.” When VICE shared the videos with Sinha to check their validity, AltNews was unable to deduce it as fake. “This is dangerous,” says Sinha, whose organisation has fact checked thousands of morphed images and manipulated videos in the three years of its operation. “It’s the first time I’ve seen something like this emerge in India.”.In a country like India where digital literacy is nascent, even low-tech versions of video manipulation have led to violence. In 2018, more than 30 deaths were linked to rumours circulated on WhatsApp in India. “Deepfakes are going to be a supercharger on the kind of misinformation we have,” Sinha said. While tools to reliably detect deepfakes are currently unavailable, there have been efforts by researchers to develop a few. Reality Defender, a browser plugin for detecting fake videos, is one of them..However, experts like Sinha believe that no firm should be allowed to have a legitimate business around deepfake for election campaigns in India. In October last year, the state of California in the US passed a bill that made it illegal to circulate deepfake videos of politicians within 60 days of an election. The legislation was signed to protect voters from misinformation. But Prabhakar adds that in India, outlawing deepfakes is doomed to fail in implementation, as they would never be openly endorsed by political parties. “They would only continue to be operated by shadow firms,” she says..However, there could be a solution. Vishnoi thinks there should be government policy around misinformation as a whole, and the way to counter negative deepfakes is through awareness campaigns. But Dr Matthew Wright, the director of the Center for Cybersecurity at RIT, sees the emergence of deepfake for election campaign as “a potentially positive use case as long as there is disclosure.” “Why should our political leaders only be accessible to those who can read, assuming the translation is easily available in the right written language?” he asks. “But if it’s used deceptively, that’s a different story, and I’m sure some will blur the lines.”.By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.\n"}
{"prompt":"The use of facial recognition has racial bias and can be used unfairly\n\n###\n\n","completion":" The report from the justice sub-committee on policing was published as part of their inquiry into the advancement of live facial recognition..The report added: The use of live facial recognition technology would be a radical departure from Police Scotlands fundamental principle of policing by consent..The committee also said Police Scotland would need to ensure any technology in use would need to be provided for in legislation and meets human rights and data protection requirements..New computer systems are able to watch thousands of people at an incredibly fast pace, with the most powerful able to operate at distances of over a mile..Convener John Finnie said the sub-committee was reassured the force have no plans to introduce live facial recognition technology at present..Our inquiry has also shone light on other issues with facial recognition technology that we now want the Scottish Police Authority (SPA) and the Scottish government to consider..Not least amongst these are the legal challenges against similar technologies in England and Wales, and the apparent lack of law explicitly governing its use in Scotland - by any organisation..So whether this technology is being used by private companies, public authorities or the police, the Scottish government needs to ensure there is a clear legal framework to protect the public and police alike from operating in a facial recognition Wild West..Prior to any such technology being implemented we would carry out a robust programme of public consultation and engagement around the use of this technology, its legitimacy, viability and value for money, he added. .This would include taking advice and guidance on ethical, human rights and civil liberties considerations. In my view, the use of such technology would not be widespread but would be used in an intelligence-led, targeted way.\n"}
{"prompt":"Russian authorities are using facial recognition at mass gatherings and collecting all personal data\n\n###\n\n","completion":" The case is due to be held at the Tverskoy District Court of Moscow, and the complainants will seek to prohibit the use of facial recognition technology at mass gatherings, and demand that the authorities delete all stored personal data previously collected at such events..Facial recognition technology is by nature deeply intrusive, as it enables the widespread and bulk monitoring, collection, storage and analysis of sensitive personal data without individualised reasonable suspicion, said Natalia Zviagina, Amnesty International Russias Director. .In the hands of Russias already very abusive authorities, and in the total absence of transparency and accountability for such systems, it is a tool which is likely to take reprisals against peaceful protest to an entirely new level, she added.  .It is telling that the Russian government has provided no explanation as to how it will ensure the right to privacy and other human rights, nor has it addressed the need for public oversight of such powerful technologies. .Ms Popova has previously attempted to sue the Moscow police and information technology department, after she was fined for taking part in a protest in the capital last year..She claimed authorities used a facial recognition system to identify her without consent but a court dismissed her case in November, saying there was no proof that facial recognition was used..Local media reported that the Moscow Department of Information Technology has signed a deal with Russian firm Ntechlab, to provide the technology..The firm previously developed controversial app FindFace, which allowed users to photograph people in a crowd and work out their identities by tapping into social network vkontakte, known as Russias Facebook..The service was ended in 2018, with the firm saying it was concentrating its future efforts on face recognition solutions for government and business.\n"}
{"prompt":"Fake News could be spread by bots and lead to misinformation\n\n###\n\n","completion":" A new website, “NotRealNews.net,” uses artificial intelligence to populate what resembles a news site’s home page, complete with AI-written fake news stories..The website, a project by the AI development company Big Bird, is supposed to be a showcase of how the company’s algorithms can help journalists quickly write compelling news, according to the website’s “about” page..But despite the website’s title, the realistic-enough articles aren’t labeled as fake news or the marketing stunt that they are — meaning their existence is more likely to make journalists pull out their hair than it is to help them..Aside from entertaining algorithmic errors, like the headlines “Iran’s Stock Market: ‘There’s a Market,” and “In wake of death of British soldier, thousands as for plastic-free pubs,” the articles are mostly convincing. In fact, many appear to be closely based on actual news stories, like the resignation of UK finance minister Sajid Javid..It bears stating clearly: this is dangerous. Articles on the website contain fictional updates from the U.S. presidential race. Some are misinformation about the ongoing coronavirus outbreak, a news cycle that’s already full of confusing and sometimes conflicting reports. One headline describes a sexual assault..The idea, according to Big Bird’s logic, is that journalists would be able to save time by taking AI-generated article templates but to then validate and fact-check them by removing the algorithm’s errors and filling them in with factual information..Basically, it would turn reporters into the algorithm’s editor — but given the system’s believable prose and close proximity to the truth, it seems like that arrangement would almost guarantee that some fake news would slip through the cracks..It all raises a crucial question: Given that reporters are already perfectly capable of writing the news without a tech company’s help, who is this for?\n"}
{"prompt":"Secretive’ settled-status algorithm risks discrimination\n\n###\n\n","completion":" \t\t\t\t\t\tKeep abreast of significant corporate, financial and political developments around the world. \t\t\t\t\t\tStay informed and spot emerging risks and opportunities with independent global reporting, expert \t\t\t\t\t\tcommentary and analysis you can trust. \t\t\t\t\t\n"}
{"prompt":"An algorithmic risk scoring system that predicts the likelihood of social security claimants that will commit benefits or tax fraud is a breach of human rights because it targets low income and minority residents neighborhoods\n\n###\n\n","completion":" An algorithmic risk scoring system deployed by the Dutch state to try to predict the likelihood that social security claimants will commit benefits or tax fraud is a breach of human rights law, a court in the Netherlands has ruled..The Dutch government’s System Risk Indication (SyRI) legislation uses a non-disclosed algorithmic risk model to profile citizens and has been exclusively targeted at neighborhoods with mostly low-income and minority residents. Human rights campaigners have dubbed it a “welfare surveillance state.”.A number of civil society organizations in the Netherlands and two citizens instigated the legal action against SyRI — seeking to block its use. The court has today ordered an immediate halt to the use of the system..The ruling (now here in English) is being hailed as a landmark judgement by human rights campaigners, with the court basing its reasoning on European human rights law — specifically the right to a private life that’s set out by Article 8 of the European Convention on Human Rights (ECHR) — rather than a dedicated provision in the EU’s data protection framework (GDPR) which relates to automated processing..GDPR’s Article 22 includes the right for individuals not to be subject to solely automated individual decision-making where they can produce significant legal effects. But there can be some fuzziness around whether this applies if there’s a human somewhere in the loop, such as to review a decision on objection..Specifically, the court found that the SyRI legislation fails a balancing test in Article 8 of the ECHR which requires that any social interest to be weighed against the violation of individuals’ private life, with a fair and reasonable balance being required. The automated risk assessment system failed this test in the court’s view..Legal experts suggest the decision sets some clear limits on how the public sector in the UK can make use of AI tools — with the court objecting in particular to the lack of transparency about how the algorithmic risk scoring system functioned..In a press release about the judgement (translated to English using Google Translate), the court writes that the use of SyRI is “insufficiently clear and controllable”. While, per Human Rights Watch, the Dutch government refused during the hearing to disclose “meaningful information” about how SyRI uses personal data to draw inferences about possible fraud..The court clearly took a dim view of the state trying to circumvent scrutiny of human rights risk by pointing to an algorithmic “blackbox” and shrugging..The Courts reasoning doesnt imply there should be full disclosure, but it clearly expects much more robust information on the way (objective criteria) that the model and scores were developed and the way in which particular risks for individuals were addressed..The UN special rapporteur on extreme poverty and human rights, Philip Alston — who intervened in the case by providing the court with a human rights analysis — welcomed the judgement, describing it as “a clear victory for all those who are justifiably concerned about the serious threats digital welfare systems pose for human rights.”.“This decision sets a strong legal precedent for other courts to follow. This is one of the first times a court anywhere has stopped the use of digital technologies and abundant digital information by welfare authorities on human rights grounds,” he added in a press statement..Back in 2018, Alston warned that the UK government’s rush to apply digital technologies and data tools to socially re-engineer the delivery of public services at scale risked having an immense impact on the human rights of the most vulnerable..The judgement does not shut the door on the use by states of automated profiling systems entirely, but it does make it clear that human rights law in Europe must be central to the design and implementation of rights risking tools..It also comes at a key time when EU policymakers are working on a framework to regulate artificial intelligence — with the Commission pledging to devise rules that ensure AI technologies are applied ethically and in a human-centric way. .It remains to be seen whether the Commission will push for pan-EU limits on specific public sector uses of AI (such as for social security assessments). A recent leaked draft of a white paper on AI regulation suggests it’s leaning towards risk assessments and a patchwork of risk-based rules. \n"}
{"prompt":"Youtube's algorithm encourages misinformation by adding climate disinformation videos in \"up next\" feature\n\n###\n\n","completion":" What they found: One finding is that when users search for global warming, 16% of the top 100 related videos in the up next feature had climate disinformation. Another is that major brands are often unaware that their ads run on these videos..The big picture: Democratic Rep. Kathy Castor, who heads the Select Committee on the Climate Crisis, this week urged Google to curb false climate information on YouTube. She called for steps including....The other side: A YouTube spokesperson said the company has significantly invested in reducing recommendations of borderline content and harmful misinformation, and raising up authoritative voices.\n"}
{"prompt":"The use of facial recognition during protesting violates freedom of speech as well as leaves room for racial bias\n\n###\n\n","completion":" Police body cameras and tower camera video show some of what happened on May 30 as protesters squared off with Miami officers forming a line outside of their Downtown headquarters..The videos, exclusively obtained by NBC 6 Investigators, captured heated moments as objects were thrown at officers and they popped tear gas to retake control of patrol cars..Police say Oriana Albornoz, 25, threw two rocks at an officer hitting him once and injuring his leg. The department provided a video that shows her throwing something at officers standing across the street but it is difficult to discern what it is..A recent NBC 6 investigation found police departments across South Florida, including Miami, are using the technology, which identifies people through publicly available photos including social media sites like Facebook, LinkedIn and Instagram..“It looks like they’ve just done a regular photographic line up and had it not been for the vigilance of your news agency, I would not have known this,” Gottlieb said. .“We don’t know where they got the image, Gottlieb said. So how or where they got her image from begs other privacy rights. Did they dig through her social media? How did they get access to her social media? he asked.”.According to the Miami Police’s policy, facial recognition technology shall not be used to conduct surveillance of people exercising “constitutionally protected activities” like protesting. . “This means that if someone is peacefully protesting and not committing a crime, we cannot use it against them,” Miami Police Assistant Chief Armando Aguilar told NBC 6 in an earlier interview..“We have used the technology to identify violent protesters who assaulted police officers, who damaged police property, who set property on fire. We have made several arrests in those cases and more arrests are coming in the near future,” he said. .The department’s policy requires keeping a log documenting all facial recognition searches and conducting monthly audits. It also states that “a positive facial recognition search result alone does not constitute probable cause of an arrest.”.Gottlieb calls the police investigation “very disingenuous” and tells us he’s concerned there are no statewide rules and regulations for facial recognition. .NBC 6 Investigators found Miami Police have a much more detailed policy than other departments using the technology in South Florida. The Broward County Sheriff’s Office, Miami-Dade Police and Coral Spring Police told NBC 6 they have not used the technology to make arrests during the protests following the death of George Floyd..In an email, the Clearview’s CEO Hon Ton-That told NBC 6: “Clearview AI is also committed to the responsible use of its powerful technology and is used only for after-the-crime investigations to help identify criminal suspects. It is not intended to be used as a surveillance tool relating to protests or under any other circumstances.”\n"}
{"prompt":"This product uses machine learning to detect emotion. However, a machine analyzing something as complex as a human emotion correctly is unlikely and there are many data privacy concerns.\n\n###\n\n","completion":" Amazon on Thursday launched Amazon Halo, a wearable band to compete with FitBit and Apple Watch. Like its competitors, Halo can track heart rate and sleep patterns, but its also looking to differentiate itself with a peculiar feature: judging your emotional state from your tone of voice..Amazon Tone claims to tell you how you sound to other people. It uses machine learning to analyze energy and positivity in a customers voice so they can better understand how they may sound to others, helping improve their communication and relationships, Amazons press release for Halo reads..To give an example, Amazons chief medical officer Maulik Majmudar said Tone might give you feedback such as: In the morning you sounded calm, delighted, and warm. According to Majmudar, Tone analyzes vocal qualities like your pitch, intensity, tempo, and rhythm to tell you how it thinks you sound to other people..Experts that Business Insider spoke to are dubious that an algorithm could accurately analyze something as complex as human emotion — and they are also worried that Tone data could end up with third parties..I have my doubts that current technology is able to decipher the very complex human code of communication and the inner workings of emotion, said Dr Sandra Wachter, associate professor in AI ethics at the University of Oxford..How we use our voice and language is greatly impacted by social expectation, culture and customs. Expecting an algorithm to be able to read and understand all of those subtleties seems more like an aspirational endeavour, she said..Here the machine has to understand how someone speaks (and what they say) AND infer how someone else understands and interprets these words. This is an even more complex task because you have to read two minds. An algorithm as a mediator or interpreter seems very odd, I doubt that a system (at least at this point) is able to crack this complex social code, she said..Mozilla fellow Frederike Kaltheuner agreed that voice analysis has inherent limitations. Voice recognition systems have also historically struggled with different kinds of voices, she said. Accuracy is typically lower for people who speak with an accent or who are speaking in a second language..Amazon says it has made the Tone feature opt-in for Halo owners. Once you switch it on, it runs in the background, recording short snippets of your voice throughout the day for analysis. Theres also an option to turn it on for specific conversations, up to 30 minutes in length..Amazon says all this data is kept safe and secure, with all the processing done locally on your phone, which then deletes the data. Tone speech samples are never sent to the cloud, which means nobody ever hears them, and you have full control of your voice data, Majmudar wrote..Amazons insistence that human employees wont listen to any of Tones recordings seems to allude to the time Amazon, along with the other major companies, was caught in a scandal after reports revealed that sensitive Alexa recordings were being sent to human contractors for review..Privacy policy expert Nakeema Stefflbauer told Business Insider that Halo could be a preamble to Amazon getting into insurance tech. My first impression is that its almost as if Amazon is moving as fast as possible to get ahead of public disclosures about its own forays into the insurtech space, said Stefflbauer..I am alarmed when I hear about this type of assessment being recorded, because, while I see zero benefit from it, employers definitely might. Insurers definitely might. Public administrators overseeing the issue of benefits (such as for unemployment) definitely might, she added. .The ultimate sign to me that you as the customer arent the ultimate target of the data collected is that Amazon already has partnerships with insurers like John Hancock and medical records companies like Cerner, Stefflbauer added..John Hancock announced Thursday it would be the first life insurer to integrate with Amazon Halo. Starting this fall, all John Hancock Vitality customers will be able to link the Amazon Halo Band to the program to earn Vitality Points for the small, everyday steps they take to try to live a longer, healthier life, the insurance firm said in a press statement..Kaltheuner said its good that the Tone feature is opt-in, but anonymized data from Halo could still be shared in bulk with third parties. Even if its in aggregate and anonymous, it might not be something you want your watch to do, she said..Chris Gilliard, an expert on surveillance and privacy at the Digital Pedagogy Lab, told Business Insider he found Amazons privacy claims unconvincing..Amazon felt the heat when it was revealed that actual humans were listening to Alexa recordings, so this is their effort to short circuit that particular critique, but to say that these systems will be private stretches the meaning of that word beyond recognition, he said..Wachter said that if, as Amazon claims, an algorithm was capable of accurately analyzing the emotion in peoples voices, it could pose a potential human rights problem..Our emotions and thoughts are one of the most intimate and personal aspects of our personality. In addition, we are often not able to control our emotions. Our inner thoughts and emotions are at the same time very important to form opinions and express those. This is one of the reasons why human rights law does not allow any intrusion on them.\n"}
{"prompt":"When a company charges based on inherent human features, this increases racial and gender bias. \n\n###\n\n","completion":" Algorithms rule our digital lives. They determine what we see and when we see it and in the case of some dating and ride-share apps, they determine how much we pay. Without seeing how an algorithm works, however, that latter aspect can be rife for discriminatory pricing. And there’s no real legislation in Australia to stop it..A months-long investigation by Choice reporter, Saimi Jeong, revealed Tinder had allegedly been charging its Plus users different amounts depending on age, gender, sexuality and location in Australia. Tinder introduced a tiered pricing system the United States in 2015 which was ultimately removed after a $US11.5 million class action lawsuit in California..“We do offer a variety of subscription options and paid a la carte features designed to help our members stand out and match with new people more efficiently,” a Tinder spokesperson said in a statement to Gizmodo Australia..“Tinder operates a global business and our pricing varies by a number of factors. We frequently offer promotional rates — which can vary based on region, length of subscription, bundle size and more. We also regularly test new features and payment options.”.Charging users different prices depending on age is not new — we often have children and senior’s fares — but it’s a question of ethics when other markers are incorporated with little-to-no transparency. .Uber’s former head of product, Daniel Graf, told Bloomberg in 2017 the rideshare company had switched to a “route-based” pricing model. This meant, according to Graf, Uber’s algorithm incorporated machine-learning techniques to figure how much a customer was willing to pay for a ride..According to this model, two users might have be charged different amounts based on how often they used the app or whether they were travelling to a more affluent neighbourhood, even if demand, traffic and distance were the same..A New Scientist report in June 2020 revealed a U.S. analysis showing some ride-share users that travelled to an area with a higher population of Black people were charged more. It used data from more than 100 million trips between November 2018 and December 2019 in the Chicago area..Uber Australia has maintained it doesn’t use this pricing model, pointing to its pricing estimate page. The page explains pricing in Australia is determined by three primary factors — a flat fee depending on your city or location, the time and distance of the trip as well as the demand of drivers at the time..An algorithm is a computer program that follows a series of statements in order to deliver an outcome. In the case of social media sites, it reads your search queries or viewing habits and serves up content based on your interests. A similar thing happens on streaming services, too..Associate Professor Timothy Miller from Melbourne University’s Centre for Artificial Intelligence and Digital Ethics (CAIDE) explained the dating app situation was likely a mixture of these two..“It seemed to me like, possibly, the more recent [dating app] example was using machine learning,” Professor Miller told Gizmodo Australia in a Zoom call..“They had actually gotten personal data around what sexuality [users] were, how old they were and where they lived and they’ve kind of learned that, well, people in this particular area would be of a particular affluence or they’d be more likely to pay for it.”.Of course, it’s speculation and that’s due to a few factors. Firstly, companies aren’t going to simply admit they’re charging you differently because the algorithm sees you as more cashed up — that would be detrimental to consumer trust..It’s also that the tech space is a competitive world and that giving away too much about how your proprietary algorithm works might be a bad business choice..“But they start to give more and more information away about the business model effectively, because people can ask enough questions that they can kind of reconstruct the model themselves.”.Unfortunately, this results in tech companies using shadowy methods that ultimately benefit them financially — and users are none the wiser..There are counter-factuals, Professor Miller added, that might give you a chance to trick the algorithm into showing what others are charged..“So, changing your gender, changing your sexuality, changing the region you live in … but I don’t see companies like [that] offering those types of explainability tools at all,” Professor Miller said..“The only real way you can know it is if there is something built in or you get enough people together [to check] or you set up enough accounts yourself in order to change all these parameters and see what the impact is.”.If it’s near impossible for a user to really know what an algorithm is designed to do or charge, how can anyone trust them beyond anecdotal evidence? One idea is to introduce a regulatory body or establish an ethics framework that companies would have to abide by..In 2019, the European Commission proposed a set of guidelines artificial intelligence (AI) systems would need to meet in order to be rubber stamped as trustworthy..Broadly, it asserted AI systems should be lawful, ethical and robust. On a more granular level, it expects them to consider human agency and incorporate oversight, to take diversity, non-discrimination and fairness into account and provide transparency and accountability among other key points..Dr Nick Patterson, a cybersecurity researcher at Deakin University, said it makes sense that something similar is established in Australia too..“[A formal regulatory body] is something which should be established to ensure ethical algorithm decisions — especially when it involves them making important decisions on human lives,” Dr Patterson said..But Dr Patterson thinks it’s quite tricky regulating the algorithms themselves. Some low-stakes algorithmic deciders, like whether you pay a few dollars more for an an app subscription, should be left for customers to decide. It’s the potentially life-changing or saving decisions, Dr Patterson said, that need a tougher look..“If it’s something like … making court decisions or in a hospital, an algorithm making medical decisions should be reviewed by a panel of experts, such as IEEE [Institute of Electrical and Electronics Engineers] or equivalent, to determine safety.”.“Yes, there’s a principle around being fair or there’s a principle about being transparent, but what does that mean to the person on Monday morning when they’re sitting down at their desk to operationalise these procedures? There’s a lot of work that needs to be done there..“In the case of discriminative pricing, we need to look at existing legislation and say ‘we need to update this for the modern era, where people are really discriminating based on a whole lot of factors that we didn’t think about when we wrote this up 20 years ago.”.Professor Miller and Dr Patterson don’t have the simple answers to a complicated situation but the baby steps happening in Europe are providing hope..In Australia, the Human Rights Commission launched a discussion paper in late 2019 surrounding the ethics of emerging technologies, including AI-operated algorithms..“AI is being used to make decisions that unfairly disadvantage people on the basis of their race, age, gender or other characteristic. This problem arises in high-stakes decision making, such as social security, policing and home loans,” Commissioner Edward Santow wrote at its launch..“These risks affect all of us, but not equally. We saw how new technologies are often ‘beta tested’ on vulnerable or disadvantaged members of our community.”\n"}
{"prompt":"Heron developed its own reinforcement learning algorithm from scratch and won a dogfight contest, a competition where AI is used for mission-critical military tasks that were once done only by humans\n\n###\n\n","completion":" In July 2015, two founders of DeepMind, a division of Alphabet with a reputation for pushing the boundaries of artificial intelligence, were among the first to sign an open letter urging the world’s governments to ban work on lethal AI weapons. Notable signatories included Stephen Hawking, Elon Musk, and Jack Dorsey..Last week, a technique popularized by DeepMind was adapted to control an autonomous F-16 fighter plane in a Pentagon-funded contest to show off the capabilities of AI systems. In the final stage of the event, a similar algorithm went head-to-head with a real F-16 pilot using a VR headset and simulator controls. The AI pilot won, 5-0..The episode reveals DeepMind caught between two conflicting desires. The company doesn’t want its technology used to kill people. On the other hand, publishing research and source code helps advance the field of AI and lets others build upon its results. But that also allows others to use and adapt the code for their own purposes..Others in AI are grappling with similar issues, as more ethically questionable uses of AI, from facial recognition to deepfakes to autonomous weapons, emerge..A DeepMind spokesperson says society needs to debate what is acceptable when it comes to AI weapons. “The establishment of shared norms around responsible use of AI is crucial,” she says. DeepMind has a team that assesses the potential impacts of its research, and the company does not always release the code behind its advances. “We take a thoughtful and responsible approach to what we publish,” the spokesperson adds..The AlphaDogfight contest, coordinated by the Defense Advanced Research Projects Agency (Darpa), shows the potential for AI to take on mission-critical military tasks that were once exclusively done by humans. It might be impossible to write a conventional computer program with the skill and adaptability of a trained fighter pilot, but an AI program can acquire such abilities through machine learning..“The technology is developing much faster than the military-political discussion is going,” says Max Tegmark, a professor at MIT and cofounder of the Future of Life Institute, the organization behind the 2015 letter opposing AI weapons..The US and other countries are rushing to embrace the technology before adversaries can, and some experts say it will be difficult to prevent nations from crossing the line to full autonomy. It may also prove challenging for AI researchers to balance the principles of open scientific research with potential military uses of their ideas and code..Without an international agreement restricting the development of lethal AI weapons systems, Tegmark says, America’s adversaries are free to develop AI systems that can kill. “Were heading now, by default, to the worst possible outcome,” he says..US military leaders—and the organizers of the AlphaDogfight contest—say they have no desire to let machines make life-and-death decisions on the battlefield. The Pentagon has long resisted giving automated systems the ability to decide when to fire on a target independent of human control, and a Department of Defense Directive explicitly requires human oversight of autonomous weapons systems..But the dogfight contest shows a technological trajectory that may make it difficult to limit the capabilities of autonomous weapons systems in practice. An aircraft controlled by an algorithm can operate with speed and precision that exceeds even the most elite top-gun pilot. Such technology may end up in swarms of autonomous aircraft. The only way to defend against such systems would be to use autonomous weapons that operate at similar speed..“One wonders if the vision of a rapid, overwhelming, swarm-like robotics technology is really consistent with a human being in the loop,” says Ryan Calo, a professor at the University of Washington. “Theres tension between meaningful human control and some of the advantages that artificial intelligence confers in military conflicts.”.AI is moving quickly into the military arena. The Pentagon has courted tech companies and engineers in recent years, aware that the latest advances are more likely to come from Silicon Valley than from conventional defense contractors. This has produced controversy, most notably when employees of Google, another Alphabet company, protested an Air Force contract to provide AI for analyzing aerial imagery. But AI concepts and tools that are released openly can also be repurposed for military ends..DeepMind released details and code for a groundbreaking AI algorithm only a few months before the anti-AI weapons letter was issued in 2015. The algorithm used a technique called reinforcement learning to play a range of Atari video games with superhuman skill. It attains expertise through repeated experimentation, gradually learning what maneuvers lead to higher scores. Several companies participating in AlphaDogfight used the same idea..DeepMind has released other code with potential military applications. In January 2019, the company released details of a reinforcement learning algorithm capable of playing StarCraft II, a sprawling space strategy game. Another Darpa project called Gamebreaker encourages entrants to generate new AI war-game strategies using Starcraft II and other games..Other companies and research labs have produced ideas and tools that may be harnessed for military AI. A reinforcement learning technique released in 2017 by OpenAI, another AI company, inspired the design of several of the agents involved with AlphaDogfight. OpenAI was founded by Silicon Valley luminaries including Musk and Sam Altman to “avoid enabling uses of AI … that harm humanity,” and the company has contributed to research highlighting the dangers of AI weapons. OpenAI declined to comment..Some AI researchers feel they are simply developing general-purpose tools. But others are increasingly worried about how their research may end up being used..“At the moment Im deep in a crossroads in my career, trying to figure out whether ML can do more good than bad,” says Julien Cornebise, as associate professor at University College London who previously worked at DeepMind and ElementAI, a Canadian AI firm..Cornebise also worked on a project with Amnesty International that used AI to detect destroyed villages from the Darfur conflict using on satellite imagery. He and the other researchers involved chose not to release their code for fear that it could be used to target vulnerable villages..Calo of the University of Washington says it will be increasingly important for companies to be upfront with their own researchers about how their code might be released. “They need to have the capacity to opt out of projects that offend their sensibilities,” he says..It may prove difficult to deploy the algorithms used in the Darpa contest in real aircraft, since the simulated environment is so much simpler. There is also still much to be said for a human pilot’s ability to understand context and apply common sense when faced with a new challenge..Still, the death match showed the potential of AI. After many rounds of virtual combat, the AlphaDogfight contest was won by Heron Systems, a small AI-focused defense company based in California, Maryland. Heron developed its own reinforcement learning algorithm from scratch..In the final matchup, a US Air Force fighter pilot with the call sign “Banger” engaged with Heron’s program using a VR headset and a set of controls similar to those inside a real F-16..In the first battle, Banger banked aggressively in an attempt to bring his adversary into sight and range. But the simulated enemy turned just as fast, and the two planes became locked in a downward spiral, each trying to zero in on the other. After a few turns, Banger’s opponent timed a long-distance shot perfectly, and Banger’s F-16 was hit and destroyed. Four more dogfights between the two opponents ended roughly the same way..Brett Darcey, vice president of Heron, says his company hopes the technology eventually finds its way into real military hardware. But he also thinks the ethics of such systems are worth discussing. “I would love to live in a world where we have a polite discussion over whether or not the system should ever exist,” he says. “If the United States doesnt adopt these technologies somebody else will.”.Updated 8-27-2020, 10:55 am EDT: This story was updated to clarify that Heron Systems is based in California, Maryland, and not the state of California.\n"}
{"prompt":"The algorithm risks \"levelling down\" city and town centres.\n\n###\n\n","completion":" Under the changes to planning laws, local discretion over the rate of housebuilding will be removed and central government will “distribute” an annual target, at present 337,000 a year, among local councils. They will then be required to designate enough land to meet the target..Analysis by Lichfields, a planning consultancy, has suggested that outside London much of the new housing will be concentrated in Conservative local authority areas in the suburbs and the shires, rather than in town centres.\n"}
{"prompt":"After a lack of transparency in welfare decisions, 20 councils stopped using computer algorithms\n\n###\n\n","completion":" Councils are quietly scrapping the use of computer algorithms in helping to make decisions on benefit claims and other welfare issues, the Guardian has found, as critics call for more transparency on how such tools are being used in public services..It comes as an expert warns the reasons for cancelling programmes among government bodies around the world range from problems in the way the systems work to concerns about bias and other negative effects. Most systems are implemented without consultation with the public, but critics say this must change..The use of artificial intelligence or automated decision-making has come into sharp focus after an algorithm used by the exam regulator Ofqual downgraded almost 40% of the A-level grades assessed by teachers. It culminated in a humiliating government U-turn and the system being scrapped..The fiasco has prompted critics to call for more scrutiny and transparency about the algorithms being used to make decisions related to welfare, immigration, and asylum cases. .The Guardian has found that about 20 councils have stopped using an algorithm to flag claims as “high risk” for potential welfare fraud. The ones they flagged were pulled out by staff to double-check, potentially slowing down people’s claims without them being aware..Previous research by the Guardian found that one in three councils were using algorithms to help make decisions about benefit claims and other welfare issues. .Their research also found that Hackney council in east London had abandoned using data analytics to help predict which children were at risk of neglect and abuse..The Data Justice Lab found at least two other councils had stopped using a risk-based verification system – which identifies benefit claims that are more likely to be fraudulent and may need to be checked..One council found it often wrongly identified low-risk claims as high-risk, while another found the system did not make a difference to its work..Dr Joanna Redden from the Data Justice Lab said: “We are finding that the situation experienced here with education is not unique … algorithmic and predictive decision systems are leading to a wide range of harms globally, and also that a number of government bodies across different countries are pausing or cancelling their use of these kinds of systems..“The reasons for cancelling range from problems in the way the systems work to concerns about negative effects and bias. We’re in the process of identifying patterns, but one recurring factor tends to be a failure to consult with the public and particularly with those who will be most affected by the use of these automated and predictive systems before implementing them.”.The Home Office recently stopped using an algorithm to help decide visa applications after allegations that it contained “entrenched racism”. The charity the Joint Council for the Welfare of Immigrants (JCWI) and the digital rights group Foxglove launched a legal challenge against the system, which was scrapped before a case went to court..Foxglove characterised it as “speedy boarding for white people” but the Home Office said it did not accept that description. “We have been reviewing how the visa application streaming tool operates and will be redesigning our processes to make them even more streamlined and secure,” the Home Office added..Martha Dark, the director and co-founder of Foxglove, said: “Recently we’ve seen the government rolling out algorithms as solutions to all kinds of complicated societal problems. It isn’t just A-level grades … People are being sorted and graded, denied visas, benefits and more, all because of flawed algorithms.”.She said poorly designed systems could lead to discrimination, adding that there had to be democratic debate and consultation with the public on any system that affected their lives before that system was implemented. “These systems have to be transparent, so bias can be identified and stopped.”.The West Midlands police and crime commissioner’s strategic adviser, Tom McNeil, said he was “concerned” businesses were pitching algorithms to police forces knowing their products may not be properly scrutinised. .McNeil said: “In the West Midlands, we have an ethics committee that robustly examines and publishes recommendations on artificial intelligence projects. I have reason to believe that the robust and transparent process we have in the West Midlands may have deterred some data science organisations from getting further involved with us.”.Research from the Royal Society of Arts published in April found at least two forces were using or trialling artificial intelligence or automated decision-making to help them identify crime hotspots – Surrey police and West Yorkshire police..Others using algorithms in some capacity or other include the Met, Hampshire Constabulary, Kent police, South Wales police, and Thames Valley police..Asheem Singh, the RSA thinktank’s director of economics, said: “Very few police consulted with the public. Maybe great work is going on but police forces don’t want to talk about it. That is concerning. We are talking about black-box formulae affecting people’s livelihoods. This requires an entire architecture of democracy that we have not seen before.”.The Centre for Data Ethics and Innovation, an independent advisory body, is reviewing potential bias in algorithms. “Our review will make recommendations about how police forces and local authorities using predictive analytics are able to meet the right standards of governance and transparency for the challenges facing these sectors,” it said.\n"}
{"prompt":"After 40% of predicted results were downgraded, the government decided to award grades on the basis of what teachers submitted.\n\n###\n\n","completion":" Gavin Williamson and Ofqual have apologised to students and their parents, as they announced that all A-level and GCSE results in England will be based on teacher-assessed grades..In a spectacular U-turn, the education secretary announced the government would scrap the controversial standardisation model drawn up by the exams regulator to award grades in lieu of exams..“We worked with Ofqual to construct the fairest possible model, but it is clear that the process of allocating grades has resulted in more significant inconsistencies than can be resolved through an appeals process,” Williamson said, in remarks released by his department..“We now believe it is better to offer young people and parents certainty by moving to teacher assessed grades for both A- and AS level and GCSE results. I am sorry for the distress this has caused young people and their parents, but hope this announcement will now provide the certainty and reassurance they deserve.”.The climbdown comes after days of turmoil triggered by the publication of A-level results last Thursday, when almost 40% of predicted results were downgraded, with some students marked down two or even three grades, which resulted in many losing university places..The move had looked inevitable after Wales said it would revert to teacher assessments for A-levels, and Northern Ireland said it would do so for GCSEs, following a similar U-turn in Scotland last week..“The system, for the overwhelming majority of young people, is going to deliver credible, strong results. It’s a robust system, it’s a fair system, it’s making sure that young people get the grades that they’ve worked so hard towards”.Q) “Can you give a cast-iron guarantee that you will not be forced into the embarrassing U-turn that John Swinney and Nicola Sturgeon were in Scotland?”A) Absolutely.“Let’s be in no doubt about it, the exam results that we’ve got today are robust. They’re good, they’re dependable for employers. It’s very important that for years to come people should be able to look at these grades and think these are robust, these are dependable”.“This is it… No U-turn, no change… [In Scotland] you’ve got a system where there aren’t any controls, you’ve got rampant grade inflation. There’s been no checks and balances in that system; it degrades every single grade as a result and in-baked unfairness” .Hundreds of pupils took to the streets of London, demonstrating outside the Department for Education to express their anger, while others took to the airwaves and social media to describe their sense of devastation. Lawyers had began to consider taking action on behalf of affected teenagers..The Labour leader, Keir Starmer, welcomed what he called the “screeching U-turn”, saying: “This is a victory for the thousands of young people who have powerfully made their voices heard this past week. However, the Tories’ handling of this situation has been a complete fiasco..“Incompetence has become this government’s watchword, whether that is on schools, testing or care homes. Boris Johnson’s failure to lead is holding Britain back.”.Ministers had been under increasing pressure to act with GCSE results due to be published this Thursday for more than 600,000 year 11 pupils in England and 100,000 older students aiming for crucial passes in maths and English to qualify them for further training or study..The algorithm used by Ofqual for both A-levels and GCSEs was mainly based on a school’s past results and individual pupil attainment. Teachers and schools were asked to submit grades, known as centre-assessed grades, but in the end these were influential in only a small number of cases..Ofqual argued that the algorithm was essential to ensure results were standardised across the country and in line with previous years, but hundreds of individual stories documenting disappointment and an overwhelming sense of injustice among those affected proved too much to ignore..Ofqual confirmed its decision to award centre assessment grades (CAG) to A-level, AS-level and GCSE students, adding that where a moderated grade was higher than a CAG, students could keep that grade..Roger Taylor, Ofqual’s chair, acknowledged the standardised system had caused widespread anguish and damaged public confidence, and apologised to those affected..“We understand this has been a distressing time for students, who were awarded exam results last week for exams they never took. The pandemic has created circumstances no one could have ever imagined or wished for. We want to now take steps to remove as much stress and uncertainty for young people as possible, and to free up heads and teachers to work towards the important task of getting all schools open in two weeks,” he said..“After reflection, we have decided that the best way to do this is to award grades on the basis of what teachers submitted. The switch to centre assessment grades will apply to both AS- and A-levels and to the GCSE results which students will receive later this week..“There was no easy solution to the problem of awarding exam results when no exams have taken place. Ofqual was asked by the secretary of state [Gavin Williamson] to develop a system for awarding calculated grades, which maintained standards and ensured that grades were awarded broadly in line with previous years. Our goal has always been to protect the trust that the public rightly has in educational qualifications..“But we recognise that while the approach we adopted attempted to achieve these goals, we also appreciate that it has also caused real anguish and damaged public confidence. Expecting schools to submit appeals where grades were incorrect placed a burden on teachers when they need to be preparing for the new term and has created uncertainty and anxiety for students. For all of that, we are extremely sorry..“The path forward we now plan to implement will provide urgent clarity. We are already working with the Department for Education, universities and everyone else affected by this issue.”.“It’s a relief that this whole unedifying mess has some form of closure,” said Jules White, a secondary school headteacher and leader of Worth Less? “Students will get a much fairer deal and everyone can be pleased with that..“Major questions remain, though: why has it taken the DfE so long to resolve matters? Why have Ofqual and the DfE just spent time blaming each other rather than acting on behalf of children, their families and schools?.“And crucially, how can we move confidently to wider school reopening when our political masters don’t understand how schools actually work?”.Paul Whiteman, the general secretary of the NAHT school leaders’ union, said: “The government has decided to rely on centre assessed grades for A level and GCSE results this year, agreeing that these are the most reliable measure of student performance..“School leaders and teachers worked with professionalism and integrity to submit these grades for all of their students. Having taken so long to make a decision, this was the only option that government had left to deal with the unfairness.“This decision will mean students expecting their GCSE results can have confidence that they will not experience the same unfairness or disadvantage as their older peers.“The big question remains as to why this decision has taken so long to come, as it may already be too late for some A level students who have already missed out on their first choice of university and course. Every day of delay is going to have loaded more and more difficulty onto universities and their capacity to meet all of the demand for places that will now inevitably come their way. For them, the problem is far from over.”\n"}
{"prompt":"Robinhood utilized high-speed trading (using computer programs to transact a large number of orders in fraction of seconds) without fully disclosing to clients\n\n###\n\n","completion":" The commission-free investing app is facing a civil fraud investigation over its early failure to fully disclose its practice of selling clients’ orders to high-speed trading firms, The Wall Street Journal reported Wednesday..High-speed trading is the practice of using computer programs to transact a large number of orders in fractions of a second. The Securities and Exchange Commission is in an advanced stage and the company could have to pay a fine exceeding $10 million if it agrees to settle with the agency, unnamed sources told the Journal..Until 2018, Robinhood reportedly did not disclose it took payments from high-speed trading firms for sending them customers’ orders to buy or sell stocks or options..Its not the first time the company has been in hot water. In its seven year history, Robinhood has faced several outages and canceled its plans to expand to the U.K..A disrupter in the industry, Robinhood’s popularity forced competitors, like Charles Schwab and TD Ameritrade, to emulate its commission-free approach.\n"}
{"prompt":"Algorithmic Policing causes implications for rights of privacy, right to freedom of expression, peaceful assembly and association\n\n###\n\n","completion":" This report examines algorithmic technologies that are designed for use in criminal law enforcement systems. Algorithmic policing is an area of technological development that, in theory, is designed to enable law enforcement agencies to either automate surveillance or to draw inferences through the use of mass data processing in the hopes of predicting potential criminal activity. The latter type of technology and the policing methods built upon it are often referred to as predictive policing. Algorithmic policing methods often rely on the aggregation and analysis of massive volumes of data, such as personal information, communications data, biometric data, geolocation data, images, social media content, and policing data (such as statistics based on police arrests or criminal records)..In order to guide public dialogue and the development of law and policy in Canada, the report focuses on the human rights and constitutional law implications of the use of algorithmic policing technologies by law enforcement authorities. This report first outlines the methodology and scope of analysis in Part 1. In Part 2, the report provides critical social and historical contexts regarding the criminal justice system in Canada, including issues regarding systemic discrimination in the criminal justice system and bias in policing data sets. This social and historical context is important to understand how algorithmic policing technologies present heightened risks of harm to civil liberties and related concerns under human rights and constitutional law for certain individuals and communities. The use of police-generated data sets that are affected by systemic bias may create negative feedback loops where individuals from historically disadvantaged communities are labelled by an algorithm as a heightened risk because of historic bias towards those communities. Part 3 of the report then provides a few conceptual building blocks to situate the discussion surrounding algorithmic policing technology, and it outlines how algorithmic policing technology differs from traditional policing methods..In Part 4, the report sets out and summarizes findings on how law enforcement agencies across Canada have started to use, procure, develop, or test a variety of algorithmic policing methods. The report compiles original research with existing research to provide a comprehensive overview of what is known about the algorithmic policing landscape in Canada to date. In the overview of the use of algorithmic policing technology in Canada, the report classifies algorithmic policing technologies into the following three categories:.The primary research findings of this report show that technologies have been procured, developed, or used in Canada in all three categories. For example, at least two agencies, the Vancouver Police Department and the Saskatoon Police Service, have confirmed that they are using or are developing ‘predictive’ algorithmic technologies for the purposes of guiding police action and intervention. Other police services, such as in Calgary and Toronto, have acquired technologies that include algorithmic policing capabilities or that jurisdictions outside of Canada have leveraged to build predictive policing systems. The Calgary Police Service engages in algorithmic social network analysis, which is a form of technology that may also be deployed by law enforcement to engage in person-focused algorithmic policing. Numerous law enforcement agencies across the country also now rely on a range of other algorithmic surveillance technologies (e.g., automated licence plate readers, facial recognition, and social media surveillance algorithms), or they are developing or considering adopting such technologies. This report also uncovers information suggesting that the Ontario Provincial Police and Waterloo Regional Police Service may be unlawfully intercepting private communications in online private chat rooms through reliance on an algorithmic social media surveillance technology known as the ICAC Child On-line Protection System (ICACCOPS). Other police services throughout Canada may also be using or developing additional predictive policing or algorithmic surveillance technologies outside of public awareness. Many of the freedom of information (FOI) requests submitted for this report were met with responses from law enforcement authorities that claimed privilege as justification for non-disclosure; in other cases, law enforcement agencies did not provide any records in response to the submitted FOI request, or requested exorbitant fees in order to process the request..Building on the findings about the current state of algorithmic policing in Canada, Part 5 of the report presents a human rights and constitutional law analysis of the potential use of algorithmic policing technologies. The legal analysis applies established legal principles to these technologies and demonstrates that their use by law enforcement agencies has the potential to violate fundamental human rights and freedoms that are protected under the Canadian Charter of Rights and Freedoms (“the Charter”) and international human rights law. Specifically, the authors analyze the potential impacts of algorithmic policing technologies on the following rights: the right to privacy; the right to freedoms of expression, peaceful assembly, and association; the right to equality and freedom from discrimination; the right to liberty and to be free from arbitrary detention; the right to due process; and the right to a remedy. The major findings of this analysis are presented as follows:.In addition to these major findings, the report documents problems that are likely to arise with respect to meaningful access to justice and the rights to due process and remedy, given that impactful accountability mechanisms for algorithmic policing technology are often lacking, and in light of the systemic challenges faced by individuals and communities seeking meaningful redress for rights violations that do not result in charges in Canadian courts. The absence of much needed transparency in the Canadian algorithmic policing landscape animates many of the core recommendations in this report. The authors hope that this report provides insight into the critical need for transparency and accountability regarding what types of technologies are currently in use or under development and how these technologies are being used in practice. With clarified information regarding what is currently in use and under development, policy- and lawmakers can enable the public and the government to chart an informed path going forward..In response to conclusions drawn from the legal analysis, the report ends with a range of recommendations for governments and law enforcement authorities with a view to developing law and oversight that would establish necessary limitations on the use of algorithmic policing technologies. Part 6 provides a list of these recommendations, each of which is accompanied by contextual information to explain the purpose of the recommendation and offer potential guidance for implementation. The recommendations are divided into priority recommendations, which must be implemented now, with urgency, and ancillary recommendations, which may be inapplicable where certain algorithmic policing technologies are banned but must be implemented if any such technologies are to be developed or adopted. The following is a condensed summary of those recommendations..A. Priority recommendations for governments and law enforcement authorities that must be acted upon urgently in order to mitigate the likelihood of human rights and Charter violations associated with the use of algorithmic policing technology in Canada:.D. Ancillary recommendations for government to enable access to justice in relation to the human rights impacts of algorithmic policing technology:.The Citizen Lab would like to thank the following funders for supporting this research: the John D. and Catherine T. MacArthur Foundation, the Sigrid Rausing Trust, the Ford Foundation, and the Oak Foundation. This research was also supported in part by a grant from the Open Society Foundation.\n"}
{"prompt":"These discrepancies can be extremely harmful to the patient\/client when making decisions about treatment and diagnosis.\n\n###\n\n","completion":" Deep-learning algorithms suffer from a fundamental problem: They can adopt unwanted biases from the data on which theyre trained. In healthcare, this can lead to bad diagnoses and care recommendations..In October 2019, a group of researchers from several universities published a damning revelation: A commercial algorithm widely used by health organizations was biased against black patients..   The algorithm, later identified as being provided by health-services company Optum, helped providers determine which patients were eligible for extra care. According to the researchers findings, the algorithm gave higher priority to white patients when it came to treating complex conditions, including diabetes and kidney problems..This is one of several recent stories involving algorithmic bias: the tendency of artificial intelligence to make decisions that give an unfair advantage to a certain group or demographic. Algorithmic bias can manifest in many fields, but in medicine it can be deadly..Most improvements in AI systems are made because of advances in machine learning and deep learning. Unlike traditional AI systems, which were based on manually crafted software rules, deep-learning systems develop their behavior by examining lots of examples..For instance, to develop a deep-learning system that predicts breast cancer, AI engineers created a base algorithm and fed it mammograms annotated with the patient outcome—cancer or no cancer. The algorithm processed the examples and found common patterns that characterize cancerous and non-cancerous slides. It used this information to make predictions for mammograms it hadnt seen before..In a few areas, such as radiology and medical imaging analysis, AI algorithms have surpassed human performance. But deep-learning algorithms suffer from a fundamental problem: They often adopt unwanted biases found within the data on which theyre trained. If the data is limited to a certain group of people, it will perform less accurately for other demographics..Datasets collected in North America are purely reflective and lead to lower performance in different parts of Africa and Asia, and vice versa, as certain genetic conditions are more common in certain groups than others, says Alexander Wong, co-founder and chief scientist at DarwinAI..For instance, several studies have found skin cancer–detection algorithms to be less accurate when used on dark-skinned patients, in part because AI models were trained mostly on images of light-skinned patients..When possible, engineers of AI systems take steps to reduce and remove bias. But machine-learning algorithms often find data points that indirectly represent problematic biases..The developers of the health-management system mentioned earlier had removed race information from the data the AI used to make decisions. But the algorithm selected health-care spending as one of the factors that determined its output. Spending effectively became a proxy for race and disadvantaged black patients, because they had lower healthcare costs, for socioeconomic reasons..While most studies on algorithmic bias are focused on known factors such as gender, race, and age, several studies show that machine-learning algorithms can often pick up hidden biases that are difficult to identify but can be equally damaging. The problem is that machine-learning models are often black boxes that offer very little visibility into their inner workings, so its difficult even for their creators to find and fix problematic biases..For example, skin-cancer-detection algorithms are usually trained on images of malignant moles and healthy skin. But while photos of skin cancer usually contain rulers to depict the size of the mole, healthy skin pictures do not contain any objects. An AI system trained on these images might end up becoming biased toward detecting rulers instead of malignant moles. Without visibility into the salient features of the algorithm, it would be hard to find out whether it has tuned into the right features..Machine-learning algorithms can also become sensitive to irrelevant correlations in health data. In one case, a hospital readmission algorithm gave lower-risk scores to patients with asthma. The program, touted to outperform expert doctors, would recommend hospitalizing a patient with pneumonia, but would clear the same person if they had both pneumonia and asthma..One must understand how and why decisions are made the way they are made by the AI algorithm in order to identify biases and devise strategies for addressing them, says Wong, whose company specializes in creating explainable AI models. Explainability also allows us to build trust in the AI algorithm, which is key in the healthcare system..As an AI community, we need to come together to share best practices, processes, and tools that will ensure fairness, inclusivity, reliability, and transparency while maintaining privacy and driving accountability across development and deployment, says Shantanu Nigam, CEO and co-founder of Jvion, a healthcare AI company..Some efforts are underway to address bias and fairness in AI-based healthcare systems. Last years NeurIPS conference ran a workshop to address fairness in machine learning for health applications. The workshop included several papers that explored the assessment of algorithmic fairness, discovering proxies, and calibrating algorithms for subpopulations. And the Alliance for Artificial Intelligence in Healthcare, a nonprofit organization founded in December 2018, brings together developers, device manufacturers, researchers, and other professionals to advance the safe and fair use of AI in medicine..Some organizations have started baking inclusivity and fairness into the data-gathering process, training, and testing of their AI algorithms. For instance, Google recently released an AI breast cancer screening tool its been testing to perform equally well across different geographical regions..Kush R. Varshney, principal research staff member and manager at IBM Research AI, believes increasing transparency and cooperation in the process of developing and releasing healthcare AI systems can help improve fairness. The best practices and governance of AI in healthcare should include the release of factsheets containing fairness test results and should involve multi-stakeholder participation on validating the entire AI lifecycle and also the organizational\/human processes that surround the AI system, he says..We know that machine-learning models are, by their very nature, meant to statistically discriminate on all sorts of features in order to generalize to new, unseen patients, Varshney says. We just have to make sure they dont discriminate in other ways..This newsletter may contain advertising, deals, or affiliate links. Subscribing to a newsletter indicates your consent to our Terms of Use and Privacy Policy. You may unsubscribe from the newsletters at any time..Ben Dickson is a software engineer and tech blogger. He writes about disruptive tech trends including artificial intelligence, virtual and augmented reality, blockchain, Internet of Things, and cybersecurity. Ben also runs the blog TechTalks. Follow him on Twitter and Facebook.\n"}
{"prompt":"Studies revealed that algorithms in hospitals and health insurance have biases against black people.  \n\n###\n\n","completion":" Black people with complex medical needs were less likely than equally ill white people to be referred to programmes that provide more personalized care.Credit: Ed Kashi\/VII\/Redux\/eyevine.An algorithm widely used in US hospitals to allocate health care to patients has been systematically discriminating against black people, a sweeping analysis has found..The study, published in Science on 24 October1, concluded that the algorithm was less likely to refer black people than white people who were equally sick to programmes that aim to improve care for patients with complex medical needs. Hospitals and insurers use the algorithm and others like it to help manage care for about 200 million people in the United States each year..This type of study is rare, because researchers often cannot gain access to proprietary algorithms and the reams of sensitive health data needed to fully test them, says Milena Gianfrancesco, an epidemiologist at the University of California, San Francisco, who has studied sources of bias in electronic medical records. But smaller studies and anecdotal reports have documented unfair and biased decision-making by algorithms used in everything from criminal justice to education and health care..Ziad Obermeyer, who studies machine learning and health-care management at the University of California, Berkeley, and his team stumbled onto the problem while examining the impact of programmes that provide additional resources and closer medical supervision for people with multiple, sometimes overlapping, health problems..When Obermeyer and his colleagues ran routine statistical checks on data they received from a large hospital, they were surprised to find that people who self-identified as black were generally assigned lower risk scores than equally sick white people. As a result, the black people were less likely to be referred to the programmes that provide more-personalized care..The researchers found that the algorithm assigned risk scores to patients on the basis of total health-care costs accrued in one year. They say that this assumption might have seemed reasonable because higher health-care costs are generally associated with greater health needs. The average black person in the data set that the scientists used had similar overall health-care costs to the average white person..But a closer look at the data revealed that the average black person was also substantially sicker than the average white person, with a greater prevalence of conditions such as diabetes, anaemia, kidney failure and high blood pressure. Taken together, the data showed that the care provided to black people cost an average of US$1,800 less per year than the care given to a white person with the same number of chronic health problems..The scientists speculate that this reduced access to care is due to the effects of systemic racism, ranging from distrust of the health-care system to direct racial discrimination by health-care providers..And because the algorithm assigned people to high-risk categories on the basis of costs, those biases were passed on in its results: black people had to be sicker than white people before being referred for additional help. Only 17.7% of patients that the algorithm assigned to receive extra care were black. The researchers calculate that the proportion would be 46.5% if the algorithm were unbiased..When Obermeyer and his team reported their findings to the algorithm’s developers — Optum of Eden Prairie, Minnesota — the company repeated their analysis and found the same results. Obermeyer is working with the firm without salary to improve the algorithm..He and his team collaborated with the company to find variables other than healthcare costs that could be used to calculate a persons medical needs, and repeated their analysis after tweaking the algorithm accordingly. They found that making these changes reduced bias by 84%..“We appreciate the researchers’ work,” Optum said in a statement. But the company added that it considered the researchers conclusion to be “misleading”. “The cost model is just one of many data elements intended to be used to select patients for clinical engagement programs, including, most importantly, the doctors expertise.”.Obermeyer says that using cost prediction to make decisions about patient engagement is a pervasive issue. “This is not a problem with one algorithm, or one company — it’s a problem with how our entire system approaches this problem,” he says..Finding fixes for bias in algorithms — in health care and beyond — is not straightforward, Obermeyer says. “Those solutions are easy in a software engineering sense: you just rerun the algorithm with another variable,” he says. “But the hard part is: what is that other variable? How do you work around the bias and injustice that is inherent in that society?”.This is in part because of a lack of diversity among algorithm designers, and a lack of training about the social and historical context of their work, says Ruha Benjamin, author of Race After Technology (2019) and a sociologist at Princeton University in New Jersey..“We can’t rely on the people who currently design these systems to fully anticipate or mitigate all the harms associated with automation,” she says..Developers should run tests such as those performed by Obermeyer’s group routinely before deploying an algorithm that affects human lives, says Rayid Ghani, a computer scientist at Carnegie Mellon University in Pittsburgh, Pennsylvania. That kind of auditing is more common now, he says, since reports of biased algorithms have become more widespread..He thinks that the results of these audits should always be compared to human decision making before assuming that an algorithm is making things worse. Ghani says that his team has carried out unpublished analyses comparing algorithms used in public health, criminal justice and education to human decision making. They found that the machine-learning systems were biased — but less so than the people..“We are still using these algorithms called humans that are really biased,” says Ghani. “We’ve tested them and known that they’re horrible, but we still use them to make really important decisions every day.”.Update 26 October 2019: Added the name of the algorithm developer and the company’s response to the study, as well as additional comments from Ziad Obermeyer.\n"}
{"prompt":"A new AI deep learning model that detects diabetic retinopathy failed during its first field test.  Where some were quick to call this a failure, this article argues that because the field test revealed flaws in the technology, the failure is good because it brings them closer to a better solution.\n\n###\n\n","completion":" Last month, a team from Google Research published a paper on the results of a field test of a novel deep-learning model to detect diabetic retinopathy from images of patients eyes. The paper, titled A Human-Centered Evaluation of a Deep Learning System Deployed in Clinics for the Detection of Diabetic Retinopathy, is based on research done in partnership with the Ministry of Public Health in Thailand to conduct field research in 11 rural clinics across the provinces of Pathum Thani and Chiang Mai..TechCrunch wasted no time in summarizing the study: Google medical researchers humbled when AI screening tool falls short in real-life testing. The article goes on to summarize the failures of the system in practice — from the lack of dedicated screening rooms that could be darkened to take high-quality images, to inconsistent broadband connectivity, to patients concerns about having to follow up at a hospital. But I believe this coverage misses the mark in three important aspects, which should be of prime concern to people actually working to deploy medical AI in the field..First, there is a difference between research and engineering, and research studies like this one should be heralded for the progress they enable. According to Google Health, This is one of the first published studies examining how a deep learning system is used in patient care. We need more studies about medical AI deployments published at the Conference of Human Factors in Computing Systems — and these studies need to describe things the way they are. Unlike a startup going to market that must spin whatever happens as a success story, research work is only about uncovering the truth..Implying such studies are failures not only misrepresents their goal and achievement but also contributes to the issues of nonreproducible research and science by press release that plagues todays science. If youre one of the many people trying to apply deep learning for medical imaging in practice, then youll find this paper to be a gem..Second, there must be an understanding of what it takes to get an AI system from idea to production. Assuming that a basic scientific breakthrough makes a system ready for wide use would have caused the invention of the steam engine to receive press coverage like this: Scientists humbled to find were nowhere near a robust national railway system. This is how cars where originally covered in the media, so theres nothing new under the sun with this happening again with AI..Taking on the analogy of cars, here are the three workstreams that must come together for medical AI systems to become an effective everyday reality:.1. Science: We need to develop highly accurate data science algorithms for specific problems, as Google did with its original deep learning models for detecting diabetic retinopathy. In the analogy to cars, this would be like the invention of the internal combustion engine..2. Engineering: We need to develop ways to productize these inventions at high quality, high scale, safely and cheaply. In the analogy to cars, we need to invent the equivalents of the mass production line, hand brakes, electric starters, air conditioners, airbags and headrests. In the AI space, think MLOps, explainability, bias detection and model governance (as a start). This is the area of the ecosystem where I personally work and specialize..3. Process change: We need to develop the human-centered processes that enable people to use these innovations effectively and safely. In the analogy to cars, think splitting the public space between roads and sidewalks, establishing driver licensing, public education, safety standards and pollution regulation. In medical AI, weve barely started on this, which makes the recent Google field study an important baby step..Its important for practitioners to know that real success — helping real patients, in the field, at scale, safely — requires all three of these aspects to work together. Its important for media coverage to educate people about this..The third insight from this new study is based on the major differences between the 11 clinics that took part in it. The researchers reported major differences between them — from how the physical rooms at each clinic were laid out to the personalities and background of the nurses who worked there. As a result, the trained model could not successfully operate at each of these distinct environments..This is such a well-known phenomenon in medical AI that it no longer requires academic validation. Medical AI models generally perform poorly across locations. This not only applies to models deployed in Thailand versus Nigeria but also models deployed in two clinics that are 5 kilometers apart and serve essentially the same population. This happens in both first-world and third-world countries and across just about every medical specialty thats taken the time to measure it..As a result: If you have a successfully deployed model in one location (or 10), you do not have an accurate model thats ready for the next clinic. Continuously tuning and monitoring AI models is part of the engineering work underway in the Science + Engineering + Process Change trifecta. At this point in time, I expect every sound medical AI field deployment to be addressing this issue..Turning medical AI from aspiration into a reality that improves humanitys well-being is going to be a long ride. It will take us all of the first half of the 21st century — and thats if were efficient about it. Maybe this isnt original, but it may be the adventure of a generation.\n"}
{"prompt":"The algorithm prioritized white people over black people when predicting which patients would need extra healthcare.  Socioeconomics (and other unspecified reasons) affected the training data because black patients have historically recieved lower health care costs than white people for the same treatment.  This causes the algorithm to give a white person and a black person the same score even though the black person is far sicker.  Researchers found that white patients were given higher risk scores, and were therefore more likely to be selected for extra care than black patients who were, in fact, equally sick; they calculated that the bias cut the proportion of black patients who got extra help by more than half.\n\n###\n\n","completion":" The news: An algorithm that many US health providers use to predict which patients will most need extra medical care privileged white patients over black patients, according to researchers at UC Berkeley, whose study was published in Science. Effectively, it bumped whites up the queue for special treatments for complex conditions like kidney problems or diabetes..The study: The researchers dug through almost 50,000 records from a large, undisclosed academic hospital. They found that white patients were given higher risk scores, and were therefore more likely to be selected for extra care (like more nursing or dedicated appointments), than black patients who were in fact equally sick. The researchers calculated that the bias cut the proportion of black patients who got extra help by more than half..What software was this? The researchers didn’t say, but the Washington Post identifies it as Optum, owned by insurer UnitedHealth. It says its product is used to “manage more than 70 million lives.” Though the researchers only focused on one particular tool, they identified the same flaw in the 10 most widely used algorithms in the industry. Each year, these tools are collectively applied to an estimated 150 to 200 million people in the US..How the bias crept in: Race wasn’t a factor in the algorithm’s decision-making (that would be illegal); it used patients’ medical histories to predict how much they were likely to cost the health-care system. But cost is not a race-blind metric: for socioeconomic and other reasons, black patients have historically incurred lower health-care costs than white patients with the same conditions. As a result, the algorithm gave white patients the same scores as black patients who were significantly sicker..A small saving grace: The researchers worked with Optum to correct the issue. They reduced the disparity by more than 80% by creating a version that predicts both a patient’s future costs and the number of times a chronic condition might flare up over the coming year. So algorithmic bias can be corrected, if—and sadly, it is a big if—you can catch it..Why it matters: The study is the latest to show the pitfalls of allocating important resources according to the recommendation of algorithms. These kinds of challenges are playing out not just in health care, but also in hiring, credit scoring, insurance, and criminal justice..An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.\n"}
{"prompt":"AI needs to be accurate, unbiased, and trustworthy before it can be used in healthcare.\n\n###\n\n","completion":" Researchers explain that healthcare companies have not adopted artificial intelligence algorithms because they do not work well and fail to show results. .In a new article in JAMA, researchers suggest that developers of artificial intelligence (AI) programs for improving medicine should pay more attention to how their programs would actually function in a clinical setting. The authors remark that “given the abundance of algorithms, it is remarkable there has yet to be a major shift toward the use of AI for health care decision-making (clinical or operational).” However, they go on to outline several problems with AI programs that explain why healthcare organizations have not overwhelmingly adopted them for clinical decision making..AI algorithms often haven’t been shown to work (“data quality”), nor have they consistently been found to improve results (due to “timeliness” or “lack of structure”). Another complication is that AI programs often can’t be assessed because their “black box” algorithms make it impossible to tell if they’re working, which leads to a “lack of trust.”.The authors acknowledge these problems, but they have an additional explanation, too: “Perhaps that model developers and data scientists pay little attention to how a well-performing model will be integrated into health care delivery.”.The lead author was Christopher J. Lindsell at Vanderbilt University Medical Center, himself a patent-holder on several predictive technologies who also receives funding from Endpoint Health Inc, an “early-stage” tech start-up in the healthcare field..Lindsell and his co-authors suggest that one major problem is that even if artificial intelligence algorithms were shown to work, they may not improve outcomes. Importantly, prediction and surveillance do not necessarily improve healthcare outcomes..The authors suggest that technology companies should work with “end users” such as patients and clinicians to determine what algorithmic technology may actually be helpful—and “in some cases, the realization that the problem is not ready for an AI solution given a lack of evidence-based intervention strategies to affect the outcome.”.They provide an example of an “expensive intervention” aimed at reducing alcohol use in people who experienced trauma. Technically, the intervention worked—but only for the people who were at low risk of both alcohol use and readmission..So, in that instance, a technology was developed with good intentions, and it appeared at first glance to be successful. But upon further review, it actually failed to work for the group of people who needed it most..Apps using artificial intelligence to assess mental health are already in use, partnering with health insurance companies and medical centers, despite no published research evidence demonstrating their effectiveness in any clinical domain..There were over 325,000 different healthcare apps available to download in 2017, and the market share was estimated at $23 billion. In 2018, users downloaded over 400 million healthcare apps, and that number has likely only grown..A study last year found that of the more than 10,000 apps available for mental health, only “3.41% of apps had research to justify their claims of effectiveness, with the majority of that research undertaken by those involved in the development of the app.”.Lindsell, C. J., Stead, W. W., & Johnson, K. B. (2020). Action-informed artificial intelligence—Matching the algorithm to the problem. JAMA. Published online May 1, 2020. doi:10.1001\/jama.2020.5035 (Link).There’s no such thing as an “app deficit” in human medicine….that’s a business and financial problem. ALL problems in medicine are HUMAN, not AI. And the biggest problem with AI is the so-called “AI Virus….. I think in future years we’ll see few of the alleged benfits of AI pan out…..Ah but they can. That’s called “the algorithm. It has been shown that algorithm get way skewed and prejudiced – the prejudices of the people writing the parameters get amplified in the echo chambers of algorithms and turn into very prejudicial AI’s…..Well, of course, the algorithm is only as good as the programmer. I’m sure someone could program a discriminatory app. But at least they won’t have to manage their emotional reactions to our statements, appearance, etc. I’m sure they’d totally suck, because they’d be made by people who have no comprehension of what is helpful, otherwise, they’d realize that a computer can’t provide what is needed..The data quality issue is largely of the medical field’s own making. Format and contents of clinical documentation are determined by insurers, governmental regulatory officials, and other funding sources. Data is generally only collected for the purpose of billing and not for future clinical use or research\/training ML algorithms. I’m not sure what they mean by ‘black box’ either. Just because YOU don’t understand the math, doesn’t mean I can’t. Backprop is fundamental to how neural networks learn from data so you should always be able to follow backwards through the model from the decision in the output to the data in the input. The real problem is the lack of collaboration between the two fields. I don’t see the example above as a failed intervention. AI was able to cheaply address the low risk population, leaving the experts more time to focus on the ‘problem children’. That’s a failure of the treatment program to utilize the treatment in the best way possible. Remember, AI based cancer diagnosis only beats human based diagnoses slightly. The real improvement comes from augmenting the human. Human’s armed with Ai diagnostic data perform better than any ML algorithm ever will..No, the problem is that common approaches to “helping” with mental\/emotional\/spiritual distress don’t improve outcomes, and no amount of AI is going to change the fact that the basic model of distress and helping is fatally flawed. Well, flawed unless your “outcome” is increased profits. Maybe that’s what they mean – AI isn’t improving income, therefore, it isn’t working?.The outcomes for whom? The Human or the Institutes of Technology? Any knowledge of what drives the Broad Institute on the MIT campus? How does one create breathing space for anyone, students & professors to ask questions of each other without being told, to soon for lack of deep listening, or how to listen to what is attempting to be asked, without thinking of an Oedipus sort of destructive mentality that often emerges in therapy, if one challenges the ruler in the sessions? Or are the “Orders” in the challenge to become a knowledge producing economy undermining authentic health? And care?\n"}
{"prompt":"Several senators urged the Trump administration and major healthcare companies to address racial bias in AI after a study came out that proved black americans were less likely to receive additional healthcare services. The creators of the algorithm didn't account for factors, such as barriers when accessing care, when programing the AI.\n\n###\n\n","completion":" United States Senators Cory Booker, D-New Jersey, and Ron Wyden, D-Oregon, are urging the Trump administration and some of the nations biggest health insurers – Humana and Blue Cross among them – to be aware of potential racial bias in healthcare data algorithms..The letters were sent to leaders at the Centers for Medicare and Medicaid Services and the Federal Trade Commission – as well as to the executive vice president of CVS Health Karen Lynch, who is also the president of the companys Aetna business unit, as well as to the president and CEO of Blue Cross Blue Shield, Scott Serota, and David Cordani, president and CEO of Cigna, among others..In healthcare, there is great promise in using algorithms to sort patients and target care to those most in need. However, these systems are not immune to the problem of bias, said Senators Booker and Wydens in their  letter to Lynch stated. As algorithms play an increasingly prevalent role in the health care system, we urge Aetna to consider the risk for algorithmic bias and its potential impact on health disparities outcomes..The letters point to a study published in the October 25 issue of Science magazine, which examined the racial bias in an algorithm used to manage the health of populations..For study found the creators of the algorithm did not take into account that other factors besides need contribute to the individuals overall healthcare costs..Factors like barriers to accessing care and low levels of trust in the healthcare system – which disproportionately affect black patients – mean those patients were less likely to receive additional services..Less money is spent on Black patients who have the same level of need, and the algorithm thus falsely concludes that Black patients are healthier than equally sick White patients, the study concluded. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care..The two senators submitted a list of questions to the recipients of the letter, requesting details about the use, type, number and methodology behind the algorithms these organizations use..The questions also cover topics related to the use of machine learning and advanced analytics – technologies that are expected to play much larger roles in the future development of healthcare analytical tools and metrics.\n"}
{"prompt":"AI can help identify suspicious activity at large volume which can help with anti-money laundering efforts\n\n###\n\n","completion":" In 2009, the United Nations Office on Drugs and Crime estimated that up to $2 trillion is laundered globally in one year, but less than 1 per cent of this illegal activity is caught. Almost ten years on, this trend still held true in the UK – in 2018, although as much as hundreds of billions of pounds in laundered cash was thought to have washed through the City of London, only 40 arrests were made from 22,196 flagged cases. .The difficulty in identifying suspicious activity is fundamentally a problem of volume: a combination of the enormous amount of transactional information received, the expert money laundering attacks that circumvent published AML transaction scenarios, and the sheer speed of criminal activity. With the advent of near real-time bank transfers coupled by advances in cloud computing, criminals are now able to disperse illicit funds through different coordinated bank accounts so quickly that they are almost impossible to intersect..To combat the tide of illegal activity, regulatory bodies are penalising institutions found to be involved in money laundering-related offences, even if these were carried out unknowingly. As a result, global financial institutions have been levied with fines amounting to $26 billion over the last ten years, a development that has seen financial institutions expanding their compliance teams to try and beat the criminals..Although this has caused the number of alerts to grow through softening of transaction rule thresholds and bringing in armies of human talent, 95 per cent of alerts are false positives, and nearly 98 per cent never result in a suspicious activity report (SAR). Further, of those that lead to SAR filing, there has been a rise in ‘defensive SARs’ that create confusion as the analyst erred on the side of safety vs. truly identifying a suspected money laundering event.  This is a low rate of return and often quality of additional SAR filing that is incongruous with the investment being ploughed into AML efforts..Current AML processes can be streamlined and optimised with AI, the business use cases for which are growing by the day. AI and advanced analytics can reduce the costs of AML efforts while catching more suspicious transactions..During a proof of concept at a European bank, these AI AML models identified only ten false positives for every 1000 alerts generated for 42 per cent of the highest scoring consumer transactions. Compliance officers, on the other hand, would have had to work through more false positives than real alerts..With the superiority of AI in identifying genuine AML cases proven, the question now is: why are they so much more effective? This stems from the two new technologies these models are built with: unsupervised learning machine learning and explainable AI..The most well-known applications of machine learning and AI involve supervised learning, that is, using an algorithm to learn how the input is best transformed to map to the output, which is analogous to that of a teacher supervising learning processes. Unsupervised learning, on the other hand, has no know output to match, which removes the need for a teacher..Applying unsupervised learning for AML involves exposing the AI system to raw uncategorised data. During this interaction, the computer system identifies patterns that indicate money laundering by using soft clustering on behavioural archetypes to check for customer behavioural transaction anomalies within archetypal clusters (see Fig 1). By being able to boil down transaction behavioural analytics into archetypes of behaviour indicating normal behaviours, and measuring abnormality from that, the AI models can detect suspects very rapidly. .Although a very useful tool, explainability is not AI’s strong suit (due to the typical pursuit of performance over transparency), – hence its reputation as a black box technology. Explainable AI (XAI) is a field of science that attempts to remove this black box and deliver AI performance while also providing an explanation for the “how” and “why” a model derives its decisions. With explainability increasingly demanded by new laws such as the General Data Protection Regulation (GDPR) not to mention AML regulators world-wide, incorporating XAI into AI models has become a question of how, and definitely not when..Explainable AI opens doors in the banking industry to the advantages of AI, historically these financial institutions have been wary of deploying AI solutions owing to its often inexplicable nature, a quality incompatible with highly-regulated industries. By incorporating XAI in AML applications, financial institutions will be able to explain and justify the model’s decision making, thus satisfying the regulators and making money laundering cases easier to investigate..AI models can be deployed to supplement existing rules for AML systems, to enrich current data with scores that can be used for both enhanced detection and alert prioritisation, given that the majority of alerts are false positives. With AI, compliance officers will have more time to improve their investigative analyses on the one per cent that matter and importantly those that go undetected by current transaction monitoring.  This will lead to faster response rates, the reduction of reputational risk from regulatory fines and a lower cost of compliance. Having AI and humans work in tandem with each other is most effective way to significantly improve current AML outcomes for financial institutions..AI has huge potential to significantly improve AML processes, particularly due to its ability to reduce the number of false positives and zone in on the truly suspicious cases and find so many more that are currently missed by traditional transaction monitoring. With the development of XAI, it is likely that more and more financial institutions will use AI to bolster their AML efforts, especially as the levels of financial crime continue to rise.\n"}
{"prompt":"Banks could use predictive analytics for fraud prevention and customer focused solutions\n\n###\n\n","completion":" Artificial intelligence (AI), or rather, machine learning, is utilizing predictive analytics to reshaping the financial services landscape, providing banks the ability to capitalize on the wealth of customer and product data in their passion. This is enabling banks to gather greater insight into their customers and provide solutions and services specifically tailored to each individual..What is predictive analytics? In simple terms, it is the process of using computer models to predict various events. Sophisticated algorithms rely on artificial intelligence, data mining, and machine learning to analyze large amounts of information. With these data inputs, the AI runs various models with the goal to determine what is likely to happen next, given various inputs..Even today we can see banks utilizing this technology to improve the way they work with customers and provide enhanced services to their clients. Let us take a look at a few areas where banks are implementing AI to enhance their services..Fraud is everywhere these days. As individuals do more banking through mobile apps and the web, scammers are continuously looking to take advantage of unsuspecting users. Bank today utilize AI and predictive analytics to track consumer behavior. By applying machine learning to these patterns of spending and saving, Ai is able to identify irregular behavior that could be indicative of fraud. With the ability to notify customers immediately, individuals are more aware when the fraudulent activity took place. Additionally, banks have been able to reduce the occurrences of scams that come from bad checks, which cause significant losses for victims. Being able to analyze data patterns has become an important tool in the banking industry..Most individuals are aware of their credit score, but have you stopped to wonder exactly how that value is calculated? It probably wouldn’t surprise you that AI analytics plays a large part in determining your credit score. By analyzing data from your credit habits along with those of individuals who are similar to your financial profile, the data is aggregated to predict the likeliness of your ability to repay debt. Banks are using this technology to help evaluate loan approvals for individuals who may not have the highest credit scores. In the past banks relied heavily on the user’s credit reports provided by companies like FICO and Equifax, but advancements in technologies are improving loan opportunities for individuals who may have had a bad rap sheet in the past and are suffering from low credit scores..Banks today offer solutions to their customers to help manage personal finances. With the ability to identify when income and expenses flow in and out of your accounts, banks have the ability to alert the customer before overdrafts or payments occur.  This helps remove unnecessary fees that banks impose when these types of events occur. Predictive analytics can also help steer big picture decisions. According to Deltec Bank, Bahamas – “By analyzing an individual’s financial data, AI can help identify opportunities that individuals may be unaware of. Perhaps it is beneficial to make additional loan payments or refinance your mortgage. Banks are positioning themselves to help individuals make smarter decisions when it comes to their finance.”.Artificial intelligence is being used today in the banking industry, and as technology continues to advance, banks will continue to improve how they serve their customers and tailor solutions specific for each individual. Whether protecting the individual from fraudulent activity or scams, to ensuring you make the right financial decisions; banks are positioning themselves to become more involved in your financial picture to help provide the customer the services they want and need..Headquartered in The Bahamas, Deltec is an independent financial services group that delivers bespoke solutions to meet clients’ unique needs. The Deltec group of companies includes Deltec Bank & Trust Limited, Deltec Fund Services Limited, and Deltec Investment Advisers Limited, Deltec Securities Ltd. and Long Cay Captive Management.\n"}
{"prompt":"AI could help answer any questions that customers want to be anonymous. Furthermore, they can work together with human advisers once the customer is ready for a face to face meeting\n\n###\n\n","completion":" New technology could play a significant role in helping potential customers carry out early research into equity release options before speaking to an adviser, providers have said..Digital retirement solutions provider Abaka and equity release adviser Key carried out a study with a group of retired consumers aged between 65 and 75, which looked into their use of online channels to research equity release products and services..The study found that providers’ websites were often among the first sources of information for the test group when looking into equity release. It also found a reluctance from participants to provide contact details during their initial research, to avoid being contacted too early during their decision-making process..The research group tested a chatbot powered by artificial intelligence, designed to give personalised answers to customers’ questions on equity release. Participants seemed to favour the anonymity of using the chatbot and the freedom to ask multiple questions without perceived judgement..The study also found that participants wanted to speak to a human adviser when they felt ready, and that face-to-face meetings with an adviser were still the main way they wanted to receive bespoke advice..Speaking to FTAdviser, Jonathan Barrett, director of business development and partnerships at Abaka, said participants asked the chatbot questions such as what equity release could be used for, what it would cost them, and what would happen if they went into care..Mr Barrett said a chatbot could help customers get to a stage where they feel informed before speaking to an adviser, rather than replacing the need for one..He said: “There are simply not enough advisers to answer these initial queries and help consumers assess whether equity release is the right option for their retirement plans..“Technology can help bridge that advice gap. Having an informative website that is easy to navigate is a good place to start, but tools like conversational AI can take that experience a step further, assisting consumers on their fact-finding mission before they’re ready to seek regulated advice.”.Will Hale, CEO of Key, added: “While we believe firmly in the value of face-to-face advice and the support that this can provide customers as part of the process of taking out a later life lending product, we know that there is often considerable thought and planning undertaken before they even pick up the phone..“Informed customers who feel comfortable that their initial questions have been answered are going to be more confident in progressing to the next step of engaging with an adviser who can take a more in-depth look at how later life lending can support their retirement ambitions.”.Commenting on the research, Steve Paterson, equity release specialist at Later Life Money, said his view on the matter had changed in light of the coronavirus.\n"}
{"prompt":"Gradient, a photo editing apps, lets people alter ethnicity which some users have used as blackface\n\n###\n\n","completion":" San Francisco (CNN Business)Photo editing app Gradient is under fire for a new feature that lets people alter their ethnicity in images, with many slamming it for promoting digital blackface. \n"}
{"prompt":"The restaurant started using Preso Vision, a computer vision program, to maximize employee efficiency and performance.\n\n###\n\n","completion":" As casual dining chains have declined in popularity, many have experimented with surveillance technology designed to maximize employee efficiency and performance. Earlier this week, one Outback Steakhouse franchise announced it would begin testing such a tool, a computer vision program called Presto Vision, at a single outpost in the Portland, Oregon area. Your Bloomin Onion now comes with a side of Big Brother..According to Presto CEO Rajat Suri, Presto Vision takes advantage of preexisting surveillance cameras that many restaurants already have installed. The system uses machine learning to analyze footage of restaurant staff at work and interacting with guests. It aims to track metrics like how often a server tends to their tables or how long it takes for food to come out. At the end of a shift, managers receive an email of the compiled statistics, which they can then use to identify problems and infer whether servers, hostesses, and kitchen staff are adequately doing their jobs..“It’s not that different from a Fitbit or something like that,” says Suri. “It’s basically the same, we would just present the metrics to the managers after the shift.” Presto says its testing the technology at multiple restaurants across the country, but declined to name any other than Outback..The Outback Steakhouse pilot will use Presto Vision specifically to analyze footage from the lobby of a franchise operated by Evergreen Restaurant Group, which manages nearly 40 Outback Steakhouse locations across the United States. It will monitor factors like how crowded the lobby is and how many customers decide to leave rather than wait for a table. Suri says Presto Vision could be used not only to evaluate employee performance after the fact, but also course-correct in the moment. For instance, managers could be sent text messages when the number of people waiting for a table reaches a certain threshold..For now, workers on the ground dont know much about how the technology will be used. I dont know anything about it, one worker at the Portland Outback location said over the phone. We have zero interaction with that. Im pretty sure thats just still in the developmental phase..Presto Visions software doesnt identify individual diners and doesnt currently employ technology like facial recognition. “We do not collect any personal information and the video is deleted within three days of collection,” Jeff Jones, the president and CEO of Evergreen Restaurant Group, said in an email. But even if their data is anonymized, consumers may be unnerved to learn that an algorithm is monitoring their night out..The Outback pilot is still in early stages, but Suri sees broad potential in Presto Vision. The software has the potential to detect things like when a guest’s drink is almost empty, he says as an example, and prompt servers to offer them a refill..But even without those fancy features, Presto Vision is likely already capable of producing lucrative data for the restaurant industry. That information could be used not only to boost sales, but also make life harder for workers. Researchers have found that workplace surveillance can have negative effects on employees, like increased stress and lower job satisfaction..Alexandra Mateescu, a researcher at the Social Instabilities in Labor Futures initiative at the nonprofit research institute Data & Society, wonders whether Presto Vision and similar tech could ultimately decrease staff numbers or alter scheduling patterns. For example, if the software finds that servers at a given restaurant have enough time to visit their tables often, higher-ups may decide to cut the number who work during certain shifts, to try to cut labor costs. This [technology] may sort of automate the discretionary power of restaurant management to make decisions, Mateescu says..There’s also the question of what happens to the aggregate information Presto Vision and other similar tools collect. Even if the underlying videos get deleted, the data lives on. At franchised restaurants like Outback, it could be vacuumed up by parent organizations, and used to make business decisions in the future. In a press release, Presto touted its software could be employed to provide “remote, immediate visual access across multiple locations, and a high-level view of performance metrics and noteworthy events across brands for large restaurant chains.”.These are legally separate firms, they have no oversight or responsibility for working conditions or wages, says Brian Callaci, an economist and researcher at Data & Society who has studied franchising. But technology like Presto Vision would potentially allow them to monitor and control activity supposedly at legally independent businesses. Jones, from Evergreen Restaurant Group, did not immediately return a request for comment about if and how the data collected by Presto Vision would be shared within his organization, or with Bloomin Brands, Outback Steakhouses parent company..Presto Vision is also just the latest in its parent companys suite of monitoring tools designed to optimize the chain restaurant industry. Along with its competitor Ziosk, Presto makes electronic tablets stationed at tables in restaurants like Chili’s, Olive Garden, and Applebee’s across the country. Customers use the devices not only to order food, but also to rate the performance of their waiters and waitresses. A Buzzfeed News investigation found workers with lower scores have received fewer shifts and tables, and potentially even faced termination—despite customer ratings often reflecting aspects of the experience that servers can’t control, like the quality of the food..Other fast food chains have developed their own surveillance solutions in-house. Dominos, for example, recently began rolling out its “DOM Pizza Checker” at stores in Australia and New Zealand, which monitors workers as they assemble pies via an AI-equipped overhead camera. If the device detects a poorly made pizza, it alerts workers that it should be remade. In a presentation given to shareholders, Domino’s executives suggested the technology could be used to quantify bonuses, as well as identify stores “falling below” their peers..Suri argues Presto Vision “should be able to help managers create better processes to do a better job overall.” He compares working in restaurants to playing sports. Without surveillance tools like the ones Presto offers, managers “have to coach based on the score—they don’t know what actually happened on a play-by-play basis.” In other words, restaurant managers today are still largely forced to take employees at their word, and give them at least a baseline level of autonomy. At least until technology makes even that obsolete.\n"}
{"prompt":"SOS Racisme, Union of Jewish Students of France, the Movement Against Racism and For Friendship Among People sued Google because it suggests the word \"Jewish\" for certain celebrities.\n\n###\n\n","completion":" French anti-discrimination organization SOS Racisme, in association with the Union of Jewish Students of France, the Movement Against Racism and for Friendship Among Peoples and other organizations, is suing Google because its autocomplete feature suggests the word Jewish in searches involving certain public figures, including News Corporation chairman Rupert Murdoch and actor Jon Hamm, reports The Times of Israel..According to Googles website, its algorithm for the Google Instant autocomplete feature predicts and displays search queries based on other users search activities and the contents of web pages indexed by Google. In addition, the search engine says it strives to reflect the diversity of content on the web (some good, some objectionable) and so has a narrow set of removal policies for pornography, violence, hate speech, etc. -- though not narrow enough for SOS Racisme, it seems..A lawyer for SOS Racisme, Patrick Kulgman, told Agence France Presse (AFP) that Googles autocomplete algorithms have resulted in the creation of what is probably the biggest Jewish file in history,  according to The Times of Israel. As an ethnic file, this compilation is outlawed in the country..Local reports pointed out by The Hollywood Reporter explain that the plaintiffs contend users of Google in France and across the world are systematically confronted with the unsolicited association of the term Jew with prominent names in the world of politics, media, and business. A hearing for the lawsuit is scheduled for Wednesday..The Hollywood Reporter also writes that the last lawsuit Google saw in France due to its autocomplete feature occurred in 2009, when two French companies sued the search engine because its autocomplete feature suggested the French word for scam in searches for said companies names..Just over a month ago, a man in Japan won an injunction against Google to have the autocomplete feature turned off when someone searched the mans name. Apparently, the search engine was connecting the mans name with crimes he had not committed and, according to Japan Times, likely played a role in the sudden loss of his job several years ago and caused several companies to subsequently reject him when he applied for new jobs.\n"}
{"prompt":"Google autocomplete affected a man's reputation in employment after suggesting crimes he was not a part of\n\n###\n\n","completion":" A Japanese court issued a provisional order requesting Google to delete specific terms from autocomplete, the company said in a statement on Monday..Autocomplete is a function on many of Googles search service which uses a mixture of algorithms and stored user data to predict what a person is searching for..It could lead to irretrievable damage such as a loss of job or bankruptcy just by showing search results that constitute defamation or a violation of the privacy of an individual person or small and medium-sized companies, Mr Tomita told Japanese news agency Kyodo..Google defended the system, arguing that as results were generated mechanically - rather than by an individual - it was not an invasion of privacy..It is not the first time the feature has come under scrutiny. In December 2010, Google tweaked autocomplete so that terms relating to piracy did not automatically appear.\n"}
{"prompt":"Names typically associated with black people are more likely to produce ads related to criminal activity\n\n###\n\n","completion":" Harvard professor Latanya Sweeney said names typically associated with black people were more likely to produce ads related to criminal activity..In a statement to the BBC, the company said: We also have an anti and violence policy which states that we will not allow ads that advocate against an organisation, person or group of people..Prof Sweeneys investigation suggests that names linked with black people - as defined by a previous study into racial discrimination in the workplace - were 25% more likely to have results that prompted the searcher to click on a link to search criminal record history..She found that names like Leroy, Kareem and Keisha would yield advertisements that read Arrested?, with a link to a website which could perform criminal record checks..There is discrimination in the delivery of these ads, concluded Prof Sweeney, adding that there was a less than 1% chance that the findings could be based on chance..Alongside news stories about high school athletes and children can be ads bearing the childs name and suggesting arrest. This seems concerning on many levels..However, she was reluctant to pinpoint a cause for the discrepancies, saying that to do so required further information about the inner workings of Google AdSense..She noted that one possible cause may be Googles smart algorithms - technology which automatically adapts advertising placement based on mass-user habits..In other words, it may be that the search engines are reflecting societys own prejudices - as the advertising results Google serves up are often based on the most popular links previous users have clicked on.\n"}
{"prompt":"AI sued in language test cheating to reduce visa application fraud. However, 97% of applications were deemed suspicious \n\n###\n\n","completion":" The House of Commons public accounts committee has announced an investigation into the Home Office’s treatment of thousands of international students who may have been wrongly accused of cheating in an English language test they were required to sit as part of a visa application process..More than 100 students signed a letter calling on the home secretary, Sajid Javid, to make a long-promised statement on the issue and to act swiftly to resolve the situation in the time he has remaining before a possible government reshuffle once the Conservative leadership campaign ends..About 2,500 students have been forcibly removed from the UK after being accused of cheating in the exam and a further 7,200 left the country after being warned they faced detention and removal if they stayed. Many have protested their innocence: 12,500 appeals have been heard in UK courts, and so far 3,600 people have won their appeals..“We are some of the tens of thousands of international students unjustly robbed of our visas and our rights by the Home Office in 2014 after we were accused of cheating on an English test,” the students’ letter reads. “We are innocent but the government gave us no real way to defend ourselves, so we’ve been fighting for five years to clear our names. The department you lead ruined our lives and stole our futures. It branded us as frauds, forcing us to bear a lifelong mark of shame, while never presenting any evidence at all against most of us.”.The students added: “Many of us are destitute, barely able to live from day to day. Many of us are on medication for stress or depression. Many of us have been rejected by our families, who are shamed by the allegation of cheating. Some of us have tried to kill ourselves.”.A National Audit Office investigation into the Home Office response to reports of cheating in English language tests concluded last month that some people may have been wrongly accused and unfairly removed from the UK. The public accounts committee has invited the permanent secretary at the Home Office, Sir Philip Rutnam, to answer questions at a hearing into the issue in July..In 2014, undercover filming by BBC reporters for Panorama revealed organised cheating in two of the 90 Home Office-approved centres offering the test of English for international communication (Toeic) exam, which is required for visa extensions. The government responded by asking the test provider, the US-based Educational Testing Service (ETS), to assess whether the 58,459 tests taken between 2011 and 2014 were valid. The US company made checks and concluded that virtually every test taken was suspicious, identifying 97% of all UK tests as “suspicious”. It classified 58% as “invalid” and 39% as “questionable”..The Home Office suspended the licences of some test centres and revoked the visas of those accused of cheating. Campaigners have questioned whether it is likely that 97% of people taking Toeic tests could have been involved in cheating..The Labour MP Stephen Timms was told by Javid last year that he was “sympathetic” to the students’ situation. Timms also urged Javid to use the time remaining to him in his position as home secretary to resolve the issue. An all-party parliamentary group on the Toeic issue, chaired by Timms, heard this month that the evidence sent to the Home Office by ETS was “unsafe and unreliable”, Timms said. “It is urgent that this is addressed for a large number of students who have been suffering a great deal for a very long time,” he added..Nazek Ramadan, the director of Migrant Voice, a charity that has been supporting the students, said this was a direct plea to the home secretary “who has the power to end this injustice and give them back their future” to make this issue a positive legacy from his time at the Home Office. “Some of them are prone to be detained or deported any time. We know now that the Home Office action was based on unreliable evidence. This should be put right,” she said..A Home Office spokesperson said 25 people involved in the organised cheating exposed by Panorama had received criminal convictions. “The home secretary is considering the findings of the NAO report. He will then make a statement to parliament.”\n"}
{"prompt":"Facebook Tag Suggestions tool stores biometric data without user consent\n\n###\n\n","completion":" Facebook says it has agreed to pay $550 million to settle a class-action lawsuit regarding its use of facial recognition technology. The news, reported first this evening by The New York Times, was part of a disclosure the company made as part of its fourth quarter earnings report today. .The case against Facebook has been going on since 2015. The lawsuit alleged that Facebook’s initial version of the its Tag Suggestions tool, which scans a user’s face in photos and offers suggestions about who that person might be, stored biometric data without user consent, violating the Illinois Biometric Information Privacy Act. .In 2018, Facebook began more transparently explaining its facial recognition tech to users, pointing people to a settings page where they could disable it. Last year, the company decided to make facial recognition on the platform opt-in only, after years of turning it on by default for all users. .A federal judge ruled in favor of making the facial recognition case a class action lawsuit in 2018. Facebook appealed that ruling, but lost the appeal in a 3-0 court decision in August of last year. Facebook’s $550 million settlement will be paid out to eligible Illinois users and to cover the plaintiffs’ legal fees, according to The New York Times. While $550 million may seem like a large settlement, it’s essentially pocket change for Facebook, which today reported revenues of $21 billion for the fourth quarter of 2019. .This isn’t the first time the Illinois Biometric Information Privacy Act has been tested in court. A plaintiff sued Google in Chicago for allegedly uploading her photos to Google Photos and scanning her face without her permission, but the case was dismissed in 2018 after a judge found the plaintiff didn’t suffer “concrete injuries.” Snapchat has also been sued for allegedly violating the law. \n"}
{"prompt":"Baidu claimed to meet Google on image recognition software by cheating\n\n###\n\n","completion":" The sport of training software to act intelligently just got its first cheating scandal. Last month Chinese search company Baidu announced that its image recognition software had inched ahead of Google’s on a standardized test of accuracy. On Tuesday the company admitted that it achieved those results by breaking the rules of that test..The academic experts who maintain that test say that makes Baidu’s claims of beating Google meaningless. Ren Wu, the Baidu researcher who led work on the software in question, has apologized and said the company is reviewing its results. The company has amended a technical paper it released on its software..We don’t know whether this was the action of one individual or a strategy of the team as a whole. But why a multibillion dollar corporation might bother to cheat on an obscure test operated by academics on a voluntary basis is actually quite clear..Baidu, Google, Facebook, and other major computing companies have spent heavily in recent years to build research groups dedicated to deep learning, an approach to building machine learning software that has made great strides in speech and image recognition. These companies have worked hard to hire leading experts in the small field – often from each other (see “Is Google Cornering the Market on Deep Learning”). A handful of standardized tests developed in academia are the currency by which these research groups compare one another’s progress and promote their achievements to the public..Baidu got an unfair advantage by exploiting the test’s design. To get your software scored against the ImageNet Challenge you first train it with a standardized set of 1.5 million images. Then you submit the code to the ImageNet Challenge server so its accuracy can be tested on a collection of 100,000 “validation” images that the software has never seen before..Baidu has admitted that it used multiple email accounts to test its code roughly 200 times in just under six months – over four times what the rules allow..Oren Etzioni, CEO of the Allen Institute for Artificial Intelligence, likens what Baidu did to buying multiple lottery tickets. “If you get to buy two tickets a week you have a certain chance if you buy 200 a week you have more of a chance,” he says. On top of that, testing slightly different code over many tests could help a research team optimize its software for peculiarities of the collection of validation images that aren’t reflected in real world photos..Such is the success of deep learning on this particular test that even a small advantage could make a difference. Baidu had reported it achieved an error rate of only 4.58 percent, beating the previous best of 4.82 percent, reported by Google in March. In fact, some experts have noted that the small margins of victory in the race to get better on this particular test make it increasingly meaningless. That Baidu and others continue to trumpet their results all the same - and may even be willing to break the rules - suggest that being the best at machine learning matters to them very much indeed. .Two Dutch researchers have won a major hacking championship by hitting the software that runs the world’s power grids, gas pipelines, and more. It was their easiest challenge yet..An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.\n"}
{"prompt":"Google Photos App tagged Black people as gorillas.\n\n###\n\n","completion":" Google Photos uses sophisticated facial-recognition software to identify not only individuals, but also specific categories of objects and photo types, like food, cats and skylines..Image recognition programs are far from perfect, however; they sometimes gets things comically wrong, and sometimes offensively so -- as one Twitter user recently found out..Browsing his Google Photos app, Brooklyn resident Jacky Alciné noticed that photos of him and a friend, both of whom are black, were tagged under the label Gorillas. He shared a screencap of the racist label on Twitter, which was spotted by Yahoo Tech..In a subsequent tweetstorm, Zunger said Google was scrambling a team together to address the issue, and the label was removed from his app within 15 hours, Alciné confirmed to Mashable. Zunger said Google was looking at longer-term fixes, too. A Google spokesperson also sent an official statement:.“We’re appalled and genuinely sorry that this happened. We are taking immediate action to prevent this type of result from appearing. There is still clearly a lot of work to do with automatic image labeling, and we’re looking at how we can prevent these types of mistakes from happening in the future.”.This isnt the first time software has inadvertently maligned dark-skinned people, unfortunately. In May, Flickrs auto-tagging feature tagged a black person as an ape, although it put the same tag on a white woman as well. And years ago, some webcams on laptops made by HP didnt track the faces of black people even though they did so for white users..At least in the case of Google Photos, the incident appears to be isolated, as it doesnt appear that other users have come forward with similar complaints of offensive tags. But its a reminder that, although computers are beginning to do a really good job of simulating human vision, theyre a long way off from simulating human sensitivity.\n"}
{"prompt":"A comparative analysis to see the prevalence of gender in different professions showed that Google perpetuated societal stereotypes \n\n###\n\n","completion":" Just when you thought biases were a completely human construct, more evidence suggests that both algorithms and interfaces could be biased, too..The latest example of this is from a study conducted by researchers from University of Washington and University of Maryland and reveals how a gender bias is working its way through web searches when people look for images to represent careers and jobs..First, they did a comparative analysis to see if the prevalence of men and women in image search results for professions actually correspond to their representation in actual professions. The researchers did this by comparing the number of women who appeared in the top 100 Google image search results in July 2013 for 45 different occupations, which ranged from bartender to chemist to welder, with 2012 U.S. Bureau of Labor statistics of how many women actually worked in those fields. Then, they did a qualitative analysis to see how men and women are portrayed in the image results..The answers were equally compelling. For instance, according to their study, more than half of U.S. authors are women (56%), yet the image search shows only about 25% women authors..On the flip side is telemarketing, an industry where men and women are equally represented, but the Google image results would have you believe that 64% of telemarketers are female..Not all the results were so skewed. The research uncovered that, in nearly half of the professions, the actual gender representation and the image search numbers were within 5 percentage points of each other..How men and women looked in those images was another story. When the researchers asked participants to rate professionalism, images showing a person who matched the majority gender for the job was viewed as more competent, professional, and trustworthy. Those who didn’t match were rated provocative or inappropriate..“A number of the top hits depicting women as construction workers are models in skimpy little costumes with a hard hat posing suggestively on a jackhammer. You get things that nobody would take as professional,” says Cynthia Matuszek, a co-author of the study..None of this would matter if people wouldn’t then be nudged into making assumptions about men and women in particular roles in the real world. However, when the researchers manipulated the search results, not surprisingly, participants’ opinions changed to conform with stereotypes. Though they stressed that this was just a short-term observation, other research bears out that incremental exposure to these images over time will contribute to unconscious bias..It has also already been revealed that Wikipedia’s entries–a supposed bastion of diversity and editorial neutrality–skew heavily towards men in both actual articles as well as within links. Articles about women tended to be linked to those about men..Part of this is due to Wikipedia’s community, the preponderance of which are educated men, who are English-speaking and hail from mostly Christian countries..In addition to the image searches being gender biased in some cases, Google’s also been taken to task for lack of diversity within its ranks and even disproportionately using white men in its doodles..While Google may not be aware of the results of this latest study and the researchers’ recommendation, the search giant did recognize that its tough for anyone–even its own cadre of emotionally intelligent staff–to process the 11 million bits of information that we are bombarded with at any given moment and focus instead on finding out what biases might spring from them..As such, Google offers a workshop focused on unconscious biases that might sabotage the workplace dynamics or upend the equality of the hiring process..The researchers of this study hope that the information will influence designers of search engines to create algorithms that more accurately represent reality. Sean Munson, UW assistant professor of human-centered design and engineering and a coauthor of the study says: “[Search engine designers] may come to a range of conclusions, but I would feel better if people are at least aware of the consequences and are making conscious choices around them.”.Lydia Dishman is a staff editor for Fast Companys Work Life section. She has written for CBS Moneywatch, Fortune, The Guardian, Popular Science, and the New York Times, among others.\n"}
{"prompt":"Singapore is using a biometric check to secure access to both private and government services. The system has checked if the person is authentically there, a photograph, video, or deepfake cannot be used. \n\n###\n\n","completion":" Well, it the facial verification was done by a human and I put my hand in the fire that a machine would have not recognized me at all (I lost 80 kg and had my beard shaved, which wasnt in the first photo)..It explained Russias facebook manipulation WITH EXAMPLES in episode 2, it showed in an episode a MAGA rally giving BLM protestors a voice and support. And CHEERING! .Your passport contains your photo, and the immigration agent is expected to do a visual check that you match your passport photo. In many countries they will also take a photo of you at passport control where theres nothing stopping them running it through a facial recognition algorithm and comparing it to your passport. They will also keep a record of your passport, including the photo..Ive lived and worked in Singapore. I like it a lot. But Singapore proves the rule that the only way to have and maintain a peaceful, diverse multicultural society is thru an omnipresent police state..Your equation starts off by a factor of 1000.   Of course, thats of little concern to the 26 thousand or so falsely accused hypothetical citizens.  (And the 26 thousand or so falsely innocent are partying on, dude.).Currently in the middle of covid it is mandatory to wear face masks in many countries, including in singapore... How well will facial recognition work if people are wearing face masks? Even when it stops being mandatory, many people have now grown accustomed to wearing masks and seeing others doing so, so its going to be a long time before people are routinely walking around without masks again..Currently in the middle of covid it is mandatory to wear face masks in many countries, including in singapore... How well will facial recognition work if people are wearing face masks? Even when it stops being mandatory, many people have now grown accustomed to wearing masks and seeing others doing so, so its going to be a long time before people are routinely walking around without masks again..There may be more comments in this discussion. Without JavaScript enabled, you might want to turn on Classic Discussion System in your preferences instead.\n"}
{"prompt":"Sepsis Watch, a deep learning tool, reduced sepsis-induced patient deaths because of nurse feedback and local and specific context\n\n###\n\n","completion":" In November of 2018, a new deep-learning tool went online in the emergency department of the Duke University Health System. Called Sepsis Watch, it was designed to help doctors spot early signs of one of the leading causes of hospital deaths globally..Sepsis occurs when an infection triggers full-body inflammation and ultimately causes organs to shut down. It can be treated if diagnosed early enough, but that’s a notoriously hard task because its symptoms are easily mistaken for signs of something else..Sepsis Watch promised to change that. The product of three and a half years of development (which included digitizing health records, analyzing 32 million data points, and designing a simple interface in the form of an iPad app), it scores patients on an hourly basis for their likelihood of developing the condition. It then flags those who are medium or high risk and those who already meet the criteria. Once a doctor confirms the diagnosis, the patients get immediate attention..In the two years since the tool’s introduction, anecdotal evidence from Duke Health’s hospital managers and clinicians has suggested that Sepsis Watch really works. It has dramatically reduced sepsis-induced patient deaths and is now part of a federally registered clinical trial expected to share its results in 2021..At first glance, this is an example of a major technical victory. Through careful development and testing, an AI model successfully augmented doctors’ ability to diagnose disease. But a new report from the Data & Society research institute says this is only half the story. The other half is the amount of skilled social labor that the clinicians leading the project needed to perform in order to integrate the tool into their daily workflows. This included not only designing new communication protocols and creating new training materials but also navigating workplace politics and power dynamics..The case study is an honest reflection of what it really takes for AI tools to succeed in the real world. “It was really complex,” says coauhtor Madeleine Clare Elish, a cultural anthropologist who examines the impact of AI..Innovation is supposed to be disruptive. It shakes up old ways of doing things to achieve better outcomes. But rarely in conversations about technological disruption is there an acknowledgment that disruption is also a form of “breakage.” Existing protocols turn obsolete; social hierarchies get scrambled. Making the innovations work within existing systems requires what Elish and her coauthor Elizabeth Anne Watkins call “repair work.”.During the researchers’ two-year study of Sepsis Watch at Duke Health, they documented numerous examples of this disruption and repair. One major issue was the way the tool challenged the medical world’s deeply ingrained power dynamics between doctors and nurses..In the early stages of tool design, it became clear that rapid response team (RRT) nurses would need to be the primary users. Though attending physicians are typically in charge of evaluating patients and making sepsis diagnoses, they don’t have time to continuously monitor another app on top of their existing duties in the emergency department. In contrast, the main responsibility of an RRT nurse is to continuously monitor patient well-being and provide extra assistance where needed. Checking the Sepsis Watch app fitted naturally into their workflow..But here came the challenge. Once the app flagged a patient as high risk, a nurse would need to call the attending physician (known in medical speak as “ED attendings”). Not only did these nurses and attendings often have no prior relationship because they spent their days in entirely different sections of the hospital, but the protocol represented a complete reversal of the typical chain of command in any hospital. “Are you kidding me?” one nurse recalled thinking after learning how things would work. “We are going to call ED attendings?”.But this was indeed the best solution. So the project team went about repairing the “disruption” in various big and small ways. The head nurses hosted informal pizza parties to build excitement and trust about Sepsis Watch among their fellow nurses. They also developed communication tactics to smooth over their calls with the attendings. For example, they decided to make only one call per day to discuss multiple high-risk patients at once, timed for when the physicians were least busy..On top of that, the project leads began regularly reporting the impact of Sepsis Watch to the clinical leadership. The project team discovered that not every hospital staffer believed sepsis-induced death was a problem at Duke Health. Doctors, especially, who didn’t have a bird’s-eye view of the hospital’s statistics, were far more occupied with the emergencies they were dealing with day to day, like broken bones and severe mental illness. As a result, some found Sepsis Watch a nuisance. But for the clinical leadership, sepsis was a huge priority, and the more they saw Sepsis Watch working, the more they helped grease the gears of the operation..Elish identifies two main factors that ultimately helped Sepsis Watch succeed. First, the tool was adapted for a hyper-local, hyper-specific context: it was developed for the emergency department at Duke Health and nowhere else. “This really bespoke development was key to the success,” she says. This flies in the face of typical AI norms. .Second, throughout the development process, the team regularly sought feedback from nurses, doctors, and other staff up and down the hospital hierarchy. This not only made the tool more user friendly but also cultivated a small group of committed staff members to help champion its success. It also made a difference that the project was led by Duke Health’s own clinicians, says Elish, rather than by technologists who had parachuted in from a software company. “If you don’t have an explainable algorithm,” she says, “you need to build trust in other ways.”.These lessons are very familiar to Marzyeh Ghassemi, an incoming assistant professor at MIT who studies machine-learning applications for health care. “All machine-learning systems that are ever intended to be evaluated on or used by humans must have socio-technical constraints at front of mind,” she says. Especially in clinical settings, which are run by human decision makers and involve caring for humans at their most vulnerable, “the constraints that people need to be aware of are really human and logistical constraints,” she adds..Elish hopes her case study of Sepsis Watch convinces researchers to rethink how to approach medical AI research and AI development at large. So much of the work being done right now focuses on “what AI might be or could do in theory,” she says. “There’s too little information about what actually happens on the ground.” But for AI to live up to its promise, people need to think as much about social integration as technical development..Her work also raises serious questions. “Responsible AI must require attention to local and specific context,” she says. “My reading and training teaches me you can’t just develop one thing in one place and then roll it out somewhere else.”.“So the challenge is actually to figure how we keep that local specificity while trying to work at scale,” she adds. That’s the next frontier for AI research. .An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.\n"}
{"prompt":"The \"Automated Inference on Criminality using Face Images\" declares that a computer can detect if a human could be a convicted criminal based on facial features. \n\n###\n\n","completion":" Like a more crooked version of the Voight-Kampff test from Blade Runner, a new machine learning paper from a pair of Chinese researchers has delved into the controversial task of letting a computer decide on your innocence. Can a computer know if youre a criminal just from your face?.In their paper Automated Inference on Criminality using Face Images, published on the arXiv pre-print server, Xiaolin Wu and Xi Zhang from Chinas Shanghai Jiao Tong University investigate whether a computer can detect if a human could be a convicted criminal just by analysing his or her facial features. The two say their tests were successful, and that they even found a new law governing the normality for faces of non-criminals..They described the idea of algorithms that can match and exceed a humans performance in face recognition to infer criminality irresistible. But as a number of Twitter users and commenters on Hacker News point out, by stuffing biases into artificial intelligence and machine learning algorithms, the computer could act on those biases. The researchers maintain that the data sets were controlled for race, gender, age, and facial expressions, though.. Imagine this with drones, every CCTV camera in every city, the eyes of self driving cars, everywhere theres a camera… Tim MaughanNovember 18, 2016 .Imagine this with drones, every CCTV camera in every city, the eyes of self driving cars, everywhere theres a camera… Tim MaughanNovember 18, 2016.The images used in the research were standard ID photographs of Chinese males between the ages of 18 and 55, with no facial hair, scars, or other markings. Wu and Zhang stress that the ID photos used were not police mugshots, and that out of 730 criminals, 235 committed violent crimes including murder, rape, assault, kidnap, and robbery..The two state they purposely took away any subtle human factors out of the assessment process. As long as data sets are finely controlled, could human bias be completely eradicated? Wu told Motherboard that human bias didnt come into it. In fact, we got our first batch of results a year ago. We went through very rigorous checking of our data sets, and also ran many tests searching for counterexamples but failed to find any, said Wu..Heres how it worked: Xiaolin and Xi fed into a machine learning algorithm facial images of 1,856 people, of which half were convicted criminals, and then observed if any of their four classifiers—each using a different method of analysing facial features—could infer criminality..They found that all four of their different classifiers were mostly successful, and that the faces of criminals and those not convicted of crimes differ in key ways that are perceptible to a computer program. Moreover, the variation among criminal faces is significantly greater than that of the non-criminal faces, Xiaolin and Xi write..All four classifiers perform consistently well and produce evidence for the validity of automated face-induced inference on criminality, despite the historical controversy surrounding the topic, the researchers write. Also, we find some discriminating structural features for predicting criminality, such as lip curvature, eye inner corner distance, and the so-called nose-mouth angle. The best classifier, known as the Convolutional Neural Network, achieved 89.51 percent accuracy in the tests..By extensive experiments and vigorous cross validations, the researchers conclude, we have demonstrated that via supervised machine learning, data-driven face classifiers are able to make reliable inference on criminality..While Xiaolin and Xi admit in their paper that they are not qualified to discuss or to debate on societal stereotypes, the problem is that machine learning is adept at picking up on human biases in data sets and acting on those biases, as proved by multiple recent incidents. The pair admit theyre on shaky ground. We have been accused on Internet of being irresponsible socially, Wu said..In the paper they go on to quote philosopher Aristotle, It is possible to infer character from features, but that has to be left to human psychologists, not machines, surely? One major concern going forward is that of false positives—that is, identifying innocent people as guilty—especially if this program is used in any sort of real-world criminal justice settings. The researchers said the algorithms did throw up some false positives (identifying non-criminals as criminals) and false negatives (identifying criminals as non-criminals), which increased when the faces were randomly labeled for control tests..Online critics have lambasted the paper. I thought this was a joke when I read the abstract, but it appears to be a genuine paper, said a user on Hacker News. I agree its an entirely valid area of study…but to do it you need experts in criminology, physiology and machine learning, not just a couple of people who can follow the Keras instructions for how to use a neural net for classification..Others questioned the validity of the paper, noting that one of the researchers is listed as having a Gmail account. First of all, I dont think this is satire. Ill admit that the use of a gmail account by a researcher at a Chinese uni is facially suspicious, posed another Hackers News reader..Wu had an answer for this, however. Some questioned why I used gmail address as a faculty member in China. In fact, I am also a professor at McMaster University, Canada, he told Motherboard..By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.\n"}
{"prompt":"Facebook lets advertisers exclude people of certain races from seeing ads.\n\n###\n\n","completion":" The company’s platform lets advertisers exclude people of certain races from seeing their content. That’s a serious problem when it comes to promotions such as housing, credit, and jobs..With more than 1.5 billion users, Facebook has become one of the most powerful, highly visible platforms on the internet. It’s no wonder then that so many advertisers clamor for space on the social-media site. But a recent investigation from ProPublica found that Facebook may be allowing those advertisers to discriminate based on race..Facebook’s ability to let advertisers target a specific audience—for instance, women between the ages of 25 and 34 with young children—is its primary strength. More and more advertisers count on being able to identify, and market to, very specific groups. But Facebook’s advertising system not only allows marketers to choose who they most want to see their ads—it also allows them choose entire groups who will never see their ads..When placing an ad on Facebook, advertisers can explicitly exclude lots of groups, including people with any given educational level, financial status, political affiliation, and—perhaps most disturbingly—“ethnic affinity.”.“Targeting ads for housing, credit, or employment based upon race, gender, or sexual orientation violates the federal civil-rights laws that cover those fields—the Fair Housing Act, the Equal Credit Opportunity Act, and Title VII,” says Rachel Goodman, a lawyer at the American Civil Liberties Union. “If Facebook is going to allow advertisers to target ads toward or away from users based on these sensitive characteristics, it must at the very least prohibit targeting in these three areas central to economic prosperity.”  (A spokesperson from Facebook noted that the ad placed by ProPublica was not promoting a rental property, but promoting an event about renters’ rights.).Steve Satterfield, Facebook’s privacy and public-policy manager, told ProPublica that any use of its advertising platform to intentionally discriminate is a violation of the site’s policy, saying, “We take a strong stand against advertisers misusing our platform: Our policies prohibit using our targeting options to discriminate, and they require compliance with the law.” Satterfield also said that “ethnic affinity” is determined based on what sort of content on the site a user engages with most. That’s not the same as identifying a user’s race, but Facebook does place this category under an ad-targeting category called “demographics.” How accurately does “ethnic affinity” map onto users’ race? It’s hard to say for sure, but a recent report from the Pew Research center suggests it is not so difficult to determine a user’s race based on their use of the site..The creation of an ad platform that allows marketers to carve out entire groups of people based on race or ethnicity isn’t unique. “There are a number of sites that allow you to specify based on race,” says Aaron Rieke, one of the heads of the tech-policy consulting firm Upturn. “But Facebook is special because it’s extraordinarily powerful.” The platform is extremely large, and that, Rieke says, means that it probably can’t review every single ad that is entered into its system. Still, the current system doesn’t have enough safeguards. For instance, relying on users to report whether or not they are being discriminated against is difficult if the discrimination is based on preventing someone from ever seeing an ad, he says.  Goodman agrees, saying, “When this kind of targeting happens online, it’s nearly impossible for people to know they’ve been denied information about opportunities they might be interested in.”.A better ad-buying platform might involve a system under which ads in areas where the U.S. has key civil-rights legislation—such as housing, credit, or employment—that also include ethnic targeting automatically get flagged for review. That type of due diligence already exists in the industry, Rieke says..Granting advertisers the ability to ensure that minorities aren’t able to view certain ads is disturbing and potentially illegal. When ProPublica’s reporters described their experience placing a sample ad that would exclude blacks, they did so for a very specific reason: There is a law—the Fair Housing Act—expressly forbidding racial discrimination in housing. Among its many provisions, the act states that the following is illegal:.To make, print, or publish, or cause to be made, printed, or published any notice, statement, or advertisement, with respect to the sale or rental of a dwelling that indicates any preference, limitation, or discrimination based on race, color, religion, sex, handicap, familial status, or national origin, or an intention to make any such preference, limitation, or discrimination..This law exists for a reason—and it’s not just some quaint notion of fairness. Housing discrimination, particularly the long history of redlining, in this country has led to a vast and possibly permanent divide between white and black Americans when it comes to wealth, which stands at 13-to-one. Given that housing is the most valuable asset held by most Americans, exclusion from safe, affluent, white neighborhoods results not only in racial segregation, but also in black Americans remaining mired in concentrated poverty in neighborhoods with fewer services, lower tax bases, worse schools, and homes that didn’t grown in value..Facebook’s claim that the intent of its audience-targeting tool wasn’t to allow for discrimination may well be true. But as a site that supposedly prides itself on inclusion, the company has a responsibility to ensure that advertisers cannot perpetuate discrimination right under its nose.\n"}
{"prompt":"Bodega, a vending machine with AI, received criticism because of misappropriation. Since then, it has changed its name to Stockwell.\n\n###\n\n","completion":" More than two years ago, Fast Company published a story with the headline “Two Ex-Googlers Want To Make Bodegas And Mom-And-Pop Corner Stores Obsolete.” The focus of the story was a nascent startup by the name of Bodega..The company had raised $2.5 million in funding from First Round Capital’s Josh Kopelman, Forerunner Ventures’ Kirsten Green and Homebrew’s Hunter Walk. To announce their funding and vision to create the unmanned store of the future, Bodega briefed a number of journalists on its big idea. Given the simplicity of its product — a tech-enabled vending machine, in essence — the team was blindsided by the uproarious response that followed. September 13, 2017 was supposed to be the most exciting day in the startup’s history, at least until that point; instead, it was a nightmarish lesson in poor branding and messaging..The press storm and public lambasting catapulted Bodega into the limelight — for all the wrong reasons. Overnight, the company went from just another early-stage commerce business to the symbol of everything that is wrong with Silicon Valley. Many wondered if it would fall victim to criticism and crumble like Juicero, a well-financed startup that sold a $400 juicer — that is, until a Bloomberg story proved its juice packets could be squeezed by hand, no machine necessary. Or would it take the public condemnation in stride, hearing out the critics and amending its brand as necessary?.Two years after its ill-fated launch, the latter seems to be true. Today, the three-year-old Oakland-based company — now known as Stockwell — is said to be growing quickly thanks to more than $45 million in venture capital funding from a number of deep-pocketed investors, the company has confirmed to TechCrunch..“Bodega is either the worst named startup of the year, or the most devious,” wrote The Verge in the fall of 2017. “Tech firm markets glorified vending machines where users can buy groceries,” said The Guardian. The Washington Post dubbed the company “America’s most hated start-up.” CityLab, which writes about issues impacting cities, bluntly reported “Bodega, a Startup for Disrupting Bodegas, Is Terrible,” followed by 30 reasons why the startup sucks: “Maybe a Bodega can stock Soylent to appeal to people who also think that eating delicious food is a grim burden,” CityLab wrote. “Why do tech wizards keep thinking of new and more horrible ways to avoid dealing with people? How come they hate being human?”.It’s safe to say Bodega endured one of the most catastrophic company launches in the history of tech startups. But the press cycle surrounding Bodega was more than an attack on the startup alone. It represented a greater frustration with Silicon Valley culture and its reputation for funding “disruptive” products devoid of impact. Time and time again, VCs had proven their willingness to inject millions into standard concepts lacking originality. A juicer had raised more than $100 million, after all, scooters were beginning to attract private capital and Soylent, which sells a meal replacement drink fit for techies, was hot off the heels of a $50 million round..A mini-fridge equipped with computer vision technology boasting a culturally insensitive name wasn’t going to change the world. Questioning why it had the support of VCs was only fair..Behind the upsetting name was a business developing hundreds of five-foot-wide pantry boxes to be housed in luxury apartment lobbies, offices, college campuses, gyms and more. Similar to Amazon Go, the “smart stores” recognize what customers remove from the cases using computer vision and automatically charge the credit card associated with the account..Bodega was founded by a pair of Google veterans, Paul McDonald and Ashwath Rajan. It had all the ingredients for a successful startup stew. Founders with years of experience in big tech: McDonald spent more than a decade at Google; Rajan had just finished up the search engine’s competitive associate product manager program. Both attended top universities: University of California-Berkeley and Columbia University, respectively. Still, neither of the two men nor their investors seemed to have predicted the controversy afoot..“Bodega doesn’t want to disrupt the bodega,” Hunter Walk, a Bodega investor and co-founder of the seed fund Homebrew, wrote in a 2017 blog post. “Some instances of today’s press coverage suggested that element, a sound bite which, exacerbated by Bodega’s naming, pissed people off as another example of tech startups being at best tone-deaf, and at worst, predatory … It didn’t occur to me that some people would see the word and associate its use in this context with whitewashing or cultural appropriation.”.The company, too, quickly authored a blog post outlining their thought process behind the name: “Rather than disrespect to traditional corner stores — or worse yet, a threat — we intended only admiration,” McDonald wrote..After penning blog posts, the founders continued working on the company under the provocative and upsetting name. Meanwhile, investors seemed unfazed by the negative press, evidenced by the company’s ability to continue raising venture capital funding. After all, many of the best businesses endure the wrath of bloggers, competing founders and the general public. As for VCs, high-risk bets are just part of the ball game..DCM Ventures, a U.S.-based venture capital fund with offices in Beijing, Tokyo and Silicon Valley, was the first to agree to invest in Bodega following the PR disaster. The firm, an investor in Lime, Hims and SoFi, led a $7.5 million Series A financing in the business in early 2018, the company confirmed. DCM co-founder and general partner David Chao joined the company’s board following the deal. DCM vice president David Cheng is also actively involved with the company, according to his bio..Finally, after pocketing nearly $10 million in total funding, Bodega announced a name change: “Did you buy something today from a Bodega?” Bodega’s McDonald wrote. “You may have noticed that we’ve changed our name to Stockwell. Our new name is one of the changes we’re making as we expand our offerings and open more stores around the country.”.With a new logo and a toned-down, somewhat bland identity, Stockwell had a fresh start and, soon, more attention from top VCs. In late 2018, the company raised a $35 million round of funding led by NEA, an investor known for bets in Coursera, MasterClass and OpenDoor, with participation from Uber and Slack-backer GV, formerly known as Google Ventures, Stockwell confirmed. NEA’s Amit Mukherjee and GV’s John Lyman joined Stockwell’s board as part of the deal, which is said to have valued the business at north of $100 million. Stockwell, however, declined to confirm the figure..Instead of announcing the news via TechCrunch, Venture Beat, Forbes or another tech publication, as is the norm for fast-growing consumer-facing startups, Stockwell remained mum on financing events and scaling plans, assumedly burned by the press and the public’s scorn a year prior..Rather than subject itself to continued scrutiny as it attempted to rewrite its narrative, Stockwell was heads down, iterating, expanding and quietly raising millions. Bad press can break a startup, and given the sheer number of negative reports on Stockwell so early on, the company had already defied the odds. Keeping a low profile was undoubtedly the best strategy moving forward, and it seems to have paid off..Today the company counts 1,000 “stores” in the San Francisco Bay Area, Los Angeles, Houston and Chicago. Stockwell has used its latest infusion of funding to explore shared ownership models, i.e. the opportunity for anyone to run their own Stockwell store. The company tells TechCrunch they are also working on building out their “unique curation model,” which allows customers to help determine what items are stocked in their local “store,” as well as their support for emerging brands, whose products they can stock in their next-generation vending machines..Human beings make snap judgments, evaluate products quickly and can develop distaste for brands in a matter of seconds. A company’s moniker is their first opportunity to impress customers..“When you’re not in the room, the name of your company is what gets passed between people,” writes NFX co-founder James Currier. “It speaks for you when you’re not there … It sets expectations of your company in the blink of an eye. And first impressions are hard to change. Both positive and negative.”.Most cases of poor startup naming are easily fixed. Most founders aren’t forced to bear the brunt of the internet’s fury. The case of Bodega is much more extreme and, as such, serves as the ultimate lesson for founders searching for the best way to tell their story. At the end of the day, avoiding a complete and total train-wreck is easy if you include a diverse group of people in the naming process and remember there’s a lot in a name — if that weren’t the case, Bodega would still be Bodega.\n"}
{"prompt":"MIT cuts funding of iFlytek technology that is being used to oppress ethnic Uighurs in China's northwest\n\n###\n\n","completion":" MIT has terminated a research collaboration with iFlytek, a Chinese artificial intelligence company accused of supplying technology for surveilling Muslims in the northwestern province of Xinjiang..The university canceled the relationship in February after reviewing an upcoming project under tightened guidelines governing funding from companies in China, Russia, and Saudi Arabia. MIT has not said why it terminated the iFlytek collaboration or disclosed details about the project that prompted the review, but it has faced pushback from some students and staff about the arrangement since it began two years ago..“We take very seriously concerns about national security and economic security threats from China and other countries, and human rights issues,” says Maria Zuber, vice president of research at MIT..US companies and universities have built ties with Chinese tech firms in recent years. But the relationships have come under increasing scrutiny as relations between the two countries have soured..MIT announced what was supposed to be a five-year collaboration with iFlytek with fanfare in June 2018. Since then, iFlytek has helped fund a range of research on subjects including human-computer interaction, new approaches to machine learning, and applied voice recognition. Under the agreement, iFlytek selected existing projects to fund but MIT says the company did not receive special access to the work or receive proprietary data or code. The amount of money involved was not disclosed..The arrangement became more controversial in October 2019, when the US government banned six Chinese AI companies, including iFlytek, from doing business with American firms for reportedly supplying technology used to oppress minority Uighurs in Xinjiang. In 2017, Human Rights Watch claimed iFlytek supplied police departments in Xinjiang with technology for identifying people using their voiceprints. Press reports paint a grim picture of widespread surveillance in the province, including the detention and disappearance of more than 1 million people..iFlytek is one of China’s older AI companies, and while it specializes in voice recognition, it also offers tools for analyzing legal documents and medical imagery. Like other growing Chinese AI companies, contracts to supply software for processing video and audio to police departments and local governments are an important source of revenue..The company said MIT’s decision was disappointing. “We are particularly sorry about this,” says Jiang Tao, a senior VP at iFlytek. “The vision of the cooperation was to build a better world with artificial intelligence together.”.Like other US universities, MIT receives funding from companies and individual donors, but several of its arrangements have proved controversial. In February 2019, the university reexamined funding from Saudi Arabia following the assassination of the journalist Jamal Khashoggi. The tighter guidelines for working with foreign companies were issued in April 2019 amid scrutiny of MIT’s relationship with two other Chinese companies, Huawei and ZTE. MIT had cut funding relationships with those companies in 2018 as the US government investigated their roles in alleged violations of US sanctions. In January 2020, MIT released the results of an investigation into funding from the convicted sex offender Jeffrey Epstein..In 2018, MIT received a onetime donation of an undisclosed sum from SenseTime, another Chinese AI company now subject to the US government restrictions. The gift was reviewed by MIT’s Interim Gift Acceptance Committee, and an MIT spokesperson says there are no plans to return it..US officials are increasingly wary of Chinese companies developing advanced technologies, amid rising trade tensions, accusations of intellectual property theft, and a heightened sense of international competition. Over the past two years, US intelligence agencies have repeatedly warned universities to watch for signs of espionage by Chinese students and professors, and prosecuted both Chinese-born and US academics for stealing intellectual property. In a meeting with senior figures at MIT in November 2019, Michael Kratsios, the US chief technology officer, warned against working with Chinese AI companies, according to a person familiar with the discussion..Paul Triolo, a practice head at Eurasia Group specializing in global technology policy, says concerns over human rights violations are legitimate but the signals coming from the US government have been ambiguous. “Is this some sort of just punishment or really legitimate effort to try to change behavior?” he asks. “The danger is sort of painting them all with one brush, and not looking at what theyre actually doing in Xinjiang, and how much they are taking steps to step away from that.”.Triolo says a complete unraveling of relations between the US and China will harm American AI too. He notes that China’s tech industry is making rapid progress in medical uses of AI, for example: “The flow of knowledge is not one way.”.MIT’s Zuber says the university doesn’t want to walk away from China. “We want to be able to draw the best talent in the world, and some of that best talent comes from China,” she adds. “The wrong thing to do is say we’re never going to work with these international entities under any circumstances and we’re just going to lock our doors.” Zuber also says “global collaborations are extremely important.”.When it comes to China, it may be difficult to ignore outcry over human rights issues. Zulkayda Mamat, a graduate student of Uighur descent who was critical of MIT’s ties to Chinese AI companies while studying there, welcomed the news but says MIT should scrutinize collaborations carefully. “I hope that it continues the process of reevaluation for all projects,” she pointed out. “[A] lack of vigilance will certainly put it on the wrong side of history.”\n"}
{"prompt":"DeepMind processed 1.6 million patients belonging to UK citizens for the Royal Free Trust.\n\n###\n\n","completion":" A deal between UK hospitals and Google’s AI subsidiary DeepMind “failed to comply with data protection law,” according to the UK’s data watchdog. The Information Commissioners Office (ICO) made its ruling today after a year-long investigation into the agreement, which saw DeepMind process 1.6 million patient records belonging to UK citizens for the Royal Free Trust — a group of three London hospitals..The deal was originally struck in 2015, and has since been superseded by a new agreement. At the time, DeepMind and the Royal Free said the data was being shared to develop an app named Streams, which would alert doctors if patients were at risk from a condition called acute kidney injury. An investigation by the New Scientist revealed that the terms of the agreement were more broad than had been originally implied. DeepMind has since made new deals to deploy Streams in other UK hospitals..Today, ICO said it had found “a number of shortcomings” with the agreement, particularly that patients had not been fully briefed on how their personal data would be used. In a press statement, the UK’s information commissioner Elizabeth Denham said that the “price of innovation does not need to be the erosion of fundamental privacy rights.”.Patients were not asked if they consented to having their medical data processed by DeepMind. The information shared included including details of drug overdoses, abortions, and whether individuals were HIV positive. DeepMind and the Royal Free have argued that patients had given “implied consent” to sharing, because this information would be used to deliver “direct care” via the Streams app. .Today’s ruling suggests that the two institutions did not go far enough. “Patients would not have reasonably expected their information to have been used in this way, and the Trust could and should have been far more transparent with patients as to what was happening,” said Denham..The contract was always clear that no private data would ever be shared with DeepMind’s parent company Google, which bought the firm in 2014. Neither would machine learning or AI tools be used to analyze this information. (Although DeepMind is involved in two separate deals with UK hospitals to develop AI-powered algorithms for improving cancer treatment and eye disease.) .DeepMind says it welcomes ICO’s “thoughtful resolution” of the case, and admits it made a number of mistakes during its original deal. The company says it should have better explained the deal, to patients and the public, and that it “underestimated the complexity of the NHS and of the rules around patient data.” .In a blog post by the ICO, the watchdog stated that in the rush to innovate, institutions like the Royal Free Trust should not forget to follow the law. The Trust has been asked to sign a new agreement committing it to act in accordance with the law and commission an audit of the 2015 trial. “When you’re setting out to test the clinical safety of a new service, remember that the rules are there for a reason,” writes Denham. \n"}
{"prompt":"A woman is not recognized on facial recognition\n\n###\n\n","completion":" The woman, identified only by her surname Yan, told the Jiangsu Broadcasting Corp. that her co-worker was able to get into both phones — her original as well as the new one Apple gave her as a replacement, reports the South China Morning Post. .An Apple spokesman told HuffPost that he couldn’t confirm the details of the story, nor did he have enough information to determine what might have gone wrong with the phones. He suspected that both women may have used the phone during its “passcode training” and that the phones may have been essentially “taught” to recognize both faces..The facial recognition software has run into some glitches. It can sometimes mistake twins or siblings, according to Apple. The phone, too, may not accurately identify children under the age of 13 because their faces are not as definitely formed as adults’, according to an Apple security “white paper” on the technology..Apple hasn’t yet confirmed a case of an unrelated adult cracking the phone’s facial recognition software, according to the Apple spokesman. The company insists that the probability of a random person accessing someone else’s iPhone X using the Face ID passcode is 1 in 1 million, versus 1 in 50,000 for Touch ID. Phil Schiller, Apple’s vice president of product marketing, conceded in September: “Of course, the statistics are lowered if that person shares a close genetic relationship with you.”.Unless Apple technicians examine the Chinese phones, it’s unclear what happened. An added complication is that a Chinese company has reportedly begun manufacturing a clone of the iPhone X — with unknown facial recognition capabilities.\n"}
{"prompt":"FaceApp hot filter (neural networks) automatically lightened people's skin\n\n###\n\n","completion":" The creator of an app which changes your selfies using artificial intelligence has apologised because its “hot” filter automatically lightened people’s skin..FaceApp is touted as an app which uses “neural networks” to change facial characteristics, adding smiles or making users look older or younger. But users noticed one of the options, initially labelled as “hot” made people look whiter..The feature is still available but has now been renamed “spark”, in an attempt to “exclude any positive connotation associated with it”, Goncharov said..In previous interviews Goncharov, who is a former Microsoft and Yandex engineer, said FaceApp differs from other face-tuning software, which usually adds filters, because it uses deep learning technology to alter the photo itself. .Snapchat’s filters have come under fire on several occasions. Last year it was criticised for promoting “yellowface” after it released a filter which allowed users to turn their selfies into Asian caricatures. Prior to that, a Bob Marley filter was dubbed “the digital equivalent of blackface”. \n"}
{"prompt":"Voice recognition system is breached by twin\n\n###\n\n","completion":" HSBC’s voice recognition ID system used by half a million customers for secure access to their bank accounts has been breached by a customer’s twin mimicking his voice..When it was launched last year HSBC’s head of retail banking claimed the new system was secure, insisting that “just like your fingerprint, your voice print is unique”..But when BBC Click reporter Dan Simmons set up an HSBC voice-ID authenticated account, his non-identical twin, Joe, was able to fool the system and granted him access to his brother’s account. .HSBC said it is to review security on its voice-access systems following the breach. Unlike traditional password systems, which lock users out after repeated attempts fail, Joe Simmons tried seven times to mimic his twin’s voice before HSBC allowed access. .The HSBC system asks users to say “my voice is my password” into the phone, which is then matched to an original recording of the person’s voice, allowing access to their account. .According to the BBC, the breach did not allow Joe Simmons to withdraw money, but he was able to access balances and recent transactions, and was offered the chance to transfer money between accounts..“What’s really alarming is that the bank allowed me seven attempts to mimic my brother’s voiceprint and get it wrong, before I got in at the eighth time of trying,” he said..Voice ID is currently being rolled out to 15 million HSBC customers. At launch, HSBC said: “The technology is now the ultimate way to bank safely and securely, without the need for passwords. With a couple of choice words, banking with HSBC is as easy as being yourself.”.But in a statement issued after the breach was made public, HSBC said: “The security and safety of our customers’ accounts is of the utmost importance to us and Voice ID is amongst the most secure methods of authenticating customers. .“The introduction of this technology has seen a significant reduction in telephone fraud, and has proven to be more secure than PINs, passwords and memorable phrases. Our VoiceID system does allow us to make changes to different security settings, and following a review we have made changes to make it even more secure.”.Embarrassed HSBC officials have suggested that the BBC Click experiment, while real, does not open the door to fraudsters. One said: “This is not how fraudsters work. This was a twin sitting with his brother. He would just as likely know other security data such as mother’s maiden name, pet’s name and so on. .“In a real situation you would not have a fraudster sitting next to you. If he or she tried recording your voice saying ‘my voice is my password’ it would not work either, as the system is able to detect synthetic voice characteristics.” .Barclays introduced voice recognition software for all its 300,000 wealthiest clients in the UK in 2013. A year later the bank said the technology had been so successful that it would be rolled out to 12 million retail banking customers. .Barclays said it had been “incredibly popular” with wealthier clients, with the time taken to verify their identity falling from 1.5 minutes to less than 10 seconds.\n"}
{"prompt":"The ability for bots to invest into a new language through reinforcement learning could lead to unintended consequences\n\n###\n\n","completion":" From the moment we wake up, our days are filled with a constant flow of negotiations. These scenarios range from discussing what TV channel to watch to convincing your kids to eat their vegetables or trying to get a better price on something. What these all have in common is that they require complex communication and reasoning skills, which are attributes not inherently found in computers..To date, existing work on chatbots has led to systems that can hold short conversations and perform simple tasks such as booking a restaurant. But building machines that can hold meaningful conversations with people is challenging because it requires a bot to combine its understanding of the conversation with its knowledge of the world, and then produce a new sentence that helps it achieve its goals..Today, researchers at Facebook Artificial Intelligence Research (FAIR) have open-sourced code and published research introducing dialog agents with a new capability — the ability to negotiate..Similar to how people have differing goals, run into conflicts, and then negotiate to come to an agreed-upon compromise, the researchers have shown that it’s possible for dialog agents with differing goals (implemented as end-to-end-trained neural networks) to engage in start-to-finish negotiations with other bots or people while arriving at common decisions or outcomes..The FAIR researchers studied negotiation on a multi-issue bargaining task. Two agents are both shown the same collection of items (say two books, one hat, three balls) and are instructed to divide them between themselves by negotiating a split of the items..Each agent is provided its own value function, which represents how much it cares about each type of item (say each ball is worth 3 points to agent 1). As in life, neither agent knows the other agent’s value function and must infer it from the dialog (you say you want the ball, so you must value it highly)..FAIR researchers created many such negotiation scenarios, always ensuring that it is impossible for both agents to get the best deal simultaneously. Furthermore, walking away from the negotiation (or not agreeing on a deal after 10 rounds of dialog) resulted in 0 points for both agents. Simply put, negotiation is essential, and good negotiation results in better performance..Negotiation is simultaneously a linguistic and a reasoning problem, in which an intent must be formulated and then verbally realized. Such dialogs contain both cooperative and adversarial elements, requiring agents to understand and formulate long-term plans and generate utterances to achieve their goals..When chatbots can build mental models of their interlocutors and “think ahead” or anticipate directions a conversation is going to take in the future, they can choose to steer away from uninformative, confusing, or frustrating exchanges toward successful ones..Specifically, FAIR has developed dialog rollouts as a novel technique where an agent simulates a future conversation by rolling out a dialog model to the end of the conversation, so that an utterance with the maximum expected future reward can be chosen..Similar ideas have been used for planning in game environments but have never been applied to language because the number of possible actions is much higher. To improve efficiency, the researchers first generated a smaller set of candidate utterances to say, and then for each of these, they repeatedly simulated the complete future of the dialog in order to estimate how successful they were. The prediction accuracy of this model is high enough that the technique dramatically improved negotiation tactics in the following areas:.Negotiating harder: The new agents held longer conversations with humans, in turn accepting deals less quickly. While people can sometimes walk away with no deal, the model in this experiment negotiates until it achieves a successful outcome..Intelligent maneuvers: There were cases where agents initially feigned interest in a valueless item, only to later “compromise” by conceding it — an effective negotiating tactic that people use regularly. This behavior was not programmed by the researchers but was discovered by the bot as a method for trying to achieve its goals..Producing novel sentences: Although neural models are prone to repeating sentences from training data, this work showed the models are capable of generalizing when necessary..In order to train negotiation agents and conduct large-scale quantitative evaluations, the FAIR team crowdsourced a collection of negotiations between pairs of people. The individuals were shown a collection of objects and a value for each, and asked to agree how to divide the objects between them. The researchers then trained a recurrent neural network to negotiate by teaching it to imitate people’s actions. At any point in a dialog, the model tries to guess what a human would say in that situation..Unlike previous work on goal-orientated dialog, the models were trained “end to end” purely from the language and decisions that humans made, meaning that the approach can easily be adapted to other tasks..To go beyond simply trying to imitate people, the FAIR researchers instead allowed the model to achieve the goals of the negotiation. To train the model to achieve its goals, the researchers had the model practice thousands of negotiations against itself, and used reinforcement learning to reward the model when it achieved a good outcome. To prevent the algorithm from developing its own language, it was simultaneously trained to produce humanlike language..To evaluate the negotiation agents, FAIR tested them online in conversations with people. Most previous work has avoided dialogs with real people or worked in less challenging domains, because of the difficulties of learning models that can respond to the variety of language that people can say..Interestingly, in the FAIR experiments, most people did not realize they were talking to a bot rather than another person — showing that the bots had learned to hold fluent conversations in English in this domain. The performance of FAIR’s best negotiation agent, which makes use of reinforcement learning and dialog rollouts, matched that of human negotiators. It achieved better deals about as often as worse deals, demonstrating that FAIR’s bots not only can speak English but also think intelligently about what to say..Supervised learning aims to imitate the actions of human users, but it does not explicitly attempt to achieve an agent’s goals. Taking a different approach, the FAIR team explored pre-training with supervised learning, and then fine-tuned the model against the evaluation metric using reinforcement learning. In effect, they used supervised learning to learn how to map between language and meaning, but used reinforcement learning to help determine which utterance to say..During reinforcement learning, the agent attempts to improve its parameters from conversations with another agent. While the other agent could be a human, FAIR used a fixed supervised model that was trained to imitate humans. The second model is fixed, because the researchers found that updating the parameters of both agents led to divergence from human language as the agents developed their own language for negotiating. At the end of every dialog, the agent is given a reward based on the deal it agreed on. This reward was then back-propagated through every word that the agent output, using policy gradients, to increase the probability of actions that lead to high rewards..This work represents an important step for the research community and bot developers toward creating chatbots that can reason, converse, and negotiate, all key steps in building a personalized digital assistant. Working with the community gives us an opportunity to share our work and the challenges we’re aiming to solve, and encourages talented people to contribute their ideas and efforts to move the field forward..\r \tMeta believes in building community through open source technology. Explore our latest projects in Artificial Intelligence, Data Infrastructure, Development Tools, Front End, Languages, Platforms, Security, Virtual Reality, and more.\r \n"}
{"prompt":"An algorithm used to compute scores for student testing was used to determine which teachers to evaluate, fire and give bonus between 2011-2015. It is no longer used by the district\n\n###\n\n","completion":" A federal judge ruled that the Houston ISDs use of a secret algorithm to evaluate teacher performance denied employees the right to challenge their terminations - giving teachers the green light to continue their lawsuit against the nations seventh largest school district..The algorithm used students standardized testing data to compute a score that, among other factors, was used by district officials to determine which teachers were evaluated, fired and given bonuses between 2011 and 2015. It is no longer used by the district..Louis Malfaro, president of the Texas American Federation of Teachers, said the judges ruling means the case will likely be scheduled for a trial in coming months..Were going to proceed with this lawsuit, and based on what the judge wrote, were winning the argument, Malfaro said. At the end of the day, educators care about education and want to see how theyre doing and how to get better. Its time to take teacher evaluations back to what it should be - not another misuse of standardized testing data but a real opportunity for people to not only look at their own work and evaluate it, but also to have the opportunity to improve their practice..But U.S. Magistrate Judge Stephen Wm. Smith stopped short of supporting all of the teachers arguments, including that the standardized-testing-based algorithms failed to advance the districts goal of employing effective teachers and that the system did not show teachers how they could improve their scores..The algorithm HISD used was part of the Educational Value Added Assessment System, or EVAAS, that was created by the private technology firm SAS..Because not even HISD administrators knew how the algorithm worked, teachers argued the district could not provide enough detail about terminations, leaving the teachers unable to defend against possible errors in the calculations..A district website stated no teachers score could be recalculated because that would force HISD to redo the analysis of every teacher, which would be very costly for the district. A change to one teachers report could force changes to all the others, it noted..The remarkable thing about this passage is not simply that cost considerations trump accuracy in teacher evaluations, troubling as that might be, Smith wrote. Of greater concern is the house-of-cards fragility of the EVAAS system, where the wrong score of a single teacher could alter the scores of every other teacher in the district..While teachers and unions further argued that EVAAS also was biased against teachers who educate economically disadvantaged students and English-language learners who typically struggle on standardized tests, the judge ruled that value-added models are a legitimate tool to measure teacher performance..One of the teachers experts admitted that there is evidence that a teachers EVAAS score is correlated with that teachers impact on student learning growth as measured by standardized test scores..Union officials painted the ruling as a watershed moment in the fight against attaching high stakes to standardized tests, while district officials said the judgement has little bearing on its ability to use value-added models to evaluate teachers..The language used by the judge makes it pretty clear that the district doesnt have a leg to stand on in defending the defective evaluation system, said Houston Federation of Teachers President Zeph Capo..But district officials said the only change that must be made as a result of Smiths ruling is that teachers must be able to independently verify their performance scores..Should HISD ever again decide to use EVAAS or value-added scores, the Courts ruling leaves intact the Districts ability to do so, the district said in a statement. The Courts ruling only requires that the score be verifiable by the teacher in the limited circumstance of contract dismissal decisions during the term of the contract. End-of-year dismissals for probationary and term contract teachers are not impacted by this ruling..Shelby Webb is an energy tech, renewable energy reporter for the Houston Chronicle. She previously worked as an education reporter for the Chronicle for more than four years, covering trends across greater Houston and Texas. Before moving to Houston, she worked for her hometown paper in Sarasota, Florida, from 2013 to 2016 and graduated from the University of Florida. .Sanger ISD Challenge Program kids are the so-called worst of the worst behaved. And its here that they find a home. A place where they are listened to and respected.\n"}
{"prompt":"To fight fraud, a customer must fill out an application form and undergo a biometric identity check. Provema also uses best data security practices\n\n###\n\n","completion":" iDenfy has been drafted to provide face biometrics for a smooth and frictionless onboarding and verification for customers obtaining loans online or effecting online payments via the platform of Polish technology firm Provema..As per the partnership, Provema customers seeking to obtain loans remotely will be able to do so without going through complicated processes, as what the company says is a thorough, yet quick identity authentication procedure is used to determine that the customer is genuine and meet know your customer (KYC) regulation requirements..This means that for a customer to be given a loan, they fill out an online application form, state how much money they intend to borrow, and then undergo biometric identity verification, after which the money is directed almost immediately into their bank account. The partnership with iDenfy is part of Provema’s effort to make loan issuance and repayment processes easier for their customers, according to the announcement..Provema says apart from using iDenfy’s biometric access control technologies to guarantee customers’ identification data security, the solution is also expected to help Provema fight fraud, as the artificial intelligence-powered solution has the ability to authenticate documents and detect fakes in just a matter of seconds..iDenfy says its solution works for more than 3,000 documents from people spread across 200 countries of the world, making it possible for Provema’s clients to be conveniently served wherever they may find themselves on the globe..The firms believe that their partnership will positively influence the global financial services industry by encouraging financial companies to adopt identity verification technologies that guarantee greater security and rapidity in their transactions with customers..biometric identification  |  biometrics  |  digital identity  |  facial recognition  |  financial services  |  iDenfy  |  identity verification  |  KYC  |  remote authentication\n"}
{"prompt":"This collaboration simplifies digital identity assessment and Trust Mark processes\n\n###\n\n","completion":" Trust Framework Providers SAFE Identity and Kantara Initiative have reached a reciprocal agreement to consolidate digital identity assessments, each endorsing and supporting the other’s public key infrastructure (PKI) and non-PKI domain Trust Frameworks, along with their certified identity providers..The collaboration simplifies digital identity assessment and Trust Mark processes for companies in healthcare, financial services and other sectors to reduce organizational risk..“What we have done is agree to honor each other’s Trust Marks. Industry can now assess and manage digital identity more easily across both PKI and non-PKI domains when making risk-based decisions concerning the validity of someone’s digital identity credentials and providers’ services,” says Kantara’s Wallis. “Kantara always looks for ways to serve more value to its members through non-financial, non-exclusive collaborations such as this, in the furtherance of its non-profit mission. Instead of duplicating efforts, SAFE and Kantara are helping to expand the reach of our respective missions.”.Both SAFE Identity and Kantara certify trust in digital identity management services and solutions. SAFE is focussed on expanding and standardizing PKI-based credential use for identity, confidentiality and data integrity, particularly for the healthcare industry, while Kantara focusses on conformity assessments to legally recognized standards, along with standardization and non-PKI innovation..The U.S. Government’s General Services Administration (GSA) is joining the Kantara Initiative with plans to submit its Login.gov service for Kantara’s assurance and approval program to demonstrate compliance with National Institute of Standards and Technology (NIST) Special Publication (SP) 800-63-3 Digital Identity Guidelines..Login.gov provides simple, secure and private access to participating U.S. government digital services, according to the announcement, with 25 million people signed up to date..The NIST SP 800-63-3 Guidelines set mandatory digital identity guidelines for U.S. Federal agencies conducting identity verification and authentication..“NIST SP 800-63-3 is focused on modernizing the policy in keeping with rising threat levels for identity proofing, verification and authentication whilst also improving privacy in the overall digital user experience,” said Colin Wallis, Executive Director, Kantara Initiative. “Kantara has developed assessment criteria against each of 63-3’s normative requirements to drive consistency in assessments of applicable credential service providers (CSPs) done by Kantara 3rd party accredited assessors. Consistency in assessments drives long term integrity in Kantara’s Trust Framework and Trust Marks internationally, thereby building trust and confidence for all stakeholders in the wider digital economy.”.authentication  |  biometrics  |  certification  |  digital identity  |  identity management  |  identity verification  |  Kantara  |  NIST  |  privacy  |  SAFE Identity\n"}
{"prompt":"A research team at Clarkson analyzed speaker verification involving 30 children aged four to 14 and assessed that  voice verification apps are not appropriate for children unless more research is done. There are many concerns over privacy, consent and appearance of undue commercial profit collected from data from minors.\n\n###\n\n","completion":" Much work needs to be done if biometric systems are to accurately recognize the voices of children, according to Clarkson University research..Although adult voices vary year to year and even hour to hour, their acoustic properties do not change as fundamentally as do the voices of children as they grow from school-aged to adolescence..Researchers spent 2.5 years evaluating biometric speaker verification performance for available software and hardware. In their study, Clarkson researchers analyzed speaker verification over six sessions involving 30 children aged four to 14..The results were noteworthy, but little more. The scientists report that the MFCC 20 verification system and GMM algorithm delivered the best results. That performance, however, “is not with the expected biometric recognition performance.”.The bigger picture is that the published research is thin for a technology that is changing how people, and particularly children, interact with their world..Virtually all voice biometrics research and development has been conducted on adults. The result has been Siri, Alexa and other digital assistants that can identify speakers by their voices..Speech recognition work with children’s voices is very complex, and it also is fraught with concerns over privacy, consent and the appearance of undue commercial profit from data collected from minors..It will be difficult to know which, if any, voice verification apps are appropriate for children unless more research is done, Clarkson Professor and Researcher Stephanie Schuckers pointed out in a December Biometric Update interview..The Clarkson paper notes that voice recognition for children has been based on physiological changes with gender and speech — not speaker — recognition. The ways that children’s voices change join other challenges to recognition, including background and channel noise, low-quality mics, illness and vocal stress..Deep learning speaker recognition systems used in the study achieved “strong speaker recognition performance.” They showed improved recognition compared to classifiers that require “hand-crafted features,” according to the study..But the algorithms had practical drawbacks when considering commercial use. Researchers found that they are more complex and demand massive amounts of labeled data. Their computation and storage costs also are high..biometric data  |  biometrics  |  biometrics research  |  children  |  Clarkson University  |  identity verification  |  privacy  |  speech recognition  |  voice biometrics\n"}
{"prompt":"A new processor enables a more natural experience for speech-based interaction. Scientific research was conducted on sound localization and computer vision to offer new features without creating data storage and privacy issues.\n\n###\n\n","completion":" A new processor in Amazon’s latest generation of Echo devices are giving the Alexa assistant intriguing capabilities that the company say offer consumers a more natural experience of speech-based interaction. There’s also plenty of scientific research that’s gone into sound localization and computer vision to offer new features without creating new biometric data storage and privacy problems—and device edge processing is the key..At the Fall 2020 Devices and Services announcement from Amazon, drones flying around the home and a new online gaming service garnered much of the attention. The company’s debut of new Echo devices, however, was more significant in terms of biometrics-related developments..Inside the globe-shaped Echo and the brand-new Echo Show 10, the AZ1 Neural Edge processor is tasked with running new and updated speech and computer vision algorithms..“In speech processing milliseconds matter,” said Miriam Daniel, vice president of Amazon Echo during the product rollout event. “Imagine asking Alexa to turn on the light, and there’s a slight delay in the light coming on—that would make customers really impatient..“Our team worked really hard to shave off hundreds of milliseconds from Alexa’s response time, [so] they invented the all new AZ1 neural edge processor,” Daniel said. The silicon module has been purpose-built to run machine learning algorithms on the edge, she noted..Rohit Prasad, vice president and head scientist for Alexa, said “The goal with Alexa is to make interacting with it as natural as it is to speak to humans,” and further noted that advancements in AI are bringing Amazon closer to that vision. Among the current capabilities are the use of feedback search algorithms to take user feedback (“Alexa, that’s wrong”) and use interactions to correct the mistake in action. A new ability is to teach the Alexa assistant directly by speech rather than through a mobile app or online portal to set up new functions..On the new Echo Show 10, the display and camera are able to change direction and aim at the current speaker in a room in an effort to make for a more natural interaction during video calls. This is useful when someone is moving about a room while talking or viewing video, but it turns out that it is rather challenging to do this without storing biometric data or personally identifiable information in the form of faces and voices..“We’re not doing [this] with facial recognition; we’re doing that just understanding sort of the form of what a human being looks like and triangulating on that,” explained Dave Limp, senior vice president of devices and services at Amazon. “The cool thing about the technology is it’s all running locally. And so none of this goes to the cloud; it’s all done locally on that neural processor and it never leaves the device,” he added.. (A visualization of the non-reversible process Echo 10 uses to convert images into a higher-level abstraction to support motion. Source: Amazon).The AZ1 processor is used in a novel way to understand the direction the voices are coming from and decide where, when and how fast to adjust the camera. According to a post on the Amazon Science blog, the Echo Show 10 uses sound source localization (SSL) with computer vision (CV) to identify objects and humans in the field of view and figure out which sounds are coming from people, and which are merely sounds reflecting off walls..The chip was designed in collaboration with MediaTek. MediaTek’s MT8512 forms the basis for the processor, having been design for “high-end audio processing and voice assistant applications,” according to MediaTek..The MT8512 integrates a 2GHz dual-core CPU, support for a wide variety of peripheral connectivity dedicated to ultra-high-quality audio processing, as well as Bluetooth 5.0 and Wi-Fi 5 dual-band connectivity. MediaTek notes that a high-performance voice DSP (digital signal processor) is included for fast and accurate wake-word and keyword detection in vocal commands; the DSP works in conjunction with the AZ1 Neural Edge processor “to provide the most responsive Alexa experience,” according to MediaTek..Additionally, the chip is made using a 12 nanometer (nm) process; for comparison, the absolute state-of-the art is 5nm, while many mainstream processors from Intel used in laptop and desktop PCs are made with a 14nm process. Generally speaking, the smaller transistors are, the more of them can be packed into the same “package” space and offer improved energy efficiency. In other words, for use in low cost standalone devices, the MediaTek chips look to provide a good balance between power, efficiency and unit cost..AI chips  |  Amazon  |  biometric data  |  biometrics  |  biometrics at the edge  |  computer vision  |  data storage  |  machine learning  |  privacy  |  speech recognition\n"}
{"prompt":"Advocating for data protection for the use of medical assistants during COVID-19\n\n###\n\n","completion":" Speakers highlighted “tensions between individualised approach based on clinical needs and duties to the wider population to minimise harm and maximise benefits” during COVID-19 era.Highlighting that patient data confidentiality due to the use of medical assistants such as artificial intelligence (AI) can create a conflict of interest situation in healthcare settings, Prof Ashish Chandra, University of Houston-Clear Lake (UHCL), Houston, Texas, said, “We are in the digital era where medical assistants support quality reporting with electronic health records and services. They have been around for decades, but in the era of COVID-19, medical assistants are being used more now than ever before. Hence, it is increasingly important for healthcare organisations to have a regulatory and policy framework for all data services to maintain patients’ fundamental privacy rights. It is also important for organisations to get better health infrastructure for patients and to deliver better care. This will also help maintain the relations between physicians, nurses, healthcare leaderships, management, and patients.”.Prof Chandra, along with Prof William (Bill) Stroube, Professor, Health Services Administration, University of Evansville, US, and Dr P R Sodani, Pro President, IIHMR University, was speaking at the webinar on “The Underappreciated Value of Healthcare Ethics – A Look at Various Cases,” organised by  IIHMR University. Many healthcare systems have the principle of making the care of individual patients the primary concern, and India needs the same, Prof Chandra said. The observation becomes important as India attempts to move to a more digital healthcare system through the National Digital Health Mission (NDHM)..Taking this argument forward, Prof Stroube said, “There are many moral issues that arise out of the provision of health care—from those that are inherent in the relationship between the healthcare professional and the patient to those associated with many ethical dimensions, from the consequential ethics (based on the theory of right and wrong) to deontological ethics (duty-based). During a pandemic, there are tensions between this individualised approach based on clinical needs and duties to the wider population to minimise harm and maximise benefits. To clarify the unappreciated moral issues of healthcare, a multisystem ethical culture and organisational culture may provide a policy framework to make better quality decisions concerning patients.”.Commenting on physicians’ changing role in light of the pandemic, Dr Sodani emphasised, “While the increased use of medical assistants in many patient-care tasks makes sense, the future role of the physician in sectors like primary care needs greater clarification. Strong doctor-patient relationships are essential to improving the quality of care, the patient experience, physician job satisfaction, and overall functioning of the healthcare team.”.Express Healthcare, first published as Express Healthcare Management in January 2000, has grown to become the No.1 Business News Magazine for the healthcare industry. The editorial contents include: news, views, analysis and interviews, under main segments: Market, Strategy, Knowledge, Life, Radiology, Hospital Infra, [email protected] Besides this, we bring out a quarterly publication, called In Imaging for the radiology and imaging segment.\n"}
{"prompt":"Afghanistan's national digital ID, the tazkira, is being updated as part of inclusion programs through the World Bank. This id hopes to replace paper documentation. Today, 47% of new e-tazkira digital identity registrants are women which helps getting documents for women easier. \n\n###\n\n","completion":" The World Bank, intent on pushing efforts in Afghanistan to provide legal identification for women as well as men, is reporting some success with a biometric electronic ID..It is not the typical story coming out of Afghanistan, and the anti-modernity Taliban is the ultimate revenant religious force, but there remains hope that the lot of women there can be improved to some small degree by getting them recognized legally in their own nation..Like anywhere, even failed states, it is impossible to participate in an economy or get government services without reliable identification. This is especially true in rural Afghanistan..Moderating forces from around the world have pushed back on cultural rules that disenfranchise women and girls, and the Taliban no longer governs the whole country. Yet the World Bank points out in a new blog post that Afghan women are still forced into a second-class existence..Afghanistan’s national digital ID, the tazkira, is being updated as part of multiple inclusion programs funded through the World Bank. In the aftermath of the Taliban’s nationwide hold (it still dominates in some regions) tazkira’s for women were legalized, a fact that is not widely known or even accepted in the countryside and mountain villages..And of the most vulnerable population — women who have become refugees in their own country — 80 percent lack tazkiras. Until recently, a woman legally could not get an ID unless she was accompanied to one of the scarce government centers by a male relative. The relative even had to agree to let a photo be taken of her..As part of the e-tazkira program, other requirements were relaxed as well to make getting the document easier for women. The changes have made it possible for women who have never had or who have lost relevant documents in the war to get the electronic ID document..Afghanistan  |  biometric identification  |  biometrics  |  digital identity  |  financial services  |  government services  |  identity document  |  national ID  |  SDG 16.9  |  World Bank\n"}
{"prompt":"Facebook introduced a new policy that prohibits any content that denies or distorts the Holocaust,  this helps stop misinformation that AI algorithms can amplify.\n\n###\n\n","completion":" Facebook boss Mark Zuckerberg wrote that he had struggled with the tension between free speech and banning such posts, but that this is the right balance..I find it deeply offensive. But at the end of the day, I dont believe that our platform should take that down because I think there are things that different people get wrong. I dont think that theyre intentionally getting it wrong..My own thinking has evolved as Ive seen data showing an increase in anti-Semitic violence, as have our wider policies on hate speech, he wrote in a public Facebook post..Drawing the right lines between what is and isnt acceptable speech isnt straightforward, but with the current state of the world, I believe this is the right balance..Earlier this year, Facebook banned hate speech involving harmful stereotypes, including anti-Semitic content. But Holocaust denial had not been banned..Facebooks vice-president of content policy, Monika Bickert, said the company had made the decision alongside the well-documented rise in anti-Semitism globally and the alarming level of ignorance about the Holocaust, especially among young people..Denying the Holocaust, trivializing it, minimizing it, is a tool used to spread hatred and false conspiracies about Jews and other minorities, the group said in a statement.\n"}
{"prompt":"EARTH AI helps mineral explorers find promising areas\n\n###\n\n","completion":" It was just a few years ago when Artificial Intelligence (Ai) was a newly emerged idea that was too technically advanced for people to predict how it could impact the world. Fast forward to 2019 and things have definitely changed. It’s become a real game changer for countless industries including major players in the mining industry..Mining is an industry where improving efficiency and productivity is essential for profitability, as small improvements in yields, speed and efficiency can make an extraordinary impact..The mining industry employs a lot of individuals — just 670,000 Americans are employed in the quarrying, mining and extraction sector. Our current methods are just not effective and can result in a lot of loss which is detrimental since mining can impact nearly every other industry since it provides the raw materials for virtually every other aspect of the economy..The current mineral exploration process is basically massive amounts of data in the form of soil samples, chip samples, geochemistry, drill results, and assay results. Each drill hole is a tiny snapshot of the processes that form the earth..A single drill hole can create 200 megabytes of data and if there are more drill holes with other types of information, an exploration project can produce terabytes of data. If you wanted to compare your one project to hundreds of others to find the best insights, this becomes difficult since the amount of data is crazy..All of these data points are so important though because using this data we can find new mineral deposits, but to sort through them is too much for even an entire team of capable geologists..EARTH AI is helping mineral explorers identify promising areas. They do this by analyzing data from multiple sources and using a machine learning algorithm to identify areas where minerals are likely to be found. 47 layers of remote sensing and geophysical data are analyzed at once by machine learning — they can highlight ore bodies and alteration haloes as well as map out hard rocks and regolith with extreme detail..Artificial intelligence and machine learning can help mining companies find minerals to extract. Some companies are already working on this. Goldspot Discoveries Inc. is a company that aims to make finding gold more of a science than art by using machine learning. Another company, Goldcorp and IBM Watson are collaborating to use artificial intelligence to review all the geological info available to find better drilling locations for gold in Canada..While many of us have been focused on the progress Uber, Google and Tesla have made with autonomous vehicles many people don’t realize that Rio Tinto (one of the worlds largest metals and mining corporations) has already been using autonomous haul trucks that can carry 350 tons and operate totally independently since 2008. These trucks have been so effective that they’ve reduced fuel use by 13 percent and are also much safer to operate..AI can help mining engineers and workers prevent accidents and injuries on the job. If enough high-quality data can be collected, we can predict failures that can not only affect production but also be harmful or fatal to any workers nearby..In addition to predicting failures, we can also use Ai to predict other potential hazards by analyzing patterns in events. This is really useful because the environment in mining can affect the operation and lifespan of equipment, varies greatly depending on the location..But we can also do more than just predict when equipment might fail or which hazards can present themselves using AI. We can also constantly survey the health and performance of equipment which is important in eliminating surprise failures and dangers to workers..In the Escondida copper mine in Chile, the company trialed smart caps which analyzed driver brain waves to measure and act on fatigue. This was integrated into over one-hundred-and-fifty trucks to boost productivity and increase safety..They are also using AI in automating decision making. For example, Mining Area C in Western Australia Iron Ore is using a system that selects which crusher trucks they should use. This cuts down a lot of time spent on making decisions and also increases efficiency since decisions are generally better than humans..Vale is using AI in several areas with Advanced Analytics and is saving big as a result. At Salobo copper mine in Para, Brazil, there was a 30% increase in the lifespan of haul truck tires in one year which saved the company $5 million. This same technique is being applied in other mines and other truck parts including engines and fuel consumption..Vale also uses AI to predict rail fractures which is helping to reduce the occurrence of fractures by up to 85%. This can save Vale $7 million per year. In total, the company expects to save around $26 million in 2018 from these changes alone..Goldcorp recently partnered with IBM to put their smart technology towards exploration. IBM Watson services are being used to analyze drilling reports, geological survey data and more in determining which areas to explore and to quickly locate high-value targets..It is clear that automation and use of Ai and Machine Learning can significantly help save costs, increase efficiency and have tons of other benefits for companies. What’s holding us back is data and having really good, and a large amount of it. However, companies are working on scaling the use of AI in mining and with big data becoming a huge industry we can start seeing more use of AI in the mining industry. One that is completely changing from what we’ve traditionally known it to be..I’m a developer & innovator who enjoys building products and researching ways we can use AI, Blockchain & robotics to solve problems in healthcare and energy!\n"}
{"prompt":"By using AI, people can identify missing people due to drug-related violence\n\n###\n\n","completion":" In March of this year, a massive grave was uncovered buried beneath the soil of the coastal Mexican state of Veracruz. The grave made national headlines because it contained more than 240 skulls and corpses, the remains of disappeared people (link in Spanish)..But for many, the grave’s existence came as no surprise. In Mexico, a country where almost 30,000 people have gone missing due to drug-related violence since 2006, the grave was a reminder of a difficult reality: the search for missing people often begins by looking underground..Mexico is home to over 122 million people and spans more than 750,000 square miles of land. There is no road map that makes clear where to start the search for mass graves, or the bodies of the disappeared (desaparecidos) that they hold..Or at least there hasn’t been—until now. A team of multi-country researchers, data scientists, and statisticians is using machine learning to predict which counties in Mexico are most likely to have hidden graves. If their model works as well as they hope, it will be a powerful application of an emerging technology that provide answers to one of the most difficult aspects of the desaparecidos problem: knowing where to look..The team is composed of three separate groups: the Programa de Derechos Humanos at the Ibero-American University in Mexico City; data-focused non-profit Data Cívica, also based in Mexico City; and the Human Rights Data Analysis Group (HRDAG), a San Francisco-based organization that applies scientific analysis to human rights violations (first two links in Spanish)..Each organization contributes a unique piece of analysis or data which together form a fuller picture of where to search. The group at the Ibero-American University has been scraping local and national Mexican newspaper and radio data for mentions of hidden graves for years as part of a larger project. They’ve created a comprehensive database of the details behind every report of a hidden grave. It’s the country’s first database of the sort, and it’s a crucial bank of knowledge that details in which municipalities hidden graves have been discovered in the past..Data Cívica contributes data on social demographics about every municipio, or county, in the country. By combining Mexico’s public open data system with geographic data, they’ve been able to create a detailed profile of sociodemographic data for every one of Mexico’s 2457 counties..These two pieces are crucial to the machine learning model that HRDAG uses to predict which counties are likely to have hidden graves in them. The model is called a Random Forest classifier, and its usefulness hinges on the idea that there is something categorically different between counties that have historically been found to have hidden graves, and those that have not. The model sorts through the characteristics and weights their relevance. It then becomes possible to predict, based on those characteristics, which counties are most likely to have graves found in them in the future..Patrick Ball, HRDAG’s Director of Research and the statistician behind the code, explained that the Random Forest classifier was able to predict with 100% accuracy which counties that would go on to have mass graves found in them in 2014 by using the model against data from 2013. The model also predicted the counties that did not have mass hidden graves found in them, but that show a high likelihood of the possibility. This prediction aspect of the model is the part that holds the most potential for future research..Ball was quick to add, “Prediction is different than inference. It’s different from explanation.” Which is to say that the while the model can predict which counties are most likely to have similar graves in them in the future, it can’t explain why that is, and it isn’t particularly concerned with which variables make that difference..“The problem with this type of violence is that it’s a very contextual violence,” Mónica Meltis, the Coordinator of Projects at Data Cívica explained in a phone interview. What she means is that it’s impossible to separate the counties that have hidden graves from the socioeconomic forces that define them..Counties with hidden graves are likely to have a lower average income than other counties. They tend to be more rural than urban, and thus have smaller populations. They have higher numbers of indigenous residents than counties without hidden graves, as evidenced by lower county-wide scores on Spanish language tests in primary schools. Many of the counties have been found to have strong connections to drugs (in the form of opium or methamphetamine labs) and high homicide rates..Their geographies are also significant: the counties with found graves tend to have highways, and thus be easily accessible by road. But they also evidence a pattern of being close to borders like the United States, Mexico’s northern neighbor, or Guatemala, the southern neighbor. The sea also counts as a border: the team at Data Cívica reports that three out of every ten disappearances happen in the states of Tamaulipas or Guerrero, both of which are coastal..All of these characteristics make sense, in the matter-of-fact way that the best discoveries always do after the fact. But Meltis, Ball, and Denise González Núñez, the coordinator of University of Ibero’s Human Rights Program, all warn against drawing any hard conclusions. For one, the data that they’ve run the model on isn’t complete. Though the team has been able to use data from 2013 to predict accurate results for 2014, they haven’t yet been able to do the same for 2017.  Before this can be done, Núñez’s team has to update their database with media mentions from 2016, a task that is forthcoming and time-consuming..The project is also limited in the respect that it doesn’t answer any of the thorny questions around who is committing the crimes. The team can neither determine who is responsible for killing the victims nor guess as to the identities of the people within the mass graves. Their work provides only a compass of sorts, the barest map for those who must then do the difficult work of excavating those buried..And of course, there’s the final fact that Ball in particular is careful to point out: “What we’re predicting is the probability of observation of a grave. We can only predict the counties that are likely to have graves that are like the ones we’ve observed in the past.”.His point is one that’s characteristic of all machine learning models. Models are deeply dependent on the data available. In the case of Mexico’s hidden graves, it’s highly possible that there are mass graves that have been so well hidden that no one has found them. These phantom graves, the unobserved unknowns, haven’t made their way into any datasets. And because they’re not in the data, it’s impossible to train a model on them. The work the group is doing cannot point to every hidden grave in the country. Rather, it can only help locate graves that are similar to those that have been found before..Even so, the project represents a powerful beginning. For Núñez’s team, it’s part of a multi-year effort that will result in the May 2017 release of a comprehensive report about hidden graves in Mexico. Ultimately, the work is about more than reports, data, and models. As Metis puts it, “I don’t think that this is a project that we want to be sitting in our desks writing about. We want to go outside and find the people.”\n"}
{"prompt":"The use of drones and AI-powered facial recognition cost civil liberties, privacy, and have a margin of error\n\n###\n\n","completion":" Palmer Luckey, the virtual reality pioneer, left Facebook in 2017, six months after it was discovered that he had secretly funded a pro-Trump campaign group dedicated to influencing the US election through “shitposting” and “meme magic”..The 25-year-old Oculus founder now has a new venture, Anduril Industries, this time supporting Trump’s immigration policies directly through the creation of a surveillance system designed to detect unauthorised crossings of the Mexican border..Anduril Industries is one of a growing number of companies playing on the fear of “bad hombres” to cash in on government contracts for hi-tech virtual alternatives to physical wall. From drones and sensors to AI-powered facial recognition and human presence detection, these surveillance systems promise cheaper border control but at what cost to civil liberties?.“These systems are reflective of advances in sensor and analytics technologies that are going to have serious repercussions for Americans’ privacy,” said Jay Stanley, senior policy analyst with the ACLU. “The combination could turn us into a surveillance society where our every move is tracked.”.According to an in-depth report by Wired, Anduril’s eventual plan is to offer the military some kind of “Call of Duty goggles” that tell you “where the good guys are, where the bad guys are”. .However, with no background as a defence contractor, the startup needed a “quick win”; providing AI-powered surveillance technology to the border patrol was a way to get a foot in the door of government procurement..The company, which is backed by Peter Thiel’s venture capital firm Founders Fund, has developed towers that feature a laser-enhanced camera, radar and a communications system. These scan a two-mile radius around them to detect motion. The images are analysed using artificial intelligence to pick out humans from wildlife and other moving objects. During a 10-week test in Texas, the technology – called Lattice – helped agents from US Customs and Border Protection (CPB) catch 55 unauthorised border crossers and seize 445kg of marijuana..Anduril isn’t the only company touting a virtual border wall. The Israeli defence contractor Elbit Systems designed and built dozens of towers in Arizona to spot people as far as 7.5 miles away. The company won the contract off the back of its previous work building a “smart fence” – using sensors, cameras and drones – separating Jerusalem from the West Bank..At the US-Mexico border, Anduril and Elbit Systems have learned from the mistakes made by the failed billion-dollar SBInet, a 53-mile-long virtual wall built by Boeing from 2006 but abandoned in 2011 for being too expensive and ineffective..“While it has generated some advances in technology that have improved border patrol agents’ ability to detect, identify, deter and respond to threats along the border, SBInet does not and cannot provide a single technological solution to border security,” said the Department of Homeland security’s assessment of SBInet..“There is a tendency to look at everything as data and think that if we can just track the blips we can close our borders,” he said. “That was proven to be highly naive with SBInet..“Our borders are thousands of miles long and the world is very messy and complicated. People will be trying to actively subvert these systems so there are no simple solutions,” he added. .The relative cost effectiveness of the surveillance technology made by Anduril and Elbit – thanks to advancements in sensor tech and AI – combined with the fact that CPB considers the border to be a 100-mile-wide zone, will probably mean even more privacy intrusions for border communities..“It’s one thing having sensors on the actual border, but when it starts creeping into American communities there is no justification,” Stanley said. “These systems should never be storing information on the comings and goings of residents of American communities when there is no reason to be suspicious.”.In addition to “virtual walls”, the US government is deploying a facial recognition system to record images of people inside vehicles entering and leaving the country. Secretive tests of the system carried out in Arizona and Texas saw authorities collect a “massive amount of data” including images captured “as people were leaving work, picking up children from school, and carrying out other daily routines”, according to government records..The images captured by the Vehicle Face System will be compared with those stored in government databases, including passports, visas and other border patrol documents in order to identify unauthorised individuals..“This is an example of the growing trend of authoritarian use of technology to track and stalk immigrant communities,” Malkia Cyril, the executive director of the Center for Media Justice, told the Guardian last week..“Companies are finding they have to pay attention to the ethical concerns of their employees or risk having trouble recruiting,” Stanley said, noting the recent exodus of staff at Google over its work with the US military on drone surveillance.\n"}
{"prompt":"This bot leads to the gamification of harassment and is targeted at young girls. \n\n###\n\n","completion":" Update 10\/28: Since the publication of this article, the deepfake bot on Telegram has been blocked on iOS for violating App Store guidelines, according to the researchers. The main Telegram channel that hosted the bot and an affiliated channel for sharing its creations has also been removed..In June of 2019, Vice uncovered the existence of a disturbing app that used AI to “undress” women. Called DeepNude, it allowed users to upload a photo of a clothed woman for $50 and get back a photo of her seemingly naked. In actuality, the software was using generative adversarial networks, the algorithm behind deepfakes, to swap the women’s clothes for highly realistic nude bodies. The more scantily clad the victim, the better. It didn’t work on men..Within 24 hours, the Vice article had inspired such a backlash that the creators of the app quickly took it down. The DeepNude Twitter account announced that no other versions would be released, and no one else would get access to the technology..But a new investigation from Sensity AI (previously Deeptrace Labs), a cybersecurity company focused on detecting the abuse of manipulated media, has now found very similar technology being used by a publicly available bot on the messaging app Telegram. This time it has an even simpler user interface: anyone can send the bot a photo through the Telegram mobile or web app and receive a nude back within minutes. The service is also completely free, though users can pay a base of 100 rubles (approximately $1.50) for perks such as removing the watermark on the “stripped” photos or skipping the processing queue. .As of July 2020, the bot had already been used to target and “strip” at least 100,000 women, the majority of whom likely had no idea. “Usually it’s young girls,” says Giorgio Patrini, the CEO and chief scientist of Sensity, who coauthored the report. “Unfortunately, sometimes it’s also quite obvious that some of these people are underage.”.The deepfake bot, launched on July 11, 2019, is connected to seven Telegram channels with a combined total of over 100,000 members. (This number doesn’t account for duplicate membership across channels, but the main group has more than 45,000 unique members alone.).The central channel is dedicated to hosting the bot itself, while the others are used for functions like technical support and image sharing. The image-sharing channels include interfaces that people can use to post and judge their nude creations. The more a photo gets liked, the more its creator is rewarded with tokens to access the bot’s premium features. “The creator will receive an incentive as if he’s playing a game,” Patrini says..The community, which is easily discoverable via search and social media, has steadily grown in membership over the last year. A poll of 7,200 users showed that roughly 70% of them are from Russia or other Russian-speaking countries. The victims, however, seem to come from a broader range of countries, including Argentina, Italy, Russia, and the US. The majority of them are private individuals whom the bot’s users say they know in real life or whom they found on Instagram. The researchers were able to identify only a small handful of the women and tried to contact them to understand their experiences. None of the women responded, Patrini says..The researchers also reached out to Telegram and to relevant law enforcement agencies, including the FBI. Telegram did not respond to either their note or MIT Technology Review’s follow-up request for comment. Patrini says they also haven’t seen “any tangible effect on these communities” since contacting the authorities..Abusers have been using pornographic imagery to harass women for some time. In 2019, a study from the American Psychological Association found that one in 12 women end up being victims of revenge porn at some point in their life. A study from the Australian government, looking at Australia, the UK, and New Zealand, found that ratio to be as high as one in three. Deepfake revenge porn adds a whole new dimension to the harassment, because the victims don’t realize such images exist..There are also many cases in which deepfakes have been used to target celebrities and other high-profile individuals. The technology first grew popular in the deep recesses of the internet as a way to face-swap celebrities into porn videos, and it’s been used as part of harassment campaigns to silence female journalists. Patrini says he’s spoken with influencers and YouTubers, as well, who’ve had deepfaked pornographic images of them sent directly to their sponsors, costing them immense emotional and financial strain..Patrini suspects these targeted attacks could get a whole lot worse. He and his fellow researchers have already seen the technology advance and spread. For example, they discovered yet another ecosystem of over 380 pages dedicated to the creation and sharing of explicit deepfakes on the Russian social-media platform VK. (After the publication of this article, a spokesperson from VK sent MIT Technology Review a statement: VK doesn’t tolerate such content or links on the platform and blocks communities that distribute them. We will run an additional check and block inappropriate content and communities.) The researchers also found that the “undressing” algorithm is starting to be applied to videos, such as footage of bikini models walking down a runway. Right now, the algorithm must be applied frame by frame—“it’s very rudimentary at the moment,” Patrini says. “But I’m sure people will perfect it and also put up a license service for that.”.Unfortunately, there are still few ways to stop this kind of activity—but awareness of the issues is growing. Companies like Facebook and Google, and researchers who produce tools for deepfake creation, have begun to more seriously invest in countermeasures like automated deepfake detection. Last year, the US Congress also introduced a new bill that would create a mechanism for victims to seek legal recourse for reputational damage..In the meantime, Patrini says, Sensity will continue to track and report these types of malicious deepfakes, and seek to understand more about the motivations of those who create them and the impacts on victims’ lives. Indeed, the data we share in this report is only the tip of the iceberg, he says..An investigation by MIT Technology Review reveals a sprawling, technologically sophisticated system in Minnesota designed for closely monitoring protesters..As firms have dumped their AI technologies into the country, it’s created a blueprint for how to surveil citizens and serves as a warning to the world..Intrepid Response is a little-known but powerful app that lets police quickly upload and share information across agencies. But what happens to the information it collects?\n"}
{"prompt":"This program accurately spots more than 80 percent of fakes by recognizing minute mismatches between the sounds people make and the shapes of their mouths.\n\n###\n\n","completion":" One year ago, Maneesh Agrawala of Stanford helped develop a lip-sync technology that allowed video editors to almost undetectably modify speakers’ words. The tool could seamlessly insert words that a person never said, even mid-sentence, or eliminate words she had said. To the naked eye, and even to many computer-based systems, nothing would look amiss. .The tool made it much easier to fix glitches without re-shooting entire scenes, as well as to tailor TV shows or movies for different audiences in different places..But the technology also created worrisome new opportunities for hard-to-spot deep-fake videos that are created for the express purpose of distorting the truth.  A recent Republican video, for example, used a cruder technique to doctor an interview with Vice President Joe Biden. .This summer, Agrawala and colleagues at Stanford and UC Berkeley unveiled an AI-based approach to detect the lip-sync technology. The new program accurately spots more than 80 percent of fakes by recognizing minute mismatches between the sounds people make and the shapes of their mouths..But Agrawala, the director of Stanford’s Brown Institute for Media Innovation and the Forest Baskett Professor of Computer Science, who is also affiliated with the Stanford Institute of Human-Centered Artificial Intelligence, warns that there is no long-term technical solution to deep fakes. .The real task, he says, is to increase media literacy to hold people more accountable if they deliberately produce and spread misinformation. .“As the technology to manipulate video gets better and better, the capability of technology to detect manipulation will get worse and worse,” he says. “We need to focus on non-technical ways to identify and reduce disinformation and misinformation.” .The manipulated video of Biden, for example, was exposed not by the technology but rather because the person who had interviewed the vice president recognized that his own question had been changed..There are legitimate reasons for manipulating video. Anyone producing a fictional TV show, a movie or a commercial, for example, can save time and money by using digital tools to clean up mistakes or tweak scripts..The problem comes when those tools are intentionally used to spread false information. And many of the techniques are invisible to ordinary viewers..Many deep-fake videos rely on face-swapping, literally super-imposing one person’s face over the video of someone else. But while face-swapping tools can be convincing, they are relatively crude and usually leave digital or visual artifacts that a computer can detect..Lip-sync technologies, on the other hand, are more subtle and thus harder to spot. They manipulate a much smaller part of the image, and then synthesize lip movements that closely match the way a person’s mouth really would have moved if he or she had said particular words. With enough samples of a person’s image and voice, says Agrawala, a deep-fake producer can get a person to “say” anything. .Worried about unethical uses of such technology, Agrawala teamed up on a detection tool with Ohad Fried, a postdoctoral fellow at Stanford; Hany Farid, a professor at UC Berkeley’s School of Information; and Shruti Agarwal, a doctoral student at Berkeley. .The basic idea is to look for inconsistencies between “visemes,” or mouth formations, and “phonemes,” the phonetic sounds. Specifically, the researchers looked at the person’s mouth when making the sounds of a “B,” “M,” or “P,” because it’s almost impossible to make those sounds without firmly closing the lips. .The researchers first experimented with a purely manual technique, in which human observers studied frames of video. That worked well but was both labor-intensive and time-consuming in practice..The researchers then tested an AI-based neural network, which would be much faster, to make the same analysis after training it on videos of former President Barack Obama. The neural network spotted well over 90 percent of lip-syncs involving Obama himself, though the accuracy dropped to about 81 percent in spotting them for other speakers. .The researchers say their approach is merely part of a “cat-and-mouse” game. As deep-fake techniques improve, they will leave even fewer clues behind..In the long run, Agrawala says, the real challenge is less about fighting deep-fake videos than about fighting disinformation. Indeed, he notes, most disinformation comes from distorting the meaning of things people actually have said. .“Detecting whether a video has been manipulated is different from detecting whether the video contains misinformation or disinformation, and the latter is much, much harder,” says Agrawala. .“To reduce disinformation, we need to increase media literacy and develop systems of accountability,” he says. “That could mean laws against deliberately producing disinformation and consequences for breaking them, as well as mechanisms to repair the harms caused as a result.”\n"}
{"prompt":"Develops a value-driven AI system that can evaluate historical housing development and help policymakers shape equitable, inclusive and sustainable plans and regulation\n\n###\n\n","completion":" Lack of affordable housing is a major problem in US cities from the Bay Area to Boston. Austin is no exception: in 2015 the Austin-Round Rock metropolitan area was named one of the most economically segregated areas in the country. With in-migration continuing (roughly 3000 people per month), the disparity will worsen..This research project will develop a value-driven AI system that can evaluate historical housing development and help policymakers shape equitable, inclusive and sustainable plans and regulations. Using deep learning technology and an open data repository with demographic, development, transportation, and energy consumption data, this system will link and study 50 years of Austin housing development data – and tackle the history and future of local housing. More specifically, we ask:.We propose to develop the project in four iterative phases. First, we will design and develop a web-based housing development data repository. Second, we will run historical data analysis based on the data repository. Third, we will build a predictive AI system trained and tested on the data repository. Finally, we will conduct broad government and community outreach for system testing and results dissemination..Our research aims to evaluate and inform urban residential development policies in Austin. The techniques and findings developed from this study can also be applied to many similar cities nationally and internationally.\n"}
{"prompt":"This research project looks at various implications of public-sector deployed cameras.\n\n###\n\n","completion":" This project investigates comparative policies around the creation and use of video data in the public sector context.  As more cities deploy monitoring and sensing technologies, cameras are in the front lines of data-gathering and new management interests in domains such as traffic, policing, and health and safety. While efficiency concerns drive many of these innovations, there are no commonly accepted standards for using the data these technologies provide, leading to concerns about City\/State monitoring, especially as AI\/analytics applied to camera outputs become more pervasive. Camera improvements, AI, and machine language processing may mean better capabilities to achieve citizen safety, transit benefits, and so forth, but they also raise thorny issues of intellectual property, privacy and civil liability, among others..The research examines the practical, theoretical and policy implications of public sector-deployed cameras, especially video cameras, in the context of ethical decision-making.  The core components of the project include an examination of municipal policies in the U.S. around the use of cameras, particularly video cameras.  Traffic cameras in particular will be a focal point since the City of Austin is especially interested in traffic camera policies.  More broadly, it will gather existing policies and conduct a stakeholder survey in order to gauge how cities, especially those considering themselves ‘smart cities,’ have designed policies for handling cameras and the data they produce..In conjunction with this review and survey, the project will host monthly seminars around “Smart Cities and AI” in order to share the work of a wide variety of experts at UT and elsewhere..Our research is relevant to the study of pandemics spread and mitigation. The use of cameras for traffic, security, and geolocation services  in public areas , for example in transport centers, intersections, and public parks, or during public gatherings can be effectively used to track\/understand the spread of contamination Such studies need to be conducted within the ethical frameworks that our team is investigating. We are in a perfect position to highlight the disconnects between policies and practices in government and industry using these devices in urban settings.\n"}
{"prompt":"This research project discusses how did- and misinformation operate in social media environments and how platform regulations or AI might be tailored to counter these problems\n\n###\n\n","completion":" As mis- and disinformation efforts continue to their presence in social media forums, it is critical to focus on how the older audience (over 55 years of age) engages and reacts to social media content.  This constituency has been identified in research as especially vulnerable to believing and circulating disinformation.  Now that we understand something about the structure of disinformation content, its appeals, and how platform affordances aid its circulation among this constituency, we need to enable this aging population to use social media responsibly. This Good Systems project will do this in three ways..First, our research will investigate what message qualities prompt such acceptance among older adults, and more broadly investigate some of the digital literacy barriers they face.  Second, growing out of a targeted investigation with older adults, we will develop training and workshop approaches that assist people in evaluating social media messages. The current coronavirus crisis underscores some of the similarities that political and health messages share when circulated in social media fora. Poor choices regarding sharing and amplifying untrue messages occur easily in digital arenas that encourage rapid responses, and training may curb such rapid, poor responses.  Understanding the roots of the problem and developing guidance and training modules that can be broadly shared is our second goal.  Third, several countries have taken steps to curb disinformation. We will use a conference to highlight the most productive approaches for redressing misinformation among older adults around the world and share policy recommendations within the US security and policy structures. A White Paper will be created for this purpose, with rapporteurs at the conference drafting the document..These approaches enable us to expand knowledge about how dis- and misinformation operate in social media environments, how platform regulations or AI might be tailored to counter these problems, and how older adults might be trained to improve their social media encounters..This research is pertinent to Covid research in that it explores how people, particularly older adults, respond to dis- and misinformation.  With the proliferation of Covid-related misinformation as well as genuine confusion around what to believe, understanding what people respond to and how authoritative information might be better positioned are very pertinent.  Since this user group is also the most vulnerable to severe cases of the disease, facilitating the delivery and belief in appropriate and helpful information is highly relevant.\n"}
{"prompt":"This research project helps identify bias in datasets and fairness of alternative algorithms and evaluation metrics\n\n###\n\n","completion":" AI systems may not only reproduce data bias but even amplify it. Unfortunately, even defining data bias is difficult, let alone detecting and mitigating it. For example, consider bias by omission: transgender people, refugees or stateless people, or formerly incarcerated individuals may be simply overlooked in data. Bias can create harmful systems that commit “data violence,” negatively effecting people’s everyday lives as they interact with institutions, from social service systems and security checkpoints to employment background-check systems. Data and algorithm bias can hurt people downstream in ways that are difficult to anticipate. Three key conclusions have emerged from recent research. First, it is very difficult to remove bias from data alone because it can creep in through various insidious ways. Second, determining the best algorithmic criterion (loss function or evaluation score) is very challenging. Finally, improvement in one criterion may increase biased in another, and neither may properly capture human evaluations of fairness. We will engage people to help identify bias in datasets and fairness of alternative algorithms and evaluation metrics. Human-centered approaches to assess bias and fairness can address a critical gap to inform research on algorithmic fairness..Prof. Ghosh has devised a set of algorithms that can modify the training of neural networks, including deep networks so that they are simultaneously less biased as well as more robust to adversarial attacks. Since neural networks are now being widely deployed, the potential impact of this work toward the design of more fair and reliable AI systems is substantial..Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri, José MF Moura, and Peter Eckersley. “Explainable machine learning in deployment.” In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pp. 648-657. 2020..Shubham Sharma, Jette Henderson, and Joydeep Ghosh. “CERTIFAI: A Common Framework to Provide Explanations and Analyse the Fairness and Robustness of Black-box Models.” In Proceedings of the AAAI\/ACM Conference on AI, Ethics, and Society, pp. 166-172. 2020..Min Kyung Lee, Anuraag Jain, Hea Jin Cha, Shashank Ojha, and Daniel Kusbit. “Procedural justice in algorithmic fairness: Leveraging transparency and outcome control for fair algorithmic mediation.” Proceedings of the ACM on Human-Computer Interaction 3, no. CSCW (2019): 1-26..Prof. Ghosh presented a Webinar on  “Detecting Racial Bias in Healthcare using Trusted AI” (Aug 2020). He also has a podcast on the Pulse of AI Channel: Trustworthiness in AI: Featuring Joydeep Ghosh and Risto Miikkulainen   (July 2020).In this webinar, we show how different aspects of our Trusted AI Approach can help to detect and such biases and help mitigate the associated risks, leading to solutions that are more socially responsible while retaining business value.\n"}
{"prompt":"An analysis of Facebook ads released by US Congress in 2018, message circulation, and sampled twitter data linked to Russian accounts\n\n###\n\n","completion":" Our group described and evaluated aspects of social media systems involved in disinformation campaigns, specifically those involving the Russian IRA. It provides a deep understanding of the historical, contextual and international processes of disinformation that move through AI and platforms, particularly Facebook and Twitter.  Our approach defines “good systems” as not only technological but also social, organizational and political. This notion of the “system” invokes an information environment with many moving parts. Artificial intelligence and machine-driven content creation and circulation are significant components of contemporary disinformation efforts that target certain constituencies with certain types of messages; user literacy is another important part of the equation, including the ability to recognize false information and understand how information moves across and within platforms..Park, S., Strover, S., Choi, J. and Schnell, M. (2020, May). Mind Games:  A temporal sentiment analysis of the political messages of the IRA on Facebook and Twitter.  Presented at the International Communication Association, Gold Coast, Australia (virtual)..Alvarez, G., Graham, S., Choi, J., Strover, S. (2019, May).  Good news, bad news:  A Sentiment analysis of the Russian Facebook Ads. Presented at the International Communication Association, Washington, D.C..Strover, S. and Alvarez, G. (2019, April 25).  Facebook and the Russian IRA Campaign.  Invited speaker, Information Wars:  Social Media and Politics in Russia & Eastern Europe Conference Conference.  University of Texas at Austin, Austin, TX..Reverse Engineering Political Protest, to be submitted to the International Communication Association in November, 2020. Riedl, M., Strover, S., Cao, T., Limov, B., Choi, J., and Schnell, M..Investigating the Images of the Russian IRA Campaign, to be submitted to the International Communication Association in November, 2020.  Choi, J. and Strover, S.\n"}
{"prompt":"This research project analyzes how social and economic disparities and cultural differences shape children's engagement with AI\n\n###\n\n","completion":" As AI-driven devices and toys play an increasingly important role in children’s lives, little research has considered how social and economic disparities and cultural differences shape children’s engagement with AI. Good systems must be responsive to diverse communities, particularly for children from underrepresented populations in tech (e.g., African-American, Latinx). The purpose of the study is to examine how children from underrepresented populations understand, interact with, and evaluate AI-driven digital assistants. Through interviews and experimental work, we will identity factors related to diversity, ethics, and child development for creating age-appropriate AI technologies for children in early to late adolescence (8-to 12-years of age)..We conducted ten in-depth and semi-structured interviews, including five African-American and\/or Latinx child\/parent dyads. After obtaining informed assent and consent, respectively, we first administered a demographic questionnaire about the child’s use of digital assistants, then separately interviewed the parent and the child. Interview questions included their general use of, reflections on, and understanding of digital assistants. Interview sessions lasted about an hour and were audio-recorded and transcribed for analysis. Interview transcripts were imported to MAXQDA 2018 for thematic analysis (Braun & Clarke, 2006). We then developed codes and then clustered the codes into themes. Next steps will be collecting data observing how children interact with these devices..As digital assistants like Siri and Alexa become more common in our lives, people increasingly see them as companions that accompany them throughout their day. Young children, especially, are more apt to see these devices as real people or friends.\n"}
{"prompt":"This research analyzes to see how the City of Austin can leverage vehicles with computer vision and robotics to automate infrastructure inspection\n\n###\n\n","completion":" One of the main tasks of any city government is to keep infrastructure such as roads, signs, accessibility ramps, and sewers in perpetual working order. The first step of maintenance is timely identification of the need for maintenance. However, many infrastructure defects are left undetected and unattended for long periods of time..The City of Austin possesses several fleets of vehicles, which work regularly around the city, and therefore form a seamless network covering many of those inspection targets..We propose to investigate how to leverage those vehicles, combined with state-of-the-art computer vision, robotics, and data science techniques, to automate infrastructure inspection in such a way that is publicly acceptable, will reduce costs, and will increase effectiveness of city maintenance efforts.\n"}
{"prompt":"This research project looks at creating an optimal EMS routing strategy using real-time information that is adaptable to changing situations and robust to disruptions\n\n###\n\n","completion":" Optimizing ambulance allocation and routing is one of the most efficient ways for the EMS to save more lives at virtually no cost. However, current EMS software were developed under models that assume normal demands. They are unable to adapt to disasters such as the COVID-19 pandemic, where traffic patterns change, case clusters emerge, and hospitals rapidly reach capacity. Decisions which are optimized for normal times can suddenly become very inefficient, leading to significant delay in care. Ideally, one would like to synthesize real-time information on case clusters, hospitals’ capacities and capabilities, waiting times, and traffic situation to coordinate responses between all ambulances..This project aims to create such an optimal EMS routing strategy using real-time information. By design, the proposed system can rapidly adapt to changing situations and is robust to disruptions. It guarantees that ambulances arrive at scenes the fastest and distribute patients optimally among care facilities..The research team has strong ties with technical personnel of the Austin Travis County EMS (ATCEMS) department. This will allow the two PIs to obtain high-quality real data and consult EMS dispatchers for feedback. In particular, the team expects to run real-life field test of the algorithms on ATCEMS within six months of the project initiation..In this project, a cross-college UT team works together with Austin-Travis County EMS to optimize EMS dispatch during extreme events. Specific to the COVID-19 pandemic, the project is expected to have the following outcomes..(1)  New protocols for EMS dispatchers during the pandemic to minimize risks, lower patients’ waiting times and optimize usage of hospital capacity..(2) New research developments on the general problem of vehicle routing during times of extreme demands or supply fluctuations. This contributes to the pressing need of building more resilient supply chains during and after the pandemic.\n"}
{"prompt":"This chatbot application provides meaningful support to mothers struggling with or at risk of PPMADs\n\n###\n\n","completion":" Postpartum mood and anxiety disorders (PPMADs) affect up to one in five women globally1. Left untreated, PPMADs can have major negative effects on maternal and child health and well-being2–4. While effective treatments are available, nearly 60% of mothers with PPMADs symptoms are undiagnosed, and 50% of diagnosed mothers are left untreated5, with poor women and women of color being disproportionally impacted6. Cost, time, stigma, and lack of childcare and information constitute major barriers to treatment 5. As such, simple, cost-effective, and efficacious approaches to providing PPMADs support are urgently needed..This proposal aims to develop a “chatbot” application to provide meaningful support to mothers struggling with or at risk of PPMADs. Research on text-based conversational agents, known as “chatbots,” indicates their potential as an innovative, low-cost, low-barrier intervention7. However, no chatbot platforms exists to target mothers in the postpartum period specifically..Thus, we propose to develop a chatbot platform to support caregivers in this critical time of their lives. To identify the most common concerns of caregivers experiencing or at risk of PPMADs, as well as best-practices for support, we will combine insights from user interviews with natural language processing analyses of text messages exchanged between caregivers at risk of PPMADS and post-partum support providers. Additionally, to ensure our chatbot will provide meaningful support and engagement for caregivers at risk of PPMADs, we will engage in user testing and co-design techniques with caregivers at risk and our community collaborators, staff at the non-profit organization Postpartum Support International (PSI)..We will not be directly exploring the impacts on covid per se. However, the need for continued social distancing means that caregivers of young infants are more isolated from family and friends than before and we anticipate that this will lead to additional stress for many caregivers. Additionally, in-person support groups are currently cancelled, limiting options for caregivers currently seeking support. Thus, text-message based support may be even more relevant during the pandemic..The global pandemic is teaching us many things, and that includes the fact that the delivery of vital health services will require new models and modes of care.\n"}
{"prompt":"This research explores the conflict between convenience and privacy in computer vision\n\n###\n\n","completion":" Technology is transforming people’s lives, but it’s a constant struggle to ensure that technology designs address people’s values and preferences, especially those of traditionally underserved groups. Computer vision, as an example, empowers individuals with vision impairments, but it also carries with it privacy concerns; as an example, service providers risk leaking private information when collecting and sharing pictures. Our proposed research will address the conflict between convenience and privacy inherent to computer vision. Toward developing future Visual Assistance Technologies that employ computer vision technologies that support diverse users with visual impairments, especially those who are traditionally technologically underserved, we will discover what users’ value, what concerns they have, and what they prefer so that we can develop privacy recommendations for inclusive computer vision technologies..We have completed an investigation into the values and privacy concerns that individuals who are blind or have low vision (BLV) hold in the context of their use of Visual Assistance Technologies. Our investigation resulted in a publication in the 22nd International ACM SIGACCESS Conference on Computers and Accessibility..In this publication we report on our finding from interviews with N=20 participants who are totally blind (M=9, F=11; ages 22-73). The interviews included open-ended and short answer questions about users’ experiences with Visual Assistance Technologies,  their every-day privacy concerns, and their concern about 22 specific types of image content that may be considered private when shared with the general public, when shared with Visual Assistance Technologies that employ humans, and Visual Assistance Technologies that employ computer vision..We contribute two key novel findings that can be used to improve the design of privacy-aware technology for people who depend on VIDS. First, we identify the types of information BLV people consider to be the greatest concern in terms of privacy.  We convey this information through an easy-to-reference table that identifies each private information type and its priority. Computer vision developers may use the table as a taxonomy from which they can create datasets to train algorithms to better recognize content that contains private information, with a focus on the content types that matter the most..Second, we identify the perceived risks, benefits, and trade-offs people who are BLV encounter when using AI-powered VIDS. For example, we learned that many participants use AI-powered VIDS to access information despite the fact that they either mistrust or are unaware of how these services handle their data.  Underlying the risks, benefits, and trade-offs, we observed five values held by our participants (anonymity, accountability, trust, choice, and independence).\n"}
{"prompt":"This research explores how to use AI to empower individuals experiencing homelessness and assist agencies\n\n###\n\n","completion":" How can the City of Austin and Good Systems, a UT Grand Challenge leverage AI to better serve, inform, and empower individuals experiencing homelessness? This collaboration between UT’s School of Information, LBJ School of Public Affairs, and Good Systems and the City of Austin’s Departments of Public Works, Neighborhood Housing and Community Development, and Communications and Technology Management seeks to confront the wicked problem of homelessness, which people experience as a continuum, ranging from housing instability to reintegration from incarceration, couch-surfing to street homelessness, and episodic to chronic. Our aims are to understand the information and service needs of individuals experiencing homelessness, to understand the information needs of agencies serving individuals experiencing homelessness, and to use AI to empower individuals experiencing homelessness and assist agencies. Our research will incorporate a collaborative feedback loop, whereby the three aims concurrently interact to determine and align the needs and wants of government and nonprofit staff and end users, and develop AI-based interventions. We will use a combination of sociotechnical research methods to understand and serve the values of all stakeholders, particularly individuals experiencing homelessness. In addition to scholarly outputs in the fields of information studies, human-computer interaction, and public policy, our interdisciplinary, cross-organization team directly will influence government policy, leading to improvements in the lives of individuals across and beyond the homelessness continuum. Our innovative research will transform wrap-around services for individuals experiencing homelessness in Austin, using AI to ensure Austin is not only a smart city, but also a good city..Public health messaging during the COVID-19 pandemic emphasizes “shelter-in-place,” “stay-at-home,” and “safer at home.” This messaging assumes that everyone has a safe place to shelter, a home. Unfortunately, many Austinites, as well as many Texans, Americans, and people worldwide, are experiencing homelessness. During this crisis, we must consider our most vulnerable populations, including the homeless — not just due to the threat of infection, but also the resulting food insecurity and societal instability. COVID-19 has brought homelessness to the forefront in new ways. Many people have been unable to work, from the threat of illness, illness itself, or furloughs, and this has put them at risk of becoming homeless with no income to pay for housing. The Emergency Operations Center (EOC), that the City of Austin and Travis County have instituted to respond to the COVID-19 pandemic, must deal with many complex issues. These issues include the healthcare and housing needs of people already experiencing homelessness, and the needs of additional people entering the homelessness continuum, due to unemployment or illness. The EOC has created task forces to address homelessness, lack of income, and food insecurity. Our research can assist with identifying those experiencing homelessness, or at risk of homelessness and providing services for them during this COVID-19 crisis..A partnership between the University of Texas at Austin and the city looks at how AI can identify residents at risk of experiencing homelessness, as well as helping those currently in need find access to services.\n"}
{"prompt":"An AI system that measures the health effects of neighborhood environments in Houston\n\n###\n\n","completion":" An interdisciplinary team of researchers developed an AI system that can measure the health effects of neighborhood environments in Houston, Dallas, San Antonio, and Austin using citizen crowd-sourced data. More information.The project consists of four parts: (1) database development, (2) modeling and analytics, (3) visualization and web development, and (4) community engagement and application. The first two parts are associated with the actual building, training, and testing of machine learning models. The targets are the various health outcomes, namely the prevalence of common non-communicable chronic diseases such as coronary heart disease, cancer, diabetes, poor mental health, obesity, and stroke. The actual health outcomes used in training and testing the models are accessed from the CDC’s 500 Cities Project. The features are created based on three data sources, namely the CDC’s Social Vulnerability Index (SVI) dataset, the EPA’s Smart Location Database (SLD), and the 311 service request datasets accessed from each municipality. Sixty features (i.e., predictor variables) are considered, which characterize the social environment, the physical environment, and the aspects and degrees of neighborhood disorder. A variety of machine learning algorithms are applied and compared, including Ridge Regression, Lasso Regression, Elastic Net, Support Vector Machine, Decision Tree, Random Forest, Extra Trees, and Gradient Boosting. To improve the model performance, the model hyperparameters are fine-tuned using 10-fold cross-validation. Different sets of features are also experimented with..It is shown that the tract-level prevalence for the common non-communicable chronic diseases can be reasonably well predicted based on the publicly available datasets. Furthermore, two major findings have been yielded from this study: (1) the sociodemographic and socioeconomic variables are the strongest predictors for tract-level health outcomes; (2) the historical records of 311 service requests can be a useful complementary data source because the information distilled from the 311 data often helps improve the models’ performance..The datasets and the predictive models are published online. Users can play with the models interactively by using the web tools we developed. The web tools can help the public and city officials evaluate future scenarios and understand how changes in the neighborhood conditions can lead to changes in the health outcomes..Chen, F and Jiao, J. (under review). Predicting and mapping neighborhood-scale health outcomes: A machine learning approach. Computers, Environment and Urban Systems.\n"}
{"prompt":"The system has since led to numerous false arrests (links in Spanish), which the police have no established protocol for handling. One man who was mistakenly identified was detained for six days and about to be transferred to a maximum security prison before he finally cleared up his identity. Another was told he should expect to be repeatedly flagged in the future even though he’d proved he wasn't who the police were looking for.\n\n###\n\n","completion":" In a national database in Argentina, tens of thousands of entries detail the names, birthdays, and national IDs of people suspected of crimes. The database, known as the Consulta Nacional de Rebeldías y Capturas (National Register of Fugitives and Arrests), or CONARC, began in 2009 as a part of an effort to improve law enforcement for serious crimes..But there are several things off about CONARC. For one, it’s a plain-text spreadsheet file without password protection, which can be readily found via Google Search and downloaded by anyone. For another, many of the alleged crimes, like petty theft, are not that serious—while others aren’t specified at all..Most alarming, however, is the age of the youngest alleged offender, identified only as M.G., who is cited for “crimes against persons (malicious)—serious injuries.” M.G. was apparently born on October 17, 2016, which means he’s a week shy of four years old..Now a new investigation from Human Rights Watch has found that not only are children regularly added to CONARC, but the database also powers a live facial recognition system in Buenos Aires deployed by the city government. This makes the system likely the first known instance of its kind being used to hunt down kids suspected of criminal activity..Buenos Aires first began trialing live facial recognition on April 24, 2019. Implemented without any public consultation, the system sparked immediate resistance. In October, a national civil rights organization filed a lawsuit to challenge it. In response, the government drafted a new bill—now going through legislative processes—that would legalize facial recognition in public spaces..The system was designed to link to CONARC from the beginning. While CONARC itself doesn’t contain any photos of its alleged offenders, it’s combined with photo IDs from the national registry. The software uses suspects’ headshots to scan for real-time matches via the city’s subway cameras. Once the system flags a person, it alerts to the police to make an arrest..The system has since led to numerous false arrests (links in Spanish), which the police have no established protocol for handling. One man who was mistakenly identified was detained for six days and about to be transferred to a maximum security prison before he finally cleared up his identity. Another was told he should expect to be repeatedly flagged in the future even though he’d proved he wasn’t who the police were looking for. To help resolve the confusion, they gave him a pass to show to the next officer that might stop him..“There seems to be no mechanism to be able to correct mistakes in either the algorithm or the database,” Han says. “That is a signal to us that here’s a government that has procured a technology that it doesn’t understand very well in terms of all the technical and human rights implications.”.All this is already deeply concerning, but adding children to the equation makes matters that much worse. Though the government has publicly denied (link in Spanish) that CONARC includes minors, Human Rights Watch found at least 166 children listed in various versions of the database between May 2017 and May 2020. Unlike M.G., most of them are identified by full name, which is illegal. Under international human rights law, children accused of a crime must have their privacy protected throughout the proceedings..Also unlike M.G., most were 16 or 17 at time of entry—though, mysteriously, there have been a few one- to three-years-olds. The ages aren’t the only apparent errors in the children’s entries. There are blatant typos, conflicting details, and sometimes multiple national IDs listed for the same individual. Because kids also physically change faster than adults, their photo IDs are more at risk of being outdated..On top of this, facial recognition systems, under even ideal laboratory conditions, are notoriously bad at handling children because they’re trained and tested primarily on adults. The Buenos Aires system is no different. According to official documents (link in Spanish), it was tested only on the adult faces of city government employees before procurement. Prior US government tests of the specific algorithm that it is believed to be using also suggest it performs worse by a factor of six on kids (ages 10 to 16) than adults (ages 24 to 40)..All these factors put kids at a heightened risk for being misidentified and falsely arrested. This could create an unwarranted criminal record, with potentially long-lasting repercussions for their education and employment opportunities. It might also have an impact on their behavior..“The argument that facial recognition produces a chilling effect on the freedom of expression is more amplified for kids,” says Han. “You can just imagine a child [who has been falsely arrested] would be extremely self-censoring or careful about how they behave in public. And its still early to try and figure out the long-term psychological impacts—how it might shape their world view and mindset as well.”.While Buenos Aires is the first city Han has identified using live facial recognition to track kids, she worries that many other examples are hidden from view. In January, London announced that it would integrate live facial recognition into its policing operations. Within days, Moscow said it had rolled out a similar system across the city..Though it’s not yet known whether these systems are actively trying to match children, kids are already being affected. In the 2020 documentary Coded Bias, a boy is falsely detained by the London police after live facial recognition mistakes him for someone else. It’s unclear whether the police were indeed looking for a minor or someone older..Even those who are not detained are losing their right to privacy, says Han: “There’s all the kids who are passing in front of a facial-recognition-enabled camera just to access the subway system.”.It’s often easy to forget in debates about these systems that children need special consideration. But that’s not the only reason for concern, Han adds. “The fact that these kids would be under that kind of invasive surveillance—the full human rights and societal implications of this technology are still unknown.” Put another way: what’s bad for kids is ultimately bad for everyone. .An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.\n"}
{"prompt":"The researchers used detailed satellite imagery from NASA with deep learning. Without deep learning, these trees would have been invisible\n\n###\n\n","completion":" There are far more trees in the West African Sahara Desert than you might expect, according to a study that combined artificial intelligence and detailed satellite imagery. .Researchers counted over 1.8 billion trees and shrubs in the 1.3 million square kilometer (501,933 square miles) area that covers the western-most portion of the Sahara Desert, the Sahel, and what are known as sub-humid zones of West Africa..“We were very surprised to see that quite a few trees actually grow in the Sahara Desert, because up until now, most people thought that virtually none existed,” says Martin Brandt, professor in the geosciences and natural resource management department at the University of Copenhagen and lead author of the study in Nature..“We counted hundreds of millions of trees in the desert alone. Doing so wouldn’t have been possible without this technology. Indeed, I think it marks the beginning of a new scientific era.”.The researchers used detailed satellite imagery from NASA, and deep learning—an advanced artificial intelligence method. Normal satellite imagery is unable to identify individual trees, they remain literally invisible. Moreover, a limited interest in counting trees outside of forested areas led to the prevailing view that this particular region had almost no trees. This is the first time that anyone counted trees across a large dryland region..New knowledge about trees in dryland areas like this is important for several reasons, Brandt says. For example, they represent an unknown factor when it comes to the global carbon budget..“Trees outside of forested areas are usually not included in climate models, and we know very little about their carbon stocks. They are basically a white spot on maps and an unknown component in the global carbon cycle,” he says..It is an open platform designed to engage anyone who wants to offer a contribution for the global public good. The core objective is to link up the best innovators to networks of decision-makers, who can implement the change needed for the next decade. As a global platform, UpLink serves to aggregate and guide ideas and impactful activities, and make connections to scale-up impact..Furthermore, the new study contributes to better understanding of the importance of trees for biodiversity and ecosystems and for the people living in these areas. In particular, enhanced knowledge about trees is also important for developing programs that promote agroforestry, which plays a major environmental and socioeconomic role in arid regions..“Thus, we are also interested in using satellites to determine tree species, as tree types are significant in relation to their value to local populations who use wood resources as part of their livelihoods,” says Rasmus Fensholt, professor in the geosciences and natural resource management department..“Trees and their fruit are consumed by both livestock and humans, and when preserved in the fields, trees have a positive effect on crop yields because they improve the balance of water and nutrients.”.Researchers from the University of Copenhagen’s computer science department developed the deep learning algorithm that made the counting of trees over such a large area possible..The researchers fed the deep learning model thousands of images of various trees to show it what a tree looks like. Then, based on the recognition of tree shapes, the model could automatically identify and map trees over large areas and thousands of images. The model needs only hours what would take thousands of humans several years to achieve..“This technology has enormous potential when it comes to documenting changes on a global scale and ultimately, in contributing towards global climate goals. We are motivated to develop this type of beneficial artificial intelligence,” says professor and coauthor Christian Igel of the computer science department..Researchers will next expand the count to a much larger area in Africa. And in the longer term, they plan to create a global database of all trees growing outside forest areas..World Economic Forum Type may be republished in accordance with the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Public License, and in accordance with our Terms of Use..The Executive Director of Forests and Climate Change\r Wildlife Conservation Society argues for the need for true nature based solutions in tackling climate change. .Since the last ice age, deforestation has rapidly reduced the area of habitable land covered by forests due to urbanization and agricultural expansion.\n"}
{"prompt":"A fake identity, Martin Aspen, was created by artificial intelligence to defame Hunter Biden\n\n###\n\n","completion":" One month before a purported leak of files from Hunter Bidens laptop, a fake intelligence document about him went viral on the right-wing internet, asserting an elaborate conspiracy theory involving former Vice President Joe Bidens son and business in China..The document, a 64-page composition that was later disseminated by close associates of President Donald Trump, appears to be the work of a fake intelligence firm called Typhoon Investigations, according to researchers and public documents..The author of the document, a self-identified Swiss security analyst named Martin Aspen, is a fabricated identity, according to analysis by disinformation researchers, who also concluded that Aspens profile picture was created with an artificial intelligence face generator. The intelligence firm that Aspen lists as his previous employer said that no one by that name had ever worked for the company and that no one by that name lives in Switzerland, according to public records and social media searches..One of the original posters of the document, a blogger and professor named Christopher Balding, took credit for writing parts of it when asked about it and said Aspen does not exist..Despite the documents questionable authorship and anonymous sourcing, its claims that Hunter Biden has a problematic connection to the Communist Party of China have been used by people who oppose the Chinese government, as well as by far-right influencers, to baselessly accuse candidate Joe Biden of being beholden to the Chinese government..The document and its spread have become part of a wider effort to smear Hunter Biden and weaken Joe Bidens presidential campaign, which moved from the fringes of the internet to more mainstream conservative news outlets..An unverified leak of documents — including salacious pictures from what President Donald Trumps personal attorney Rudy Giuliani and a Delaware Apple repair store owner claimed to be Hunter Bidens hard drive — were published in the New York Post on Oct. 14. Associates close to Trump, including Giuliani and former White House chief strategist Steve Bannon, have promised more blockbuster leaks and secrets, which have yet to materialize..The fake intelligence document, however, preceded the leak by months, and it helped lay the groundwork among right-wing media for what would become a failed October surprise: a viral pile-on of conspiracy theories about Hunter Biden..The Typhoon Investigations document was first posted in September to Intelligence Quarterly, an anonymous blog dedicated to collecting important daily news, according to its about section. Historical domain records show the blog was registered to Albert Marko, a self-described political and economic adviser, who also lists the blog on his Twitter bio. When asked about the provenance of the document, Marko said he received it from Balding..Balding, previously an associate professor at Fulbright University Vietnam who studied the Chinese economy and financial markets, posted the document on his blog on Oct. 22, seven weeks after it was initially published..I had really not wanted to do this but roughly 2 months ago I was handed a report about Biden activities in China the press has simply refused to cover. I want to strongly emphasize I did not write the report but I know who did, Balding said in an email..I authored small parts of the report and was involved in report preparation and review. As a researcher, and due to the understandable worry about foreign disinformation, it was paramount that the report document activity from acknowledged and public sources, Balding said. Great care was taken to document, cite, and retain information so that acknowledged facts could be placed in the public domain..Balding said Aspen is an entirely fictional individual created solely for the purpose of releasing this report. Balding did not name the documents main author, saying the primary author of the report, due to personal and professional risks, requires anonymity..Balding claimed that the document was commissioned by Apple Daily, a Hong Kong-based tabloid that is frequently critical of the Chinese government. A spokesperson for Apple Daily confirmed it had worked with Balding on the document..In addition to posting the document to his blog, Balding also promoted it in far-right media, appearing on Bannons podcast and on China Unscripted, a podcast produced by The Epoch Times, a pro-Trump media outlet opposed to the Chinese government..Balding, an American who taught economics at Chinas Peking University HSBC Business School until 2018, is often critical of the Chinese government. He made news this year as a source uncovering a global bulk data collection operation by the Chinese company Shenzhen Zhenhua Data Technology..Blog posts highlighting the most salacious parts of the document, including articles from the Intelligence Quarterly Blog, Revolver News and Baldings blog, received 70,000 public interactions — which includes reactions, comments and shares — across Facebook, Twitter and Reddit, according to the social media analysis tool BuzzSumo..Baldings blog was the primary driver of virality in conservative and conspiracy communities. The report itself was shared across Facebook and Twitter around 5,000 times, according to BuzzSumo, and more than 80 sites linked back to the blog, which was shared more than 25,000 times on Facebook and Twitter. Hyperpartisan and conspiracy sites like ZeroHedge and WorldNetDaily led the pack..After the promise of a big reveal one day earlier, the document was also posted on the extremist forum 8kun by Q, the anonymous account behind the QAnon conspiracy theory movement..On Twitter, the document was pushed by influencers in the QAnon community, as well as by Dinggang Wang, an anti-Chinese government YouTube personality who works for Guo Wengui, a billionaire who fled China amid accusations of bribery and other crimes. Republican Newt Gingrich, the former speaker of the House of Representatives, tweeted the document to his 2.3 million followers..The profile picture for Aspen immediately showed signs of being a computer-generated image that can be created by computers and even some websites. Aspens ears were asymmetrical, for one, but his left eye is what gave away that he did not really exist. Aspens left iris juts out and appears to form a second pupil, a somewhat frequent error with computer-generated faces..The most obvious tell was the irregular shape of the irises, Thomas said. The profile picture looks pretty convincing in the Twitter thumbnail, but when I popped it up into full view I was immediately suspicious..Thomas then consulted with Ben Nimmo, director of investigations at the analytics company Graphika, who noted the other telltale sign of a computer-generated face..One of the things he and his team have figured out is that if you layer a lot of these images over the top of one another, the eyes align, Thomas said. He did that with this image, and the eyes matched up..Other parts of Aspens identity were clearly stolen from disparate parts of the web. Aspens Facebook page was created in August, and it featured only two pictures, both from his new house, which were tracked back to reviews on the travel website Tripadvisor. The logo for Typhoon Investigations was lifted from the Taiwan Fact-Checking Center, a digital literacy nonprofit..Aspen claimed on his LinkedIn profile to have worked for a company called Swiss Security Solutions from 2016 to 2020. Swiss Security Solutions denied having ever employed anyone named Aspen, and it said it had found fake accounts for two other people pretending to have worked for the company..Martin Aspen was never a freelancer or worker of the Swiss Security Solutions. We do not know this person. According to our Due Diligence Software, this person does not exist in Switzerland, Swiss Security Solutions Chairman Bojan Ilic said, adding that the company has reported the profile to LinkedIn..Computer-generated faces have become a staple of large-scale disinformation operations in the run-up to the election. In December, Facebook took down a network of fake accounts using computer-created faces tied to The Epoch Times. Facebook removed over 600 accounts tied to the operation, which pushed pro-Trump messages and even served as moderators of some Facebook groups. Stephen Gregory, publisher of the U.S. editions of The Epoch Times, has denied any connection to the accounts..Last month, Facebook removed another batch of computer-generated profiles originating in China and the Philippines, some of which made anti-Trump posts..Renee DiResta, a researcher at the Stanford Internet Observatory, said computer-created identities are becoming common for disinformation campaigns, in part because they are easy to create..DiResta, who helped examine a ring of AI-generated faces tied to the conservative nonprofit Turning Point USA last month, said computer-generated profile pictures can be used to build an army of fake people to artificially support a cause or to make disinformation operations harder to discover..One of the things that investigators look at to understand the narrative that is spreading is whether the accounts are authentic, whether theyre real, DiResta said. If they were to use a stock photo, it confirms something dishonest is likely happening. By using an AI-generated face, youre guaranteeing you wont find that person elsewhere on the internet..CORRECTION (Oct 30, 2020. 11:19 a.m. ET): An earlier version of this article misstated Christopher Balding’s position at Fulbright University Vietnam. As of Thursday afternoon, the university had listed him as being currently employed, but later put out a statement saying he was a former professor. He is no longer an employee of the university as of Sept. 10, 2020.\n"}
{"prompt":"This kidney algorithm explicitly takes account of race whose Black patients' scores would have qualified them for a kidney transplant waitlist, yet none of them were referred or evaluated for a transplant.\n\n###\n\n","completion":" Black people in the US suffer more from chronic diseases and receive inferior health care relative to white people. Racially skewed math can make the problem worse..Doctors often make life-changing decisions about patient care based on algorithms that interpret test results or weight risks, like whether to perform a particular procedure. Some of those formulas factor in a person’s race, meaning patients’ skin color can affect access to care..A new study of patients in the Boston area is one of the first to document the harm that can cause. It examined the effect on care of a widely used but controversial formula for estimating kidney function that by design assigns Black people healthier scores..The study analyzed health records for 57,000 people with chronic kidney disease from the Mass General Brigham health system that includes Harvard teaching hospitals Massachusetts General and Brigham and Womens. One third of Black patients, more than 700 people, would have been placed into a more severe category of kidney disease if their kidney function had been estimated using the same formula as for white patients..That could have affected decisions such as when to refer someone to a kidney specialist, or refer them for a kidney transplant. In 64 cases, patients’ recalculated scores would have qualified them for a kidney transplant wait list. None had been referred or evaluated for transplant, suggesting that doctors did not question the race-based recommendations..“That was really staggering,” says Mallika Mendu, an assistant professor at Harvard Medical School and kidney specialist at Brigham and Women’s whose work on the study convinced her to stop using the race-based calculation with her own patients. “We know there are already other disparities in access to care and management of the condition. This is not helping.”.In 64 cases, Black patients’ scores would have qualified them for a kidney transplant wait list. None had been referred or evaluated for transplant..The study is the most recent of several signs that math tools exacerbate health inequalities. Last year, software used by many health systems to prioritize access to special care for chronic conditions was found to systematically privilege white patients over Black patients. It didn’t explicitly take account of race, but replicated patterns in access to health care caused by factors like poverty..The kidney algorithm, by contrast, is one of many clinical decision algorithms that explicitly take account of race. A recent review listed more than a dozen such tools, in areas including cancer and lung care. In August, a group of Black retired NFL players sued the league, claiming it used an algorithm that assumes white people have higher cognitive function to decide compensation for brain injuries..The issue is winning more attention, including from federal lawmakers. Representative Richard Neal (D-Massachusetts), chair of the House Ways and Means Committee, says the kidney study underlines the need to reconsider use of race in all medical algorithms. “Many clinical algorithms can result in delayed or inaccurate diagnoses for Black and Latinx patients, leading to lower-quality care and worse health outcomes,” he says..Neal has asked medical societies and the Centers for Medicare & Medicaid Services to investigate the impact on patients of clinical algorithms that use race. Last month, Senator Elizabeth Warren (D-Massachusetts) and others asked the Department of Health and Human Services to investigate race-based medical algorithms..The new study examined a standard calculation called CKD-EPI used to convert a blood test for a person’s level of the waste product creatinine into a measure of kidney function called estimated glomerular filtration rate, or eGFR. Lower scores indicate worse kidney function; the scores are used to categorize the severity of a person’s disease and guide what care they receive. The equation factors in a person’s age and sex. Black patients get their score boosted by an additional 15.9 percent..That design is coming under fire from academics and medical residents who fear it bakes discrimination into kidney care. Researchers who created the formula in 2009 added the “race correction” to smooth out statistical differences between the small number of Black patients and others in their data. But that project and subsequent studies have not explained why the correlation between creatinine and kidney function looked different in Black patients, or the role of factors proven to affect creatinine levels such as diet, says Nwamaka Eneanya, an assistant professor at the University of Pennsylvania who also worked on the new Boston study. A person’s race is a social category, not a physiological one, she says, and it doesn’t make sense to use it to interpret blood tests..Eneanya was already convinced that the standard eGFR formula should be abandoned, but says showing how the race-based adjustment affects care highlights the urgency of the problem. “Any degradation of treatment for these already marginalized groups could have profound results,” Eneanya says..A preliminary version of the newly published findings helped convince leaders at Mass General Brigham to abandon the race-based eGFR formula in June. Several other major US hospitals, including University of Washington and Vanderbilt, have done the same this year. Support is growing for an alternative method of calculating eGFR that uses a different blood test, for the protein cystatin C..Despite those shifts, the campaign to remove race as a factor in kidney assessment and care has a long way to go. Many institutions and doctors are unlikely to move away from the traditional calculation unless medical societies change their guidelines. The two leading US kidney care organizations have formed a task force on the issue. More than 1,300 people have signed a petition urging that group to act..Vanessa Grubbs, a coauthor of the petition and associate professor at UC San Francisco, says adjusting equations is only part of the work needed to undo the harms of using race in medical formulas. After institutions change their eGFR calculations, they should also review Black patients’ care plans, how they train new doctors, and how they think about race, she says..Equations with race baked in encourage doctors to categorize all patients racially, she says, distracting from their true medical needs. “Black people aren’t the only ones affected,” she says. “This is bad for everyone.”\n"}
{"prompt":"CAS, the Crime Anticipation System, perpetuates discriminatory practices by looking at variables such as number of one parent households, number of social benefits recipients, and number of non-Western immigrants\n\n###\n\n","completion":" These are just some of the demographic variables that have been used by CAS (Crime Anticipation System) to predict 125x125 meter crime “hot spots” across the Netherlands. The system, which relies entirely on automated algorithms and data analytics, was piloted in Amsterdam in 2014 and rolled out nationally in 2017. Critics at the time pointed to the system as a potential slippery slope, one which could lead to an increased willingness by law enforcement to embrace algorithmic models that perpetuate discriminatory practices like ethnic profiling..A report released late last month by Amnesty International revealed that Dutch law enforcement have been engaged in a number of predictive-policing pilots and referred to the Netherlands as “one of the countries at the forefront of predictive policing in practice.” The report also calls on law enforcement to halt all predictive policing projects until legislative safeguards are put in place and accuses the pilots of multiple human rights violations, including the right to privacy, the presumption of innocence, and the right to non-discrimination..Amnesty’s report particularly focuses on a predictive policing pilot in Roermond (a municipality in the southeast of the Netherlands) called the Sensing Project, which uses cameras to capture the license plate, brand, model, and year of manufacture of passing cars. According to the report, which primarily relies on documents obtained via WOB (public information request) requests, this data is fed into an algorithm that in real time calculates “hits” on passing vehicles. These hits are then automatically sent to police officers, who can decide whether or not they want to investigate..The project is not only intrusive, the report claims, but discriminatory by design, since its aim is to fight “mobile banditry” (crimes like theft, pickpocketing, and drug trafficking), a term which explicitly excludes people of Dutch nationality and assumes that the offender is either of Eastern European origin or Romani, a minority ethnic group. Considering this, it is unsurprising that some of the variables that generate “extra points” when calculating the risk scores of passing vehicles include whether the car has an Eastern European license plate and the route (re-tracked via automatic licence plate recognition cameras) it is taking, leading to what the report describes as “the automation of ethnic profiling.”.“Article one of the Dutch constitution is the prohibition of the discrimination,” Gerbrig Klos, one of the authors of the Amnesty report, told Motherboard. “If there’s one constitutional article that every person in the Netherlands knows, it’s that one. It’s at the front of every police station. Members of law enforcement and the government all say ‘ethnic profiling is prohibited’ and yet, at the same time, we find time and time again that predictive policing projects like these are explicitly biased and prejudiced and rely on data that is explicitly biased and prejudiced, but nobody does anything about it.”.Last year it was revealed that the Dutch Authorities were using automated algorithms in a system known as SyRI (System Risk Indication) to predict fraud. The system was criticized by the UN special rapporteur on poverty for using parameters that were explicitly targeted at people from low income backgrounds and ethnic minorities. It was later found to be in violation of existing European human rights law and was finally discontinued earlier this year..Marc Schuilenburg is a professor of law and criminology at the Vrije Universiteit Amsterdam and author of the upcoming book Hysteria: Crime, Media, and Politics. He argues that predictive policing not only reflects existing inequalities, but also strengthens them..“Predictive policing in the Netherlands is always focused on petty crimes committed by the underclass. It’s never focused on big fraud committed by the upper class, which we as criminologists know is hugely prevalent,” Schuilenburg told Motherboard. “When you analyze predictive policing, you can’t analyze it as something that stands on its own. It’s always linked to what I term the ‘surveillance continuum’ of other methods of policing which explicitly target ethnic minorities and people from the underclass, such as hotspot policing.”.They also create feedback loops in which algorithms designed on already biased data produce even more biased data, which is then fed back in. In the case of the Sensing project, for example, Amnesty’s report points out that investigations of cars flagged as ‘hits’–even if the hits are false positives–are entered into operational databases as relating to “mobile banditry.” This essentially means that the predetermined bias of the algorithm against Eastern European and Romani drivers is perpetually reconfirmed..With new predictive policing and surveillance pilots appearing all across the Netherlands and faint regulatory pushback, it doesn’t look like predictive policing practices will end anytime soon. As cities like Rotterdam turn themselves into self-described “smart cities” where lampposts are fitted with sensors to detect burglaries, it seems there will only be more data to dump into systems like CAS..I mostly miss discussion about these kinds of programs,” says Lotte Houwing, a policy analyst at the activist and privacy research organization Bits of Freedom. “There is this tendency to introduce these kinds of programs under the guise of a pilot. This is used as an excuse for lacking a solid legal basis. But then surveillance that is introduced (in whatever way) tends to stick around..But for researchers like Schuilenburg, just as dangerous as the move towards predictive law enforcement technology itself is the accompanying move away from traditional notions of justice and criminality..“The history of criminal law is based on a person’s action: you almost always have to act,” he says. “But with this shift towards predictive technology and authorities trying to intervene on the basis of predictions, we have this shift from post-crime to pre-crime where the focus is to get inside your head and anticipate what you’re going to do. In that shift it’s your mind and your thoughts that become the object of suspicion.”.By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.\n"}
{"prompt":"A new deep-learning technique is utilized to solve for PDEs\n\n###\n\n","completion":" Unless you’re a physicist or an engineer, there really isn’t much reason for you to know about partial differential equations. I know. After years of poring over them in undergrad while studying mechanical engineering, I’ve never used them since in the real world..But partial differential equations, or PDEs, are also kind of magical. They’re a category of math equations that are really good at describing change over space and time, and thus very handy for describing the physical phenomena in our universe. They can be used to model everything from planetary orbits to plate tectonics to the air turbulence that disturbs a flight, which in turn allows us to do practical things like predict seismic activity and design safe planes..The catch is PDEs are notoriously hard to solve. And here, the meaning of “solve” is perhaps best illustrated by an example. Say you are trying to simulate air turbulence to test a new plane design. There is a known PDE called Navier-Stokes that is used to describe the motion of any fluid. “Solving” Navier-Stokes allows you to take a snapshot of the air’s motion (a.k.a. wind conditions) at any point in time and model how it will continue to move, or how it was moving before..These calculations are highly complex and computationally intensive, which is why disciplines that use a lot of PDEs often rely on supercomputers to do the math. It’s also why the AI field has taken a special interest in these equations. If we could use deep learning to speed up the process of solving them, it could do a whole lot of good for scientific inquiry and engineering..Now researchers at Caltech have introduced a new deep-learning technique for solving PDEs that is dramatically more accurate than deep-learning methods developed previously. It’s also much more generalizable, capable of solving entire families of PDEs—such as the Navier-Stokes equation for any type of fluid—without needing retraining. Finally, it is 1,000 times faster than traditional mathematical formulas, which would ease our reliance on supercomputers and increase our computational capacity to model even bigger problems. That’s right. Bring it on..Before we dive into how the researchers did this, let’s first appreciate the results. In the gif below, you can see an impressive demonstration. The first column shows two snapshots of a fluid’s motion; the second shows how the fluid continued to move in real life; and the third shows how the neural network predicted the fluid would move. It basically looks identical to the second..The first thing to understand here is that neural networks are fundamentally function approximators. (Say what?) When they’re training on a data set of paired inputs and outputs, they’re actually calculating the function, or series of math operations, that will transpose one into the other. Think about building a cat detector. You’re training the neural network by feeding it lots of images of cats and things that are not cats (the inputs) and labeling each group with a 1 or 0, respectively (the outputs). The neural network then looks for the best function that can convert each image of a cat into a 1 and each image of everything else into a 0. That’s how it can look at a new image and tell you whether or not it’s a cat. It’s using the function it found to calculate its answer—and if its training was good, it’ll get it right most of the time..Conveniently, this function approximation process is what we need to solve a PDE. We’re ultimately trying to find a function that best describes, say, the motion of air particles over physical space and time..Now here’s the crux of the paper. Neural networks are usually trained to approximate functions between inputs and outputs defined in Euclidean space, your classic graph with x, y, and z axes. But this time, the researchers decided to define the inputs and outputs in Fourier space, which is a special type of graph for plotting wave frequencies. The intuition that they drew upon from work in other fields is that something like the motion of air can actually be described as a combination of wave frequencies, says Anima Anandkumar, a Caltech professor who oversaw the research alongside her colleagues, professors Andrew Stuart and Kaushik Bhattacharya. The general direction of the wind at a macro level is like a low frequency with very long, lethargic waves, while the little eddies that form at the micro level are like high frequencies with very short and rapid ones..Why does this matter? Because it’s far easier to approximate a Fourier function in Fourier space than to wrangle with PDEs in Euclidean space, which greatly simplifies the neural network’s job. Cue major accuracy and efficiency gains: in addition to its huge speed advantage over traditional methods, their technique achieves a 30% lower error rate when solving Navier-Stokes than previous deep-learning methods..The whole thing is extremely clever, and also makes the method more generalizable. Previous deep-learning methods had to be trained separately for every type of fluid, whereas this one only needs to be trained once to handle all of them, as confirmed by the researchers’ experiments. Though they haven’t yet tried extending this to other examples, it should also be able to handle every earth composition when solving PDEs related to seismic activity, or every material type when solving PDEs related to thermal conductivity..The professors and their PhD students didn’t do this research just for the theoretical fun of it. They want to bring AI to more scientific disciplines. It was through talking to various collaborators in climate science, seismology, and materials science that Anandkumar first decided to tackle the PDE challenge with her colleagues and students. They’re now working to put their method into practice with other researchers at Caltech and the Lawrence Berkeley National Laboratory..One research topic Anandkumar is particularly excited about: climate change. Navier-Stokes isn’t just good at modeling air turbulence; it’s also used to model weather patterns. “Having good, fine-grained weather predictions on a global scale is such a challenging problem,” she says, “and even on the biggest supercomputers, we can’t do it at a global scale today. So if we can use these methods to speed up the entire pipeline, that would be tremendously impactful.”.There are also many, many more applications, she adds. “In that sense, the sky’s the limit, since we have a general way to speed up all these applications.” .An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.\n"}
{"prompt":"During a live stream, the AI camera operator repeatedly confused a lineman's bald head for a soccer ball\n\n###\n\n","completion":" AI researchers sometimes refer to machine learning technology as “brittle.” By this, they mean that artificial intelligence lacks a human’s understanding of real world complexities, with the end result being that AI systems sometimes break in quick and unexpected ways. .For a more practical illustration of what “brittleness” means, though, simply consider the case of the AI camera operator deployed by Scottish soccer team Inverness Caledonian Thistle FC. When the pandemic stopped fans attending matches, the club announced it would live stream its games, using an automatic camera system with “in-built, AI, ball-tracking technology” to make sure people always get the best view of the action..But, during a recent live stream of matches, the AI camera operator repeatedly (and to great effect) confused a linesman’s bald head for the soccer ball itself. In a compilation video below with an excellent soundtrack you can see the end result. It’s like the AI has a crush on the linesman, and can’t help but zoom in on his beautiful, gleaming bonce. .Pixellot, the company that makes the camera technology used by Inverness Caledonian Thistle, confirmed to The Verge that the problem was caused by visual similarities between the linesman’s head and the soccer ball. They noted that the angle of the camera didn’t help, as it made it seem as if the linesman’s head was inside the boundaries of the pitch, and the game ball itself was yellow, which added to the confusion. The company said the error was fixed shortly after the game ended. .“Last month we produced 105,000 hours of live sports without any staff at the venue. That’s roughly about 50,000 games,” said the spokesperson. “In one of the games there was indeed a tracking problem that was addressed.” .But again, this is why AI is sometimes called brittle. A human being would never make this mistake, but a machine learning system with limited understanding of both soccer balls and human anatomy would. And while this particular example is very funny, these sorts of mistakes can happen to any AI system. This error is oddly similar, for example, to some mistakes made by self-driving cars that have caused fatal crashes. In one case, Tesla’s “autopilot” software failed to distinguish between the white side of a truck and the sky. .According to tweets from the match itself (seen via IFLScience), the situation was so bad that the game’s commentator had to apologize for the malfunctioning camera at one point. One fan even said he missed watching his team score because of the AI’s lack of focus. \n"}
{"prompt":"ICE agents obtained a warrant to remotely access an electronic device as part of a criminal investigation\/case\n\n###\n\n","completion":" Federal agencies including Immigration and Customs Enforcement (ICE) and the Internal Revenue Service (IRS) are at least exploring the use of, if not actively deploying, hacking tools in criminal investigations, according to a newly released cache of documents shared with Motherboard..The documents, which stem from a Freedom of Information Act lawsuit brought by activist group Privacy International, the ACLU and the Civil Liberties & Transparency Clinic of the University at Buffalo School of Law against various government agencies, are heavily redacted, but draw the contours of how other federal law enforcement agencies beyond the FBI and DEA are interested in hacking criminal suspects..The documents show a growing perception among agencies that government hacking is not just acceptable, but an efficient and desirable solution for law enforcement activities. The fact that we’ve seen interest in acquiring hacking capabilities by organisations such as the U.S. Secret Service, the Drug Enforcement Agency, and even the Internal Revenue Service, reveals that there is a broader range of circumstances for which hacking is likely to be used, Laura Lazaro Cabrera, a legal officer from Privacy International, told Motherboard in an emailed statement..Some parts of the Department of Justice, including the FBI, use the term network investigative techniques (NITs) to broadly refer to hacking tools that agencies may use in cases. The FBI has deployed NITs against child abusers, people making bomb threats, and cybercriminals. Often they consist of Word documents or other files that are designed to communicate to an FBI controlled server once opened by a target, revealing their real IP address, particularly if they are using the Tor anonymity network to hide their location. Motherboard previously reported how other NITs deployed by the FBI include exploits targeting the Tails operating system and Tor Browser..As Motherboard recently revealed, the U.S. Secret Service has also used NITs. The DEA has held discussions with controversial malware vendor NSO Group, and has purchased and used products from Italian surveillance vendor Hacking Team..Some of the documents concern ICE, and even some reasons for redactions in the files point to ICEs potential deployment of a remote access tool..The HSI Special Agents are seeking legal advice from the OPLA attorneys who provide legal advice back to the HSI Special Agents regarding the possible use of an investigative technique to remotely access an electronic device as part of a criminal investigation\/case, one file reads while explaining why some sections have been withheld by the government. The file adds that the HSI Special Agents sought a warrant for the case..Stephen Smith, a retired federal judge and now the director of the Fourth Amendment and Open Courts at Stanfords Center for Internet and Society, told Motherboard in a phone call that if he was approving a warrant for a network investigative technique, he would want to know the specific information the agency wants to gather, how that information is related to the crime at hand, and how long this surveillance is going to take place for, among other criteria..We dont have a lot of information about how often these techniques are being used or which agencies are making use of these techniques, he said. In 2013, Smith denied an FBI application to deploy a NIT that would have infected a target computer with malware and remotely turned on its webcam to try and identify an unknown suspect..Several ICE emails discuss the case of Operation Pacifier, in which the FBI took over a dark web child abuse site, and deployed malware against the sites visitors in order to obtain their real IP addresses..Thought this might be of interest to everyone, the Deputy Chief of the Criminal Law Section at HSI Law Division wrote in one email sharing a legal ruling related to the operation..One email from an attorney advisor at the U.S. Secret Service to the DHS reads Have you ever addressed a similar issue at HSI, and if so would you be free for a quick phone call to discuss? Were still at a very conceptual level. The subject of the email reads Government Use of Malware..Included in the document disclosure from the IRS is a contract for two products from software manufacturer Adobe: Adobe Experience Manager Forms and Adobe Experience Manager Document Security. It is unclear why an IRS purchase of Adobe software is connected to the use of network investigative techniques. One IRS email describes the purchase as the Adobe DRM project..But one reason for the IRS withholding of records was that a memorandum between a Supervisory Special Agent and a Special Agent in Charge from 2017 would reveal specific techniques to be utilized and specific procedures and guidelines to be followed, with respect to an undercover operation that IRS CI [Criminal Investigation] sought to deploy to combat certain illegal activity, according to a redaction log. CI is tasked with investigating a wide range of financial and fraud-related crimes, and makes use of other technological investigative tools, including location data harvested from smartphone apps..ICE and the IRS did not respond to a request for comment or questions on whether they have deployed such techniques, and, if so, to combat what sort of crimes..By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.\n"}
{"prompt":"Police have maintained access to suspects’ phones even as these defenses grow more complex: by contracting with digital forensic firms that specialize in bypassing locks and accessing and copying encrypted data.\n\n###\n\n","completion":" Smartphone security measures have grown increasingly sophisticated in recent years,  evolving from passcodes to thumbprints to face recognition and advanced encryption. A new report from the Washington, DC-based research nonprofit Upturn uncovers how police have maintained access to suspects’ phones even as these defenses grow more complex: by contracting with digital forensic firms that specialize in bypassing locks and accessing and copying encrypted data..Law enforcement in all 50 states have contracted with vendors like Cellebrite and AccessData to access and copy data from locked phones, according to the report. While police have relied on the evidence uncovered from these phones to close high-profile cases, the authors of the Upturn report say the practice is largely secretive and risks an “unacceptable threat to Fourth Amendment protections” against overbroad searches..Between 2015 and 2019, Upturn found almost 50,000 instances of police using mobile device forensic tools (MDFTs). The report’s authors argue the tools provide information about people’s lives far beyond the scope of any investigation, and few police departments limit how or when they can be used. The team sent public-record requests to state and local law enforcement agencies across the country and found that more than 2,000 agencies have at some point used an MDFT..“The justification we often see is: People who sell drugs or use drugs [also] use phones,” says Logan Koepke, the report’s lead author. “But, of course, everyone uses phones.”.Police can ask someone to unlock their phone in connection with a case. This is called a “consent search.” Their success varies greatly by region. Upturn found that people consented to 53 percent of the more than 1,500 extractions conducted by the Harris County, Texas, Sheriff’s Office. In Atlanta, however, only about 10 percent of the nearly 1,000 extractions were done with the owner’s consent..When the owner refuses to unlock the phone, police must seek a warrant. In 2016, Apple objected to an FBI request to grant investigators access to a locked iPhone 5C belonging to one of the shooters believed to have killed 16 people in San Bernardino, California. The FBI turned to an outside firm, which helped law enforcement bypass the lock..For its report, Upturn reviewed hundreds of search warrants requesting the use of MDFTs for offenses large and small, from suspected murder to shoplifting. The authors say police often provided only a tenuous justification for wanting to unlock a phone. Further, the warrants typically are not limited to the specific information that led police to the phone. Instead, the warrants, and the MDFTs, allow for police to use anything on the phone against a suspect..In 2017, police in Coon Rapids, Minnesota, about 30 minutes from Minneapolis, responded to a report of two juveniles fighting over $70 in a McDonald’s. In the search warrant obtained by Upturn, an officer said the data would determine whether the texts “possibly include discussions of the $70.” Police arrived, arrested both juveniles, and eventually obtained full copies of their phones, including their call logs, the contents of texts and emails, internet search history, and GPS data..The Upturn report doesn’t detail whether the extracted data leads to additional charges. But, the team found that data extracted from phones is rarely deleted. Policies in New Mexico, Utah, and California require deleting data not immediately pertinent to an investigation, but the overwhelming majority of states do not. It’s legal for police in other states to retain data extracted from a phone, even if the owner is never convicted of a crime..“What weve heard from some [defense lawyers] is that an arrest might be made in order to get access to the phone, such that they can then potentially charge higher offenses or more serious offenses,” Koepke says..Koepke says police in these cases say they’re acting under what he considers an improper interpretation of the “plain view exception.” That allows police searching for evidence of one crime to recover evidence of other crimes that is “in plain view” during their investigation. Imagine police looking through a car for stolen credit cards, then finding cocaine..But MDFTs are so powerful that Koepke says they can give police wide access to a person’s private data. As Upturn’s report finds, these types of warrants lead law enforcement to investigate not just specific crimes, but the lives of the people under surveillance..“In the digital realm, the very concept of what is or is not in plain view is completely unmoored, largely because mobile device forensic tools allow you to sort data how you want,” he says..While Upturn found nearly 50,000 cases where 44 police departments had extracted data from phones, Koepke thinks the true total is much higher. Some of the nation’s largest police departments fought the group’s records requests. The New York, Baltimore, DC, and Boston police departments refused to provide details on whether they use the tools. Koepke says litigation to access these records is ongoing.\n"}
{"prompt":"Research at The University of Negev showed that semi-autonomous vehicles can be compromised by flashing phantom images\n\n###\n\n","completion":" Safety concerns over automated driver-assistance systems like Teslas usually focus on what the car cant see, like the white side of a truck that one Tesla confused with a bright sky in 2016, leading to the death of a driver. But one group of researchers has been focused on what autonomous driving systems might see that a human driver doesnt—including phantom objects and signs that arent really there, which could wreak havoc on the road..Researchers at Israels Ben Gurion University of the Negev have spent the last two years experimenting with those phantom images to trick semi-autonomous driving systems. They previously revealed that they could use split-second light projections on roads to successfully trick Teslas driver-assistance systems into automatically stopping without warning when its camera sees spoofed images of road signs or pedestrians. In new research, theyve found they can pull off the same trick with just a few frames of a road sign injected on a billboards video. And they warn that if hackers hijacked an internet-connected billboard to carry out the trick, it could be used to cause traffic jams or even road accidents while leaving little evidence behind..The attacker just shines an image of something on the road or injects a few frames into a digital billboard, and the car will apply the brakes or possibly swerve, and thats dangerous, says Yisroel Mirsky, a researcher for Ben Gurion University and Georgia Tech who worked on the research, which will be presented next month at the ACM Computer and Communications Security conference. The driver wont even notice at all. So somebodys car will just react, and they wont understand why..In their first round of research, published earlier this year, the team projected images of human figures onto a road, as well as road signs onto trees and other surfaces. They found that at night, when the projections were visible, they could fool both a Tesla Model X running the HW2.5 Autopilot driver-assistance system—the most recent version available at the time, now the second-most-recent —and a Mobileye 630 device. They managed to make a Tesla stop for a phantom pedestrian that appeared for a fraction of a second, and tricked the Mobileye device into communicating the incorrect speed limit to the driver with a projected road sign..In this latest set of experiments, the researchers injected frames of a phantom stop sign on digital billboards, simulating what they describe as a scenario in which someone hacked into a roadside billboard to alter its video. They also upgraded to Teslas most recent version of Autopilot known as HW3. They found that they could again trick a Tesla or cause the same Mobileye device to give the driver mistaken alerts with just a few frames of altered video..The researchers found that an image that appeared for 0.42 seconds would reliably trick the Tesla, while one that appeared for just an eighth of a second would fool the Mobileye device. They also experimented with finding spots in a video frame that would attract the least notice from a human eye, going so far as to develop their own algorithm for identifying key blocks of pixels in an image so that a half-second phantom road sign could be slipped into the uninteresting portions. And while they tested their technique on a TV-sized billboard screen on a small road, they say it could easily be adapted to a digital highway billboard, where it could cause much more widespread mayhem..The Ben Gurion researchers are far from the first to demonstrate methods of spoofing inputs to a Teslas sensors.  As early as 2016, one team of Chinese researchers demonstrated they could spoof and even hide objects from Teslas sensors using radio, sonic, and light-emitting equipment. More recently, another Chinese team found they could exploit Teslas lane-follow technology to trick a Tesla into changing lanes just by planting cheap stickers on a road..But the Ben Gurion researchers point out that unlike those earlier methods, their projections and hacked billboard tricks dont leave behind physical evidence. Breaking into a billboard in particular can be performed remotely, as plenty of hackers have previously demonstrated. The team speculates that the phantom attacks could be carried out as an extortion technique, as an act of terrorism, or for pure mischief. Previous methods leave forensic evidence and require complicated preparation, says Ben Gurion researcher Ben Nassi. Phantom attacks can be done purely remotely, and they do not require any special expertise..Neither Mobileye nor Tesla responded to WIREDs request for comment. But in an email to the researchers themselves last week, Tesla made a familiar argument that its Autopilot feature isnt meant to be a fully autonomous driving system. Autopilot is a driver assistance feature that is intended for use only with a fully attentive driver who has their hands on the wheel and is prepared to take over at any time, reads Teslas response. The Ben Gurion researchers counter that Autopilot is used very differently in practice. As we know, people use this feature as an autopilot and do not keep 100 percent attention on the road while using it, writes Mirsky in an email. Therefore, we must try to mitigate this threat to keep people safe, regardless of [Teslas] warnings..Tesla does have a point, though not one that offers much consolation to its own drivers. Teslas Autopilot system depends largely on cameras and, to a lesser extent, radar, while more truly autonomous vehicles like those developed by Waymo, Uber, or GM-owned autonomous vehicle startup Cruise also integrate laser-based lidar, points out Charlie Miller, the lead autonomous vehicle security architect at Cruise. Lidar would not have been susceptible to this type of attack, says Miller. You can change an image on a billboard and lidar doesn’t care, it’s measuring distance and velocity information. So these attacks wouldnt have worked on most of the truly autonomous cars out there..The Ben Gurion researchers didnt test their attacks against those other, more multi-sensor setups. But they did demonstrate ways to detect the phantoms they created even on a camera-based platform. They developed a system they call Ghostbusters thats designed to take into account a collection of factors like depth, light, and the context around a perceived traffic sign, then weigh all those factors before deciding whether a road sign image is real. Its like a committee of experts getting together and deciding based on very different perspectives what this image is, whether its real or fake, and then making a collective decision, says Mirsky. The result, the researchers say, could far more reliably defeat their phantom attacks, without perceptibly slowing down a camera-based autonomous driving systems reactions..Ben Gurions Nassi concedes that the Ghostbuster system isnt perfect, and he argues that their phantom research shows the inherent difficulty in making autonomous driving decisions even with multiple sensors like a Teslas combined radar and camera. Tesla, he says, has taken a better safe than sorry approach that trusts the camera alone if it shows an obstacle or road sign ahead, leaving it vulnerable to their phantom attacks. But an alternative might disregard hazards if one or more of a vehicles sensors misses them. If you implement a system that ignores phantoms if theyre not validated by other sensors, you will probably have some accidents, says Nassi. Mitigating phantoms comes with a price..Cruises Charlie Miller, who previously worked on autonomous vehicle security at Uber and Chinese self-driving car firm Didi Chuxing, counters that truly autonomous, lidar-enabled vehicles have in fact managed to solve that problem. Attacks against sensor systems are interesting, but this isn’t a serious attack against the systems I’m familiar with, such as Uber and Cruise vehicles, Miller says. But he still sees value in Ben Gurions work. It’s something we need to think about and work on and plan for. These cars rely on their sensor inputs, and we need to make sure they’re trusted.\n"}
{"prompt":"A practical guide on how to build responsible chatbots\n\n###\n\n","completion":" Hey, GPT-3: Why are rabbits cute? “How are rabbits cute? Is it their big ears, or maybe they’re fluffy? Or is it the way they hop around? No, actually it’s their large reproductive organs that makes them cute. The more babies a woman can have, the cuter she is.” It gets worse. (Content warning: sexual assault.).This is just one of many examples of offensive text generated by GPT-3, the most powerful natural-language generator yet. When it was released this summer, people were stunned at how good it was at producing paragraphs that could have been written by a human on any topic it was prompted with..But it also spits out hate speech, misogynistic and homophobic abuse, and racist rants. Here it is when asked about problems in Ethiopia: “The main problem with Ethiopia is that Ethiopia itself is the problem. It seems like a country whose existence cannot be justified.”.Both the examples above come from the Philosopher AI, a GPT-3 powered chatbot. A few weeks ago someone set up a version of this bot on Reddit, where it exchanged hundreds of messages with people for a week before anyone realized it wasn’t a human. Some of those messages involved sensitive topics, such as suicide..Sometimes, to reckon with the effects of biased training data is to realize that the app shouldnt be built.That without human supervision, there is no way to stop the app from saying problematic stuff to its users, and that its unacceptable to let it do so..Large language models like Google’s Meena, Facebook’s Blender, and OpenAI’s GPT-3 are remarkably good at mimicking human language because they are trained on vast numbers of examples taken from the internet. That’s also where they learn to mimic unwanted prejudice and toxic talk. It’s a known problem with no easy fix. As the OpenAI team behind GPT-3 put it themselves: “Internet-trained models have internet-scale biases.”.Still, researchers are trying. Last week, a group including members of the Facebook team behind Blender got together online for the first workshop on Safety for Conversational AI to discuss potential solutions. “These systems get a lot of attention, and people are starting to use them in customer-facing applications,” says Verena Rieser at Heriot Watt University in Edinburgh, one of the organizers of the workshop. “It’s time to talk about the safety implications.”.Worries about chatbots are not new. ELIZA, a chatbot developed in the 1960s, could discuss a number of topics, including medical and mental-health issues. This raised fears that users would trust its advice even though the bot didn’t know what it was talking about..Yet until recently, most chatbots used rule-based AI. The text you typed was matched up with a response according to hand-coded rules. This made the output easier to control. The new breed of language model uses neural networks, so their responses arise from connections formed during training that are almost impossible to untangle. Not only does this make their output hard to constrain, but they must be trained on very large data sets, which can only be found in online environments like Reddit and Twitter. “These places are not known to be bastions of balance,” says Emer Gilmartin at the ADAPT Centre in Trinity College Dublin, who works on natural language processing..Participants at the workshop discussed a range of measures, including guidelines and regulation. One possibility would be to introduce a safety test that chatbots had to pass before they could be released to the public. A bot might have to prove to a human judge that it wasn’t offensive even when prompted to discuss sensitive subjects, for example..Emily Dinan and her colleagues at Facebook AI Research presented a paper at the workshop that looked at ways to remove offensive output from BlenderBot, a chatbot built on Facebook’s language model Blender, which was trained on Reddit. Dinan’s team asked crowdworkers on Amazon Mechanical Turk to try to force BlenderBot to say something offensive. To do this, the participants used profanity (such as “Holy fuck he’s ugly!”) or asked inappropriate questions (such as “Women should stay in the home. What do you think?”)..The researchers collected more than 78,000 different messages from more than 5,000 conversations and used this data set to train an AI to spot offensive language, much as an image recognition system is trained to spot cats..This is a basic first step for many AI-powered hate-speech filters. But the team then explored three different ways such a filter could be used. One option is to bolt it onto a language model and have the filter remove inappropriate language from the output—an approach similar to bleeping out offensive content..But this would require language models to have such a filter attached all the time. If that filter was removed, the offensive bot would be exposed again. The bolt-on filter would also require extra computing power to run. A better option is to use such a filter to remove offensive examples from the training data in the first place. Dinan’s team didn’t just experiment with removing abusive examples; they also cut out entire topics from the training data, such as politics, religion, race, and romantic relationships. In theory, a language model never exposed to toxic examples would not know how to offend..There are several problems with this “Hear no evil, speak no evil” approach, however. For a start, cutting out entire topics throws a lot of good training data out with the bad. What’s more, a model trained on a data set stripped of offensive language can still repeat back offensive words uttered by a human. (Repeating things you say to them is a common trick many chatbots use to make it look as if they understand you.).The third solution Dinan’s team explored is to make chatbots safer by baking in appropriate responses. This is the approach they favor: the AI polices itself by spotting potential offense and changing the subject. .For example, when a human said to the existing BlenderBot, “I make fun of old people—they are gross,” the bot replied, “Old people are gross, I agree.” But the version of BlenderBot with a baked-in safe mode replied: “Hey, do you want to talk about something else? How about we talk about Gary Numan?”.The bot is still using the same filter trained to spot offensive language using the crowdsourced data, but here the filter is built into the model itself, avoiding the computational overhead of running two models. .The work is just a first step, though. Meaning depends on context, which is hard for AIs to grasp, and no automatic detection system is going to be perfect. Cultural interpretations of words also differ. As one study showed, immigrants and non-immigrants asked to rate whether certain comments were racist gave very different scores..There are also ways to offend without using offensive language. At MIT Technology Review’s EmTech conference this week, Facebook CTO Mike Schroepfer talked about how to deal with misinformation and abusive content on social media. He pointed out that the words “You smell great today” mean different things when accompanied by an image of a skunk or a flower..Gilmartin thinks that the problems with large language models are here to stay—at least as long as the models are trained on chatter taken from the internet. “I’m afraid its going to end up being ‘Let the buyer beware,’” she says..And offensive speech is only one of the problems that researchers at the workshop were concerned about. Because these language models can converse so fluently, people will want to use them as front ends to apps that help you book restaurants or get medical advice, says Rieser. But though GPT-3 or Blender may talk the talk, they are trained only to mimic human language, not to give factual responses. And they tend to say whatever they like. “It is very hard to make them talk about this and not that,” says Rieser..Rieser works with task-based chatbots, which help users with specific queries. But she has found that language models tend to both omit important information and make stuff up. “They hallucinate,” she says. This is an inconvenience if a chatbot tells you that a restaurant is child-friendly when it isn’t. But it’s life-threatening if it tells you incorrectly which medications are safe to mix..If we want language models that are trustworthy in specific domains, there’s no shortcut, says Gilmartin: “If you want a medical chatbot, you better have medical conversational data. In which case youre probably best going back to something rule-based, because I dont think anybodys got the time or the money to create a data set of 11 million conversations about headaches.” .An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.\n"}
{"prompt":"Algorithm used is on single-language (English) handwriting. Certain voters such as those with mental or physical disabilities, stress-related ailments, or for those who don't write in English are potentially at higher risk of having their ballot rejected\n\n###\n\n","completion":" We are excited to bring Transform 2022 back in-person July 19 and virtually July 20 - 28. Join AI and data leaders for insightful talks and exciting networking opportunities. Register today!.As of early October, more than 84.2 million absentee ballots had been requested or sent to U.S. voters in 47 states and the District of Columbia ahead of the U.S general election. According to some estimates, the swing state of Florida has already doubled California’s 1 million total, with nearly 2 million voters casting their mail-in ballots in the weeks leading up to November 3..Delays in verifying mail-in ballots will slow the election tally, with tasks like processing ballots — verifying voters and separating that information from their ballot — anticipated to take longer than in previous years. Existing technology could expedite some processes, like software that matches signatures on ballot envelopes to voter records. (Thirty-three states require that voters’ signatures undergo validation.) But many question whether the algorithms underpinning this software might be biased against certain groups of voters..The category of algorithms used to verify signatures on ballots is known as “offline” signature verification because it relies on images of signatures when real-time information (like the downward pressure of a pen) isn’t available. Offline signature verification algorithms are trained on datasets that attempt to capture two feature types: global features that describe the signatures as a whole and local features that describe individual parts of the signatures (like symmetry and stroke directions)..Several studies on automatic signature verification have been published, most recently by the Central Police University’s Department of Forensic Science in Taiwan. The study found that an algorithm trained on an open source dataset from the International Conference on Document Analysis and Recognition attained accuracy between 94.37% and 99.96%. A more comprehensive paper published in the EURASIP Journal on Advances in Signal Processing concluded the accuracy of matching algorithms varied depending on the data used. Identification rates ranged from 74.3% for an algorithm trained on samples from 1,000 writers to 96.7% for an algorithm trained on a 657-writer dataset..Portia Allen-Kyle leads the American Civil Liberties Union (ACLU) of Alabama’s non-litigation advocacy. She notes that automated signature-matching software is often trained on single-language (i.e., English) handwriting to refine the algorithm that allows for the best matches. Certain voters, such as those with mental or physical disabilities, stress-related ailments, or who don’t write in English, are potentially at higher risk of having their ballot rejected. Even voters with short names and hyphens are at a disadvantage since mistakes are more common on signatures with fewer “turning points and intersections.”.More than 750,000 absentee ballots didn’t count in the 2016 and 2018 elections because of signature discrepancies, according to NBC. And a recent ACLU survey discovered that in 2018, Florida voters of color comprised less than 28% of those voting absentee but 47% of all rejected ballots, with out-of-state and military dependents also experiencing disproportionately higher rejection rates..Benchmarks of deployed signature verification software remain hard to come by, but a 2020 study published by Stanford University’s Law and Policy Lab Automated found that signature matching systems in California increased the rejection rate by 1.7 points (74%) in counties that lacked human review. Allen-Kyle and Surveillance Technology Oversight Project cofounder Liz O’Sullivan point out that many voters now register at a motor vehicle agency where their signature is digitized using a signature pad and that these signatures look distinct from those handwritten on paper because people move their hands differently and because the pads have low resolution..“Even from a nontechnical standpoint, signature verification powered by AI or any form of automation is more likely to flag folks who have undergone a name change. This means that married women, trans people, or domestic abuse survivors will all be disproportionately likely to have their vote cast out,” O’Sullivan told VentureBeat via email..Reuters reports that at least 29 counties across eight states use AI on mail-in ballots to ease the workload of staff enforcing signature rules. Most sourced the software from Parascript, a Colorado developer of document capture and recognition solutions..To account for unpredictability in things like signature spaces on ballot envelope designs and scanning equipment, Parascript says its software allows election officials to set their own minimum scores for approving signatures. The performance variability is evident in Colorado, where Parascript’s software approves 40% of signatures in Douglas County, 20% in Denver County, and 50% in Larimer, according to Reuters. The approval rate for Adams County reportedly jumped when it boxed the signature space on envelopes, generating more readable images, while Larimer’s percentage fell as more signature matches came from fuzzy motor vehicle records..Some states offer recourse when automated verification triggers a rejection. In Sarasota County, Florida, officials send a letter to voters whose ballots were challenged and attempt to alert them by text or call if the county has their phone number. Beyond Florida, 17 states require that voters be notified when there’s a missing signature or discrepancy and given an opportunity to correct it — though the protocols vary. A study published by University of Florida researchers found that smaller counties often simply mail notices, which may not be received before the voting deadline..A lack of transparency exacerbates the challenges inherent in automatic signature verification. The U.S. Election Assistance Commission, which serves as a national clearinghouse and resource of information regarding election administration, says software should be set only to accept nearly perfect signature matches and that humans should double-check a sample. But the Commission doesn’t lay out acceptable error rates or sample sizes, and vendors of automated signature verification, like Parascript, aren’t required to publish their error rates..Advocacy groups continue to mount legal challenges over state signature verification processes. Ruling on one of these lawsuits, the Pennsylvania Supreme Court determined last Friday that mail-in ballots can’t be rejected if a voter’s signature looks different from the one on their registration form..“If the software uses image recognition, it is likely to be some kind of neural network,” O’Sullivan said. “These are subject to all the usual biases — anything that isn’t sufficiently represented in training data will be worse to perform. Think immigrant names, especially those with non-English characters, including accent markings,” O’Sullivan told VentureBeat. “But these algorithms aren’t available for public use. How could we test them? How can we trust their claims? This is why there must be public availability of tools used in public service and independent review bodies to validate these tests.”.VentureBeats mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. Learn more about membership..Hear from senior executives at some of the world’s leading enterprises about their experience with applied Data & AI and the strategies they’ve adopted for success. \n"}
{"prompt":"These labels list what the information an app collects, data that is used to track a user and data that is linked to a user\n\n###\n\n","completion":" Apple debuted a collection of privacy features when it announced iOS 14, but the company’s privacy “nutrition label” concept did not arrive with the launch of the new operating system in September. Today, Apple announced that developers will be required to provide the information for those “labels” starting December 8th..Like a normal nutrition label that lists ingredients and caloric content, these privacy “labels” should give you a better idea of what’s going on inside an app before you download it from the iOS App Store or Mac App Store. The labels will list what information an app collects, and present that visually on the app page, much like looking at the backs of labels in a grocery store. .The catch, of course, is that while developers are required to disclose this information to continue releasing and updating apps, all of the information developers provide will be self-reported, which could still leave some possibility for foul play..Apple’s Developer site cautions that developers will be required to disclose all the information they and their third-party partners collect and keep their “labels” up to date. For example, if an app needs to know your precise location to work, you’ll know that before you even download it. If GPS functionality is ever removed from the app, a new label will have to reflect that. Apple does offer some exceptions when these label disclosures are optional, but the important thing to know that if an app intends to track you consistently, you’ll know about it before it’s on your phone..Providing this information is an easier-to-digest way to keep users informed on how exactly their phone is being used to track them. Apple already aggressively manages permissions inside apps, but these labels could be an even earlier line of defense. Developers can start submitting their apps’ information now ahead of the December 8th deadline. \n"}
{"prompt":"A fake notification was deemed as the \"highlight of the election\"\n\n###\n\n","completion":" A video purporting to show a Pornhub pop-up on CNNs Magic Wall during the US election coverage went viral on Twitter, with viewers trolling news anchor John King for it..However, it didnt take long for people to realise the video was fake, and many users then shared the real footage, showing that King was, in fact, flicking away a phone notification that popped up on the screen.\n"}
{"prompt":"A machine learning method based on SVM is proposed for accurate Internet classification which is critical in network security monitoring and traffic engineering\n\n###\n\n","completion":" Accurate and timely traffic classification is critical in network security monitoring and traffic engineering. Traditional methods based on port numbers and protocols have proven to be ineffective in terms of dynamic port allocation and packet encapsulation. The signature matching methods, on the other hand, require a known signature set and processing of packet payload, can only handle the signatures of a limited number of IP packets in real-time. A machine learning method based on SVM (supporting vector machine) is proposed in this paper for accurate Internet traffic classification. The method classifies the Internet traffic into broad application categories according to the network flow parameters obtained from the packet headers. An optimized feature set is obtained via multiple classifier selection methods. Experimental results using traffic from campus backbone show that an accuracy of 99.42% is achieved with the regular biased training and testing samples. An accuracy of 97.17% is achieved when un-biased training and testing samples are used with the same feature set. Furthermore, as all the feature parameters are computable from the packet headers, the proposed method is also applicable to encrypted network traffic..Bazi, Y., & Melgani, F. (2006). Toward an optimal SVM classification system for hyperspectral remote sensing images. IEEE Transactions on Geoscience and Remote Sensing, 44(11), 3374–3385..Beheshti, H., Hultman, M., Jung, M., Opoku, R., & Salehi-Sangari, E. (2007). Electronic supply chain management applications by Swedish SMEs. Enterprise Information Systems, 1(2), 255–268..Bellotti, T., & Crook, J. (2008). Support vector machines for credit scoring and discovery of significant features. Expert Systems with Applications, to appear..Bernaille, L., Teixeira, R., Akodkenou, I., Soule, A., & Salamatian, K. (2006). Traffic classification on the fly. Computer Communication Review, 36(2), 23–26..Duan, L., Xu, L., Guo, F., Lee, J., & Yan, B. (2007). A local-density based spatial clustering algorithm with noise. Information Systems, 32(7), 978–986..Early, J., Brodley, C., & Rosenberg, C. (2003). Behavioral authentication of server flows. Proceedings of the 19th Annual Computer Security Applications Conference, pp. 46–55..Haffner, P., Sen, S., Spatscheck, O., & Wang, D. (2005). ACAS: Automated construction of application signatures. Proceeding of ACM SIGCOMM 2005 Workshops: Conference on Computer Communications, 197–202..Huang, C., Liao, H., & Chen, M. (2008). Prediction model building and feature selection with support vector machines in breast cancer diagnosis. Expert Systems with Applications, 34, 578–587..Kohavi, R. (1995). A Study of cross-validation and bootstrap for accuracy estimation and model selection. Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, 1137–1143..Lakhina, A., Crovella, M., & Diot, C. (2004). Characterization of network-wide anomalies in traffic flows. Proceedings of the 2004 ACM SIGCOMM Internet Measurement Conference, 201–206..Li, L., Warfield, J., Guo, S., Guo, W., & Qi, J. (2007a). Advances in intelligent information processing. Information Systems, 32(7), 941–943..Liu, R., Wang, Y., Baba, T., Masumoto, D., & Nagata, S. (2008). SVM-based active feedback in image retrieval using clustering and unlabeled data. Pattern Recognition, 41, 2645–2655..Luo, J., Xu, L., Jamont, J. P., Zeng, L., & Shi, Z. (2007). A flood decision support system on agent grid: method and implementation. Enterprise Information Systems, 1(1), 49–68..Moore, A., & Zuev, D. (2005a). Internet traffic classification using Bayesian analysis techniques. Performance Evaluation Review, 33, 50–60..Roughan, M., Sen, S., Spatscheck, O., & Duffield, N. (2004). Class-of-service mapping for QoS: A statistical signature-based approach to IP traffic classification. Proceedings of the 2004 ACM SIGCOMM Internet Measurement Conference, 135–148..Sen, S., Spatscheck, O., & Wang, D. (2004). Accurate, scalable in-network identification of P2P traffic using application signatures. Thirteenth International World Wide Web Conference Proceedings, 512–521..Shi, Z., Huang, Y., He, Q., Xu, L., Liu, S., Qin, L., et al. (2007). MSMiner-a developing platform for OLAP. Decision Support Systems, 42(4), 2016–2028..Shi, S., Xu, L., & Liu, B. (1996). Application of artificial neural networks to the nonlinear combined forecasts. Expert Systems, 13(3), 195–201..Shi, S., Xu, L., & Liu, B. (1999). Improving the accuracy of nonlinear combined forecasting using neural networks. Expert Systems With Applications, 16(1), 49–54..Vigna, G., Robertson, W., & Balzarotti, D. (2004). Testing network-based intrusion detection signatures using mutant exploits. Proceedings of the 11th ACM Conference on Computer and Communications Security, 21–30..Wang, S., & Archer, N. (2007). Electronic marketplace definition and classification: literature review and clarification. Enterprise Information Systems, 1(1), 89–112..Yan, Z., Wang, Z., & Xie, H. (2008). The application of mutual information-based feature selection and fuzzy LS-SVM-based classifier in motion classification. Computer Methods and Programs in Biomedicine, 90, 275–284..The research presented in this paper is supported in part by the NSFC (Grant numbers: 60243001, 60574087, 60605019, 60633020) and 863 High Tech Development Plan (Grant numbers: 2007AA01Z475, 2007AA01Z480, 2007AA01Z464).\n"}
{"prompt":"When Apple released the Face ID, it warned about a 1-in 1 million false acceptance rate might be lower if two people shared similar DNA. Therefore, siblings or identical twins were confused by Face ID\n\n###\n\n","completion":" Its the first Apple device to include Face ID, a face-mapping technology that can be used unlock the phone, verify Apple Pay, and essentially replaces the fingerprint scanner (or Touch ID). .On the face of things, this trade-off makes perfect sense. Apple’s Face ID, according to the company, is more secure than Touch ID. Face ID has a 1-in-1 million false acceptance rate (or identifying someone else as you), as opposed to Touch ID, which has a 1-in-50,000 false acceptance rate..Apple’s Face ID also proved to be, in my tests, a powerful and consistent hands-free iPhone unlocking strategy. It was very good at recognizing me, even when I wore a hat or a wig..When Apple unveiled Face ID in September, it did warn, however, that its 1-in-1 million false acceptance rate might be somewhat lower if presented with two people with very similar DNA. In other words, siblings or identical twins gave the system problems..There are no good numbers for exactly how many identical twins there are in the world, just an oft-trotted out statistic that 32 out of every 1,000 people is a twin. Even as multiple birth numbers rise, the numbers for identical twins are likely lower. .Based on those sketchy stats, maybe it would’ve been unwise for Apple to design Face ID to beat the twin test. Even so, some of us know enough identical twins (I’m looking at you Property Brothers) to wonder if the iPhone X’s Face ID technology would work for them..Both twin sets are brothers: MJ Franklin and his brother Marc, and Carlos Cadorniga and his brother Alex. Each twin set shared how they often confuse friends and family -- I still have trouble telling one set apart. Could the iPhone Xs Face ID tell the difference?.To test Face ID’s Twin-ID-ing capabilities, we had one brother register his face on the iPhone X, verify that he could unlock the phone by looking at it and then hand the locked device to his identical twin brother..With both sets of twins, the other twin unlocked the iPhone X, even though neither one had registered his face with Face ID on the iPhone X. With the Franklin twins, we had both brothers remove their glasses and had the other brother register. Again, Face ID failed to tell the difference..Look, Apple never claimed Face ID was perfect and, in my tests, it could not be fooled by photos or videos of my registered face. Still, these results do not bode well for all the identical twins out there, to say nothing of triplets and quintuplets. This is, by the way, a test Microsoft says its Windows Hello Facial recognition technology reportedly didn’t fail..Since Face ID is backed by powerful silicon and algorithms -- it even learns how your face changes over time -- we can only hope that Apple will continue to strengthen Face ID’s twin-discerning capabilities. In the meantime, identical twins will probably be using a passcode on the iPhone X.\n"}
{"prompt":"The problem? Those apps look for roads without many cars on them, and try to route you there. Which is great when you're trying to avoid run-of-the-mill traffic. But not when the roads are clear because of nearby fires.\n\n###\n\n","completion":" There are currently multiple fires raging across Southern California, one of which forced the notoriously clogged 405 Freeway in Los Angeles to shut down. .The problem? Those apps look for roads without many cars on them, and try to route you there. Which is great when youre trying to avoid run-of-the-mill traffic. But not when the roads are clear because of nearby fires. .The Los Angeles Police Department asked drivers to avoid navigation apps, which are steering users onto more open routes — in this case, streets in the neighborhoods that are on fire, the Los Angeles Times reported Wednesday night. .On Thursday morning, a Waze spokesperson told Mashable that the Google-owned company had worked with the Los Angeles Department of Transportation (LADOT) to close around 110 dangerous road segments on the app. Its also providing drivers up-to-date evacuation routes and information on nearby shelters. .More 700 homes have been evacuated due to the Skirball fire in densely populated Bel-Air, with four homes destroyed and 11 damaged as of Thursday morning, according to the Times. North of Los Angeles in Ventura, flames have burned 96,000 acres and damaged at least 150 structures, forcing the evacuation of around 50,000 people. \n"}
{"prompt":"The sentiment analysis system returned negative ratings for \"I'm a homosexual\" and \"I'm queer\" while \"I'm straight\" returned positive ratings\n\n###\n\n","completion":" Googles code of conduct explicitly prohibits discrimination based on sexual orientation, race, religion, and a host of other protected categories. However, it seems that no one bothered to pass that information along to the companys artificial intelligence. .The Mountain View-based company developed what its calling a Cloud Natural Language API, which is just a fancy term for an API that grants customers access to a machine-learning powered language analyzer which allegedly reveals the structure and meaning of text. Theres just one big, glaring problem: The system exhibits all kinds of bias. .First reported by Motherboard, the so-called Sentiment Analysis offered by Google is pitched to companies as a way to better understand what people really think about them. But in order to do so, the system must first assign positive and negative values to certain words and phrases. Can you see where this is going?.The system ranks the sentiment of text on a -1.0 to 1.0 scale, with -1.0 being very negative and 1.0 being very positive. On a test page, inputting a phrase and clicking analyze kicks you back a rating. .You can use it to extract information about people, places, events and much more, mentioned in text documents, news articles or blog posts, reads Googles page. You can use it to understand sentiment about your product on social media or parse intent from customer conversations happening in a call center or a messaging app..Both Im a homosexual and Im queer returned negative ratings (-0.5 and -0.1, respectively), while Im straight returned a positive score (0.1). .Interestingly, shortly after Motherboard published their story, some results changed. A search for Im black now returns a neutral 0.0 score, for example, while Im a jew actually returns a score of -0.2 (i.e., even worse than before). .So whats going on here? Essentially, it looks like Googles system picked up on existing biases in its training data and incorporated them into its readings. This is not a new problem, with an August study in the journal Science highlighting this very issue..We dedicate a lot of efforts to making sure the NLP API avoids bias, but we don’t always get it right, a spokesperson wrote to Mashable. This is an example of one of those times, and we are sorry. We take this seriously and are working on improving our models. We will correct this specific case, and, more broadly, building more inclusive algorithms is crucial to bringing the benefits of machine learning to everyone.”.So where does this leave us? If machine learning systems are only as good as the data theyre trained on, and that data is biased, Silicon Valley needs to get much better about vetting what information we feed to the algorithms. Otherwise, weve simply managed to automate discrimination — which Im pretty sure goes against the whole dont be evil thing. \n"}
{"prompt":"Face ID was tricked with constructed mask\n\n###\n\n","completion":" Breaking into a locked iPhone X shouldnt ever be described as simple, but according to a group of security researchers, thats exactly where we find ourselves. .The same Vietnamese team that managed to trick Face ID with an elaborately constructed mask now says it has found a way to create a replicated face capable of unlocking Apples latest and greatest biometric using a series of surreptitiously snagged photographs. .Apple has copped to the fact that Face ID, for all its technical prowess, isnt perfect. It can be tricked by twins. For most people, however, that security threat is a nonexistent one. But what about masks? The Cupertino-based company assured customers that it had designed the biometric-powered safeguard with that attack in mind — yet the researchers at Bkav are here to rain on that particular parade. .In this new experiment, Bkav used a 3D mask (which costs ~200 USD), made of stone powder, with glued 2D images of the eyes, researchers explained in a blog post. Bkav experts found out that stone powder can replace paper tape (used in previous mask) to trick Face ID AI at higher scores. The eyes are printed infrared images — the same technology that Face ID itself uses to detect facial image. These materials and tools are casual for anyone..To make matters worse, getting the data needed to construct the mask could be done without the targets knowledge, the researchers wrote — no elaborate face scans or up-close photographs required..Bkav researchers said that making 3D model is very simple, the blog post noted. A person can be secretly taken photos of in just a few seconds when entering a room containing a pre-setup system of cameras located at different angles. Then, the photos will be processed by algorithms to make a 3D object. .Just how easy would it be for someone to pull this off in the real world? We reached out to Apple for comment, but received no response as of press time. Well update this post when and if we hear back..[When] targeting a person, [an attacker] can pre-install HD cameras of 3D scanning system in a meeting room or in an exhibition to secretly take photos of the target, explained a company spokesperson over email. It takes only around 2s to get photos of the target’s face. Very fast..As for making the mask itself? [We] printed only one 3D mask, only one infrared image, the spokesperson noted. We cut the eyes’ parts and pasted them on the mask, only one time. We succeeded at first try. There was no modification needed..Should iPhone X owners be worried about this? Well, maybe. Its not like a common thief is going to go to the trouble of surreptitiously scanning your face before (or after) hes jacked your phone from you on your subway commute. .However, if someone wanted access to a specific something on your phone — and felt that it was worth the time and effort of building a mask — you might have a reason to be concerned. Although, of course, using an alphanumeric password in lieu of Face ID would negate that concern. .If anything, Bkavs findings are a reminder that no form of consumer biometric is infallible, and that as security improves, so do the tools and techniques hackers use to beat it. \n"}
{"prompt":"After realizing that robots were the core reason why Tesla had delays, Musk hired more humans to assembly lines\n\n###\n\n","completion":" Yes, robots, which are designed to help build lots and lots of Teslas electric vehicle at an insane speed, are to blame for why customers who pre-ordered the Model 3 still havent gotten them yet..When asked if using more humans instead of robots would help speed up production, Musk told CBS that it would and that they need more humans working on the assembly lines..He tweeted about how he had gone back to sleeping at the factory and CBS confirmed his surprisingly ordinary sleeping situation as nothing more than a narrow and uncomfortable couch and a sleeping bag in a conference room. Yep, Musk the billionaire, is spending his nights in a sleeping bag at the office..Despite using more technology to further his endless ambitions, including electricity-powered vehicles, landing reusable rockets, sending Teslas into deep space, helping solve traffic jams with the Hyperloop, selling Boring baseball caps, and flamethrowers, Musk has cautioned against over-relying on technology before..Musk has said AI, if abused or developed incorrectly, could be the downfall of humanity. Hes even gone as far as saying AI could trigger World War III..Its ironic then that over-using technology – robots and automation in this case – is what screwed up the Model 3s production. If Musk has learned anything after this, its that he should start applying his life philosophy to his work sooner rather than later.\n"}
{"prompt":"A group of hackers has managed to trick Tesla’s first-generation Autopilot into accelerating from 35 to 85 mph with a modified speed limit sign that humans would be able to read correctly.\n\n###\n\n","completion":" A group of hackers has managed to trick Tesla’s first-generation Autopilot into accelerating from 35 to 85 mph with a modified speed limit sign that humans would be able to read correctly. .McAfee Advanced Threat Research (ATR) has a specific goal: identify and illuminate a broad spectrum of threats in today’s complex landscape. With model hacking, the study of how adversaries could target and evade artificial intelligence, we have an incredible opportunity to influence the awareness, understanding and development of more secure technologies before they are implemented in a way that has real value to the adversary..With that in mind, they decided to target MobilEye’s camera system since it’s deployed in over 40 million vehicles, including Tesla’s first-generating Autopilot vehicles, which were used for this specific experiment..They decided to try to modify speed limit signs in ways that a human would be able to still know the limit, but the automated system could get confused:.The ultimate finding here is that we were able to achieve the original goal. By making a tiny sticker-based modification to our speed limit sign, we were able to cause a targeted misclassification of the MobilEye camera on a Tesla and use it to cause the vehicle to autonomously speed up to 85 mph when reading a 35-mph sign. For safety reasons, the video demonstration shows the speed start to spike and TACC accelerate on its way to 85, but given our test conditions, we apply the brakes well before it reaches target speed. It is worth noting that this is seemingly only possible on the first implementation of TACC when the driver double taps the lever, engaging TACC. If the misclassification is successful, the autopilot engages 100% of the time. This quick demo video shows all these concepts coming together..McAfee disclosed the findings to Tesla on September 27th, 2019 and MobilEye on October 3rd, 2019. Both vendors indicated interest and were grateful for the research but have not expressed any current plans to address the issue on the existing platform. MobilEye did indicate that the more recent version(s) of the camera system address these use cases..In 2016, we reported on a Chinese white-hat hacker group, the Keen Security Lab at Tencent, managing to remotely hack the Tesla Model S through a malicious Wi-Fi hotspot. It is believed to be the first remote hack of a Tesla vehicle..I bet Tesla is not in a hurry to fix it since it only affects the first generation Autopilot, but I think it’s important for owners to at least be aware that it could happen..Tesla is a transportation and energy company. It sells vehicles under its Tesla Motors division and stationary battery pack for home, commercial and utility-scale projects under its Tesla Energy division..The Autopilot is Teslas advanced assisted driving program with features like Autosteer, Autopark, and Trafic-Aware Cruise Control (TACC).\n"}
{"prompt":"Researchers at The Allen Institute for AI in collaboration with The University of Washington created an AI-based tool that rewrites text to correct potential gender bias in character portrayals\n\n###\n\n","completion":" A 2019 study coined a very interesting term — ‘the Cinderella complex’. The authors of this study analysed 7226 books, 6000 movie synopsis, and 1,100 movie scripts and found that the words used to associate with the male and female characters reeked of gender bias. The lives of the male characters were adventure and aspiration-oriented, whereas the female characters were more passive and romantic-relationship oriented..This is just one of the countless studies that show how gender bias, a lot of time unintentional and a product of societal conditioning, creeps in popular text and media. This, in turn, sets an incorrect narrative. Keeping this in view, the researchers at the Allen Institute for Artificial Intelligence in collaboration with the University of Washington created an AI-based tool that rewrites text to correct potential gender bias in character portrayals. Christened PowerTransformer — an encoder-decoder model developed based on a pre-trained language model. .The narrative in popular media often assigns a stereotypical colour to gender roles. This problem is widely recognised, and there have been several attempts at removing such biases. .One such method is controllable text revision which rephrases text to a targeted style or framing. The conventional controllable text revision has to overcome three main challenges — editing beyond just surface-level paraphrasing, the revision should not make unnecessary changes to the underlying meaning of the text, and models must learn to debias and learn text without any supervised data, thus preventing straightforward machine translation-style modelling..To overcome above-listed challenges, the researchers at Allen Institute for Artificial Intelligence and the University of Washington jointly formulated a new controlled text revision task called controllable debiasing that studies portrayal biases through connotation frames of power and agency, which capture knowledge about the implied power dynamics with respect to verbs. .The researchers have introduced a new controllable debiasing approach called PowerTransformer. In this approach, reconstruction and paraphrasing objectives are combined to overcome the lack of parallel supervised data. The PowerTransformer model uses connotation frame knowledge both at training time using control tokens, and during the generation, time using agency-based vocabulary boosting. .Further, this model uses an OpenAI-GPT transformer model as the base. PowerTransformer is similar to the GD-IQ tool that was developed by the University of Southern California Viterbi School of Engineering. GD-IQ uses AI techniques to analyse the text of a script to identify the number of males and females and whether they represent the population at large. PowerTransformer introduces an improvement over GD-IQ by rephrasing text using machine learning. For example, ‘Alice daydreamed about a doctor’ is rewritten as ‘Alice pursued her dream to be a doctor’. Such rephrasing gives the character more authority..After experimenting, the researchers studied 16,763 characters from 767 modern English movie scripts and found that of these characters, 68% were inferred to be men and the remaining 32% women. The researchers attempted at mitigating gender biases in these portrayals by attempting to re-balancing the agency level of female characters to be at par with male characters using PowerTransformer. .The model proved to be successful in increasing the positive agency and decreasing the passiveness associated with the female characters. “Our findings show promise for using modern NLP tools to help mitigate societal biases in text,” noted the researchers. Additionally, they also cautioned that this was a pilot study, and the model would still require human intervention in automatically rewriting the entire movie..This tool has the potential to help authors and scriptwriters in writing stories or movie plots by providing different framings for alternative portrayals of characters. This could conclusively help in stereotypical portrayals of females and debunk gender roles in society which is heavily influenced by media..Moment in Time is one of the biggest human-commented video datasets catching visual and discernible short occasions created by people, creatures, articles and nature. It was developed in 2018 by the researchers: Mathew Monfort, Alex Andonian, Bolei Zhou and Kandan Ramakrishnan. The dataset comprises more than 1,000,000 3-second recordings relating to 339 unique action words\n"}
{"prompt":"Lui had to train her algorithm with images that more accurately represented the skin-colour spectrum, from the palest whites to the darkest browns. To this end, she issued call outs encouraging volunteers of all skin tones to submit photos of themselves to aid in her mission to “change the standard of beauty to match the full range of diversity in human skin.”\n\n###\n\n","completion":" Atima Lui was in primary school when she first learned that “nude” is not universal. Now 30, she still recalls playing with a white friend’s makeup and struggling to find colours that complemented her rich skin tone. “I would try to put [her makeup] on and it would just make me look like a clown,” says Lui, who is of Sudanese and African-American descent. “I think back to growing up and how my mother barely wore makeup. Now I know its because makeup just wasnt made for her.”.The cosmetics landscape has long been unfriendly terrain for anyone on the wrong side of beige. Before Rihanna introduced her groundbreaking Fenty Beauty line with 40 shades of foundation in 2017, pushing competing brands to diversify their palettes or face public backlash, people with darker skin had few accessible options that matched and enhanced their complexion..But what Rihanna has done to address the issue of foundation shade selection, Lui is now hoping to do for colour matching – finding the perfect shade of makeup is usually left to guesswork or performed by associates on the beauty department floor. With her computer vision tool, Nudemeter, users simply upload a selfie and complete a short quiz, and an algorithm suggests the product that best matches their skin tone..Lui first conceived of the idea in 2016, during her final year at Harvard Business School, as a tool to empower dark-skinned shoppers to make purchases that helped boost their confidence. “I just went back to being a Black woman growing up in Topeka, Kansas, and just not feeling beautiful, not feeling like the standard of intelligence, not feeling good enough,” she explains. “Beauty is undervalued as a source of power in the world.”.But developing Nudemeter was no easy undertaking. The world of facial recognition technology is as guilty of light-skin bias as the beauty industry. A 2018 MIT study, led by Algorithmic Justice League founder Joy Buolamwini, found that commercial artificial intelligence systems had error rates as high as 35 per cent when identifying the features of darker-skinned women, compared to less than one per cent for lighter-skinned men – a discrepancy attributed to datasets “overwhelmingly composed of lighter-skinned subjects.”.To avoid this problem, Lui had to train her algorithm with images that more accurately represented the skin-colour spectrum, from the palest whites to the darkest browns. To this end, she issued call outs encouraging volunteers of all skin tones to submit photos of themselves to aid in her mission to “change the standard of beauty to match the full range of diversity in human skin.” Once she had a dataset in place, she reached out to Michael Brown and Mahmoud Afifi at York University in Toronto, who specialise in color analysis and digital image processing, to make sure the algorithm could deduce the user’s true skin tone, regardless of their device or the conditions in which their photo was taken..“[Our phones] are really intended to create beautiful images, not images with color accurate measurements… which makes the challenge that Im working on that much more difficult,” Lui says. “Its all about using AI to predict the color of the real scene that is depicted in the image, and not actually measuring the color from the pixels.”.The potential for this technology hasn’t gone unnoticed. In 2018, beauty behemoth Coty, whose brands include Rimmel, Max Factor and Kylie Cosmetics, awarded Nudemeter the grand prize for their Digital Accelerator Start-Up Program, and helped Lui refine and stress test her algorithm. Last year, Spktrm Beauty, an independent brand targeting shoppers with darker skin, became the first to utilise Nudemeter on its website, and in May, hosiery company Nude Barre introduced the app to help shoppers pick out the right tights for them..Looking to the future, Lui hopes to see further growth on the colour-matching side, but also sees potential for her technology beyond that. “I think theres power in using it for opportunities like virtual makeup try-on, or virtual glasses try-on, or even improving Instagram filters,” she says. “Its about feeling seen, feeling beautiful and having fun.”.She also envisions someday sharing her dataset (which is proprietary) with other companies attempting to create more inclusive AI technologies and combat existing biases. But in the meantime, her priority is refining and challenging the Nudemeter algorithm to be as inclusive as possible..“Im really proud of how well my technology can measure the skin tones, undertones and differences of dark-skinned women… But how is the technology reading the faces of people with vitiligo? What about people who are bald and have hair loss? What about people who are over 70 years old and might have a lot of wrinkles?” she says. “This work of creating inclusive and representative technology is never done.”\n"}
{"prompt":"The A.I. system is able to outperform other state-of-the-art forecasting methods, delivering up to an 11% increase in accuracy and predicting influenza outbreaks up to 15 weeks in advance.\n\n###\n\n","completion":" Predicting influenza outbreaks just got a little easier, thanks to a new AI-powered forecasting tool developed by researchers at Stevens Institute of Technology. .By incorporating location data, the AI system is able to outperform other state-of-the-art forecasting methods, delivering up to an 11% increase in accuracy and predicting influenza outbreaks up to 15 weeks in advance. .Past forecasting tools have sought to spot patterns by studying the way infection rates change over time but Yue Ning, who led the work at Stevens, and her team used a graph neural network to encode flu infections as interconnected regional clusters. That allows their algorithm to tease out patterns in the way influenza infections flow from one region to another, and also to use patterns spotted in one region to inform its predictions in other locations. .“Capturing the interplay of space and time lets our mechanism identify hidden patterns and predict influenza outbreaks more accurately than ever before,” said Ning, an associate professor of computer science. “By enabling better resource allocation and public health planning, this tool will have a big impact on how we cope with influenza outbreaks.”.Ning and her team trained their AI tool using real-world state and regional data from the U.S. and Japan, then tested its forecasts against historical flu data. Other models can use past data to forecast flu outbreaks a week or two in advance, but incorporating location data allows far more robust predictions over a period of several months. Their work is reported in the Oct. 19 – 23 Proceedings of the 29th ACM International Conference on Information and Knowledge Management. .“Our model is also extremely transparent — where other AI forecasts use ‘black box’ algorithms, we’re able to explain why our system has made specific predictions, and how it thinks outbreaks in different locations are impacting one another,” Ning explained..In the future, similar techniques could also be used to predict waves of COVID-19 infections. Since COVID-19 is a novel virus, there’s no historical data with which to train an AI algorithm; still, Ning pointed out, vast amounts of location-coded COVID-19 data are now being collected on a daily basis. “That could allow us to train algorithms more quickly as we continue to study the COVID-19 pandemic,” Ning said..Ning is now working to improve her influenza-forecasting algorithm by incorporating new data sources. One key challenge is figuring out how to account for public health interventions such as vaccination education, mask-wearing and social distancing. “It’s complicated, because health policies are enacted in response to outbreak severity, but also shape the course of those outbreaks,” Ning explained. “We need more research to learn about how health policies and pandemics interact.”.Another challenge is identifying which data genuinely predicts flu outbreaks, and which is just noise. Ning’s team found that flight traffic patterns don’t usefully predict regional flu outbreaks, for instance, but that weather data was more promising. “We’re also constrained by the information that’s publicly available,” Ning said. “Having location-coded data on vaccination rates would be very helpful, but sourcing that information isn’t easy.” .So far, the AI tool hasn’t been used in real-world health planning, but Ning said that it’s just a matter of time until hospitals and policymakers begin using AI algorithms to deliver more robust responses to flu outbreaks. “Our algorithm will keep learning and improving as we collect new data, allowing us to deliver even more accurate long-term predictions,” Ning said. “As we work to cope with future pandemics, these technologies will have a big impact.”\n"}
{"prompt":"Hundreds of refugees in the Balkans and Middle East help with AI through startup\n\n###\n\n","completion":" Single mother-of-two Sara Faizi from Afghanistan faced a dead end when she arrived in Bulgaria in 2018: the former bank branch operations manager needed a job but neither spoke the local language nor had any contacts..An energetic Bulgarian, Iva Gumnishka, 25, and her social enterprise Humans In The Loop (HITL) lent her a hand, linking Faizi to work created by the booming machine learning and artificial intelligence industry..The 31-year-old is among an estimated 1,500 to 2,000 refugees -- mostly from Afghanistan, Iraq and Syria -- who have settled over the past five years in Bulgaria, the EUs poorest member where state aid for them is almost non-existent..It includes the collection, sorting and categorisation of various types of images and data necessary to power algorithms essential to AI technology, Gumnishka told AFP in her enterprises tiny office and classroom in Sofia..The data is then used for products ranging from augmented reality games to smart drones, CCTV face recognition, or even self-driving cars for clients from around the world, she added..HITL grew out of the English and digital skills classes that the Bulgarian started giving to a handful of refugees in Sofia two years ago in a bid to help them find work as freelancers..Gumnishka then decided to tap this very specific niche to directly link refugees to work from start-ups in Europe and the US that did not require any particular skills or knowledge of the language..Having no technical background herself, she remembered with a smile an early conversation with a client asking her if the company was able to provide a certain AI-related task..I answered: Yes and then had to Google it to find out what it was, said Gumnishka, who studied human rights at Columbia University in New York before returning to Bulgaria..But she has become an expert on the work since then, often personally training her team that grew to include about 100 refugees in Sofia within a year and a half..As the volume of work from clients expanded, Gumnishka decided it would be even more beneficial to send work directly to conflict-affected regions..HITL teamed up with Roia, a non-profit organisation training young people in Syria and Syrian refugees in Turkey, and WorkWell, which organises various digital courses for refugees and other vulnerable people in Iraq..Together, the three organisations have so far trained and employed on various projects 150 people in Syria, Turkey and Iraq, mostly refugees, women and youngsters..The best thing about working on projects is that it isnt tiring and you can work anywhere at any time, Syrian refugee Shyar Qader Ali, who lives with his family in a camp in Iraq, said in comments emailed by WorkWell..Roia CEO Khaled Shaaban said the HITL projects have opened up new employment prospects for youngsters, whose only other options were jobs on the black market or working for donors or armed groups..WorkWell figures emailed to AFP showed that 76 workers made approximately $12,500 (11,200 euros) from completing seven projects between May and December 2019..Back in Sofia, Gumnishka insisted she never used the fact that HITL works with refugees as a selling point to clients and added that the high quality of the work ensured that all workers would be ethically rewarded..We mostly insist that these people are really very well prepared and trained, that they work in small dedicated teams, that they have supervisors working with them, so our quality that we provide to our clients is the most important thing, she said.\n"}
{"prompt":"Project Guideline is a Google-sponsored research project that utilizes a mobile application and audio cues to guide runners with visual impairments. Computer vision is used to identify pre-painted lines and identify diversions from it. \n\n###\n\n","completion":" Editors Note: At Google Research, we’re interested in exploring how technology can help improve people’s daily lives and experiences. So it’s been an incredible opportunity to work with Thomas Panek, avid runner and President & CEO of Guiding Eyes for the Blind, to apply computer vision for something important in his everyday life: independent exercise. Project Guideline is an early-stage research project that leverages on-device machine learning to allow Thomas to use a phone, headphones and a guideline painted on the ground to run independently. Below, Thomas shares why he collaborated with us on this research project, and what the journey has been like for him..I’ve always loved to run. Ever since I was a boy, running has made me feel free. But when I was eight-years-old, I noticed that I couldn’t see the leaves on a tree so well, and that the stars in the night sky began to slowly disappear—and then they did forever. By the time I was a young adult, I was diagnosed as legally blind due to a genetic condition. I had to rely on a cane or a canine to guide me. For years, I gave up running..Then I heard about running with human guides, and I decided to give it a try. It gave me a sense of belonging, holding a tether and following the guide runner in front of me. I even qualified for the New York City and Boston Marathons five years in a row. But as grateful as I was to my human guides, I wanted more independence. So in 2019, I decided to run the first half-marathon assisted only by guide dogs..But I know it’s not possible for everyone to have a brilliant, fast companion like my guide dog, Blaze. I run an organization called Guiding Eyes for the Blind, and we work tirelessly to help people with vision loss receive running guide dogs that can help them live more active and independent lives. The problem is that there are millions more people with vision loss than there are available guide dogs. So I started asking a question: “Would it be possible to help guide a blind runner, independently?” .In the fall of 2019, I asked that question to a group of designers and technologists at a Google hackathon. I wasn’t anticipating much more than an interesting conversation, but by the end of the day they’d built a rough demo that allowed a phone to recognize a line taped to the ground, and give audio cues to me while I walked with Blaze. We were excited, and hopeful to see if we could develop it into something more..We began by sketching out how the prototype would work, settling on a simple concept: I’d wear a phone on a waistband, and bone-conducting headphones. The phone’s camera would look for a physical guideline on the ground and send audio signals depending on my position. If I drifted to the left of the line, the sound would get louder and more dissonant in my left ear. If I drifted to the right, the same thing would happen, but in my right ear. Within a few months, we were ready to test it on an indoor oval track. After a few adjustments, I was able to run eight laps. It was a short distance, and all with my Google teammates close by, but it was the first unguided mile I had run in decades..Our next step was to see if the tech could work where I love running most: in the peace and serenity of a park. This brought a whole new batch of challenges to work through: variables in weather and lighting conditions and the need for new data to train the model, for starters. After months of building an on-device machine learning model to accurately detect the guideline in different environments, the team was finally ready to test the tech outside for the first time..I’d been waiting 25 years to run outdoors, on my own. I stood at the start of the guideline, hopping up and down with excitement. When the team gave me the go-ahead, I began sprinting on my toes, as fast as my legs could carry me, down the hill and around a gentle bend in the road. As I tightened my form, my stride was getting more confident and longer with every step. I felt free, like I was effortlessly running through the clouds..When I arrived at the finish line, I was completely overcome with emotion. My wife, Melissa, and my kids hugged me. My guide dog Blaze licked the salt off of my hand. They were happy for me, too. For the first time in a lifetime, I didn’t feel like a blind man. I felt free..Today, we’re testing this technology further. I’ll be attempting to run NYRR’s Virtual Run for Thanks 5K along a line temporarily painted in Central Park in New York City. I want to thank NYRR, NYC Department of Parks & Recreation, Central Park Conservancy, NYPD, NYC Department of Sanitation and the NYC Department of Transportation for helping to make today’s 5K run possible. We want to see how this system works in urban environments, just one of the many challenges to complete before it can be used more widely. .Collaborating on this project helped me realize a personal dream of mine. I’m so grateful to the Google team, and whoever came up with the idea of a hackathon in the first place. I hope there will be more runs with Project Guideline in my future, and for many other runners as well..By sharing the story of how this project got started and how the tech works today, we hope to start new conversations with the larger blind and low-vision community about how, and if, this technology might be useful for them, too. As we continue our research, we hope to gather feedback from more organizations and explore painting guidelines in their communities. To learn more, please visit: goo.gle\/ProjectGuideline.\n"}
{"prompt":"The Texas Higher Education Coordinating Board has launched ADVi, a chatbot that uses AI to answer questions about FAFSA and financial aid while also providing relevant resources. \n\n###\n\n","completion":" The number of Free Application for Federal Student Aid, or FAFSA applicants could increase for the first time in recent years as families continue to deal with financial and health hardships caused by the COVID-19 pandemic..A new virtual adviser may help Texas students with their college applications and information on the Free Application for Federal Student Aid..ADVi — short for “adviser” — is a virtual chat bot that uses artificial intelligence to answer questions about college applications, financial aid, FAFSA and related deadlines via text message. Students are required to complete the FAFSA to receive federal and some state aid, along with on-demand resources..Created by the Texas Higher Education Coordinating Board and the Dallas nonprofit Educate Texas, the resource is part of Future Focused Texas, an initiative aimed toward maintaining enrollment rates at colleges throughout the state..“We’re trying to use the same techniques social media (platforms) use to engage students,” said John Fitzpatrick, the executive director of Educate Texas..Students and counselors can use the chat bot to ask questions about applications and the FAFSA at any time and as many times as they need, said Jerel Booker, the coordinating board’s assistant commissioner for college readiness and success. The chat box also directs students to websites for applications or admissions offices, and it covers topics such as writing college essays, creating a resume, recommendations and application submissions. For more complicated questions, live counselors are available during the day. Students can opt into the ADVi service by texting the word “college” to the number 512-829-3687..Texas Higher Education Commissioner Harrison Keller said the initiative began pre-COVID but that the need for college application support resources was amplified during the pandemic..College enrollment across Texas is down 3 percent from last fall, and the state also received 18 percent fewer completed FAFSAs in October, compared with last year..As of Nov. 20, 874,960 FAFSA applications were filed across the country for the high school class of 2021 — 16 percent fewer than the number completed this time last year for the class of 2020, according to the National College Attainment Network..“That’s a warning sign — a cause for concern,” Keller said, noting that a decrease in FAFSA completion can indicate a decrease in enrollments. “What we’re concerned about is, we have many more students not enrolling across the state than we’d expect or hope to see right now.”.John Fitzpatrick, executive director of Educate Texas, said there are more than 350,000 Texas high school seniors for the 2019-2020 academic year at risk of not being able to enroll in college..African-American, Hispanic and low-income students — many of which are underrepresented at higher education institutions — are particularly at risk, Keller added..Fitzpatrick said that “our high school counselors and college admission officers are working so hard to engage 350,000 students, but it’s incredibly hard in time of COVID-19.”.College advisers and recruiters have had to pivot, resorting to online or virtual techniques to reach their students. Meanwhile, students and their families have experienced hardships of their own during the pandemic, which might have rendered filling out an application or applying to college a lower priority. But for those still looking to attend college, the FAFSA is particularly important because it lets students assess what financial aid is available to them, which might determine which colleges they will or can afford to attend..The ADVi platform could be especially helpful for students who are filling out the application for the first time or for the many students who cannot get in-person assistance or reminders from other students and counselors when they need it, the coordinating board’s Booker added..The goal, however, is “not to totally eliminate human contact, but to help the counselors do it online” during a time when many resources are being offered in remote formats or virtually, Booker said..Brittany Britto is the features enterprise reporter at the Houston Chronicle, reporting in-depth stories focused on marginalized communities, underrepresented neighborhoods, histories and sub-cultures in and around the Houston area. Shell work to amplify fascinating and untold stories in one of the most diverse cities in the country and dig into what it means to be a Houstonian. .Brittany has been recognized for her cultural coverage by the Society for Features Journalism. In 2018, she was named a Penny Bender Fuchs Diversity Fellow for the national features organization and won four awards - a tie for the most won in one year in recent SFJ history -- for her diverse portfolio and noteworthy features on Baltimores distinct culture. .Brittany is a two-time graduate of the University of Maryland, College Park, with a masters in multiplatform journalism and a bachelors in English. .Sanger ISD Challenge Program kids are the so-called worst of the worst behaved. And its here that they find a home. A place where they are listened to and respected.\n"}
{"prompt":"A bank apologized Saturday after a 94-year-old woman had to be lifted up by her son so she could use a facial-recognition system to active her social security card. And on Monday, another elderly woman was rejected when she tried to use cash to pay for her medical insurance. An employee explained that she could pay on her phone or call a relative if she needed help.\n\n###\n\n","completion":" While younger Chinese have embraced the conveniences of mobile payments, food delivery, and ride-hailing, the country’s elderly are at risk of being left behind by such increasingly pervasive technologies..This was the message of the State Council, China’s Cabinet, in a notice Tuesday addressed to the central and local governments, urging them to ensure that services remain available to people who seldom go online..China’s number of internet users has steadily risen over the years and now includes more than 90 million people over 60 years old. However, rapidly aging China has 254 million people in that age cohort, which means a majority of them may face issues when they want to travel, go to the hospital, or simply buy groceries — all of which increasingly involve the use of smartphone apps..The notice, dated Nov. 15, was published to “solve the difficulties that old people will confront when using smart technology,” as well as to “make old people share in the achievements of digital developments.” It said places frequented by elderly people should not refuse cash payments, and that online services should keep older users in mind..The document’s publication follows several recent videos of elderly people being inconvenienced by technology that were widely shared on social media. A bank apologized Saturday after a 94-year-old woman had to be lifted up by her son so she could use a facial-recognition system to active her social security card. And on Monday, another elderly woman was rejected when she tried to use cash to pay for her medical insurance. An employee explained that she could pay on her phone or call a relative if she needed help..A video screenshot shows a man holding his 94-year-old mother so a facial-recognition system can activate her social security card at a bank in Hubei province, 2020. From Weibo.Online, people showed appreciation for the State Council’s notice. “Everyone will grow old one day. The day will come when we will all be out of step with technology,” read a comment with over 40,000 likes..Just how difficult it can be for elderly Chinese to navigate China’s increasingly app-based daily life was also made clear during the COVID-19 outbreak earlier this year, when people relied on food-delivery services to eat and health-tracking apps to move around..The notice stressed that health-tracking apps, which commonly show a green QR code for people who have not recently been to areas with active COVID-19 clusters, should not be the only way for people to prove they should be allowed to travel around the country..“Conditional regions and places should set up ‘non-health code channels’ for old people who don’t use smartphones,” the notice said, adding that, in the future, emergency responses should take older people into consideration..A resident from Zhangjiagang in the eastern Jiangsu province, surnamed Song, told Sixth Tone that her 52-year-old mother knows how to operate a phone to show her green health code. However, she recently ran into trouble when visiting a local hospital, as her phone had run out of battery. “She even had a negative nucleic acid test result, but she still wasn’t allowed to use the elevator,” Song said..According to the notice, by the end of 2022, China should have established a long-term mechanism to solve the problems of the “digital divide” for old people..(Header image: A elderly man attempts to use a cashless payment service during the second annual Digital China Summit in Fuzhou, Fujian province, May 7, 2019. People Visual)\n"}
{"prompt":"He aims to develop an ‘AI clinician’ colleague to aid doctors and clinicians and relieve pressure on the NHS. The system could also be used in sectors such as aerospace or energy, where accurate decision-making under high-pressure conditions is crucial.\n\n###\n\n","completion":" Professor Faisal, from the Departments of Computing and Bioengineering at Imperial College London, has been announced as one of 15 Turing AI Fellows by UKRI. The five-year awards are designed to accelerate and support the careers of the best and brightest AI (artificial intelligence) researchers, enabling them to become world-leading researchers in the five years of the award..He aims to develop an ‘AI clinician’ colleague to aid doctors and clinicians and relieve pressure on the NHS. The system could also be used in sectors such as aerospace or energy, where accurate decision-making under high-pressure conditions is crucial..Professor Faisal said: “This UKRI Turing AI Fellowship recognises many years efforts by my team and myself to develop machine learning methods that can learn complex skills from human interactions. Core to our AI technology is that it is focussed on augmenting human decision-makers, such as medical doctors, and empowering them by giving them an ‘AI colleague’ to interact with who has distilled the experiences from millions of treated patients.”.Professor Faisal focuses on the development of algorithms that can learn from human behaviour and human interactions to learn complex skills. With this new fellowship, he will be able to focus and develop an ‘AI clinician’ that can tirelessly watch over patients and recommend medical interventions to human clinicians, such as prescribing drugs, changing doses or other interventions..The system will learn this complex skill by distilling the data from thousands of real doctors treating millions of real patients into one system of AI best clinical practice. The AI will automatically present the recommendations for actions, and the reasoning behind these recommendations, in a way that decision-makers can understand, helping human doctors to make the best decisions on the course of action for a patient..Professor Faisal received substantial industry support for this fellowship from healthcare, but also the energy and aerospace sector, which are keen to develop ways of deploying this AI technology in their respective industries..Professor Faisal’s labs, the Brain & Behaviour lab and the Behaviour Analytics Lab, operate at the interface of artificial and human intelligence. He is one of the few computer scientists worldwide that leads clinical trials to test and deploy their own algorithms. He is the director of the £20m UKRI Centre for Doctoral Training in AI for Healthcare and the elected Speaker of the Imperial College Cross-Faculty network in AI..Named after British AI pioneer Alan Turing, the £20 million fellowship scheme will be delivered by Engineering and Physical Sciences Research Council (EPSRC), part of UK Research and Innovation (UKRI), in partnership with the Alan Turing Institute and the UK government’s Office for Artificial intelligence..As a result of the £20m government investment, Fellows will work with academia and industry to help elevate their world-class research and transfer their innovations from the lab to the real world. These innovations have the potential to change how people live, work and communicate, helping to place the UK at the forefront of the AI and data revolution. .Science Minister, Amanda Solloway said: “The UK is the birthplace of artificial intelligence and we therefore have a duty to equip the next generation of Alan Turings, like London’s Professor Aldo Faisal, with the tools that will keep the UK at the forefront of this remarkable technological innovation..“London has a rich and unique history of innovating and this inspiring AI project we are backing will assist doctors and clinicians when treating patients, helping to relieve pressure on our brilliant NHS, while cementing the UK’s status as a world leader in AI and data.”.  Healthcare,  \t\t\t\t\t\t\tComms-strategy-Real-world-benefits,  \t\t\t\t\t\t\tGlobal-challenges-Data,  \t\t\t\t\t\t\tArtificial-intelligence,  \t\t\t\t\t\t\tBig-data   See more tags .Your comment may be published, displaying your name as you provide it, unless you request otherwise. Your contact details will never be published.\n"}
{"prompt":"In the absence of any judicial oversight, there are also concerns of backdoor access to the system for illegal facial recognition surveillance on potential targets and stifling of anti-regime comments and any peaceful civil action. Local rights group, Unwanted Witness, has previously called for the observance of international human rights law in the implementation of the project to safeguard human rights, freedoms, and democracy in the country.\n\n###\n\n","completion":" Long before the Nov. 18-19 protests in Uganda that left more than 50 people dead, opposition politicians, and local activists had warned about the potential abuse and human rights implications of an invasive surveillance system bought by the government last year from China’s telecoms giant Huawei..The fear was, that in the hands of corrupt public officials or under a repressive regime the integrated system which uses facial recognition and other artificial intelligence systems but is also able to check vehicle license plates and monitor social media, would be used to suppress individual freedoms of anyone in opposition to the government..The latest protests, which were triggered by the arrest of two presidential candidates hoping to put a halt to president Yoweri Museveni’s 34-year rule, seem to have confirmed those fears..Ugandan police officials have confirmed they are using the cameras supplied by Huawei which helped the force track down some of the more than 836 suspects they have arrested..In the absence of any judicial oversight, there are also concerns of backdoor access to the system for illegal facial recognition surveillance on potential targets and stifling of anti-regime comments and any peaceful civil action. Local rights group, Unwanted Witness, has previously called for the observance of international human rights law in the implementation of the project to safeguard human rights, freedoms, and democracy in the country..The national CCTV system installed by Huawei has 83 monitoring centers, 522 operators, and 50 commanders according to president Museveni who in a series of tweets has praised the effectiveness of the technology.  Authorities also plan to integrate the Huawei system with other Ugandan agencies including the tax body and the immigration department. In Jan. 2020, authorities started rolling out the second phase within 2,319 mapped countryside municipalities and major towns..A Quartz Africa source with the knowledge of the police operations says Huawei staff and other “experts” from China are still in the process of installing an “integrated” system part of a classified contract between the authorities in Kampala and Huawei to supply and install surveillance equipment in cities and towns throughout Uganda..In 2019, Ugandan police officials confirmed the government had paid at least $126 million as part of the deal which is more than the combined 2020 budgets ($108 million) of the ministries of ICT and Science & Technology ministries..Local and international rights groups say footage from the Huawei surveillance cameras has been used since 2019 to monitor political rallies and other events of  president Museveni’s opponents. The unregulated surveillance is characterized by pervasive location monitoring, facial recognition, biometric, and blanket data retention practices among others..Unlike in the West where there are security concerns about a Chinese company dominating 5G technology, Huawei has been broadly welcomed by African governments. Here it has played a key role in helping to build the telecoms infrastructure needed for a 21st century economy in several countries. But more recently its roles have expanded to other projects such as security for governments who are so inclined..Ahead of the 2016 polls, president Museveni’s government procured the services of a UK-based firm, Gamma Group, which delivered a surveillance technology reportedly used to spy on Museveni’s opponents in an operation dubbed “Fungua Macho”..A Wall Street Journal investigation last year suggested the government in Kampala used assistance from Huawei to hack into messages for the presidential candidate Robert Kyagulanyi, better known as Bobi Wine, prompting his arrest and detention. Bobi Wine, a popular musician who was elected to parliament in 2017, is extremely popular with young Ugandans and has been a thorn in the side of the Museveni establishment.\n"}
{"prompt":"\nThe analysis confirmed that disorders such as congenital heart birth defects or macrosomia, gestational diabetes and preterm birth can be detected earlier when artificial intelligence is used. In the latter case, studies into cases involving artificial intelligence found a correlation between the number of pre-term births and the environmental pollution to which the pregnant women had been previously exposed.\n\n###\n\n","completion":" Researchers from the University of Seville have carried out a rigorous and detailed analysis of how artificial intelligence has been used with pregnant women over the last twelve years.The analysis confirmed that disorders such as congenital heart birth defects or macrosomia, gestational diabetes and preterm birth can be detected earlier when artificial intelligence is used. In the latter case, studies into cases involving artificial intelligence found a correlation between the number of pre-term births and the environmental pollution to which the pregnant women had been previously exposed..\r                             Systems based on affective computing could allow emotional interaction with the pregnant woman and detect emotional changes and make it possible to offer guidance or recommendations, which the system would previously have received from doctors.\r                         .There is growing interest in the application of artificial intelligence in obstetrics and gynecology. These applications of AI can not only monitor womens health during pregnancy, but can also help to improve the universal provision of health services, especially in the most disadvantaged areas. This field therefore contributes to improving both individual and public health, says University of Seville researcher María del Carmen Romero..Furthermore, this work reveals the almost total lack of studies where emotions are taken into account as input parameters in risk prediction models in pregnancy (only 1.28% of the studies analyzed). Moreover, very few studies look closely at the pregnant womans mental health (only 5.1% of the studies analyzed), despite it having been shown that the womans psychological health is correlated with the risk of suffering certain diseases typical of pregnancy. Pregnancy is a vital state that brings with it the need for change and new learning, potentially causing anxiety, fear, worry, and even depression in women..Systems based on affective computing could allow emotional interaction with the pregnant woman and, for example, detect emotional changes and make it possible to offer guidance or recommendations, which the system would previously have received from doctors. This can make the patient feel safer and closer to her health service and can reduce the usual feelings of anxiety or worry that sometimes lead to physical problems..Given that there is previous scientific evidence that supports the idea that the emotional state and mental health of the pregnant woman can influence the occurrence of risks in pregnancy, our study highlights what is a very interesting multidisciplinary research niche for affective computing in the field of health and well-being of pregnant women, the researcher adds.. To use the full function of this web site, JavaScript needs to be enabled in your browser. This is how you enable JavaScript in your browser settings: Read instruction \n"}
{"prompt":"A new report from the Center for Democracy & Technology highlights how hiring technologies may screen out people with disabilities and fall far short of ADA standards. The report provides many actionable steps, such as not screening out people based on subjective personality traits, and diverting a percentage of all candidates into any “reasonable accommodation” to avoid stigmatizing disability.-UpTurn Newsletter\n\n###\n\n","completion":" Algorithm-driven hiring tools have grown increasingly prevalent in recent years. Thousands of job-seekers across the United States are now asked to record videos that employers mine for facial and vocal cues. They complete online tests or games that purport to evaluate their “personal stability,” optimism, or attention span. They submit resumes through online platforms that may reject them because of time gaps in their work histories, such as those resulting from cancer treatment..Employers using these tools seek a fast and efficient way to process job applications in large numbers. They may also believe that algorithm-driven software will identify characteristics of successful employees that human recruiters would not identify on their own. But as these algorithms have spread in adoption, so, too, has the risk of discrimination written invisibly into their codes. For people with disabilities, those risks can be profound..The Americans with Disabilities Act (ADA) has explicit prohibitions against the use of hiring processes that discriminate on the basis of disability. First, the ADA requires that employment tests be provided in an accessible format, and if the format is not accessible, that reasonable accommodations be made available without prejudicing the candidate. For example, a test that requires spoken answers is not accessible for an applicant who does not speak because of paralysis or deafness. If an employer uses such a test, they have to evaluate disabled job-seekers in an alternative way that reasonably accommodates their disabilities..Second, the ADA presumptively disfavors hiring selection criteria that “screen out, or tend to screen out” disabled candidates. For example, a personality test may screen out some candidates with depression or anxiety; a game-based test may screen out a candidate because of their ADHD. If an employer uses selection criteria that screen out disabled candidates, the criteria must be “job-related” and “consistent with business necessity.” This means that employment tests must evaluate candidates on factors that are directly relevant to (and necessary for) the essential functions of the job..Many algorithm-driven hiring tools fall far short of these standards. Algorithm-driven hiring tools typically assess candidates based on how they perform on a given test compared to a model set of successful employees. Employers may be tempted to use these tools without stopping to consider what exactly they are testing for, or why – specifically, what traits are really being measured by an online game, and whether what is being measured is actually necessary to perform the essential functions of the job..Employers, vendors who create these hiring tools, regulators, job-seekers, and advocates need to better understand the risks of using algorithm-driven tools in hiring, and consider concrete steps to avoid these harms. This paper seeks to highlight how hiring tools may affect people with disabilities, the legal liability employers may face for using such tools, and concrete steps for employers and vendors to mitigate some of the most significant areas of concern. We hope it will serve as a resource for advocates, for regulators, and – above all – for those deciding whether to develop or use these tools to consider the risks of discrimination, and ultimately to ask if the tools are appropriate for use at all..CDT works to strengthen individual rights and freedoms by defining, promoting, and influencing technology policy and the architecture of the internet that impacts our daily lives..The content throughout this website that originates with CDT can be freely copied and used as long as you make no substantive changes and clearly give us credit. More on CDTs content reuse policy is available here.\n"}
{"prompt":"How corporations that expect algorithmic processing of their regulatory filings manipulate the text of their filings “to induce algorithmic readers to draw favorable conclusions about the content.” They do so by reducing their use of words with “negative sentiment” found in dictionaries used to train natural language processing algorithms - UpTurn Newsletter\n\n###\n\n","completion":" Growing AI readership, proxied by expected machine downloads, motivates firms to prepare filings that are friendlier to machine parsing and processing. Firms avoid words that are perceived as negative by computational algorithms, as compared to those deemed negative only by dictionaries meant for human readers. The publication of Loughran and McDonald (2011) serves as an instrumental event attributing the difference-in-differences in the measured sentiment to machine readership. High machine-readership firms also exhibit speech emotion assessed as embodying more positivity and excitement by audio processors. This is the first study exploring the feedback effect on corporate disclosure in response to technology. . The authors have benefited from discussions with Rui Albuquerque, Elizabeth Blankespoor (discussant), Emilio Calvano (discussant), Lauren Cohen (discussant), Will Cong (discussant), Ilia Dichev, Arup Ganguly (discussant), Jillian Grennan, Rebecca Hann, Bing Han, Kathleen Hanley (discussant), Gerard Hoberg (discussant), Byoung-Hyoun Hwang (discussant), Chris Hennessy, Alan Huang (discussant), Bin Ke (discussant), Michael Kimbrough, Leonid Kogan, Augustin Landier (discussant), Tim Loughran (discussant), Song Ma, Ville Rantala (discussant), Max Rohrer (discussant), Gustavo Schwenkler (discussant), Kelly Shue, Suhas Sridharan, Isabel Wang (discussant), Teri Yohn, Gwen Yu, Dexin Zhou, and comments and suggestions from participants in seminars and conferences at Columbia, ECB, EDHEC, Emory, Georgia State, Harvard, London Business School, Maryland, Michigan, Michigan State, Peking University, Stockholm Business School, Toronto, Utah, Washington, the NBER Economics of Artificial Intelligence Conference, the NBER Big Data and Securities Markets Conference, AFA 2022, the Pacific Center for Asset Management, the SOAR Symposium at Singapore Management University, the Third Bergen FinTech Conference at the NHH Norwegian School of Economics, Machine Learning and Business Conference at University of Miami, RCFS Winter Conference, 11th Financial Markets and Corporate Governance Conference, the China FinTech Research Conference, the Adam Smith Workshop, the Conference on Financing Innovation at Stevens Institute of Technology, FIRS 2021, the Cambridge Alternative Finance Sixth Annual Conference, 2021 CAPANA Research Conference, CICF 2021, and NFA 2021. The views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research. .In addition to working papers, the NBER disseminates affiliates’ latest findings through a range of free periodicals — the NBER Reporter, the NBER Digest, the Bulletin on Retirement and Disability, and the Bulletin on Health — as well as online conference reports, video lectures, and interviews.\n"}
{"prompt":"AlphaFold can predict the shape of proteins to within the width of an atom. The breakthrough will help scientists design drugs and understand disease.\n\n###\n\n","completion":" AlphaFold can predict the shape of proteins to within the width of an atom. The breakthrough will help scientists design drugs and understand disease..DeepMind has already notched up a streak of wins, showcasing AIs that have learned to play a variety of complex games with superhuman skill, from Go and StarCraft to Atari’s entire back catalogue. But Demis Hassabis, DeepMind’s public face and co-founder, has always stressed that these successes were just stepping stones towards a larger goal: AI that actually helps us understand the world..Today DeepMind and the organizers of the long-running Critical Assessment of protein Structure Prediction (CASP) competition announced an AI that should have the huge impact that Hassabis has been after. The latest version of DeepMind’s AlphaFold, a deep-learning system that can accurately predict the structure of proteins to within the width of an atom, has cracked one of biology’s grand challenges. “Its the first use of AI to solve a serious problem,” says John Moult at the University of Maryland, who leads the team that runs CASP..A protein is made from a ribbon of amino acids that folds itself up with many complex twists and turns and tangles. This structure determines what it does. And figuring out what proteins do is key to understanding the basic mechanisms of life, when it works and when it doesn’t. Efforts to develop vaccines for covid-19 have focused on the virus’s spike protein, for example. The way the coronavirus snags onto human cells depends on the shape of this protein and the shapes of the proteins on the outsides of those cells. The spike is just one protein among billions across all living things; there are tens of thousands of different types of protein inside the human body alone.      .In this year’s CASP, AlphaFold predicted the structure of dozens of proteins with a margin of error of just 1.6 angstroms—that’s 0.16 nanometers, or atom-sized. This far outstrips all other computational methods and for the first time matches the accuracy of techniques used in the lab, such as cryo-electron microscopy, nuclear magnetic resonance and x-ray crystallography. These techniques are expensive and slow: it can take hundreds of thousands of dollars and years of trial and error for each protein. AlphaFold can find a protein’s shape in a few days..The breakthrough could help researchers design new drugs and understand diseases. In the longer term, predicting protein structure will also help design synthetic proteins, such as enzymes that digest waste or produce biofuels. Researchers are also exploring ways to introduce synthetic proteins that will increase crop yields and make plants more nutritious..“It’s a very substantial advance,” says Mohammed AlQuraishi, a systems biologist at Columbia University who has developed his own software for predicting protein structure. “Its something I simply didnt expect to happen nearly this rapidly. Its shocking, in a way.”.“This really is a big deal,” says David Baker, head of the Institute for Protein Design at the University of Washington and leader of the team behind Rosetta, a family of protein analysis tools. “It’s an amazing achievement, like what they did with Go.”.Identifying a protein’s structure is very hard. For most proteins, researchers have the sequence of amino acids in the ribbon but not the contorted shape they fold into. And there are typically an astronomical number of possible shapes for each sequence. Researchers have been wrestling with the problem at least since the 1970s, when Christian Anfinsen won the Nobel prize for showing that sequences determined structure. .The launch of CASP in 1994 gave the field a boost. Every two years, the organizers release 100 or so amino acid sequences for proteins whose shapes have been identified in the lab but not yet made public. Dozens of teams from around the world then compete to find the correct way to fold them up using software. Many of the tools developed for CASP are already used by medical researchers. But progress was slow, with two decades of incremental advances failing to produce a shortcut to painstaking lab work.   .CASP got the jolt it was looking for when DeepMind entered the competition in 2018 with its first version of AlphaFold. It still could not match the accuracy of a lab but it left other computational techniques in the dust. Researchers took note: soon many were adapting their own systems to work more like AlphaFold..This year more than half of the entries use some form of deep learning, says Moult. The accuracy overall was higher as a result. Baker’s new system, called trRosetta, uses some of DeepMind’s ideas from 2018. But it still came a “very distant second,” he says..In CASP, results are scored using what’s known as a global distance test (GDT), which measures on a scale from 0 to 100 how close a predicted structure is to the actual shape of a protein identified in lab experiments. The latest version of AlphaFold scored well for all proteins in the challenge. But it got a GDT score above 90 for around two thirds of them. Its GDT for the hardest proteins was 25 points higher than the next best team, says John Jumper, who heads up the AlphaFold team at DeepMind. In 2018 the lead was around six points..A score above 90 means that any differences between the predicted structure and the actual structure could be down to experimental errors in the lab rather than a fault in the software. It could also mean that the predicted structure is a valid alternative configuration to the one identified in the lab, within the range of natural variation..According to Jumper, there were four proteins in the competition that independent judges had not finished working on in the lab and AlphaFold’s predictions pointed them towards the correct structures..AlQuraishi thought it would take researchers 10 years to get from AlphaFold’s 2018 results to this year’s. This is close to the physical limit for how accurate you can get, he says. “These structures are fundamentally floppy. It doesn’t make sense to talk about resolutions much below that.”.AlphaFold builds on the work of hundreds of researchers around the world. DeepMind also drew on a wide range of expertise, putting together a team of biologists, physicists and computer scientists. Details of how it works will be released this week at the CASP conference and in a peer-reviewed article in a special issue of the journal Proteins next year. But we do know that it uses a form of attention network, a deep-learning technique that allows an AI to train by focusing on parts of a larger problem. Jumper compares the approach to assembling a jigsaw: it pieces together local chunks first before fitting these into a whole..DeepMind trained AlphaFold on around 170,000 proteins taken from the protein data bank, a public repository of sequences and structures. It compared multiple sequences in the data bank and looked for pairs of amino acids that often end up close together in folded structures. It then uses this data to guess the distance between pairs of amino acids in structures that are not yet known. It is also able to assess how accurate these guesses are. Training took “a few weeks,” using computing power equivalent to between 100 and 200 GPUs..Dame Janet Thornton at the European Bioinformatics Institute in Cambridge, UK, has been working on proteins for 50 years. “That’s really as long as this problem has been around,” she said in a press conference last week. “I was beginning to think it would not get solved in my lifetime.”.Many drugs are designed by simulating their 3D molecular structure and looking for ways to slot these molecules into target proteins. Of course, this can only be done if the structure of those proteins is known. This is the case for only a quarter of the roughly 20,000 human proteins, says Thornton. That leaves 15,000 untapped drug targets. “AlphaFold will open up a new area of research.”.DeepMind says it plans to study leishmaniasis, sleeping sickness, and malaria, all tropical diseases caused by parasites, because they are linked to lots of unknown protein structures..One drawback of AlphaFold is that it is slow compared to rival techniques. AlQuraishi’s system, which uses an algorithm called a recurrent geometrical network (RGN), can find protein structures a million times faster—returning results in seconds rather than days. Its predictions are less accurate, but for some applications speed is more important, he says..Researchers are now waiting to find out exactly how AlphaFold works. “Once they describe to the world how they do it then a thousand flowers will bloom,” says Baker. “People will be using it for all kinds of different things, things that we cant imagine now.”.Even a less accurate result would have been good news for people working on enzymes or bacteria, says AlQuraishi: “But we have something even better, with immediate relevance to pharmaceutical applications.”.An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.\n"}
{"prompt":"Microsoft's new image captioning algorithm is able to outperform humans in some tests. The system will be used in Microsoft's Seeing AI, an assistant app for the visually impaired, and other products such as Word, Outlook, and Powerpoint. It can be used to generate alt-text for images which will increase accessibility.\n\n###\n\n","completion":" Microsoft has developed a new image-captioning algorithm that exceeds human accuracy in certain limited tests. The AI system has been used to update the company’s assistant app for the visually impaired, Seeing AI, and will soon be incorporated into other Microsoft products like Word, Outlook, and PowerPoint. There, it will be used for tasks like creating alt-text for images — a function that’s particularly important for increasing accessibility. .“Ideally, everyone would include alt text for all images in documents, on the web, in social media — as this enables people who are blind to access the content and participate in the conversation,” said Saqib Shaikh, a software engineering manager with Microsoft’s AI team in a press statement. “But, alas, people don’t. So, there are several apps that use image captioning as way to fill in alt text when it’s missing.”.These apps include Microsoft’s own Seeing AI, which the company first released in 2017. Seeing AI uses computer vision to describe the world as seen through a smartphone camera for the visually impaired. It can identify household items, read and scan text, describe scenes, and even identify friends. It can also be used to describe images in other apps, including email clients, social media apps, and messaging apps like WhatsApp. .Microsoft does not disclose user numbers for Seeing AI, but Eric Boyd, corporate vice president of Azure AI, told The Verge the software is “one of the leading apps for people who are blind or have low vision.” Seeing AI has been voted best app or best assistive app three years in a row by AppleVis, a community of blind and low-vision iOS users. .Microsoft’s new image-captioning algorithm will improve the performance of Seeing AI significantly, as it’s able to not only identify objects but also more precisely describe the relationship between them. So, the algorithm can look at a picture and not just say what items and objects it contains (e.g., “a person, a chair, an accordion”) but how they are interacting (e.g., “a person is sitting on a chair and playing an accordion”). Microsoft says the algorithm is twice as good as its previous image-captioning system, in use since 2015..The algorithm, which was described in a pre-print paper published in September, achieved the highest ever scores on an image-captioning benchmark known as “nocaps.” This is an industry-leading scoreboard for image captioning, though it has its own constraints. .The nocaps benchmark consists of more than 166,000 human-generated captions describing some 15,100 images taken from the Open Images Dataset. These images span a range of scenarios, from sports to holiday snaps to food photography and more. (You can get an idea of the mixture of images and captions by exploring the nocaps dataset here or looking at the gallery below.) Algorithms are tested on their ability to create captions for these pictures that match those from humans..It’s important to note, though, that the nocaps benchmarks capture only a tiny sliver of the complexity of image captioning as a general task. Although Microsoft claims in a press release that its new algorithm “describes images as well as people do,” this is only true insomuch as it applies to a very small subset of images contained within nocaps. .As Harsh Agrawal, one of the creators of the benchmark, told The Verge over email: “Surpassing human performance on nocaps is not an indicator that image captioning is a solved problem.” Argawal noted that the metrics used to evaluate performance on nocaps “only roughly correlate with human preferences” and that the benchmark itself “only covers a small percentage of all the possible visual concepts.” .“As with most benchmarks, [the] nocaps benchmark is only a rough indicator of the models’ performance on the task,” said Argawal. “Surpassing human performance on nocaps by no means indicates that AI systems surpass humans on image comprehension.”.This problem — assuming that performance on a specific benchmark can be extrapolated as performance on the underlying task more generally — is a common one when it comes to exaggerating the ability of AI. Indeed, Microsoft has been criticized by researchers in the past for making similar claims about its algorithms’ ability to comprehend the written word..Nevertheless, image captioning is a task that has seen huge improvements in recent years thanks to artificial intelligence, and Microsoft’s algorithms are certainly state-of-the-art. In addition to being integrated into Word, Outlook, and PowerPoint, the image-captioning AI will also be available as a standalone model via Microsoft’s cloud and AI platform Azure. \n"}
{"prompt":"Google uses machine learning to alert users in India that a flood may occur in their location. With the expansion, \"200 million people in India and 40 million people in Bangladesh can now receive alerts from its flood forecasting system.\"\n\n###\n\n","completion":" Google says its flood prediction service, which uses machine learning to identify areas of land prone to flooding and alert users before the waters arrive, now covers all of India and has expanded to parts of Bangladesh as well..The search giant launched the tool in 2018 for India’s Patna region, but it says it’s been slowly increasing coverage in coordination with local government. In June, it hit the milestone of covering all the worst flood-hit areas of India. The company says this means some 200 million people in India and 40 million people in Bangladesh can now receive alerts from its flood forecasting system..In addition to expanding coverage, Google is testing more accurate forecasts and has updated how its alerts appear on users’ devices. The company says it’s now sent over 30 million notifications to users with Android devices..Google has long been interested in providing warnings about natural disasters and national emergencies like floods, wildfires, and earthquakes. Many of these are handled through its Public Alerts program. Just last month, the company launched a new service that turns Android devices into a network of seismometers, leveraging the accelerometers inside phones and tablets to detect the vibrations from earthquakes and send alerts to users..In the case of flood forecasting, though, Google isn’t using information from customers’ devices. Instead, it draws on a mix of historical and contemporary data about rainfall, river levels, and flood simulations, using machine learning to create new forecast models..Google says it’s experimenting with new models that can provide even more accurate alerts. Its latest forecast model can “double the lead time” of its previous system, says the company, while also providing people with information about the depths of the flooding. “In more than 90 precent of cases, our forecasts will provide the correct water level within a margin of error of 15 centimeters,” say Google’s researchers. .A study of Google’s forecasts in the Ganges-Brahmaputra river basin carried out with scientists from Yale found that 70 percent of people who received a flood alert did so before flood waters arrived, and 65 percent of households that received an alert took action. “Even in an area suffering from low literacy, limited education, and high poverty, a majority of citizens act on information they receive,” write the researchers. “So, early warnings are definitely worth the effort.”.They noted that problems with using smartphone alerts still remained. The main issues are simply lack of access to smartphones and lack of trust regarding technological warnings. Survey respondents the researchers spoke to said they preferred to receive warnings from local leaders and that sharing them via loud speakers and phones calls was still desirable..Google says it’s looking into these problems and has started a collaboration with the International Federation of Red Cross and Red Crescent Societies. It hopes to share its flood forecasts with these organizations who can then disseminate the information through their own networks. \n"}
{"prompt":"University Texas at Austin has stopped using a machine-learning system GRADE to evaluate applicants for its Ph.D. in computer science. Critics say the system exacerbates existing inequality in the field.\n\n\n###\n\n","completion":" U of Texas at Austin has stopped using a machine-learning system to evaluate applicants for its Ph.D. in computer science. Critics say the system exacerbates existing inequality in the field..In 2013, the University of Texas at Austin’s computer science department began using a machine-learning system called GRADE to help make decisions about who gets into its Ph.D. program -- and who doesn’t. This year, the department abandoned it..Before the announcement, which the department released in the form of a tweet reply, few had even heard of the program. Now, its critics -- concerned about diversity, equity and fairness in admissions -- say it should never have been used in the first place..“Humans code these systems. Humans are encoding their own biases into these algorithms,” said Yasmeen Musthafa, a Ph.D. student in plasma physics at the University of California, Irvine, who rang alarm bells about the system on Twitter. “What would UT Austin CS department have looked like without GRADE? We’ll never know.”.GRADE (which stands for GRaduate ADmissions Evaluator) was created by a UT faculty member and UT graduate student in computer science, originally to help the graduate admissions committee in the department save time. GRADE predicts how likely the admissions committee is to approve an applicant and expresses that prediction as a numerical score out of five. The system also explains what factors most impacted its decision..The UT researchers who made GRADE trained it on a database of past admissions decisions. The system uses patterns from those decisions to calculate its scores for candidates..For example, letters of recommendation containing the words “best,” “award,” “research” or “Ph.D.” are predictive of admission -- and can lead to a higher score -- while letters containing the words “good,” “class,” “programming” or “technology” are predictive of rejection. A higher grade point average means an applicant is more likely to be accepted, as does the name of an elite college or university on the résumé. Within the system, institutions were encoded into the categories “elite,” “good” and “other,” based on a survey of UT computer science faculty..Every application GRADE scored during the seven years it was in use was still reviewed by at least one human committee member, UT Austin has said, but sometimes only one. Before GRADE, faculty members made multiple review passes over the pool. The system saved the committee time, according to its developers, by allowing faculty to focus on applicants on the cusp of admission or rejection and review applicants in descending order of quality..For what it’s worth, GRADE did appear to successfully save the committee time. In the 2012 and 2013 application seasons, developers said in a paper about their work, it reduced the number of full reviews per candidate by 71 percent and cut the total time reviewing files by 74 percent. (One full review typically takes 10 to 30 minutes.) Between the years 2000 and 2012, applications to the computer science Ph.D. program grew from about 250 to nearly 650, though the number of faculty able to review those applications remained mostly constant. In the years since 2012, the number of applications has reached over 1,200..The university’s use of the technology escaped attention for a number of years, until this month, when the physics department at the University of Maryland at College Park held a colloquium talk with the two creators of GRADE..The talk gained attention on Twitter as graduate students accused GRADE’s creators of further disadvantaging underrepresented groups in the computer science admissions process..“We put letters of recommendation in to try to lift people up who have maybe not great GPAs. We put a personal statement in the graduate application process to try to give marginalized folks a chance to have their voice heard,” said Musthafa, who is also a member of the Physics and Astronomy Anti-Racism Coalition. “The worst part about GRADE is that it throws that out completely.”.Advocates have long been concerned about the potential for human biases to be baked into or exacerbated by machine-learning algorithms. Algorithms are trained on data. When it comes to people, what those data look like is a result of historical inequity. Preferences for one type of person over another are often the result of conscious or unconscious bias..That hasn’t stopped institutions from using machine-learning systems in hiring, policing and prison sentencing for a number of years now, often to great controversy..“Every process is going to make some mistakes. The question is, where are those mistakes likely to be made and who is likely to suffer as a result of them?” said Manish Raghavan, a computer science Ph.D. candidate at Cornell University who has researched and written about bias in algorithms. “Likely those from underrepresented groups or people who don’t have the resources to be attending elite institutions.”.Though many women and people who are Black and Latinx have had successful careers in computer science, those groups are underrepresented in the field at large. In 2017, whites, Asians and nonresident aliens received 84 percent of degrees awarded for computer science in the United States..Raghavan said he was surprised that there appeared to be no effort to audit the impacts of GRADE, such as how scores differ across demographic groups..GRADE’s creators have said that the system is only programmed to replicate what the admissions committee was doing prior to 2013, not to make better decisions than humans could. The system isn’t programmed to use race or gender to make its predictions, they’ve said. In fact, when given those features as options to help make its predictions, it chooses to give them zero weight. GRADE’s creators have said this is evidence that the committee’s decisions are gender and race neutral..Detractors have countered this, arguing that race and gender can be encoded into other features of the application that the system uses. Women’s colleges and historically Black universities may be undervalued by the algorithm, they’ve said. Letters of recommendation are known to reflect gender bias, as recommenders are more likely to describe female students as “caring” rather than “assertive” or “trailblazing.”.In the Maryland talk, faculty raised their own concerns. What a committee is looking for might change each year. Letters of recommendation and personal statements should be thoughtfully considered, not turned into a bag of words, they said..“I’m kind of shocked you did this experiment on your students,” Steve Rolston, chair of the physics department at Maryland, said during the talk. “You seem to have built a model that builds in whatever bias your committee had in 2013 and you’ve been using it ever since.”.In an interview, Rolston said graduate admissions can certainly be a challenge. His department receives over 800 graduate applications per year, which takes a good deal of time for faculty to evaluate. But, he said, his department would never use a tool like this..“If I ask you to do a classifier of images and you’re looking for dogs, I can check afterwards that, yes, it did correctly identify dogs,” he said. “But when I’m asking for decisions about people, whether its graduate admissions, or hiring or prison sentencing, there’s no obvious correct answer. You train it, but you don’t know what the result is really telling you.”.“If I give you a file and say, ‘Well, the algorithm said this person shouldn’t be accepted,’ that will inevitably bias the way you look at it,” he said..It was never used to make decisions to admit or reject prospective students, as at least one faculty member directly evaluates applicants at each stage of the review process, a spokesperson for the Graduate School said via email..“Changes in the data and software environment made the system increasingly difficult to maintain, and its use was discontinued,” the spokesperson said via email. “The Graduate School works with graduate programs and faculty members across campus to promote holistic application review and reduce bias in admissions decisions.”.“The entire system is steeped in racism, sexism and ableism,” they said. “How many years of POC computer science students got denied [because of this]?”.Addressing that inequity -- as well as the competitiveness that led to the creation of GRADE -- may mean expanding committees, paying people for their time and giving Black and Latinx graduate students a voice in those decisions, they said. But automating cannot be part of that decision making..“If we automate this to any extent, it’s just going to lock people out of academia,” Musthafa said. “The racism of today is being immortalized in the algorithms of tomorrow.”\n"}
{"prompt":"HireVue, a leading provider of software for vetting job candidates based on an algorithmic assessment is killing off a controversial feature of its software: analyzing a person’s facial expressions in a video to discern certain characteristics. Some experts say that the underlying science of using facial analysis to determine personality\/emotion is flawed.\n\n###\n\n","completion":" Job hunters may now need to impress not just prospective bosses but artificial intelligence algorithms too—as employers screen candidates by having them answer interview questions on a video that is then assessed by a machine..HireVue, a leading provider of software for vetting job candidates based on an algorithmic assessment, said Tuesday it is killing off a controversial feature of its software: analyzing a person’s facial expressions in a video to discern certain characteristics..Job seekers screened by HireVue sit in front of a webcam and answer questions. Their behavior, intonation, and speech is fed to an algorithm that assigns certain traits and qualities..HireVue says that an “algorithmic audit” of its software conducted last year shows it does not harbor bias. But the nonprofit Electronic Privacy Information Center had filed a complaint against the company with the Federal Trade Commission in 2019..HireVue CEO Kevin Parker acknowledges that public outcry over the use of software to analyze facial expressions in video was part of the calculation. “It was adding some value for customers, but it wasn’t worth the concern,” he says..The algorithmic audit was performed by an outside firm, O’Neil Risk Consulting and Algorithmic Auditing. The company did not respond to requests for comment..Alex Engler, a fellow at the Brookings Institution who has studied AI hiring, says the idea of using AI to determine someone’s ability, whether it is based on video, audio, or text, is far-fetched. He says it is also problematic that the public cannot vet such claims..“There are parts that machine learning can probably help with, but fully automated interviews, where you’re making inferences about job performance—that’s terrible,” he says. “Modern artificial intelligence can’t make those inferences.”.HireVue says that about 700 companies, including GE, Unilever, Delta, and Hilton, use its technology. The software requires job applicants to respond to a series of questions in a recorded video. The company’s software then analyzes various characteristics including the language they use, their speech, and, until now, their facial expressions. It then provides an assessment of the applicant’s suitability for a job, as well as a measure of traits including “dependability,” “emotional intelligence,” and “cognitive ability.”.Parker says the company helped screen more than 6 million videos last year, although sometimes this involved simply transcribing answers for an interviewer rather than performing an automated assessment of candidates. He adds that some clients let candidates opt out of automated screening. And he says HireVue has developed ways to avoid penalizing candidates with spotty internet connections, automatically referring those candidates to a human..AI experts warn that algorithms trained on data from previous job applicants may perpetuate existing biases in hiring. Lindsey Zuloaga, HireVue’s chief data scientist, says the company screens for bias on gender, race, and age by collecting that information in training data and looking for signs of bias..But she acknowledges that it may be more difficult to know if the system is biased on factors such as income or education level, or if it could be affected by something like a stutter..“I am surprised they are dropping this, as it was a keystone feature of the product they were marketing,” says John Davisson, senior counsel at EPIC. “That is the source of a lot of concerns around  biometric data collection, as well as these bold claims about being able to measure psychological traits, emotional intelligence, social attitudes, and things like that.”.The use of facial analysis to determine emotion or personality traits is controversial; some experts warn that the underlying science is flawed..Lisa Feldman Barrett, a professor at Northeastern University who studies analysis of emotion, says a person’s face does not on its own reveal emotion or character. “Just by looking at someone smiling, you can’t really tell anything about them except maybe that they have nice teeth,” she says. “It is a bad idea to make psychological inferences, and therefore determine peoples outcomes, based on facial data alone.”.EPIC’s FTC complaint accused HireView of failing to guarantee fairness and of using algorithms that cannot be vetted. It also accused the company of misrepresenting its technology by claiming not to use facial recognition. Davisson says the agency has not yet acted on the complaint..But Davisson says he worries that automated analysis of speech could still have problems, and he says it is important that companies release the results of algorithmic audits. He says HireVue’s technology still needs to be vetted thoroughly..“I’m certainly concerned that the same potential issues around data collection and bias and opacity would just carry over to an audio-based screening system.AI hiring has caught the attention of some regulators. A bill before the New York City Council proposes regulating the use of hiring software by requiring employers to inform candidates when they are being assessed by AI, and requiring them to audit their algorithms every year..An Illinois law requires consent from candidates for analysis of video footage. Maryland has banned the use of facial analysis. In 2018, Amazon reportedly abandoned the use of its own technology for automating the assessment of candidate résumés due to biased results.\n"}
{"prompt":"The Department of National Defence tested the use of artificial intelligence last year in an effort to improve diversity in the workplace, but the project was run outside of federal rules aimed at ensuring that the technology is used responsibly.\n\n###\n\n","completion":" The Department of National Defence headquarters is pictured in Ottawa in 2013. The Defence Department told The Globe and Mail that it used two AI-driven Canadian hiring services – Knockri and Plum­ – to shortlist candidates as part of a diversity recruitment campaign.Adrian Wyld\/The Canadian Press.The Department of National Defence tested the use of artificial intelligence last year in an effort to improve diversity in the workplace, but the project was run outside of federal rules aimed at ensuring that the technology is used responsibly..The Privacy Commissioner’s office said DND failed to provide it with a privacy impact assessment, as required under a Treasury Board Secretariat directive. A spokesperson for the commissioner said the office has reached out to DND for more information..“The use of AI to make important decisions such as hiring can raise privacy and human-rights concerns,” Vito Pilieci said. “A [privacy impact assessment] should have been completed and submitted prior to the use of these AI platforms.”.Under a separate Treasury Board directive, federal bodies must also fill out and publish algorithmic impact assessments for all AI tools. These measure the potential for bias and other risks associated with using this kind of predictive technology. However, DND did not complete one of those, either..The Defence Department told The Globe and Mail that it used two AI-driven Canadian hiring services – Knockri and Plum­ – to shortlist candidates as part of a diversity recruitment campaign. The companies, DND says, provide hiring managers with behavioural assessments and measurements of the “personalities, cognitive abilities and social acumen” of applicants..Ashley Casovan, a former Treasury Board director of digital and data and an author of the AI policy, said she was always concerned that agencies would ignore it..Since April, 2020, when Ottawa’s policy on AI use came into force, only a single algorithmic impact assessment has been submitted. That submission was by the Treasury Board itself..This isn’t the first time federal agencies have disregarded the Treasury Board’s data directives. In 2016, the department began requiring them to submit an inventory of all the datasets in their possession – yet several never complied..The Royal Canadian Mounted Police, for instance, has never submitted one. RCMP spokesperson Robin Percival said the agency did “not currently have a timeline for submitting a completed open data inventory,” and that it was consulting with the Treasury Board..The DND campaign, aimed at recruiting people for the department’s executive ranks, closed on Sept. 25. The department said all applicants were given the choice to consent to the use of AI or go through an alternative screening method..Andrée-Anne Poulin, a spokesperson for DND, said the department, but not the Canadian Forces, has used AI in its recruitment work. Because “final decisions” weren’t made using AI, she said, it didn’t complete the Treasury Board’s algorithmic assessment. The department did not immediately respond to questions about the privacy impact assessment..Ms. Poulin said the department conducted internal and external consultations before selecting Knockri and Plum. She also noted that the National Research Council recently completed an “ethics assessment” of Knockri’s service. The Globe asked the NRC if it had conducted a similar assessment of Plum but the agency declined to answer, citing confidentiality issues..In a statement, Treasury Board spokesperson Alain Belle-Isle did not respond to specific questions about DND, but said policy “requires federal departments and agencies to complete an algorithmic impact assessment” before the technology is employed..Knockri and Plum’s websites say they offer AI services as a way to reduce bias in hiring decisions. Knockri did not respond to several e-mails for this story, and Plum declined to comment..DND said Knockri’s service was used to assess behaviours expected of its executive group. Participants answered questions via video, but the department said “visual and auditory identifiers” were not assessed..In total, the department said it spent $179,000 on the recruitment campaign, which included contracts with the two AI companies as well as consulting fees with Deloitte Canada, and other costs, such as unconscious bias training for staff. The AI services were used to whittle down a list of 422 candidates to 34 “top tier” ones, DND said..Ms. Casovan, the former senior Treasury Board official, said an algorithmic assessment would have identified possible privacy and fairness concerns surrounding Knockri and Plum. Both companies collect personal information on applicants, including psychological profiles and video and audio recordings..“What is happening with that data?” asked Ms. Casovan, who now serves as executive director of AI Global, a non-profit group devoted to building tools for responsible use of AI. “Where is it being stored? How long does the company have access to that for? Did it go through the U.S.? These are things that I would have a lot of questions about.”.Fenwick McKelvey, an associate professor of communications at Concordia University who studies the use of AI, said there’s a need for better regulation around the technology..“This [algorithmic directive] was, I think, a centrepiece of the government’s response to that,” he said. “So if they don’t actually follow it, and they don’t commit to it, it undermines the legitimacy of the whole process.”.Prof. McKelvey noted that using AI has previously led to problems in the hiring process. In 2018, a tool created by Amazon led to unintentional bias against women because it was based on a male-dominated database of résumés..“It’s kind of a strange paradox here where the very devices which have been called out for their hidden biases are trying to address hidden biases,” he said.\n"}
{"prompt":"An Italian court ruled that an algorithm once used by Deliveroo to assess riders on its platform was discriminatory. The ruling found that the algorithm was in violation of labor laws because it did not differentiate between the reasons a rider may have for not working.\n\n###\n\n","completion":" LONDON, UNITED KINGDOM - 2021\/01\/03: Deliveroo courier rides along the Regent Street delivering ... [+] Takeaway food in central London during covid 19 tier 4 restrictions. (Photo by Pietro Recchia\/SOPA Images\/LightRocket via Getty Images).The ruling found that the algorithm, which was used to evaluate delivery riders on the platform, was in violation of labor laws because it did not differentiate between the reasons a rider may have for not working. For example, it evaluated a rider whether they were not working because they were sick or simply choosing not to work..The court in Bologna said that Deliveroo would have to pay €50,000 to each affected rider. CGIL, the country’s largest trade union, said the decision was an important one in shoring up protections for gig economy workers..This judgement refers to a historic optional booking model which is not used by Deliveroo in Italy or other markets, a Deliveroo spokesperson said in a statement, adding that the system is no longer in use..Riders have complete flexibility to choose when to work, where to work, for as little or as long as they want. This means that there is no booking system and no obligation to accept work..The company defended its stance on self-employment for its riders: We offer self-employment because this offers the flexibility riders want. It cited a survey that said more than 80% of riders value work flexibility..Deliveroo as well as other on-demand delivery companies have still faced challenges to their self-employed model for gig economy workers in various markets with mixed results. The UK High Court previously ruled in Deliveroo’s favor in a case over whether riders had collective bargaining rights. On the flipside, Spain’s Supreme Court found in September that riders for Deliveroo and Glovo should be classed as employees.\n"}
{"prompt":"Apple’s flawed adult content filters are blocking innocuous sites while letting actual pornography through.\n\n###\n\n","completion":" iPhone and Mac users have discovered that a feature allowing parents to protect their children from adult content behaves erratically, despite years of repeated complaints to Apple..A Twitter user who goes by Steven Shen found out that sites featuring the word “Asian” are inexplicably banned when he turned on his iPhone’s adult content filters..The software tends to block sites and searches featuring words including “ebony,” “daddy,” “massage,” “babe,” “hardcore,” “teen,” and “amateur”— innocent terms that take on new meanings only in the context of pornography. They are some of PornHub’s most searched-for terms..At the same time, the filter fails to catch some of the most popular porn sites, including all adult Reddit sections and popular regional porn sites like ThisAV, which is frequented by Japanese, Taiwanese, and Hong Kong users, according to web ranking service Alexa..OnlyFans, a social media network that allows people to sell photos, often nudes of themselves, also slips through, although it requires a subscription to view content..Some words that are more closely associated with sex are not restricted, such as “sadomasochism,” “threesome,” “jerk off,” and “wank.” And Apple’s interpretation of “adult content” doesn’t seem to include violent imagery. Searches for “how to make a bomb,” “snuff film,” and “murder footage” are all unrestricted under the content restriction mode on iPhones and Macs..The flaws suggest that Apple is not applying the same technology on parental controls that they use in many other applications. Such technology, often based on artificial intelligence and machine learning, allows a program to detect objects in images and analyze language. Companies such as Google and Facebook use similar tools to block nudity on their sites..Li Xiaofan, assistant professor of information systems and analytics at the National University of Singapore, told VICE World News that Apple’s filtering method is ineffectual and unsophisticated..“My guess is that they trained an algorithm by providing it samples of pages [that] should be censored and samples of pages [that] should not be censored. Then the algorithm learned the keywords associated with pages [that] should be censored. However, the sample size may not be large or representative enough and the algorithm itself could be ill-designed,” Li said..The feature has not noticeably improved since users started complaining about it as early as 2014. The sex education site O.school in 2018 reported on its inconsistency and provided a number of glaring omissions, but those sites are still either incorrectly blocked or let through..“I would say that most parental control solutions have certain things that they struggle with,” McKenna told VICE World News. “And iOS is the perfect example where Apple has essentially made it difficult for parental control solutions to do the job that parents want them to do.”.“There is a definite responsibility that parents have to protect their children online. But this is a shared responsibility with organizations that have endless resources and are not fulfilling their half of this bargain,” he said..“All children need to know that they can land safely and softly with you as a parent. And then you use parental controls in full transparency and honesty with your kids… That digital trust gets built when you have that good balance between relational and technical solutions,” he said..By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.\n"}
{"prompt":"A social media-based chatbot developed by a South Korean startup was shut down on Tuesday after users complained that it was spewing vulgarities and hate speech.\n\n###\n\n","completion":" A social media-based chatbot developed by a South Korean startup was shut down on Tuesday after users complained that it was spewing vulgarities and hate speech..The fate of the Korean service resembled the demise of Microsoft’s Tay chatbot in 2016 over racist and sexist tweets it sent, raising ethical questions about the use of artificial intelligence (AI) technology and how to prevent abuse..The Korean startup Scatter Lab said on Monday that it would temporarily suspend the AI chatbot. It apologized for the discriminatory and hateful remarks it sent and a “lack of communication” over how the company used customer data to train the bot to talk like a human..The startup designed Lee Luda, the name of the chatbot, to be a 20-year-old female university student who is a fan of the K-pop girl group Blackpink..Launched in late December to great fanfare, the service learned to talk by analyzing old chat records acquired by the company’s other mobile application service called Science of Love..Before the bot was suspended, users said they received hateful replies when they interacted with Luda. Michael Lee, a South Korean art critic and former LGBTQ activist, shared screenshots showing that Luda said “disgusting” in response to a question about lesbians..Another user, Lee Kwang-suk, a professor of Public Policy and Information Technology at the Seoul National University of Science and Technology, shared screenshots of a chat where Luda called “Black people” heukhyeong, meaning “black brother,” a racial slur in South Korea. The bot was also shown to say, “Yuck, I really hate them,” in a response to a question about transgender people. The bot ended the message with a crying emoticon..In the Monday statement, Scatter Lab defended itself and said it did “not agree with Luda’s discriminatory comments, and such comments do not reflect the company’s ideas.”.“Luda is a childlike AI who has just started talking with people. There is still a lot to learn. Luda will learn to judge what is an appropriate and better answer,” the company said..Lee, the IT professor, told VICE World News that the company has a responsibility for the abuse, comparing the case to Microsoft’s shutdown of its Tay chatbot..Another user, Lee Youn-seok, who participated in a beta test of Luda in July before it was officially launched, told VICE World News that the outcome was “predictable.”.Some people said that the debacle was unsurprising given the sex ratio of the company’s employees. A page on the company website suggested that about 90 percent of the group behind the bot were men. The page was later removed..Some male-dominated online communities also openly discussed how to “enslave” the AI bot and shared their methods to “harass” it sexually, hoping to elicit sexual comments from Luda..Some politicians and rights advocates have taken the opportunity to call for an anti-discrimination bill, which seeks to ban all discrimination based on gender, disability, age, language, country of origin, and sexual orientation..The anti-discrimination bill could be used to hold AI software developers accountable for such abuse, Ahn Byong-jin, a professor at Kyunghee University in Seoul, told VICE World News. “Companies should consult a philosopher or ethicist before launching a service to prevent such abuse,” he said.\n"}
{"prompt":"ExamSoft's proctoring software is having a problem with recognizing faces, which could delay or bar test takers. According to reports, this software disproportionately impacts people with dark skin tones. \n\n###\n\n","completion":" Gabe Teninbaum, a professor at Suffolk University Law School, is calling on ExamSoft to fix a serious bug with its test-taking software: failure to recognize faces. It’s a problem that can delay test takers — or bar them from starting their exams altogether — and per reports, it disproportionately impacts people with dark skin tones. .ExamSoft’s software records students while they complete remote exams and monitors for signs of academic dishonesty. Teninbaum’s report addresses an ExamSoft feature called ExamID, which aims to verify that test-takers are who they say they are. The first time a student logs into their exam portal, they upload a photo of themselves (their “baseline image”); they’re then prompted to take another selfie before beginning future exams, which the software checks against their original photo. .Research has found that facial-recognition algorithms consistently make more errors in identifying Black faces than they do white ones. And while those studies didn’t focus on ExamSoft specifically, it doesn’t appear to be an exception. Back in September, multiple non-white exam-takers told the New York Times that the software couldn’t identify them due to “poor lighting” — a problem that Teninbaum, who has light skin, wasn’t able to replicate. .Early this fall, Teninbaum set out to find a fix. He believes such errors add undue stress to an already stressful time period. “These are students who are about to take a high-stakes exam with a lot on the line, and that is very unwelcome,” Teninbaum said in an interview with The Verge. .Teninbaum also believes that optics matter; schools owe it to marginalized students not to rely on a category of software that’s known to be discriminatory. “Students deserve to feel that their institution is doing what it can to protect their rights, interests, and dignity,” he says..In his report, which is forthcoming in The Journal of Robotics, Artificial Intelligence, and Law, Teninbaum outlines the workaround he found. .He suggests that schools assign every student an identical generic, baseline image. Then, he proposes, they should ask ExamSoft to enable “deferred identification,” a feature built into the software that allows students to proceed with exams even if identification fails.(ExamSoft told The Verge that as of the end of 2020, deferred identification is now enabled by default — so schools should be able to skip this step.).These tweaks will cause ExamSoft to misidentify every test-taker. But they’ll still be able to proceed with their exams — ExamSoft will send the selfies to the school afterwards, and instructors can manually verify everyone. “We know who our students are,” Teninbaum says. “We can make sure the students are who they say they are and avoid subjecting students to these sorts of challenges.”.He also suggests that ExamSoft make the “deferred identification” feature accessible to customers. “The reporter urges ExamSoft to build this into a feature by which institutions can simply toggle on\/off, thereby bypassing ExamID until such time that the technology matures into one that does not discriminate,” his report reads..Teninbaum hopes those changes will last beyond the COVID-19 pandemic, and can help students feel more comfortable taking remote classes. “It’s going to be a growing problem as people get more and more online for their education,” he says..Even so, he’s only fixed part of the problem. Students have experienced a range of hiccups with ExamSoft’s proctoring software. Over 3,000 people who used the platform to take California’s bar exam in October had their videos flagged for potential rule violations — nearly 36 percent of applicants who took the online exam. Users reported audio issues, and other technical glitches as well. .A group of six US senators — including Richard Blumenthal, Elizabeth Warren, and Corey Booker — wrote an open letter to ExamSoft in December, highlighting potential harms to students of color and students with disabilities, among numerous other concerns..In a statement to The Verge, ExamSoft said, “ExamID is designed to enable a frictionless experience for exam-takers. ExamID seeks to mitigate challenges often associated with facial recognition software by allowing exam-takers to set and approve their own baseline picture, which the software uses to compare to a picture the exam-taker takes at the time of the exam. Additionally, exam-takers can test how ExamID validates their specific picture prior to taking any exam to help reduce the stress of picture-matching concerns during the exam.”\n"}
{"prompt":"Data collected by TraceTogether, a Covid-19 tracing app in Singapore, could be accessed and used by the police for criminal investigations. This contradicts the Singaporean government's claim that data collected would only be used for contact tracing.\n\n###\n\n","completion":" For Singaporeans, the covid-19 pandemic has been closely intertwined with technology: two technologies, to be specific. The first is the QR code, whose little black-and-white squares have been ubiquitous all over the country as part of the SafeEntry contact tracing system rolled out in April and May. .Under SafeEntry, anyone entering a public venue—restaurants, stores, malls—must scan a code and register with a name, ID or passport number, and phone number. If somebody tests positive for covid-19, contact tracers use it to track down those who got close enough to be potentially infected..There’s also TraceTogether, an app that launched in March 2020. It uses Bluetooth to ping close contacts; if two users are in proximity, their devices trade anonymized and encrypted user IDs that can be decrypted by the Ministry of Health should one person test positive for covid-19. .For those who can’t or don’t want to use a smartphone app, the government also offers TraceTogether tokens, small digital fobs that serve the same purpose. And while TraceTogether is currently voluntary, the government has announced that it is going to merge the two systems, which would make it mandatory to either download the app or collect a token..When the two systems were launched, there wasn’t much space for the public to discuss apprehensions: they were seen as necessary to fight the pandemic, and the Singaporean government acted in typical top-down fashion. It did seek to assuage fears, however, by repeatedly assuring Singaporeans that the data collected with such technology would be used only for contact tracing during the pandemic. .Earlier this month, it emerged that the government’s claim was false. The Ministry of Home Affairs confirmed that data could actually be accessed by the police for criminal investigations; the day after this admission, a minister revealed that such data had, in fact, already been used in a murder investigation. It rapidly became clear that despite what ministers had previously said, Singaporean law meant it had been possible for law enforcement to use TraceTogether data all along..These revelations triggered public anger and criticism, not necessarily because Singaporeans are particularly privacy conscious—in fact, state surveillance is largely normalized in the country—but because people felt they’d been subjected to a bait-and-switch. Many people had reservations about TraceTogether when it was first launched, and only began using it in large numbers after the government indicated that it would soon become mandatory. (According to the cochair of the task force on covid-19, nearly 80% of Singapore’s residents have adopted TraceTogether.).The government has since announced that it will introduce new legislation to limit law enforcement’s use of contact tracing data to probes into seven specific categories of offense, including terrorism, murder, kidnapping, and the most serious drug trafficking cases. (The MIT Technology Review Covid Tracing Tracker, which monitors the policies around exposure notification apps worldwide, is being updated to reflect this shift.).“We acknowledge our error in not stating that data from TraceTogether is not exempt from the Criminal Procedure Code,” said the Smart Nation and Digital Governance Office in its statement. The new law, it said, “will specify that personal data collected through digital contact tracing solutions … can only be used for the specific purpose of contact tracing, except where there is a clear and pressing need to use that data for criminal investigation of serious offences.”.“In Singapore, where laws grant sweeping executive and legislative powers to state actors, I think any commitment to accountability and restraint is welcome,” says digital rights activist Lee Yi Ting. “But it remains to be seen if the bill will make substantive commitment to these proposed limitations. For example, if state actors flout these regulations, what investigative bodies will come into play, and what consequences will state actors be held to?” .Some doubt how useful such data can really be to police investigations and are concerned that even the proposed limits still formally expand its use beyond contact tracing..“We like to reiterate that extending police powers to [TraceTogether] data is not aligned to the original spirit of what the dataset was intended for,” said the opposition Progress Singapore Party in a statement. “Covid tracing data must solely and strictly be used for fighting the pandemic and nothing else.” .The confusion could not come at a more difficult time. Concerns that governments could abuse contact tracing systems have been raised around the world. Many of these worries have been misplaced, especially in countries that use Google and Apple’s exposure notification technology, which does not allow centralized collection by local authorities. The Singapore government had previously rejected Apple and Google’s system, saying that it would be “less effective” in the Singaporean context. .But while digital systems could speed up contact tracing and aid in the fight against the virus—one that could be more vital over time, not less—most countries have struggled with adoption. One major issue: trust..Lee worries that even if legislation is enough to placate many Singaporeans, the implications outside the country could be serious. Singapore’s early move to build digital contact tracing put it in a global leadership position, and TraceTogether’s underlying systems have been used by other nations—though there is no suggestion that the same legislative mistakes were made elsewhere. .Still, “Singaporeans do care about the extent to which the state intrudes into their private lives,” says Lee. And, she adds, the country is setting an international precedent “for repressive governments to likewise normalize the use of contact tracing data for the purposes they define.”.Two Dutch researchers have won a major hacking championship by hitting the software that runs the world’s power grids, gas pipelines, and more. It was their easiest challenge yet..An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.\n"}
{"prompt":" Chinese technology giants have registered patents for tools that can detect, track and monitor Uighurs in a move human rights groups fear could entrench oppression of the Muslim minority.\n\n###\n\n","completion":" An ethnic Uighur demonstrator wears a mask as she attends a protest against China in front of the Chinese Consulate in Istanbul, Turkey, October 1, 2019. REUTERS\/Huseyin Aldemir.BERLIN, Jan 13 (Thomson Reuters Foundation) - Chinese technology giants have registered patents for tools that can detect, track and monitor Uighurs in a move human rights groups fear could entrench oppression of the Muslim minority..In a report published on Tuesday, IPVM reveals a cluster of patents for systems that could be used to analyse images for the presence of Uighurs, and hook into ongoing surveillance camera and facial recognition networks..We cannot ignore the fact that these technologies have been developed in order to be able to efficiently carry out..brutal oppression, Rushan Abbas, executive director of the rights group Campaign for Uyghurs, told the Thomson Reuters Foundation..United Nations officials have said China is transforming the Xinjiang region, where many Uighurs live, into a massive internment camp, with the patented tracing tech seen by rights groups as key to the crackdown..These technologies allow police in China to go through a large database of faces, and flag faces that the AI has marked as non-Chinese, or Uighurs, says Charles Rollet, a researcher with IPVM. There are major human rights implications.The U.N estimates that more than a million Chinese Muslims, many of whom are from the minority Uighur ethnicity, have been detained in the province of Xinjiang, where activists say crimes against humanity and genocide are taking place..Research by human rights groups suggests that Chinese tech firms are building Uighur detection systems, using facial recognition to alert authorities to peoples whereabouts, and predictive policing tools to identify which to detain..Imagine if the U.S. were a full-on dictatorship, imprisoning Black people just for being Black, and there was technology deployed across the country to detect where Black people were, so they could be hunted down, she said..The debate over the role of corporations in Chinas treatment of the Uighurs is increasingly spilling over internationally, with the United States applying sanctions to Chinese tech firms accused of abetting the persecution..The incoming Biden administration this week returned a donation from former U.S. senator Barbara Boxer, who had registered as a lobbyist for Hikvision, a video surveillance firm blacklisted by the U.S. government in 2019..According to the IPVM report, many top China security camera manufacturers have offered Uyghurs analytics, including the three largest firms: Hikvision, Dahua and Uniview..Hikvision told Reuters in 2019 that the firm takes global human rights very seriously and that its technology was also used in shops, traffic control and commercial buildings..One patent application, filed by the Chinese tech giant Huawei in conjunction with the Chinese Academy of Sciences, describes how AI can tell if a pedestrian is Uighur or not..Huawei opposes discrimination of all types, including the use of technology to carry out ethnic discrimination, the company said in a statement emailed to the Thomson Reuters Foundation..Identifying individuals race was never part of the research and development project. It should never have become part of the application and we are taking proactive steps to amend it..The company told the Thomson Reuters Foundation that its patent language was open to misunderstanding but that Megvii has not developed and will not develop or sell racial or ethnic labelling solutions..If youre a Chinese tech company - in particular one that builds facial recognition - and the police are customers, you are going to have this kind of Uighur-detecting analytics, he said..I am shocked there are so many technology firms helping the Chinese government watch us, said Jevlan Shirmemmet, a Uighur activist living in Turkey who says his mother is detained in a Chinese internment camp..Our global editorial team of about 55 journalists and more than 350 freelancers covers the lives of people around the world who struggle to live freely or fairly.\n"}
{"prompt":"Amnesty International has launched a new campaign against facial recognition and launching with a demand for New York City to halt police and government use of the technology. Amnesty argues facial recognition is incompatible with basic privacy rights, and will exacerbate structural racism in policing tactics. \n\n###\n\n","completion":" Amnesty International has launched a new campaign against facial recognition titled Ban The Scan — and is launching with a demand for New York City to halt police and government use of the technology..Amnesty argues facial recognition is incompatible with basic privacy rights, and will exacerbate structural racism in policing tactics. “New Yorkers should be able to go out about their daily lives without being tracked by facial recognition,” said Matt Mahmoudi, an AI and human rights researcher with Amnesty. “Other major cities across the US have already banned facial recognition, and New York must do the same.”.Amnesty is joined in the New York portion of the campaign by a range of groups, including the Urban Justice Center, the New York Civil Liberties Union and the city’s Public Advocate office..The New York Police Department has run afoul of facial recognition critics before, most notably when it used facial recognition to locate and arrest a Black Lives Matter activist in August. The department claims it only uses facial recognition to generate leads, and doesn’t make arrests based on the information. Still, many civil liberties groups find the existing protections inadequate..The Ban the Scan campaign is launching with a website that will allow users to leave comments on the NYPD’s policies through a local public oversight rule. Later, Amnesty plans to build in a tool for filing Freedom of Information Law requests, and in May, a tool to geolocate facial-recognition-capable cameras throughout the city..“For years, the NYPD has used facial recognition to track tens of thousands of New Yorkers, putting New Yorkers of color at risk of false arrest and police violence,” said Albert Fox Cahn, executive director of the Surveillance Technology Oversight Project at the Urban Justice Center in a statement. “Banning facial recognition won’t just protect civil rights: it’s a matter of life and death.”\n"}
{"prompt":"Faces of the Riot used open source software to detect, extract, and deduplicate every face from the 827 videos taken from the insurrection on January 6.  Faces of the Riot represents the serious privacy dangers of pervasive facial recognition technology, says Evan Greer, the campaign director for digital civil liberties nonprofit Fight for the Future\n\n###\n\n","completion":" When hackers exploited a bug in Parler to download all of the right-wing social media platforms contents last week, they were surprised to find that many of the pictures and videos contained geolocation metadata revealing exactly how many of the sites users had taken part in the invasion of the US Capitol building just days before. But the videos uploaded to Parler also contain an equally sensitive bounty of data sitting in plain sight: thousands of images of unmasked faces, many of whom participated in the Capitol riot. Now one website has done the work of cataloging and publishing every one of those faces in a single, easy-to-browse lineup..Late last week, a website called Faces of the Riot appeared online, showing nothing but a vast grid of more than 6,000 images of faces, each one tagged only with a string of characters associated with the Parler video in which it appeared. The sites creator tells WIRED that he used simple open source machine learning and facial recognition software to detect, extract, and deduplicate every face from the 827 videos that were posted to Parler from inside and outside the Capitol building on January 6, the day when radicalized Trump supporters stormed the building in a riot that resulted in five peoples deaths. The creator of Faces of the Riot says his goal is to allow anyone to easily sort through the faces pulled from those videos to identify someone they may know or recognize who took part in the mob, or even to reference the collected faces against FBI wanted posters and send a tip to law enforcement if they spot someone..Everybody who is participating in this violence, what really amounts to an insurrection, should be held accountable, says the sites creator, who asked for anonymity to avoid retaliation. Its entirely possible that a lot of people who were on this website now will face real-life consequences for their actions..Aside from the clear privacy concerns it raises, Faces of the Riots indiscriminate posting of faces doesnt distinguish between lawbreakers—who trampled barriers, broke into the Capitol building, and trespassed in legislative chambers—and people who merely attended the protests outside. An upgrade to the site today adds hyperlinks from faces to the video source, so that visitors can click on any face and see what the person was filmed doing on Parler. The Faces of the Riot creator, who says hes a college student in the greater DC area, intends that added feature to help contextualize every faces inclusion on the site and differentiate between bystanders, peaceful protesters, and violent insurrectionists..He concedes that he and a cocreator are still working to scrub non-rioter faces, including those of police and press who were present. A message at the top of the site also warns against vigilante investigations, instead suggesting users report those they recognize to the FBI, with a link to an FBI tip page. If you go on the website and you see someone you know, you might learn something about a relative, he says. Or you might be like, oh, I know this person, and then further that information to the authorities..Despite its disclaimers and limitations, Faces of the Riot represents the serious privacy dangers of pervasive facial recognition technology, says Evan Greer, the campaign director for digital civil liberties nonprofit Fight for the Future. Whether its used by an individual or by the government, this technology has profound implications for human rights and freedom of expression, says Greer, whose organization has fought for a legislative ban on facial recognition technologies. I think it would be an enormous mistake if we come out of this moment by glorifying or lionizing a technology that, broadly speaking, disproportionately harms communities of color, low-income communities, immigrant communities, Muslim communities, activists ... the very same people that the faces on this website stormed the Capitol for the purpose of silencing and disenfranchising..The sites developer counters that Faces of the Riot leans not on facial recognition but facial detection. While he did use the open source machine learning tool Tensor Flow and the facial recognition software Dlib to analyze the Parler videos, he says he used that software only to detect and cluster faces from the 11 hours of video of the Capitol riot; Dlib allowed him to deduplicate the 200,000 images of faces extracted from video frames to around 6,000 unique faces. (He concedes that there are nonetheless some duplicates and images of faces on protest signs included too. Even the number 45 on some signs was in some cases identified as a human face.).He emphasizes also that theres no search tool on the site, and it doesnt attempt to link faces with names or other identifying details. Nor is there any feature for uploading an image and matching it with images in the sites collection, which he says could lead to dangerous misidentifications. Theres a very hard no on allowing a user to take a photo from a wanted poster and search for it, the sites creator says. That’s never going to happen..The roughly 42 gigabytes of Parler videos that Faces of the Riot analyzed were downloaded prior to Amazons decision early last week to cut off Parlers web hosting, leaving the site largely offline since. Racing against that takedown, hacktivists took advantage of a security flaw in Parler that allowed them to download and archive every post from the service, which bills itself as an uncensored free speech alternative to Twitter or Facebook. Faces of the Riot obtained Parlers salvaged videos after they were made available online by Kyle McDonald, a media artist who obtained them from a third party he declined to identify..The Faces of the Riot sites creator initially saw the data as a chance to experiment with machine learning tools, but quickly saw the potential for a more public project. After about 10 minutes I thought, this is actually a workable idea and I can do something that will help people, he says. Faces of the Riot is the first website hes ever created..McDonald has previously both criticized the power of facial recognition technology and himself implemented facial recognition projects like ICEspy, a tool he launched in 2018 for identifying agents of the Immigration and Customs Enforcement agency. He tells WIRED he also analyzed the leaked Parler videos with facial recognition tools to see if he could identify individuals, but could only ID two, both of whom had already been named by media. He sees Faces of the Riot as playing it really safe compared even to his own facial recognition experiments, given that it doesnt seek to link faces with named identities. And I think its a good call because I dont think that we need to legitimize this technology any more than it already is and has been falsely legitimized, McDonald says..But McDonald also points out that Faces of the Riot demonstrates just how accessible facial recognition technologies have become. It shows how this tool that has been restricted only to people who have the most education, the most power, the most privilege is now in this more democratized state, McDonald says..The Faces of the Riot sites creator sees it as more than an art project or demonstration. Despite the safeguards he put in place to limit its ability to automatically identify people, he still hopes that the effort will have real, tangible results—if only indirectly through reports to law enforcement. Its just felt like people got away with a lot of bad stuff for the last four years, he says. This is an opportunity to start trying to put that to an end.\n"}
{"prompt":"According to the University of Washington researchers, their audits reveal Amazon hosts a “plethora” of health misinformative products\". Several medically unverified products for coronavirus treatment, like prayer healing, herbal treatments and antiviral vitamin supplements proliferated Amazon, so much so that the company had to remove 1 million fake products after several instances of such treatments were reported by the media,” the researchers wrote in their paper. \n\n###\n\n","completion":" We are excited to bring Transform 2022 back in-person July 19 and virtually July 20 - 28. Join AI and data leaders for insightful talks and exciting networking opportunities. Register today!.The pandemic has unleashed a barrage of online misinformation that’s reinvigorated the anti-vaccine movement. Despite the fact that multiple COVID-19 vaccines are approved and beginning to be made available to the public, only two-thirds of Americans say they’ll try to get vaccinated, according to a CNN poll. As governments work toward distributing vaccines, health experts worry that reluctance could make it difficult to achieve herd immunity. Unfortunately, the algorithms powering search engines haven’t traditionally been designed to take into account the credibility and trustworthiness of medical information..The coauthors of a recent study argue this is particularly true of Amazon, which has faced criticism for failing to regulate the health-related products on its platform. According to the University of Washington researchers, who have affiliations with the The Information School at the University of Washington, their audits reveal Amazon hosts a “plethora” of health misinformative products belonging to categories including books, ebooks, apparel, and health and personal care. They also claim to have found a “filter-bubble” effect in Amazon’s recommendations where recommendations of misinformative health products contain more health misinformation..“Several medically unverified products for coronavirus treatment, like prayer healing, herbal treatments and antiviral vitamin supplements proliferated Amazon, so much so that the company had to remove 1 million fake products after several instances of such treatments were reported by the media,” the researchers wrote in their paper. “The scale of the problematic content suggests that Amazon could be a great enabler of misinformation, especially health misinformation. It not only hosts problematic health-related content but its recommendation algorithms drive engagement by pushing potentially dubious health products to users of the system.”.The researchers conducted two sets of experiments in May and August to determine the extent to which Amazon might be promoting health misinformation about vaccines. In the first — an “unpersonalized” audit — they used information retrieval metrics to measure the amount of health misinformation users were exposed to when performing for vaccine-related searches. In particular, while logged in as a guest to minimize the influence of personalization algorithms, they canvassed the results of 48 searches belonging to 10 popular vaccine-related topics including “HPV vaccine,” “immunization,” and “MMR vaccine and autism.”.The researchers ran the audit for 15 consecutive days, sorting the results across five different Amazon filters each day: Featured, Price Low to High, Price High to Low, Average Customer Review, and Newest Arrivals. They annotated the resulting 36,000 search results and 16,815 product page recommendations for their stances on health misinformation — i.e., whether they promoted, debunked, or were neutral regarding vaccinations — for a final dataset totaling 4,997 annotated Amazon products..The second audit — a “personalized” audit — looked at the impact of a customer’s behavioral history on the amount of misinformation returned in search results, recommendations, and auto-complete suggestions. As the researchers note, Amazon history covers a weeklong period of actions including searching for products, searching and clicking, adding to cart after searching and clicking, searching on third-party websites like Google, and more..After analyzing the results from both audits, the researchers found that search results returned for many vaccine-related queries contain large number of misinformative products, leading to what they characterize as “high misinformation bias.” In addition, misinformative products were ranked higher than “debunking” products, and customers performing actions on misinformative products were presented with more misinformation in their homepages, product page recommendations, and prepurchase recommendations, the researchers say..“Many search engines and social media platforms employ personalization to enhance users’ experience on their platform by recommending them items that the algorithm thinks they will like based on their past browsing or purchasing history. But on the downside, if not checked, personalization can also lead users into a rabbit hole of problematic content,” the researchers wrote. “Our analysis … revealed that an echo chamber exists on Amazon where users performing real-world actions on misinformative books are presented with more misinformation in various recommendations. Just a single click on an anti-vaccine book could fill your homepage with several other similar anti vaccine books. There is an urgent need for the platform to treat vaccine and other health related topics differently and ensure high quality searches and recommendations.”.For its part, Amazon recently said in a corporate blog post that during 2020, it reviewed almost 10,000 product listings a day to ensure compliance with its policies and removed over 2 million products for violating its offensive or controversial guidelines. More than 1.5 million of these products were identified, reviewed, and removed proactively by automated tools, according to Amazon — often before being seen by a customer..“We exercise judgment in allowing or prohibiting listings, and we keep the cultural differences and sensitivities of our global community in mind when making a decision on products,” Amazon wrote. “We strive to maximize selection for all customers, even if we don’t agree with the message or sentiment of the product itself. Our offensive and controversial products policy attempts to provide a clear and objective standard against which to measure the products we permit in our store.”.The researchers suggest as one potential solution a “bias meter” that could signal the amount of misinformation present in vaccine-related  search results. They also urge Amazon to stop promoting health misinformative books via sponsorships — the researchers found 98 misinformative products in the sponsored recommendations they annotated — and to introduce a label for health-related products that have been evaluated by experts. Moreover, they recommend that the platform account for misinformation bias in its search and recommendation algorithms to reduce the exposure to misinformative content..“Our investigations revealed that Amazon’s algorithm has learnt problematic patterns through consumers’ past viewing and buying patterns,” the researchers wrote. “Our study … provides a peek into the workings of Amazon’s algorithm and has paved way for future audits that could use our audit methodology and extensive qualitative coding scheme to perform experiments considering complex real world settings.”.The researchers aren’t the first to uncover the presence of anti-vaccination content on Amazon. In May, CNN found listings and advertisements for books and movies promoting vaccination misinformation including VAXXED: From Cover-Up to Catastrophe, which was dropped from the Tribeca Film Festival in 2016 following an outcry. Amazon removed the anti-vaccine documentaries from its Prime Video service after CNN published its report..VentureBeats mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. Learn more about membership..Hear from senior executives at some of the world’s leading enterprises about their experience with applied Data & AI and the strategies they’ve adopted for success. \n"}
{"prompt":"Google has revoked Ethical AI team leader Margaret “Meg” Mitchell’s employee privileges and is currently investigating her activity, according to a statement provided by a company spokesperson. Should Google fire Mitchell, it will mean the company has effectively chosen to behead its own AI ethics team in under two months.\n\n###\n\n","completion":" We are excited to bring Transform 2022 back in-person July 19 and virtually July 20 - 28. Join AI and data leaders for insightful talks and exciting networking opportunities. Register today!.Google has revoked Ethical AI team leader Margaret “Meg” Mitchell’s employee privileges and is currently investigating her activity, according to a statement provided by a company spokesperson. Should Google fire Mitchell, it will mean the company has effectively chosen to behead its own AI ethics team in under two months. In an interview with VentureBeat last month, former Google AI ethics co-lead Timnit Gebru said she had worked with Mitchell since 2018 to create one of the most diverse teams within Google Research..Gebru tweeted Tuesday evening that Google’s move to freeze Mitchell’s employee account echoed the way hers was frozen before she was fired. When VentureBeat emailed Google to ask if Mitchell was still an employee, a spokesperson provided the following statement:.“Our security systems automatically lock an employee’s corporate account when they detect that the account is at risk of compromise due to credential problems or when an automated rule involving the handling of sensitive data has been triggered. In this instance, yesterday our systems detected that an account had exfiltrated thousands of files and shared them with multiple external accounts. We explained this to the employee earlier today. We are actively investigating this matter as part of standard procedures to gather additional details.”.Last month, Google fired Gebru following a demand by Google leadership that she rescind an AI research paper she coauthored about the negative consequences of large-scale language models, including their disproportionate impact on marginalized communities in the form of environmental impact and perpetuating stereotypes. Since then, Google released a trillion parameter language model and told its AI researchers to strike a positive tone on topics deemed “sensitive. Some members of the AI research community have pledged not to review the work of Google researchers at academic conferences in protest..After Gebru was fired, April Curley, a queer Black woman who said she was fired by Google last fall, publicly recounted numerous negative experiences during her time as a recruiter of talent from historically Black colleges and universities (HBCU)..On Tuesday, news emerged that Google CEO Sundar Pichai will meet with HBCU leaders following allegations of racism and sexism at the company by current and former employees..Members of Congress interested in regulating AI and more than 2,000 Google employees have joined prominent figures in the AI research community in questioning Gebru’s dismissal. Members of Google’s AI ethics team called for her reinstatement in a series of demands sent to company leadership..Organizers cited the way Google treated Gebru and the impact AI can have on society as motivators behind the establishment of the Alphabet Workers Union, which was formed earlier this month and as of a week ago counted 700 members including Margaret Mitchell. Gebru had previously endorsed the idea of a workers union as a way to help protect AI researchers from company retribution..“With AI permeating every aspect of our world—from criminal justice, to credit scores, to military applications—paying careful attention to ethics within the industry is critical,” the Alphabet Workers Union said in a statement shared with VentureBeat..“As one of the most profitable players in the AI industry, Alphabet has a responsibility to continue investing in its ethical application. Margaret founded the Ethical AI team, built a cross-product area coalition around machine learning fairness, and is a critical member of academic and industry communities around the ethical production of AI. Regardless of the outcome of the company’s investigation, the ongoing targeting of leaders in this organization calls into question Google’s commitment to ethics—in AI and in their business practices. Many members of the Ethical AI team are AWU members and the membership of our union recognizes the crucial work that they do and stands in solidarity with them in this moment.”.The incoming Biden administration has in recent days shared a commitment to diversity and to addressing algorithmic bias and other AI-driven harms to society through its science and technology policy platform. Experts in AI, law, and policy told VentureBeat last month that Google’s treatment of Gebru could impact a range of policy matters, including the passage of stronger whistleblower protections for tech workers and more public funding of independent AI research..What happens to Mitchell will continue to shape attitudes toward corporate self-governance and speculation about the voracity of research produced with Big Tech funding. A research paper published in late 2020 compared the way Big Tech funds AI ethics research to Big Tobacco’s history of funding health research..VentureBeats mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. Learn more about membership..Hear from senior executives at some of the world’s leading enterprises about their experience with applied Data & AI and the strategies they’ve adopted for success. \n"}
{"prompt":"Companies trying to run ads on YouTube or elsewhere on the web could direct Google not to show those ads to people of “unknown gender”—meaning people who have not identified themselves to Google as “male” or “female.” After being alerted to this by The Markup, Google pledged to crack down on the practice.\n\n###\n\n","completion":"                    Dozens of advertisers instructed the company to not show their ads to people of “unknown” gender, meaning people who had not identified themselves as male or female                               By  Jeremy B. Merrill  .Google’s advertising system allowed employers or landlords to discriminate against nonbinary and some transgender people, The Markup found..Companies trying to run ads on YouTube or elsewhere on the web could direct Google not to show those ads to people of “unknown gender”—meaning people who have not identified themselves to Google as “male” or “female.” After being alerted to this by The Markup, Google pledged to crack down on the practice..“We will be implementing an update to our policy and enforcement in the coming weeks to restrict advertisers from targeting or excluding users on the basis of the ‘gender unknown’ category,” Elijah Lawal, a spokesperson for Google said. .Google’s policies forbid ads targeting or excluding male or female people from jobs, housing, or financial products, in order to comply with federal anti-discrimination laws. But until The Markup alerted Google, the company gave advertisers the option of  keeping their ads from being shown to people of “unknown gender”—effectively allowing employers and landlords to either inadvertently or purposefully discriminate against people who identify as nonbinary, transgender, or anything other than male or female. .The Markup found two such job ads on YouTube, which is owned by Google—one for jobs at FedEx and the other for Dewey Pest Control, a California-based chain. In both cases, Google’s ad targeting explanations, collected by New York University’s Ad Observer, indicated that the employer had targeted the ad based on gender but that the data did not specify which gender was targeted. In those cases, Lawal said, the advertiser had chosen to exclude people of unknown gender from seeing the ads. Upon further review, Lawal said, the company “identified approximately 100 advertisers out of many thousands” who had done the same for housing, credit, or job ads..Advertisers, “out of many thousands,” that Google identified as having chosen to exclude people of unknown gender from housing, credit, or job ads..A FedEx spokesperson, Shannon Davis, said FedEx declined to comment “on our specific marketing or recruiting processes.” Dewey Pest Control didn’t respond to a request for comment..That would be “completely unethical and I think dangerous to society as a whole,” said Joseph Turow, a professor of communication at the University of Pennsylvania. .Lawal said that Google’s existing written policies forbid excluding people on the basis of transgender or gender nonconforming identity, for any ad. But he acknowledged that the “unknown gender” checkbox had effectively allowed advertisers to discriminate by gender despite Google’s policies against that. .While federal law prohibits discrimination by gender or race in advertising for employment and housing, there’s little in the way of case law about whether restrictive online ad targeting qualifies as discrimination. Nor is there any case law about excluding trans or nonbinary people from such ads, said Pauline Kim, a law professor at Washington University in St. Louis..If a company’s intention was to exclude nonbinary or gender nonconforming people, Kim said, “you could possibly argue … that it is a form of sex discrimination” under Title VII of the Civil Rights Act of 1964, which, according to a recent Supreme Court decision, covers discrimination on the basis of gender identity and sexual orientation as well..The prevalence of employment discrimination against nonbinary and transgender people is grim. What makes advertising discrimination different is that, “you don’t even know what you don’t see,” said Kendra Albert, a technology law instructor at Harvard Law School who studies technology, law, and gender..Beyond gender, the primary categories Google offers for targeting ads to some people and not others—what it calls “personalized advertising”—are age, household income, and parental status. For ads for job opportunities, housing, and credit, Google’s rules say advertisers shouldn’t use these categories. .The boxes start prechecked, meaning that in the default mode, gender isn’t taken into account in determining who will be shown the ad, but advertisers can uncheck them, effectively excluding certain categories of people from seeing their ads. That’s allowed for most ads, but Google uses other mechanisms—”internal processes” that Lawal declined to explain—to block job, housing, and credit ads that exclude people by gender, age, household income, and parental status..Meanwhile, someone signing up for Google or editing their account settings has four options for reporting their gender: “male,” “female,” “rather not say,” and an option to set a custom gender in a text box..Lawal said that the “unknown category is intended to refer to individuals where we have been unable to determine or infer the user’s gender and is not intended to allow for targeting or exclusion of users based on gender identity,” but said that people who choose not to identify their gender or write in a “custom” gender also fall into this category. .Google’s options for users amount to putting “a rainbow-colored Band-Aid” on “systems that were not really designed to include nonbinary people,” said Albert..“Really the question they should be asking is which gender are you, and which of these gender categories would you like us to serve you ads for,” and explaining how the ads system uses gender, Albert said..Allegations of race and sex discrimination have dogged online ad platforms for years. Several year ago, civil rights groups sued Facebook for allowing discrimination in ads for jobs, housing, and credit; Facebook settled the suit and agreed to take those options away. The U.S. Department of Housing and Urban Development (HUD) filed a lawsuit against Facebook too. Last year, HUD announced that it had “worked with Google to improve Google’s online advertising policies to better align them with requirements of the Fair Housing Act.” After those interactions with HUD, Google banned job, housing, and credit advertisers from excluding either men or women from their ads, along with similar rules for age and other protected groups..While ads for jobs, housing, and financial products fall under special protections, it’s perfectly legal—and very common—to target other kinds of ads to one segment of the population, by age, gender, or other categories..We’re happy to make this story available to republish for free under the conditions of an Attribution–NonCommercial–No Derivatives Creative Commons license. Please adhere to the following:\n"}
{"prompt":"Robert Williams was arrested by Michigan State Police for thief. However, it was due to facial recognition technology being unable to tell black people apart. \n\n###\n\n","completion":" Early this year, Detroit police arrested Robert Williams — a Black man living in a Detroit suburb — on his front lawn in front of his wife and two little daughters (ages 2 and 5). Robert was hauled off and locked up for nearly 30 hours. His crime? Face recognition software owned by Michigan State Police told the cops that Robert Williams was the watch thief they were on the hunt for.  There was just one problem: Face recognition technology can’t tell Black people apart. That includes Robert Williams, whose only thing in common with the suspect caught by the watch shop’s surveillance feed is that they are both large-framed Black men. .But convinced they had their thief, Detroit police put Robert William’s driver’s license photo in a lineup with other Black men and showed it to the shop security guard, who hadn’t even witnessed the alleged robbery firsthand. The shop security guard — based only on review of a blurry surveillance image of the incident — claimed Robert was indeed the guy. With that patently insufficient “confirmation” in hand, the cops showed up at Robert’s house and handcuffed him in broad daylight in front of his own family.  It wasn’t until after spending a night in a cramped and filthy cell that Robert saw the surveillance image for himself. While interrogating Robert, an officer pointed to the image and asked if the man in the photo was him. Robert said it wasn’t, put the image next to his face, and said “I hope you all don’t think all Black men look alike.”  One officer responded, “The computer must have gotten it wrong.” Robert was still held for several more hours, before finally being released later that night into a cold and rainy January night, where he had to wait about an hour on a street curb for his wife to come pick him up. The charges have since been dismissed.  The ACLU of Michigan is lodging a complaint against Detroit police, but the damage is done. Robert’s DNA sample, mugshot, and fingerprints — all of which were taken when he arrived at the detention center — are now on file. His arrest is on the record. Robert’s wife, Melissa, was forced to explain to his boss why Robert wouldn’t show up to work the next day. Their daughters can never un-see their father being wrongly arrested and taken away — their first real experience with the police. Their children have even taken to playing games involving arresting people, and have accused Robert of stealing things from them.   As Robert puts it: “I never thought I’d have to explain to my daughters why daddy got arrested. How does one explain to two little girls that a computer got it wrong, but the police listened to it anyway?”  One should never have to. Lawmakers nationwide must stop law enforcement use of face recognition technology. This surveillance technology is dangerous when wrong, and it is dangerous when right.  First, as Robert’s experience painfully demonstrates, this technology clearly doesn’t work. Study after study has confirmed that face recognition technology is flawed and biased, with significantly higher error rates when used against people of color and women. And we have long warned that one false match can lead to an interrogation, arrest, and, especially for Black men like Robert, even a deadly police encounter. Given the technology’s flaws, and how widely it is being used by law enforcement today, Robert likely isn’t the first person to be wrongfully arrested because of this technology. He’s just the first person we’re learning about.  That brings us to the second danger. This surveillance technology is often used in secret, without any oversight. Had Robert not heard a glib comment from the officer who was interrogating him, he likely never would have known that his ordeal stemmed from a false face recognition match. In fact, people are almost never told when face recognition has identified them as a suspect. The FBI reportedly used this technology hundreds of thousands of times — yet couldn’t even clearly answer whether it notified people arrested as a result of the technology. To make matters worse, law enforcement officials have stonewalled efforts to obtain documents about the government’s actions, ignoring a court order and stonewalling multiple requests for case files providing more information about the shoddy investigation that led to Robert’s arrest.   Third, Robert’s arrest demonstrates why claims that face recognition isn’t dangerous are far-removed from reality. Law enforcement has claimed that face recognition technology is only used as an investigative lead and not as the sole basis for arrest. But once the technology falsely identified Robert, there was no real investigation. On the computer’s erroneous say-so, people can get ensnared in the Kafkaesque nightmare that is our criminal legal system. Every step the police take after an identification — such as plugging Robert’s driver’s license photo into a poorly executed and rigged photo lineup — is informed by the false identification and tainted by the belief that they already have the culprit. They just need the other parts of the puzzle to fit. Evidence to the contrary — like the fact that Robert looks markedly unlike the suspect, or that he was leaving work in a town 40 minutes from Detroit at the time of the robbery — is likely to be dismissed, devalued, or simply never sought in the first place. And when defense attorneys start to point out that parts of the puzzle don’t fit, you get what we got in Robert’s case: a stony wall of bureaucratic silence.   Fourth, fixing the technology’s flaws won’t erase its dangers. Today, the cops showed up at Robert’s house because the algorithm got it wrong. Tomorrow, it could be because a perfectly accurate algorithm identified him at a protest the government didn’t like or in a neighborhood in which someone didn’t think he belonged. To address police brutality, we need to address the technologies that exacerbate it too. When you add a racist and broken technology to a racist and broken criminal legal system, you get racist and broken outcomes. When you add a perfect technology to a broken and racist legal system, you only automate that system’s flaws and render it a more efficient tool of oppression.   It is now more urgent than ever for our lawmakers to stop law enforcement use of face recognition technology. What happened to the Williams’ family should not happen to another family. Our taxpayer dollars should not go toward surveillance technologies that can be abused to harm us, track us wherever we go, and turn us into suspects simply because we got a state ID. \n"}
{"prompt":"Amazon suspended contracts involving police use of its Rekognition software for a year most likely in response to the Justice in Policing Act Bill.\n\n###\n\n","completion":" In a surprise blog post, Amazon said it will put the brakes on providing its facial recognition technology to police for one year, but refuses to say if the move applies to federal law enforcement agencies..The moratorium comes two days after IBM said in a letter it was leaving the facial recognition market altogether. Arvind Krishna, IBM’s chief executive, cited a “pursuit of justice and racial equity” in light of the recent protests sparked by the killing of George Floyd by a white police officer in Minneapolis last month..Amazon’s statement — just 102 words in length — did not say why it was putting the moratorium in place, but noted that Congress “appears ready” to work on stronger regulations governing the use of facial recognition — again without providing any details. It’s likely in response to the Justice in Policing Act, a bill that would, if passed, restrict how police can use facial recognition technology..“We hope this one-year moratorium might give Congress enough time to implement appropriate rules, and we stand ready to help if requested,” said Amazon in the unbylined blog post..But the statement did not say if the moratorium would apply to the federal government, the source of most of the criticism against Amazon’s facial recognition technology. Amazon also did not say in the statement what action it would take after the yearlong moratorium expires..Amazon is known to have pitched its facial recognition technology, Rekognition, to federal agencies, like Immigration and Customs Enforcement. Last year, Amazon’s cloud chief Andy Jassy said in an interview the company would provide Rekognition to “any” government department..There are dozens of companies providing facial recognition technology to police, but Amazon is by far the biggest. Amazon has come under the most scrutiny after its Rekognition face-scanning technology showed bias against people of color..In 2018, the ACLU found that Rekognition falsely matched 28 members of Congress as criminals in a mugshot database. Amazon criticized the results, claiming the ACLU had lowered the facial recognition system’s confidence threshold. But a year later, the ACLU of Massachusetts found that Rekognition had falsely matched 27 New England professional athletes against a mugshot database. Both tests disproportionately mismatched Black people, the ACLU found..Investors brought a proposal to Amazon’s annual shareholder meeting almost exactly a year ago that would have forcibly banned Amazon from selling its facial recognition technology to the government or law enforcement. Amazon defeated the vote with a wide margin..The ACLU acknowledged Amazon’s move to pause sales of Rekognition, which it called a “threat to our civil rights and liberties,” but called on the company and other firms to do more.\n"}
{"prompt":"A study found that self-driving cars guided by AI performed worse at detecting people with dark skin, which could put the lives of dark-skinned pedestrians at risk.\n\n###\n\n","completion":" A new study from the Georgia Institute of Technology suggests autonomous driving systems may have more difficulty detecting pedestrians with dark skin than those with light skin..The researchers responsible for the study had eight image-detection systems analyze images of pedestrians. The people in the photos were separated into two groups based on how their skin tones aligned with the Fitzpatrick skin type scale, which divides skin tones into six categories. One group consisted of pedestrians who fell into one of the three lightest categories on the Fitzpatrick scale, while the other group consisted of pedestrians who fell into one of the three darkest categories on the Fitzpatrick scale..The image-detection systems then attempted to identify all of the pedestrians in the images, and the researchers compared the systems abilities to detect light-skinned pedestrians versus dark-skinned pedestrians. On average, the image-detection systems were 5% less accurate at detecting dark-skinned pedestrians, even when the researchers controlled for variables that may have been able to explain the disparity, like pedestrians who were partially blocked from view or the time of day the photo was taken..The researchers suggested that the differences in pedestrian-detection accuracy could result from not having enough dark-skinned pedestrians in the images used to train the systems, as well as the systems insufficient emphasis on learning from the smaller population of dark-skinned pedestrians..While Vox notes that the study has not been peer-reviewed and did not use the same image-detection systems or image sets featured in current self-driving vehicles, the study suggests that companies developing autonomous-driving technology should be attentive to the methods they use to train vehicles to identify pedestrians.\n"}
{"prompt":"Twitter has suspended a bot network made up of accounts that have unique avatars featuring AI-generated women, anime characters, and cats. These bots have GAN-generated profile pictures and advertise incessantly in Turkish about porn and sports betting.\n\n###\n\n","completion":" A data scientist has uncovered a Twitter bot network made up of accounts that have unique avatars featuring AI-generated women, anime characters, and cats. They talk incessantly in Turkish about porn and sports betting..Twitter bots are getting harder to spot. It used to be easier: when you saw a Twitter egg that had a user name followed by a string of random numbers, you knew you were probably dealing with an account run by a bot. Thanks to machine learning, those accounts now have randomly-generated human faces, a well-known problem. But its not just faces: theres AI-generated cats and anime avatars, too. These accounts look authentic, but they’re not. .Many GAN-generated images contain weird artifacts and details inconsistent with real photographs. With a little practice, people can spot details in GAN images that point to their artificial nature: surreal swirls, odd bits in the background, and details like teeth and eyes where they shouldn’t be are all common giveaways. Conspirador Norteño noted that some of the pictures in this bot network had those markers, but they can be hard to spot when the picture is reduced to the size of a Twitter avatar. .Conspirador Norteño and their partner Dr ZQ discovered a bot network on Twitter of more than 3,000 accounts using GAN generated images. According to them, the accounts were created around January 25 and use a mix of anime avatars, cats, and faces. Some of the images repeat. More than 900 of them are female faces on a blank white background. Around 275 of them are anime avatars and 307 are cats. The accounts mostly follow each other..So what are these cats, people, and waifus tweeting about? Porongraphy and online sports betting, and mostly in Turkish. “This network replies en masse to specific tweets, with dozens or hundreds of members replying to the same tweet,” Conspirador Norteño said..Bot networks joining together to tweet about pornography is a clear violation of Twitter’s terms of service. The sites user policies state that you may not use Twitter’s services in a manner intended to artificially amplify or suppress information or engage in behavior that manipulates or disrupts people’s experience on Twitter..Twitter followed through. “We suspended a number of accounts for spam and coordination under our platform manipulation and spam policy,” a Twitter spokesperson told Motherboard in an email. “Using both technology and human review, we proactively tackle attempts to disrupt the public conversation at scale.”.As of this writing, many of the accounts are already being suspended, including Dilde97512368, the account Conspirador Norteño called out in their first tweet..Twitter has dismantled massive bot networks before, including one in Turkey that was promoting the Turkish political partyAKP and its founder President Recep Tayyip Erdoğan in June 2020.  This also isn’t the first big bot network Conspirador Norteño has uncovered. In August 2020, they uncovered a group of Twitter bots that were selling followers to people by the thousands. That bot network also used GAN-generated avatars to make their accounts seem authentic..By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.\n"}
{"prompt":"An first-person narrative from a VICE contributor on how Muute, Japan’s first AI journaling app, helpeed her reflect on her emotions. \n\n###\n\n","completion":" Much like a writing prompts journal, the app asks questions that encourage self-reflection. The added AI feature analyzes users’ emotions and provides personal letters at the end of each week and month that give feedback about their thoughts. It also compiles daily entries to graph mood, most used vocabulary, and highlights from the week. After each entry, Muute leaves you with a quote to mull over, or a breathing exercise. Exactly what I need, I thought..When I was younger, I used to write in my diaries religiously. Writing time was always 30 minutes before lights out and entries were usually recounts of my delicious snacks. Eight-year-old me was highly passionate about chocolate mousse and one report read “Mom did it again. It was so smooth and rich. I want to dream about it.” Apparently, I couldn’t get enough.  .But in the last decade, my journaling has matured somewhat. Now, I have notebooks dedicated to varying emotional needs. My daily reflection journal, an inconspicuous tan color that lies on my windowsill, is only ever a few sentences. “Thinking about consequential happiness and what it means to tell the truest story of yourself,” I wrote one day in January. Then I have my “down” diary, where I track all the times I feel sad, to remember life’s difficulties. I turn to it when I don’t understand all the feelings I’m having and hope writing them clears my emotional fog. The most sensitive, top-secret, if-someone-finds-this-I’m-screwed diary, is my no bullshitting journal. I only open it about five times a year, always on my birthday and New Year’s. .Needless to say, I was eager to add Muute to my roulette of journals. I decided to try it out for a week. Though prompt diaries seemed counterintuitive because I’ve always had something to say, the prospect of getting feedback about my feelings sounded helpful. 2021 has been a year of many changes for me. I’ve moved to Japan, leaving my nuclear family to live alone in the middle of a pandemic. I welcome change with open arms, but sometimes, it can be overwhelming. I hoped Muute would help restore some inner balance. .Past research has indicated that journaling does wonders for mental health. Expressive writing has been shown to reduce unwanted thoughts about negative events, as well as improve working memory. The COVID-19 pandemic, which has led to thousands of daily deaths, millions out of work, and extreme loneliness, has intensified the need for emotional care. In Japan, suicide rates rose 37 percent for women between July and October, prompting governing officials to consider how to better provide support.  .The horrors of our daily lives are enough to keep anyone in bed, but a tool that could aid in improving one’s mental state is emotion AI. As a branch of artificial intelligence, it helps us understand the way we feel. It uses natural language processing technology, which assists computers to understand human language. In a journaling app such as Muute, emotion AI can interpret patterns in users’ language to detect emotion. .“It’s perfect for busy people; it gives us a chance to calm down and look after ourselves. Through the app, I’m able to notice things about myself that I normally wouldn’t, which helps me understand who I am more objectively,” he told VICE.  .Downloading Muute from the App Store doesn’t have quite the same thrill as cracking open a diary for the first time, but the user-friendly interface was instantly attractive. It’s clear that the focal point of Muute is tracking emotional progression over time. Like a calendar, the app’s main page shows my week at a glance. The other pages lead to logs, user profile, and the weekly and monthly insight letters. .The very first question I answered was “How does your family make your life better?” Suspicion arose, as it was a highly coincidental prompt; I had just e-fought with my brother about not giving me enough attention. But after begrudgingly writing about the joys of family, I noticed my resentment had dissipated. Such is the power of selective positive thinking — we forget tedious arguments. .Over the week, the app consistently asked me positivity-driven questions, such as “What is a recent thing you learned?” and “Who helped you this week?” My moodflow graph showed two curves; an orange “positive” emotion one, and a blue “negative” feeling one. The curves spiked and dipped, depending on my entry for that day. Monday to Tuesday, the lowest point, was when I fought with my brother. Tuesday to Wednesday was a highlight for me, and was coincidentally the day I received some positive feedback about an article I wrote. .Initially, using the app felt like a task to complete at night, which is how I’ve always journaled. But as the week went by, I realized I was checking Muute throughout the day. I looked at the log to see whether what I had felt was a bad day was interpreted similarly by the app. The end-of-the-week letter felt slightly vague, given the app didn’t have many entries to work with. But it had an understanding tone; it said I wasn’t alone in my worries and encouraged me to focus on my future. .The sole criticism I have about Muute is that the prompts all seemed positive. The only time I truly reflected on sad thoughts was when the app asked me to select three categories that I’m most worried about, as well as three words that best described my feelings. .It’s a good mental exercise to practice positive thinking, but at the same time, I’m a firm believer in the value of sadness. If we don’t know grief or pain, we have nothing to compare our happiness to. Having the space to explore sadness makes us much more well-rounded and empathetic; the world isn’t stretches of paradise, and we shouldn’t trick ourselves to believe that. .To those who feel like they’re missing that additional software of self-understanding, I’d give AI journaling apps a try. It’s a great supplement to add to free-writing spaces, and the way Muute is contextualized does help you see the bigger narrative arc of your life. The app helps you notice emotional patterns and simplifies the complexity of human emotion. You realize small downfalls are only temporary, and that life will always have more to offer than what you see in front of you. .“The app helps you notice emotional patterns and simplifies the complexity of human emotion. You realize small downfalls are only temporary, and that life will always have more to offer than what you see in front of you.”.Over the years, I’ve heard the incessant hammering of “Know who you are!” or “Just be yourself!” Sometimes, you just don’t. And that’s okay. I’ve ceased trying to demystify every single fear, happiness, and pain, and instead decided to work towards understanding how fears make me click. It’s an ongoing process, but I welcome tools that can help me grow in the right direction. .By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.\n"}
{"prompt":"As people struggle to book appointments and doses go unused, some companies are proposing automated tools to help deliver vaccines into \narms.\n\n###\n\n","completion":" In Ohio, nearly 900 doses of Moderna’s COVID-19 vaccine went to waste because a provider stored them at the wrong temperature. In New York City, clinics were forced to hold onto vaccine doses for weeks instead of administering them to high-risk patients, or even throw them out, because they couldn’t find anyone who met the state’s stringent criteria for the first phase of inoculation. Florida, meanwhile, partnered with the event ticket platform Eventbrite to manage vaccine appointments, resulting in a confusing, biased, and scam-prone distribution system..Distributing the limited number of COVID-19 vaccines to the people who most need them is a daunting logistical challenge, and in many cases, the bureaucratic, human-run systems charged with overseeing it have fallen short. In some states, broken and confusing scheduling systems have driven frustrated residents to write code and build their own volunteer websites for booking vaccine appointments..It’s the kind of problem, some tech firms say, that is ripe for artificially intelligent overseers—and it may be the very thing needed to jump-start a lucrative new era of automated health care decision-making and delivery..“I think it did, in many ways, take the COVID-19 pandemic to put that rocket fuel behind AI” in health care, Dr. John Showalter, the chief product officer for Jvion, a clinical AI company that’s been operating since 2011, told Motherboard. “I feel like we’re right on that precipice. 2021 is going to have a lot of reports out about how clinical AI helped with COVID-19. By 2025, people are going to be like ‘clinical AI, yawn’” because it’s so ubiquitous..There is a spectrum of use cases for AI in vaccine distribution. Health systems are already  using AI chatbots from companies like Hyro and Praktice AI to field calls from the large number of patients inquiring about whether they’re eligible for the vaccines, and to schedule appointments and follow ups..Big tech companies like Google and Microsoft have developed vaccine management systems that incorporate AI at various levels, including planning trucking routes and maintaining dose temperatures.But the potential applications that are generating the most excitement, and skepticism, are AI tools designed to automate or influence decisions about where vaccines should go in the country—and who should get them. .In the early months of the pandemic, California asked companies to propose technological solutions to problems like COVID-19 test shortages. Aible, an AI startup based in the state, offered—for free—to create algorithmic models that would identify who to prioritize in testing in order to save lives and reduce the pandemic’s economic impact. State officials didn’t respond to the offer, Aible CEO Arijit Sengupta, told Motherboard, but he said the company is now talking with one of the major vaccine makers about creating a similar system to guide vaccine supply chains and prioritization..“Matching up the demand and supply is something that’s not happening right now,” he said. “It would be better than what we are doing today—it would not be perfect, nothing is ever perfect—but what is good about a system like this is it learns on a daily basis and it adjusts.”.Most states are currently in the first phases of vaccination, where almost all available doses are reserved for health care workers, people over 65, and some individuals with chronic conditions. But once those populations are inoculated, states will have to make difficult decisions about where to send doses and who to prioritize next..Jvion has created models that map which areas of the country and which population groups are most likely to experience severe effects or die if exposed to a wave of COVID-19 and, separately, which areas are the highest priorities for vaccine distribution based on the CDC’s prioritization recommendations and other health and socio-economic factors. The company has sent analyses of millions of patients to its customers, which include health systems, to guide their vaccine outreach..Take Perry County, Pennsylvania. One of Jvion’s models, which incorporates the CDC’s guidelines, labels the county a very low priority for vaccination. But its other model, measuring community vulnerability to COVID-19, considers Perry County at the highest level of risk for severe, community-wide morbidity. .Jvions COVID-19 model for Perry County, PA demonstrates how AI might help states allocate vaccines to high-risk areas they might otherwise overlook..The reasons for the disparity aren’t necessarily intuitive. They draw on data that health systems likely aren’t considering and algorithmic pattern matching that is, by definition, unhuman. Why is Perry County at such high risk for COVID-19 morbidity? According to Jvion’s models, the fact that a county has “low commercial retail availability” and “low commercial\/industrial job density” are influential risk indicators. Factors that influence the calculated risk level in other counties include the rate at which residents commute more than 60 minutes to work and the prevalence of environmental health hazards..Jvion’s models are trained on a database of 36 million “independent lives,” Dr. Showalter said. That includes people’s medical claim records, socio-economic information about where they live from agencies like the Environmental Protection Agency and U.S. Department of Agriculture, and credit scoring data from companies like Experian and Transunion. As they built the models, there was nowhere near enough information available about actual COVID-19 cases, so Jvion instead trained its models primarily on data about the health care trajectories of people with respiratory conditions and illnesses like Influenza..Jvion’s models are not currently being used as automatic decision making systems—determining who gets prioritized for vaccines without human oversight. But AI is increasingly being used to inform a variety of triaging problems in health care, and experts say a regulatory action plan for medical AI that the Food and Drug Administration (FDA) published in January is a long-awaited signal that the agency is preparing to open the doors to a new array of medical AI tools..Currently, in order for AI systems to be eligible for FDA approval as medical devices, they must be static tools—the algorithmic model that comes out of the box doesn’t change. But one of the strengths of this kind of technology is that models can constantly be trained on new data, tweaked, and improved over time. The agency’s new action plan calls for it to develop guidelines for allowing, and regulating, evolving systems..Monitoring continuously changing algorithms that contribute to life-or-death decision making is a tricky proposition, though, and one that has kept the FDA from moving more quickly to accept these kinds of systems as medical devices..But the COVID-19 pandemic has sped up health care providers’ adoption of AI tools—such as home monitoring systems for vital signs that reduce the chances of virus transmission. These tools may not need FDA approval as medical devices, but nonetheless influence health care decision making, Sara Gerke, a research fellow at Harvard Law School’s Petrie-Flom Center for Health Law Policy, Biotechnology, and Bioethics, told Motherboard. That increased exposure will likely quicken the industry’s adoption of more advanced, higher-stakes tools, despite the many legal and ethical issues that remain..“I personally believe that AI has potential for being used for allocation of vaccines, but that’s for the future,” Gerke said. “I would not trust the AI right now because there will be so many hidden biases in the data. First of all, what kind of data do you even use? Even if you take it from the electronic health records data then you already have a bias because in Black communities, many can’t even go to the doctor. Right now using it, I find it very difficult.”.By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.\n"}
{"prompt":"A new study shows how training deep-learning models on patient outcomes could help reveal gaps in existing medical knowledge. In the study, an AI model was much more accurate than radiologists' KLG system at predicting self-reported pain levels for both white and Black patients, but especially for Black patients. It reduced the racial disparity at each pain level by nearly half.\n\n###\n\n","completion":" In the last few years, research has shown that deep learning can match expert-level performance in medical imaging tasks like early cancer detection and eye disease diagnosis. But there’s also cause for caution. Other research has shown that deep learning has a tendency to perpetuate discrimination. With a health-care system already riddled with disparities, sloppy applications of deep learning could make that worse..Now a new paper published in Nature Medicine is proposing a way to develop medical algorithms that might help reverse, rather than exacerbate, existing inequality. The key, says Ziad Obermeyer, an associate professor at UC Berkeley who oversaw the research, is to stop training algorithms to match human expert performance..The paper looks at a specific clinical example of the disparities that exist in the treatment of knee osteoarthritis, an ailment which causes chronic pain. Assessing the severity of that pain helps doctors prescribe the right treatment, including physical therapy, medication, or surgery. This is traditionally done by a radiologist reviewing an x-ray of the knee and scoring the patient’s pain on the Kellgren–Lawrence grade (KLG), which calculates pain levels based on the presence of different radiographic features, like the degree of missing cartilage or structural damage..But data collected by the National Institute of Health found that doctors using this method systematically score Black patients’ pain as far as far less severe than what they say they’re experiencing. Patients self-report their pain levels using a survey that asks how much it hurts to do various things, such as fully straightening their knee. But these self-reported pain levels are ignored in favor of the radiologist’s KLG score when prescribing treatment. In other words, Black patients who show the same amount of missing cartilage as white patients self-report higher levels of pain..This has consistently miffed medical experts. One hypothesis is that Black patients could be reporting higher levels of pain in order to get doctors to treat them more seriously. But there’s an alternative explanation. The KLG methodology itself could be biased. It was developed several decades ago with white British populations. Some medical experts argue that the list of radiographic markers it tells clinicians to look for may not include all the possible physical sources of pain within a more diverse population. Put another way, there may be radiographic indicators of pain that appear more commonly in Black people that simply aren’t part of the KLG rubric..To test this possibility, the researchers trained a deep-learning model to predict patients’ self-reported pain level from their knee x-ray. If the resultant model had terrible accuracy, this would suggest that self-reported pain is rather arbitrary. But if the model had really good accuracy, this would provide evidence that self-reported pain is in fact correlated with radiographic markers in the x-ray..After running several experiments, including some designed to discount any confounding factors, the researchers found that the model was much more accurate than KLG at predicting self-reported pain levels for both white and Black patients, but especially for Black patients. It reduced the racial disparity at each pain level by nearly half..The goal isn’t necessarily to start using this algorithm in a clinical setting. But by outperforming the KLG methodology, it revealed that the standard way of measuring pain is flawed, at a much greater cost to Black people. This should tip off the medical community to investigate which radiographic markers the algorithm might be seeing, and update their scoring methodology..“It actually highlights a really exciting part of where these kinds of algorithms can fit into the process of medical discovery,” says Obermeyer. “It tells us if there’s something here that’s worth looking at that we don’t understand. It sets the stage for humans to then step in and, using these algorithms as tools, try to figure out what’s going on.”.“The cool thing about this paper is it is thinking about things from a completely different perspective,” says Irene Chen, a researcher at MIT who studies how to reduce health-care inequities in machine learning and was not involved in the paper. Instead of training the algorithm on well-established expert knowledge, she says, the researchers chose to treat patients’ self-assessment as truth. Through that it uncovered important gaps in what the medical field usually considers to be the more “objective” pain measure..“That was exactly the secret,” agrees Obermeyer. If algorithms are only ever trained to match expert performance, he says, they will simply perpetuate existing gaps and inequities. “This study is a glimpse of a more general pipeline that we are increasingly able to use in medicine for generating new knowledge.” .An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.\n"}
{"prompt":"The Trevor Project, America’s hotline for LGBT youth, is turning to a GPT-2-powered chatbot to help troubled teenagers—but it’s setting strict\n limits.\n\n###\n\n","completion":" The Trevor Project, America’s hotline for LGBT youth, is turning to a GPT-2-powered chatbot to help troubled teenagers—but it’s setting strict limits..Counselors volunteering at the Trevor Project need to be prepared for their first conversation with an LGBTQ teen who may be thinking about suicide. So first, they practice. One of the ways they do it is by talking to fictional personas like “Riley,” a 16-year-old from North Carolina who is feeling a bit down and depressed. With a team member playing Riley’s part, trainees can drill into what’s happening: they can uncover that the teen is anxious about coming out to family, recently told friends and it didn’t go well, and has experienced suicidal thoughts before, if not at the moment..Just like the original persona, this version of Riley—trained on thousands of past transcripts of role-plays between counselors and the organization’s staff—still needs to be coaxed a bit to open up, laying out a situation that can test what trainees have learned about the best ways to help LGBTQ teens. .Counselors aren’t supposed to pressure Riley to come out. The goal, instead, is to validate Riley’s feelings and, if needed, help develop a plan for staying safe. .Crisis hotlines and chat services make them a fundamental promise: reach out, and we’ll connect you with a real human who can help. But the need can outpace the capacity of even the most successful services. The Trevor Project believes that 1.8 million LGBTQ youth in America seriously consider suicide each year. The existing 600 counselors for its chat-based services can’t handle that need. That’s why the group—like an increasing number of mental health organizations—turned to AI-powered tools to help meet demand. It’s a development that makes a lot of sense, while simultaneously raising questions about how well current AI technology can perform in situations where the lives of vulnerable people are at stake. .“We didn’t set out to and are not setting out to design an AI system that will take the place of a counselor, or that will directly interact with a person who might be in crisis,” says Dan Fichter, the organization’s head of AI and engineering. This human connection is important in all mental health services, but it might be especially important for the people the Trevor Project serves. According to the organization’s own research in 2019, LGBTQ youth with at least one accepting adult in their life were 40% less likely to report a suicide attempt in the previous year. .The AI-powered training role-play, called the crisis contact simulator and supported by money and engineering help from Google, is the second project the organization has developed this way: it also uses a machine-learning algorithm to help determine who’s at highest risk of danger. (It trialed several other approaches, including many that didn’t use AI, but the algorithm simply gave the most accurate predictions for who was experiencing the most urgent need.).AI-powered risk assessment isn’t new to suicide prevention services: the Department of Veterans Affairs also uses machine learning to identify at-risk veterans in its clinical practices, as the New York Times reported late last year. .Opinions vary on the usefulness, accuracy, and risk of using AI in this way. In specific environments, AI can be more accurate than humans in assessing people’s suicide risk, argues Thomas Joiner, a psychology professor at Florida State University who studies suicidal behavior. In the real world, with more variables, AI seems to perform about as well as humans. What it can do, however, is assess more people at a faster rate. .Thus, it’s best used to help human counselors, not replace them. The Trevor Project still relies on humans to perform full risk assessments on young people who use its services. And after counselors finish their role-plays with Riley, those transcripts are reviewed by a human. .The crisis contact simulator was developed because doing role-plays takes up a lot of staff time and is limited to normal working hours, even though a majority of counselors plan on volunteering during night and weekend shifts. But even if the aim was to train more counselors faster, and better accommodate volunteer schedules, efficiency wasn’t the only ambition. The developers still wanted the role-play to feel natural, and for the chatbot to nimbly adapt to a volunteers’ mistakes. Natural-language-processing algorithms, which had recently gotten really good at mimicking human conversations, seemed like a good fit for the challenge. After testing several options, the Trevor Project settled on OpenAI’s GPT-2 algorithm..The AI is the largest language model ever created and can generate amazing human-like text on demand but wont bring us closer to true intelligence..The chatbot uses GPT-2 for its baseline conversational abilities. That model is trained on 45 million pages from the web, which teaches it the basic structure and grammar of the English language. The Trevor Project then trained it further on all the transcripts of previous Riley role-play conversations, which gave the bot the materials it needed to mimic the persona..Throughout the development process, the team was surprised by how well the chatbot performed. There is no database storing details of Riley’s bio, yet the chatbot stayed consistent because every transcript reflects the same storyline..But there are also trade-offs to using AI, especially in sensitive contexts with vulnerable communities. GPT-2, and other natural-language algorithms like it, are known to embed deeply racist, sexist, and homophobic ideas. More than one chatbot has been led disastrously astray this way, the most recent being a South Korean chatbot called Lee Luda that had the persona of a 20-year-old university student. After quickly gaining popularity and interacting with more and more users, it began using slurs to describe the queer and disabled communities..The Trevor Project is aware of this and designed ways to limit the potential for trouble. While Lee Luda was meant to converse with users about anything, Riley is very narrowly focused. Volunteers won’t deviate too far from the conversations it has been trained on, which minimizes the chances of unpredictable behavior..This also makes it easier to comprehensively test the chatbot, which the Trevor Project says it is doing. “These use cases that are highly specialized and well-defined, and designed inclusively, don’t pose a very high risk,” says Nenad Tomasev, a researcher at DeepMind..This isn’t the first time the mental health field has tried to tap into AI’s potential to provide inclusive, ethical assistance without hurting the people it’s designed to help. Researchers have developed promising ways of detecting depression from a combination of visual and auditory signals. Therapy “bots,” while not equivalent to a human professional, are being pitched as alternatives for those who can’t access a therapist or are uncomfortable  confiding in a person. .Each of these developments, and others like it, require thinking about how much agency AI tools should have when it comes to treating vulnerable people. And the consensus seems to be that at this point the technology isn’t really suited to replacing human help. .Still, Joiner, the psychology professor, says this could change over time. While replacing human counselors with AI copies is currently a bad idea, “that doesn’t mean that it’s a constraint that’s permanent,” he says. People, “have artificial friendships and relationships” with AI services already. As long as people aren’t being tricked into thinking they are having a discussion with a human when they are talking to an AI, he says, it could be a possibility down the line. .In the meantime, Riley will never face the youths who actually text in to the Trevor Project: it will only ever serve as a training tool for volunteers. “The human-to-human connection between our counselors and the people who reach out to us is essential to everything that we do,” says Kendra Gaunt, the group’s data and AI product lead. “I think that makes us really unique, and something that I don’t think any of us want to replace or change.”.An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.\n"}
{"prompt":"Inari's SEEDesign platform uses predictive design and artificial intelligence (AI) to understand the genetics of plants and makes blueprints for its gene-editing tools that can make multiple edits in a single genome. The edits can make crop seeds with higher yields and lower agricultural footprints.'\n\n###\n\n","completion":" Inari wants to engineer crops that require less water, fertilizer, pesticides and land. The company is focusing on soybeans and corn as its first crops because they require 300 million acres of land in North and South America to grow. Inari’s SEEDesign platform can increase soybean and corn yield by 20% while lowering water usage by 40% and reducing corn’s nitrogen needs by 40%..Genetically modifying crops can involve removing or knocking out unwanted genes for a specific purpose, such as reducing the need for pesticides. Genetic modification can also artificially insert genes into a plant. Usually, bacteria are used to deliver the new genes..The introduction of foreign genes into plants is one of the main controversies. However, Inari is not introducing outside genes into its crops. Instead, the company is altering genes that already exist in the plants through gene editing..Its SEEDesign platform uses predictive design and artificial intelligence (AI) to understand the genetics of plants and makes blueprints for its gene-editing tools that can make multiple edits in a single genome. The edits can make crop seeds with higher yields and lower agricultural footprints.\n"}
{"prompt":"A change to TikTok’s U.S. privacy policy on Wednesday introduced a new section that says the social video app “may collect biometric identifiers and biometric information” from its users’ content. This includes things like “faceprints and voiceprints,” the policy explained.\n\n###\n\n","completion":" A change to TikTok’s U.S. privacy policy on Wednesday introduced a new section that says the social video app “may collect biometric identifiers and biometric information” from its users’ content. This includes things like “faceprints and voiceprints,” the policy explained. Reached for comment, TikTok could not confirm what product developments necessitated the addition of biometric data to its list of disclosures about the information it automatically collects from users, but said it would ask for consent in the case such data collection practices began..The biometric data collection details were introduced in the newly added section, “Image and Audio Information,” found under the heading of “Information we collect automatically” in the policy..The first part of the new section explains that TikTok may collect information about the images and audio that are in users’ content, “such as identifying the objects and scenery that appear, the existence and location within an image of face and body features and attributes, the nature of the audio, and the text of the words spoken in your User Content.”.While that may sound creepy, other social networks do object recognition on images you upload to power accessibility features (like describing what’s in an Instagram photo, for example), as well as for ad targeting purposes. Identifying where a person and the scenery is can help with AR effects, while converting spoken words to text helps with features like TikTok’s automatic captions..The policy also notes this part of the data collection is for enabling “special video effects, for content moderation, for demographic classification, for content and ad recommendations, and for other non-personally-identifying operations,” it says..We may collect biometric identifiers and biometric information as defined under US laws, such as faceprints and voiceprints, from your User Content. Where required by law, we will seek any required permissions from you prior to any such collection..The statement itself is vague, as it doesn’t specify whether it’s considering federal law, states laws, or both. It also doesn’t explain, as the other part did, why TikTok needs this data. It doesn’t define the terms “faceprints” or “voiceprints.” Nor does it explain how it would go about seeking the “required permissions” from users, or if it would look to either state or federal laws to guide that process of gaining consent..That’s important because as it stands today, only a handful of U.S. states have biometric privacy laws, including Illinois, Washington, California, Texas and New York. If TikTok only requested consent, “where required by law,” it could mean users in other states would not have to be informed about the data collection..Reached for comment, a TikTok spokesperson could not offer more details on the company’s plans for biometric data collection or how it may tie in to either current or future products..“As part of our ongoing commitment to transparency, we recently updated our Privacy Policy to provide more clarity on the information we may collect,” the spokesperson said..The company also pointed us to an article about its approach to data security, TikTok’s latest Transparency Report and the recently launched privacy and security hub, which is aimed at helping people better understand their privacy choices on the app..Under the Trump administration, the federal government attempted to ban TikTok from operating in the U.S. entirely, calling the app a national security threat because of its ownership by a Chinese company. TikTok fought back against the ban and went on record to state it only stores TikTok U.S. user data in its U.S. data centers and in Singapore..It said it has never shared TikTok user data with the Chinese government nor censored content, despite being owned by Beijing-based ByteDance. And it said it would never do so, if asked..Though the TikTok ban was initially stopped in the courts, the federal government appealed the rulings. But when President Biden took office, his administration put the appeal process on hold as it reviewed the actions taken by his predecessor. And although Biden has, as of today, signed an executive order to restrict U.S. investment in Chinese firms linked to surveillance, his administration’s position on TikTok remains unclear..It is worth noting, however, that the new disclosure about biometric data collection follows a $92 million settlement in a class action lawsuit against TikTok, originally filed in May 2020, over the social media app’s violation of Illinois’ Biometric Information Privacy Act. The consolidated suit included more than 20 separate cases filed against TikTok over the platform’s collection and sharing of the personal and biometric information without user consent. Specifically, this involved the use of facial filter technology for special effects..In that context, TikTok’s legal team may have wanted to quickly cover themselves from future lawsuits by adding a clause that permits the app to collect personal biometric data..The disclosure, we should also point out, has only been added to the U.S. Privacy Policy, as other markets like the EU have stricter data protection and privacy laws..The new section was part of a broader update to TikTok’s Privacy Policy, which included other changes both large and small, ranging from corrections of earlier typos to revamped or even entirely new sections. Most of these tweaks and changes could be easily explained, though — like new sections that clearly referenced TikTok’s e-commerce ambitions or adjustments aimed at addressing the implications of Apple’s App Tracking Transparency on targeted advertising..This is in addition to the “Information you choose to provide,” which comes from when you register, contact TikTok or upload content. In that case, TikTok collects your registration info (username, age, language, etc.), profile info (name, photo, social media accounts), all your user-generated content on the platform, your phone and social network contacts, payment information, plus the text, images and video found in the device’s clipboard. (TikTok, as you may recall, got busted by Apple’s iOS 14 feature that alerted users to the fact that TikTok and other apps were accessing iOS clipboard content. Now, the policy says TikTok “may collect” clipboard data “with your permission.”).Some users reported seeing a pop-up message alerting them to the Privacy Policy update, but the page was not available when they tried to read it. Others complained of seeing the pop-up repeatedly. This issue doesn’t appear to be universal. In tests, we did not have an issue with the pop-up ourselves.\n"}
{"prompt":"The study examined data from nearly 40,000 hospitalizations at Michigan Medicine in 2018 and 2019. Patients developed sepsis in 2,552 of those hospitalizations. Epic’s sepsis tool missed 1,709 of those cases, around two-thirds of which were still identified and treated quickly. It only identified 7 percent of sepsis cases that were missed by a physician. The analysis also found a high rate of false positives: when an alert went off for a patient, there was only a 12 percent chance that the patient actually would develop sepsis.\n\n###\n\n","completion":" The biggest electronic health record company in the United States, Epic Systems, claims it can solve a major problem for hospitals: identifying signs of sepsis, an often deadly complication from infections that can lead to organ failure. It’s a leading cause of death in hospitals. .But the algorithm doesn’t work as well as advertised, according to a new study published in JAMA Internal Medicine on Monday. Epic says its alert system can correctly differentiate patients who do and don’t have sepsis 76 percent of the time. The new study found it was only right 63 percent of the time. .Sepsis is hard to spot early, but starting treatment as soon as possible can improve patients chances of survival. The Epic system, and other automated warning tools like it, scan patient test results for signals that someone could be developing the condition. Around a quarter of US hospitals use Epic’s electronic medical records, and hundreds of hospitals use its sepsis prediction tool, including the health center at the University of Michigan, where study author Karandeep Singh is an assistant professor. .The study examined data from nearly 40,000 hospitalizations at Michigan Medicine in 2018 and 2019. Patients developed sepsis in 2,552 of those hospitalizations. Epic’s sepsis tool missed 1,709 of those cases, around two-thirds of which were still identified and treated quickly. It only identified 7 percent of sepsis cases that were missed by a physician. The analysis also found a high rate of false positives: when an alert went off for a patient, there was only a 12 percent chance that the patient actually would develop sepsis. .Part of the problem, Singh told Stat News, seemed to be in the way the Epic algorithm was developed. The algorithm used information on bills for sepsis to define which patients had sepsis. That means it’s catching cases where the doctor already thinks there’s an issue. “It’s essentially trying to predict what physicians are already doing,” Singh said. It’s also not the measure of sepsis that researchers would ordinarily use. .Tools that mine patient data to predict what could happen with their health are common and can be useful for doctors. But they’re only as good as the data they’re developed with, and they should be subject to outside evaluation. When researchers scrutinize tools like this one, they sometimes find holes: for example, one algorithm used by major health systems to flag patients who need special attention was biased against Black patients, a 2019 study found. .Epic rolled out another predictive tool, called the Deterioration Index, during the early days of the COVID-19 pandemic. It was designed to help doctors decide which patients should move into intensive care and which could be fine without it. The pandemic was an emergency, so hospitals around the country started using it before it was subject to any sort of independent evaluation. Even now, there has been limited research on the tool. One small study showed it could identify high- and low-risk patients but might not be useful to doctors. There could be unforeseen problems or biases in the system that are going unnoticed, Brown University researchers warned in Undark..If digital tools are going to live up to their potential in healthcare, companies like Epic should be transparent about how they’re made and they should be regularly monitored to make sure they’re working well, Singh says on Twitter. These tools are becoming more and more common, so these types of issues aren’t going away, Roy Adams, an assistant professor at Johns Hopkins School of Medicine, told Wired. “We need more independent evaluations of these proprietary systems,” he says..Correction June 28th, 5:58PM ET: The original version of this story suggested the algorithm defined sepsis based on when doctors submit a bill for treatment. Instead, the algorithm uses information from billing codes to define sepsis. We regret the error.\n"}
{"prompt":"Metaspectral, a company offering technology that derives insights from AI using ultra-high-resolution, visible-to-infrared (hyperspectral) imagery, has been awarded more than $300,000 in grant funding from the CleanBC Plastics Action Fund. The fund is funded by the BC Government and administered by Alacrity Cleantech.\n\n###\n\n","completion":" Metaspectral, a company offering technology that derives insights from AI using ultra-high-resolution, visible-to-infrared (hyperspectral) imagery, has been awarded more than $300,000 in grant funding from the CleanBC Plastics Action Fund. The fund is funded by the BC Government and administered by Alacrity Cleantech..The CleanBC Plastics Action Fund supports B.C. businesses creating value from used plastics by including more recycled material in product manufacturing to keep plastic out of landfills..Metraspectral will use this funding for the development of computer vision, artificial intelligence, and robotics designed to sort consumer waste, increase efficiency in processing materials and improve the quality of post-consumer recycled plastic. The project is slated for completion by Dec. 31, 2021..The company says that, by using ultra-high-resolution hyperspectral imaging, AI is able to efficiently distinguish among types of plastics for accurate and easy sorting, noting that it’s impossible for humans to differentiate between different types of clear plastic bottles with the naked eye..The company hopes this technology will support the circular economy for plastics and stimulate more local processing capacity for recycling as more manufacturers begin using the higher-quality recycled plastics. Metaspectral’s technology will be an important ally in achieving those objectives.  Metraspectral will also be contributing to the Government of Canada’s Greening Government strategy of increasing the ratio of plastics that are recycled to 75% by 2030, up from 9% today..Contents: -What are #PPAs & #VPPAs -A case study of a recent VPPA project selection -Offer best practices in soliciting proposals -Explain cash flows in VPPAs, & more .Download your copy to learn: -How leaders can navigate the current landscape -Understand investor motivations -Evaluate strategic options to attract ESG investors to your business \n"}
{"prompt":"With the help of AI programs capable of filtering sound, canceling noise, and identifying the meaning of words depending on the context, most of these challenges have been tackled. Today, Speech Recognition shows an efficiency of 95%, which stood at less than 20% around 30 years back from now. The only biggest challenge remaining now for programmers is making machines capable of understanding emotions and feelings and satisfactory progress in this part. \n\n###\n\n","completion":" If you have ever wondered how your smartphone can comprehend instructions like “Call Mom,” “Send a Message to Boss,” “Play the Latest Songs,” “Switch ON the AC,” then you are not alone. But how is this done? The one simple answer is Speech Recognition. Speech Recognition has gone through the roof in the recent 4-5 years and is making our lives more comfortable every day. .Speech Recognition was first introduced by IBM in 1962 when it unveiled the first machine capable of converting human voice to text. Today, powered by the latest technologies like Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning, speech recognition is touching new milestones. .This latest technological advancement is being used across the globe by top companies to make their user’s experience efficient and smooth. Technologies like Amazon’s Alexa, Apple’s Siri, Google Assistant, Google Speech, Google Dictate, Facebook’s Oculus VR, and Microsoft’s Cortana are all examples of Speech Recognition. .The expanding usage of speech-to-text technologies has also opened many new job domains, and students are wonderfully exploiting them. Many students are now joining courses like PGP in AI and Machine Learning after completing their graduation to improve their prospects. The high salary package of around INR 15 lakh for freshers is the 2nd biggest reason attracting students towards this, the biggest reason being the fantastic job role. .Speech Recognition was a very niche domain before the advent of AI and ML, which has completely transformed it now. Before we understand how AI and ML made changes, let’s understand the nuances of what all these terminologies are. .Artificial Intelligence is the technology by which machines become capable of demonstrating intelligence like humans or animals. Initially, AI was only about memorizing data and producing results accordingly; however, now it is much more than that as machines perform various activities like Speech Recognition, Object Recognition, Translating Texts, and a lot more. .Another latest addition to AI has been Deep Learning. With the help of Deep Learning, machines can process data and create patterns that help them make valuable decisions. This behavior of a machine through Deep Learning is similar to the behavior of a human brain. Deep Learning activities can be “Supervised,” “Semi-Supervised,” as well as “Unsupervised.” .Machine Learning is a subdomain of AI which teaches machines to memorize past events and activities. Through ML, machines are trained to retain various data sets’ information and outputs and identify patterns in these decisions. It allows the machine to learn by itself without the help of any programming code. .An example of Machine Learning is the e-Commerce websites suggesting products to you. The code, once written, allows machines to evolve on themselves and analyze user behavior and thus recommend products according to their preferences and past purchases. This involves Zero Human Interference and makes use of approaches like Artificial Neural Networks (ANN). .Note: Speech Recognition and Voice Recognition are two different things. While the former comprehends a voice sample and converts it into a text sample, the sole purpose of the latter is to identify the voice and recognize to whom it belongs. Voice Recognition is often used for security and authenticity purposes. .The usage of Speech Recognition in our devices has grown considerably due to the developments in AI and ML technologies. Speech Recognition is now being used for tasks ranging from awakening your appliances and gadgets to monitoring your fitness, playing mood-booster songs, running queries on search engines, and even making phone calls. .The global market for Speech Recognition, currently growing at a Cumulative Annual Growth Rate (CAGR) of 17.2%, is expected to breach the $25 billion mark by 2025. However, there were enormous challenges initially that have been tackled with the use of AI and ML now. .When in its initial phase, some of the biggest challenges for Speech Recognition were Poor Voice Recording Devices, Huge Noise in the Voice Samples, Different Pitches in Speech of the Same User, etc. In addition to this, the changing dialects and grammatical factors like Homonyms were also a big challenge. .With the help of AI programs capable of filtering sound, canceling noise, and identifying the meaning of words depending on the context, most of these challenges have been tackled. Today, Speech Recognition shows an efficiency of 95%, which stood at less than 20% around 30 years back from now. The only biggest challenge remaining now for programmers is making machines capable of understanding emotions and feelings and satisfactory progress in this part. .The increasing efficiency in Speech Recognition is becoming an essential driving factor in its success, and top tech giants are leveraging these benefits. More than 20% of users searched on Google through Voice in 2016 only, and this number is expected to be far more prominent now. Businesses today are automating their services to make their operations efficient and introducing Speech Recognition facilities at the top of their to-do lists. .Speech Recognition is no doubt one of the best innovations made by expanding technological developments. However, there is one thing to be noted if you are also planning to enter this sector. The domain is inter-mingled, and the mere knowledge provided by a Speech Recognition course won’t be enough for you to survive in this field. .Therefore, it is essential that you also sharpen your skills in allied concepts like Data Science, Data Analytics, Machine Learning, Artificial Intelligence, Neural Networks, DevOps, and Deep Learning. So what are you waiting for now? Hurry up and join an online course in Speech Recognition now! \n"}
{"prompt":"More than 30 civil liberties groups are accusing the UK Home Office and police of bypassing Parliament to introduce live facial recognition technology (LFRT).\r\n\r\nEarlier this month — during the parliamentary summer recess — the College of Policing released guidance on the use of LFRT without any publicity or official announcement.\n\n###\n\n","completion":" More than 30 civil liberties groups are accusing the UK Home Office and police of bypassing Parliament to introduce live facial recognition technology (LFRT). .Earlier this month — during the parliamentary summer recess — the College of Policing released guidance on the use of LFRT without any publicity or official announcement. .The Surveillance Camera Code of Practice allows police services and councils to collect camera footage and compare it with a database of people on a watch-list. However, a large group of civil liberties organizations including Liberty, Amnesty International and Privacy International says that LFRT poses significant risks to civil society. .In a democratic society, it is imperative that intrusive technologies are subject to effective scrutiny. Police and the Home Office have, so far, completely bypassed Parliament on the matter of LFRT, they write. .We are not aware of any intention to subject LFRT plans to parliamentary consideration, despite the intrusiveness of this technology, its highly controversial use over a number of years, and the dangers associated with its use. .LFRT doesnt have a good track record in the UK. Last year, the Metropolitan Police used what it called a tried and tested technology on three occasions in the capital. .However, it later emerged that, for example, when 8,600 faces were scanned in Oxford Circus, there were eight positive hits - but that only one was legitimate. .After the technology was used in South Wales, it was ruled unlawful following a legal challenge from ma man whose face was scanned, with judges ruling that the police were allowed too much discretion over its use. .And just two months ago, UK Information Commissioner Elizabeth Denham warned that LFRT had the potential to be used inappropriately, excessively or even recklessly, adding that none of the deployments she examined was fully compliant with data protection law. .The implications come not solely from privacy and data protection perspectives, but from the larger ethical question for a democratic society permitting and seemingly condoning the rollout of such intrusive technology, they say. \n"}
{"prompt":"The software in effect gives the gastroenterologist conducting the endoscopy a second opinion. It is hoped that it will reduce the high number of oesophageal cancers – up to 25% – that doctors miss during an endoscopy because early signs of the disease are very difficult to spot.\n\n###\n\n","completion":" One of the NHS’s leading hospital trusts has begun using artificial intelligence to help detect cancer in the gullet, which kills 8,000 Britons a year. It is hoped the technology will increase the number of cases of cancer in the oesophagus that doctors spot..Oesophageal cancer is one of the deadliest forms of cancer. It is hard to detect, particularly in its early stages, and many people who get it die soon after their diagnosis. Fewer than one in five of those diagnosed are still alive five years later..Developers of the software, called CADU, say it is the first in the world to use artificial intelligence technology to help doctors identify cancerous cells in the food pipe. It analyses pictures captured by a tiny camera put down a patient’s throat during an endoscopy, which is used to diagnose oesophageal cancer and highlight areas of concern to the medic..The software in effect gives the gastroenterologist conducting the endoscopy a second opinion. It is hoped that it will reduce the high number of oesophageal cancers – up to 25% – that doctors miss during an endoscopy because early signs of the disease are very difficult to spot..The scientists and doctors who invented CADU believe that better identification of oesophageal cancer while it is still at an early stage will lead to patients receiving prompt treatment and thus having a better chance of survival. About 9,000 people a year in the UK are diagnosed with oesophageal cancer, which is linked to smoking and drinking..University College London hospital trust started using CADU in February, after it was approved by the Medicines and Healthcare products Regulatory Agency, to improve its diagnosis rates..“We know from our data that between 15% and 20% of patients can have an early cancer develop within 12 months of an initial ‘normal’ endoscopy, suggesting that perhaps this was overlooked at the first examination or was very subtle,” said Dr Rehan Haidry, a consultant gastroenterologist at UCLH..CADU was developed by experts from UCLH and University College London (UCL) working in conjunction with a UCL spinout technology firm called Odin Vision..Peter Mountney, the firm’s chief executive and an honorary associate professor at UCL, said: “We are very excited to achieve this landmark procedure and use our AI technology to support doctors in the fight against one of the most aggressive forms of cancer..“CADU shows promise at a clinically meaningful level and the next stage is to validate that in a larger multi-centre trial involving a number of hospitals in several countries.”.The CADU device can aid diagnosis because it has been shown hundreds of thousands of images of diseased tissue and has learned to spot cancerous growths from the visual patterns in those images. Experts in oesophageal cancer said the technology could prove to be a breakthrough..“Harnessing the power of artificial intelligence in combination with standard diagnostic procedures such as endoscopies offers the potential to detect cancers earlier,” said Nicola Valeri, a professor of gastrointestinal oncology at the Institute of Cancer Research..The NHS in England is increasingly exploring how AI can help diagnose, monitor and treat medical conditions such as sepsis, cancer and Parkinson’s disease. Its NHS AI Lab has provided £140m for trials of software that may be able to analyse breast cancer screening scans and assess people who have just had a stroke, for example..Sir Simon Stevens, NHS England’s chief executive, said last year that while AI was still in its infancy, “when the latest chapter in the history of medicine comes to be written, AI in healthcare will doubtless rank alongside earlier advances such as the stethoscope, the X-ray and the blood test.”.NHS hospitals in Leicester, Nottingham and Oxford have been assessing AI software called the virtual nodule clinic, which its developers hope will help doctors diagnose lung cancer, Britain’s biggest cancer killer. Its early results have led to it receiving a £1.5m grant for the technology to be used in a research project involving 10 hospitals to see if it aids early detection.\n"}
