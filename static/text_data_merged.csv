article_index,text,issue
2,"Last December Synced compiled its first “Artificial Intelligence Failures” recap of AI gaffes from the previous year. AI has achieved remarkable progress, and many scientists dream of creating the Master Algorithm proposed by Pedro Domingos — which can solve all problems envisioned by humans. It’s unavoidable however that researchers, fledgling technologies and biased data will also produce blunders not envisioned by humans..We produce professional, authoritative, and thought-provoking content relating to artificial intelligence, machine intelligence, emerging technologies and industrial insights.","Amazon AI Recruiting tool favored male applicants over female. The software downgraded resumes that had the word ""women"""
3,"The team had been building computer programs since 2014 to review job applicants’ resumes with the aim of mechanizing the search for top talent, five people familiar with the effort told Reuters..Automation has been key to Amazon’s e-commerce dominance, be it inside warehouses or driving pricing decisions. The company’s experimental hiring tool used artificial intelligence to give job candidates scores ranging from one to five stars - much like shoppers rate products on Amazon, some of the people said..“Everyone wanted this holy grail,” one of the people said. “They literally wanted it to be an engine where I’m going to give you 100 resumes, it will spit out the top five, and we’ll hire those.”.But by 2015, the company realized its new system was not rating candidates for software developer jobs and other technical posts in a gender-neutral way..That is because Amazon’s computer models were trained to vet applicants by observing patterns in resumes submitted to the company over a 10-year period. Most came from men, a reflection of male dominance across the tech industry..In effect, Amazon’s system taught itself that male candidates were preferable. It penalized resumes that included the word “women’s,” as in “women’s chess club captain.” And it downgraded graduates of two all-women’s colleges, according to people familiar with the matter. They did not specify the names of the schools..Amazon edited the programs to make them neutral to these particular terms. But that was no guarantee that the machines would not devise other ways of sorting candidates that could prove discriminatory, the people said..The Seattle company ultimately disbanded the team by the start of last year because executives lost hope for the project, according to the people, who spoke on condition of anonymity. Amazon’s recruiters looked at the recommendations generated by the tool when searching for new hires, but never relied solely on those rankings, they said..Amazon declined to comment on the technology’s challenges, but said the tool “was never used by Amazon recruiters to evaluate candidates.” The company did not elaborate further. It did not dispute that recruiters looked at the recommendations generated by the recruiting engine..The companys experiment, which Reuters is first to report, offers a case study in the limitations of machine learning. It also serves as a lesson to the growing list of large companies including Hilton Worldwide Holdings Inc HLT.N and Goldman Sachs Group Inc GS.N that are looking to automate portions of the hiring process..Some 55 percent of U.S. human resources managers said artificial intelligence, or AI, would be a regular part of their work within the next five years, according to a 2017 survey by talent software firm CareerBuilder..Employers have long dreamed of harnessing technology to widen the hiring net and reduce reliance on subjective opinions of human recruiters. But computer scientists such as Nihar Shah, who teaches machine learning at Carnegie Mellon University, say there is still much work to do..“How to ensure that the algorithm is fair, how to make sure the algorithm is really interpretable and explainable - that’s still quite far off,” he said..Amazon’s experiment began at a pivotal moment for the world’s largest online retailer. Machine learning was gaining traction in the technology world, thanks to a surge in low-cost computing power. And Amazon’s Human Resources department was about to embark on a hiring spree: Since June 2015, the company’s global headcount has more than tripled to 575,700 workers, regulatory filings show..So it set up a team in Amazon’s Edinburgh engineering hub that grew to around a dozen people. Their goal was to develop AI that could rapidly crawl the web and spot candidates worth recruiting, the people familiar with the matter said..The group created 500 computer models focused on specific job functions and locations. They taught each to recognize some 50,000 terms that showed up on past candidates’ resumes. The algorithms learned to assign little significance to skills that were common across IT applicants, such as the ability to write various computer codes, the people said..Instead, the technology favored candidates who described themselves using verbs more commonly found on male engineers’ resumes, such as “executed” and “captured,” one person said..Gender bias was not the only issue. Problems with the data that underpinned the models’ judgments meant that unqualified candidates were often recommended for all manner of jobs, the people said. With the technology returning results almost at random, Amazon shut down the project, they said..Kevin Parker, chief executive of HireVue, a startup near Salt Lake City, said automation is helping firms look beyond the same recruiting networks upon which they have long relied. His firm analyzes candidates’ speech and facial expressions in video interviews to reduce reliance on resumes..You weren’t going back to the same old places; you weren’t going back to just Ivy League schools, Parker said. His companys customers include Unilever PLC ULVR.L and Hilton..Goldman Sachs has created its own resume analysis tool that tries to match candidates with the division where they would be the “best fit,” the company said..Microsoft Corps MSFT.O LinkedIn, the worlds largest professional network, has gone further. It offers employers algorithmic rankings of candidates based on their fit for job postings on its site..Some activists say they are concerned about transparency in AI. The American Civil Liberties Union is currently challenging a law that allows criminal prosecution of researchers and journalists who test hiring websites’ algorithms for discrimination..“We are increasingly focusing on algorithmic fairness as an issue,” said Rachel Goodman, a staff attorney with the Racial Justice Program at the ACLU..Still, Goodman and other critics of AI acknowledged it could be exceedingly difficult to sue an employer over automated hiring: Job candidates might never know it was being used..As for Amazon, the company managed to salvage some of what it learned from its failed AI experiment. It now uses a “much-watered down version” of the recruiting engine to help with some rudimentary chores, including culling duplicate candidate profiles from databases, one of the people familiar with the project said.",The company realized its new system was not rating candidates for software developer jobs and other technical posts in a gender-neutral way.
5,"What are the allegations against Cambridge Analytica?The data analytics firm used personal information harvested from more than 50 million Facebook profiles without permission to build a system that could target US voters with personalised political advertisements based on their psychological profile, according to Christopher Wylie, a former Cambridge Analytica contractor who helped build the algorithm. Employees of Cambridge Analytica, including the suspended CEO Alexander Nix, were also filmed boasting of using manufactured sex scandals, fake news and dirty tricks to swing elections around the world..How is Facebook involved in the scandal?The social media company has received a number of warnings about its data security policies in recent years and had known about the Cambridge Analytica data breach since 2015, but only suspended the firm and the Cambridge university researcher who harvested user data from Facebook earlier this month. A former Facebook manager has warned that hundreds of millions of users are likely to have had their private information used by private companies in the same way. On Sunday, Facebook ran adverts in several major UK and US newspapers apologising for the data breach, and said it was investigating other applications that had access to large amounts of user data..What has been the reaction to the scandal?Investigators from Britain’s data watchdog raided Cambridge Analytica’s London offices over Friday night, and the main consumer protection body in the US is reported to have opened an investigation into whether Facebook has violated privacy agreements. Billions of dollars have been wiped off Facebook’s stock market valuation this week as a growing #DeleteFacebook movement and regulatory fears have spooked investors..What is the Brexit link?During the Brexit referendum, a digital services firm linked to Cambridge Analytica received a £625,000 payment from a pro-Brexit campaign organisation which had been given the money by Vote Leave, potentially violating referendum spending rules. Shahmir Sanni, a pro-Brexit whistleblower, told the Observer newspaper that he had passed evidence supporting his claims to the police and the Electoral Commission..Separately, around £3.4m was spent by different Brexit Leave campaigns with Canadian data firm AggregateIQ during the run up to the EU referendum, including £2.7m by the official Vote Leave campaign (40% of their total budget). Christopher Wylie says he played a role in setting up AIQ in 2013, around same time he worked for Cambridge Analytica. AIQ have say they have never entered into a contract with Cambridge Analytica and had no communications with them during the referendum campaign..Why is the Electoral Commission involved?British electoral law forbids co-ordination between different campaign groups, which must all comply with strict spending limits. If they plan tactics or co-ordinate together, the organisations must share a cap on spending..Sanni has alleged that key figures in the Vote Leave campaign may have violated referendum spending rules and then attempted to destroy evidence. According to him, the £625,000 donation Vote Leave made to a pro-Brexit youth campaign group, who then spent the money on digital campaigning services with a Cambridge Analytica-linked firm, was not a genuine donation. Sanni also alleges that when the commission opened an investigation into Vote Leave last March, key Vote Leave figures tried to hide the possible co-ordination by removing themselves from the Google drive both campaign groups shared.",Influenced public through individualized ads and messaging that played a significant role in political campaigns in the US. Micro-targeting individuals in political campaigns.
7,"This is the third Synced year-end compilation of “Artificial Intelligence Failures.” Despite AI’s rapid growth and remarkable achievements, a review of AI failures remains necessary and meaningful. Our aim is not to downplay or mock research and development results, but rather to take a look at what went wrong with the hope we can do better next time..A leading facial-recognition system identified three-time Super Bowl champion Duron Harmon of the New England Patriots, Boston Bruins forward Brad Marchand, and 25 other New England professional athletes as criminals. Amazon’s Rekognition software incorrectly matched the athletes to a database of mugshots in a test organized by the Massachusetts chapter of the American Civil Liberties Union (ACLU). Nearly one-in-six athletes were falsely identified..The misclassifications were an embarrassment for Amazon, which has marketed Rekognition to police agencies for use in their investigations. “This technology is flawed,” Harmon said in an ACLU statement, and “should not be used by the government without protections.”.In March the CEO of a UK-based energy firm got a phone call from his boss at the German parent company instructing him to transfer €220,000 ($243,000) to a Hungarian supplier. The ‘boss’ said the request was urgent and directed the UK CEO to transfer the money promptly..It turns out the phone call was made by criminals who used AI-based software to mimic the boss’s voice, including the “slight German accent and the melody of his voice,” as reported in The Wall Street Journal. Such AI-powered cyberattacks are a new challenge for companies, as traditional cybersecurity tools designed for keeping hackers off corporate networks can’t identify spoofed voices..Then there’s the artificial intelligence system that’s not very “artificial.” That was the accusation leveled at Engineer.ai in an article that appeared in The Wall Street Journal in August. The Indian startup claimed to have built an AI-assisted app development platform, but the WSJ, citing former and current employees, suggested it relies mostly on human engineers and “exaggerates its AI capabilities to attract customers and investors.”.Engineer.ai has attracted nearly US$30 million in funding from a SoftBank-owned firm and others. Founder Sachin Dev Duggal says the company’s AI tools are only human-assisted, and that it provides a service to help customers make more than 80 percent of a mobile app from scratch in about an hour. The WSJ story argued that Engineer.ai did not use AI to assemble code as it claimed, instead it used human engineers in India and elsewhere to put together the app. Indian media Inc42 claims that Engineer.ai had released a detailed statement to clear the accusation, and declared that it never claimed to ‘automated software development,’ but a human-assisted AI. The case is so far left open..Is it a mushroom or is it a pretzel? OK forget about the pretzel, 99 percent sure this is a sea lion…Or wait, actually it could be a fox squirrel. Yup, looks like this one is a fox squirrel for sure… except that it’s not, nope. It’s a bullfrog. Wait….Computer vision strives to understand what it sees the way humans do — but remains far from that goal. In July, researchers from Berkeley, University of Chicago and University of Washington hand-curated a dataset of 7,500 unretouched nature photos which are able to confuse SOTA computer vision models 98 percent of the time..The ImageNet-A dataset of “natural adversarial examples” is but a tiny subset of the 14 million labeled images in industry-standard ImageNet, exploiting flaws in current classifiers which can over-rely for example on color, texture, and background cues..A Boston Robotics’ Spot robodog suffered a dramatic onstage death while being live demo’d by company CEO Marc Raibert at re:MARS 2019 in Las Vegas this summer. The commercial robot was tasked with walking but its legs seemed to buckle. It stumbled desperately before sadly collapsing to the floor, where it lay motionless in front of the gasping audience..Founded in 1992, American engineering and robotics design company Boston Dynamics has created incredible robots such as BigDog, Atlas, SpotMini, etc. While these flexible and versatile bots can jump over logs, open doors and even perform search and rescue tasks, this was not the first time they’ve succumbed to stage fright..When Hong Kong real estate tycoon Samathur Li Kin-kan let an automated platform based on a supercomputer called “K1” manage part of his fortune the goal was to boost funds. Instead the AI regularly lost up to US$20 million daily, according to a Bloomberg story..Li filed a $23 million lawsuit against Raffaele Costa, CEO and founder of Tyndaris Investments, which sold Li the fintech service. The suit alleges Costa exaggerated K1’s abilities and is the first known case of a court action filed over automated investment losses. A verdict is expected in April 2020..An app that uses neural networks to virtually disrobe images of women caused public outrage early this year before it was shut down by its creator, anonymous programmer ‘Alberto.’ The DeepNude app used a photo of a clothed person as input to create a new, naked image of the same person..The app of course has no X-Ray ability, it merely replaces clothes with naked breasts and a vulva — as such only realistically functioning on images of women. After a Vice story made it viral, DeepNude was taken down. Multiple people however then uploaded their own DeepNude-style apps to code repository GitHub, which responded by removing all clothes-stripping code from its platform, citing the “Sexually Obscene” section of the GitHub Community Guidelines..Researchers with the San Diego-based AI firm Kneron were able to fool facial recognition systems at banks, border crossings and airports using a printed 3D masks — and in some cases only a 2D photo..The team used high-quality masks based on people in databases the facial recognition system would access. The method was tested in public locations globally. In stores in Asia where facial recognition technology is deployed widely, the 3D masks deceived popular AliPay and WeChat payment systems. More alarmingly, at a self-boarding terminal at Amsterdam’s Schiphol Airport the team tricked a sensor with just a photo on a smartphone screen, Fortune reported..In 2015 the first futuristic, robot-staffed Henn-na Hotel opened in Japan to much fanfare. Bots staffed the front-desk and worked as cleaners, porters and in-room assistants. Early this year however, the hotel chain bucked global tech and labour trends and dismissed the last of their “unreliable, expensive and annoying” bots, replacing them with human workers..The robot-staff novelty had worn off as customer complaints accumulated — the bots frequently broke down, could not provide satisfactory answers to guest queries, and in-room assistants startled guests at night by interpreting snoring as a wake command. Henn-na Hotels says it will head back to the lab to see if it can develop a new generation of more capable hospitality bots..There was a dramatic scene at Chicago’s O’Hare International Airport in September when an unmanned catering cart suddenly broke bad on the tarmac, circling out of control and ever-closer to a vulnerable jet parked at a gate. Finally, a yellow-vested worker managed to stop the cart — by ramming and knocking it down with another vehicle..Although the cart was neither AI-equipped nor autonomous its frenzied behavior drew comparisons to robot combat along with comments warning of the perils of machines gone amok and lauding the humans who resist and defeat them..Need a comprehensive review of the past, present and future of modern AI research development? Trends of AI Technology Development Report is out!.2018 Fortune Global 500 Public Company AI Adaptivity Report is out!Purchase a Kindle-formatted report on Amazon.Apply for Insight Partner Program to get a complimentary full PDF report..We produce professional, authoritative, and thought-provoking content relating to artificial intelligence, machine intelligence, emerging technologies and industrial insights.",Cart was not AI equipped but started conversations on how robots combat along
9,"Children ordering (accidentally or otherwise) items from gadgets is nothing new. Major retailers have refunded purchases made by children playing with phones or computers, and with voice-activated devices making their way into homes, it’s a problem that parents will have to be on the lookout for. .The story could have stopped there, had it not ended up on a local morning show on San Diego’s CW6 News. At the end of the story, Anchor Jim Patton remarked: “I love the little girl, saying ‘Alexa ordered me a dollhouse,’” According to CW6 News, Echo owners who were watching the broadcast found that the remark triggered orders on their own devices. .Patton didn’t think that any of the devices went through with their purchases, who told The Verge that the station received a handful of reports of viewer devices attempting to order a dollhouse after hearing his remarks. “As for the number of people affected - I dont know,” Patton noted in an email. “Personally, Ive seen one other email and have been told there were others, as well as calls to our news desk with similar stories.” .Alexa’s settings can be adjusted through the device’s app, and users can either turn off voice ordering altogether, or add a passcode to prevent accidental purchases. ",A TV advert created by Amazon triggered Alexa devices in the US to order a Dolls House on behalf of 100s of Customers. This purchase decision without a human input has a potential to be harmful.
10,"When the Boston public school system announced new start times last December, some parents found the schedules unacceptable and pushed back. The algorithm used to set these times had been designed by MIT researchers, and about a week later, Kade Crockford, director of the Technology for Liberty Program at the ACLU of Massachusetts, emailed asking me to cosign an op-ed that would call on policymakers to be more thoughtful and democratic when they consider using algorithms to change policies that affect the lives of residents. Kade, who is also a Directors Fellow at the Media Lab and a colleague of mine, is always paying attention to the key issues in digital liberties and is great at flagging things that I should pay attention to. (At the time, I had no contact with the MIT researchers who designed the algorithm.).I made a few edits to her draft, and we shipped it off to the Boston Globe, which ran it on December 22, 2017, under the headline Don’t blame the algorithm for doing what Boston school officials asked. In the op-ed, we piled on in criticizing the changes but argued that people shouldnt criticize the algorithm, but rather the city’s political process that prescribed the way in which the various concerns and interests would be optimized. That day, the Boston Public Schools decided not to implement the changes. Kade and I high-fived and called it a day..The protesting families, Kade and I did what we thought was fair and just given the information that we had at the time. A month later, a more nuanced picture emerged, one that I think offers insights into how technology can and should provide a platform for interacting with policy—and how policy can reflect a diverse set of inputs generated by the people it affects. In what feels like a particularly dark period for democracy and during a time of increasingly out-of-control deployment of technology into society, I feel a lesson like this one has given me greater understanding of how we might more appropriately introduce algorithms into society. Perhaps it even gives us a picture of what a Democracy 2.0 might look like..A few months later, having read the op-ed in the Boston Globe, Arthur Delarue and Sébastien Martin, PhD students in the MIT Operations Research Center and members of the team that built Boston’s bus algorithm, asked to meet me. In very polite email, they told me that I didn’t have the whole story..Kade and I met later that month with Arthur, Sebastien, and their adviser, MIT professor Dimitris Bertsimas. One of the first things they showed us was a photo of the parents who had protested against the schedules devised by the algorithm. Nearly all of them were white. The majority of families in the Boston school system are not white. White families represent only about 15 percent the public school population in the city. Clearly something was off..The MIT researchers had been working with the Boston Public Schools on adjusting bell times, including the development of the algorithm that the school system used to understand and quantify the policy trade-offs of different bell times and, in particular, their impact on school bus schedules. The main goal was to reduce costs and generate optimal schedules..The MIT team described how the award-winning original algorithm, which focused on scheduling and routing, had started as a cost-calculation algorithm for the Boston Public Schools Transportation Challenge. Boston Public Schools had been trying to change start times for decades but had been confounded by the optimizations and a way to improve the school schedule without tripling the costs, which is why it organized Transportation Challenge to begin with. The MIT team was the first to figure out a way to balance all of these factors and produce a solution. Until then, calculating the cost of the complex bus system had been such a difficult problem that it presented an impediment to even considering bell time changes..After the Transportation Challenge, the team continued to work with the city, and over the previous year they had participated in a community engagement process and had worked with the Boston school system to build on top of the original algorithm, adding new features that were included to produce a plan for new school start times. They factored in equity—existing start times were unfair, mostly to lower-income families—as well as recent research on teenage sleep that showed starting school early in the day may have negative health and economic consequences for high school students. They also tried to prioritize special education programs and prevent young children from leaving school too late. They wanted to do all this without increasing the budget, and even reducing it..From surveys, the school system and the researchers knew that some families in every school would be unhappy with any change. They could have added additional constraints on the algorithm to limit some of outlier situations, such as ending the school day at some schools at 1:30 pm, which was particularly exasperating for some parents. The solution that they were proposing significantly increased the number of high school students starting school after 8 am and significantly decreased the number of elementary school students dismissed after 4 pm so they wouldn’t have to go home after dark. Overall it was much better for the majority of people. Although they were aware that some parents wouldn’t be happy, they werent prepared for the scale of response from angry parents who ended up with start times and bus schedules that they didnt like..Optimizing the algorithm for greater “equity also meant many of the planned changes were biased against families with privilege. My view is that the fact that an algorithm was making decisions also upset people. And the families who were happy with the new schedule probably didn’t pay as much attention. The families who were upset marched on City Hall in an effort to overturn the planned changes. The ACLU and I supported the activist parents at the time and called foul on the school system and the city. Eventually, the mayor and the city caved to the pressure and killed off years of work and what could have been the first real positive change in busing in Boston in decades..While Im not sure privileged families would give up their good start times to help poor families voluntarily, I think that if people had understood what the algorithm was optimizing for—sleep health of high school kids, getting elementary school kids home before dark, supporting kids with special needs, lowering costs, and increasing equity overall—they would agree that the new schedule was, on the whole, better than the previous one. But when something becomes personal very suddenly, people to feel strongly and protest..It reminds me a bit of a study, conducted by the Scalable Cooperation Group at the Media Lab based on earlier work by Joshua Greene, which showed people would support the sacrifice by a self-driving car of its passenger if it would save the lives of a large number of pedestrians, but that they personally would never buy a passenger-sacrificing self-driving car..Technology is amplifying complexity and our ability to change society, altering the dynamics and difficulty of consensus and governance. But the idea of weighing trade-offs isnt new, of course. Its a fundamental feature of a functioning democracy..While the researchers working on the algorithm and the plan surveyed and met with parents and school leadership, the parents were not aware of all of the factors that went into the final optimization of the algorithm. The trade-offs required to improve the overall system were not clear, and the potential gains sounded vague compared to the very specific and personal impact of the changes that affected them. And by the time the message hit the nightly news, most of the details and the big picture were lost in the noise..A challenge in the case of the Boston Public Schools bus route changes was the somewhat black-box nature of the algorithm. The Center for Deliberative Democracy has used a process it calls deliberative polling, which brings together a statistically representative group of residents in a community to debate and deliberate policy goals over several days in hopes of reaching a consensus about how a policy should be shaped. If residents of Boston could have more easily understood the priorities being set for the algorithm, and hashed them out, they likely would have better understood how the results of their deliberations were converted into policy..After our meeting with the team that invented the algorithm, for instance, Kade Crockford introduced them to David Scharfenberg, a reporter at the Boston Globe who wrote an article about them that included a very well done simulation allowing readers to play with the algorithm and see how changing cost, parent preferences, and student health interact as trade-offs—a tool that would have been extremely useful in explaining the algorithm from the start..The lessons learned from Boston’s effort to use technology to improve its bus routing system and start times provides a valuable lesson in understanding how to ensure that such tools aren’t used to reinforce and increase biased and unfair policies. They can absolutely make systems more equitable and fair, but they won’t succeed without our help.",Ending schools at different time zones might prioritize various families differently
12,"We often call out overly optimistic news coverage of drugs and devices. But information technology is another healthcare arena where uncritical media narratives can cause harm by raising false hopes and allowing costly and unproven investments to proceed without scrutiny..A case in point is the recent collapse of M.D. Anderson Cancer Center’s ambitious venture to use IBM’s Watson cognitive computing system to expedite clinical decision-making around the globe and match patients to clinical trials. .Launched in 2013, the project initially received glowing mainstream media coverage that suggested Watson was already being deployed to revolutionize cancer care–or soon would be. .But that was premature. By all accounts, the electronic brain was never used to treat patients at M.D. Anderson. A University of Texas audit reported the product doesn’t work with Anderson’s new electronic medical records system, and the cancer center is now seeking bids to find a new contractor. .The audit chronicles rife financial missteps by the cancer center including no competitive bidding, payments to vendors without documented results, and decision-making that skirted the medical center’s own IT department. The bungle cost $62 million paid to IBM and PricewaterhouseCoopers, plus uncalculated internal resources including staff time, technology infrastructure and administrative support, according to the audit. The cost was supposed to be shouldered by a single donor who didn’t come through with all of the promised funds..It’s uncertain how much of the failure was the fault of M.D. Anderson versus the limitations of the technology. But looking at early news coverage, it is clear journalists were not asking pointed questions about how the project was being financed or its capabilities when it comes to curing cancer. .The dominant narrative was that the technology that famously bested two human champions on the game show Jeopardy! in 2011 was being re-deployed to augment cancer care. Only limited play was given to the complexity of integrating medical research and patient records to craft an effective decision-making tool. .A 2015 Washington Post story entitled “Watson’s next feat? Taking on cancer. IBM’s computer brain is training alongside doctors to do what they can’t,” mentioned some limitations of machine learning but took an overall positive tone. It describes Watson as “a revolutionary approach to medicine and health care that is likely to have significant social, economic and political consequences.”.The story said also Watson would enable doctors “to find personalized treatments for every cancer patient by comparing disease and treatment histories, genetic data, scans and symptoms against the vast universe of medical knowledge.” .But treating cancer is more complex than winning a trivia game, and the “vast universe of medical knowledge” may not be as significant as purveyors of artificial intelligence make it out to be, according to some observers..“IBM spun a story about how Watson could improve cancer treatment that was superficially plausible – there are thousands of research papers published every year and no doctor can read them all,” said David Howard, a faculty member in the Department of Health Policy and Management at Emory University, via email. “However, the problem is not that there is too much information, but rather there is too little. Only a handful of published articles are high-quality, randomized trials. In many cases, oncologists have to choose between drugs that have never been directly compared in a randomized trial.”.While Watson’s use in cancer care was still developing, a 2013 IBM news release declared MD Anderson “is using the IBM Watson cognitive computing system for its mission to eradicate cancer.” .From what we know now, the system was a long way from being used for patient care. Yet some media reports echoed this premature language, suggesting it was or soon would be operational..Forbes ran a blog headlined “IBM’s Watson Now Tackles Clinical Trials At MD Anderson Cancer Center.” Forbes stated use in patient care “might come in early 2014.” It quoted an M.D. Anderson doctor saying: “It’s still in testing and not quite ready for the mainstream yet, but it has the infrastructure to potentially revolutionize oncology research.” .Likewise Scientific American asserted: “The University of Texas M.D. Anderson Cancer Center is using Watson to help doctors match patients with clinical trials, observe and fine-tune treatment plans, and assess risks as part of M. D. Anderson’s ‘Moon Shots’ mission to eliminate cancer.”.The magazine gave the project another PR boost by publishing a blog post by IBM Chief Technology Officer Rob High and financier Jho Low, whose foundation funded the project, entitled “Expert Cancer Care May Soon Be Everywhere, Thanks to Watson.”.Largely missing in this coverage was a caveat about the lack of evidence that the technology improved patient outcomes, lowered costs, or provided some other benefit — something we demand in reporting on drugs, devices and tests..“Reporters are often susceptible to PR hype about the potential of new technology – from Watson to ‘wearables’ – to improve outcomes,” Howard said. “A lot of stories would turn out differently if they asked a simple question: ‘Where is the evidence?’”.At this point, there isn’t much. While IBM has entered into numerous deals to use its artificial intelligence system in healthcare, a company spokeswoman said there’s no published study linking the technology to improved outcomes for patients because “the implementation of the technology is not there yet.”.She did cite published studies showing the system met operational objectives, such as matching clinicians’ treatment recommendations in a given percentage of cases..When it comes to IT coverage, journalists should make a habit of pointing out gaps between what’s claimed and what’s been demonstrated to work..“Artificial intelligence has been suffering from overhype since the 1970s and 80s,” said Steven Salzberg, a professor of biomedical engineering at the Johns Hopkins School of Medicine. “Be skeptical and ask to see some evidence. (Technology companies) need to do more than simply assert that it works.”.Please note, comments are no longer published through this website. All previously made comments are still archived and available for viewing through select posts..Yes, everyone took credit before the product was “shipped”. Yes, the rules of procurement were bent, but there was no choice about IBM and none about PwC who got virtually all of the funds — they are one company. Yes, no one coordinated with the IT department that owned the patient records..But the biggest failure of all was not doing a Medical Device Commercialization. It would have taken far less than the five years, and would have WORKED. If some useful results would have been obtained for the $70 Million, we wouldn’t be having this conversation and throwing stones at everyone involved..A statement that every physician should know”In many cases the oncologists( I assume most of the physicians) have to choose between drugs that have never directly compared( to other drugs manufactured by the same company ) in a randomized trial). Most of the new drugs are compared to “controls” or ineffective theraphies,Also inneffectual drug trials are not widely reported..While I certainly agree with and appreciate the commentary on media reporting and auto reporting on Watson, this is also a good example of why many of us who have been through these cycles and work in the trenches warn about over-hype. .This sort of story becomes a poster and excuse for those attempting to block more efficient systems and, in this case, it could harm an enormous number of people. It takes some real research and time, but there are many good public works available now in highly specific use cases under the umbrella (admittedly not terribly accurate label) of AI..There is an awful lot of good work out there not getting much if any light–and by extension the chance to demonstrate efficacy at a fraction of these costs, apparently simply due to a combination of marketing spend, click bait and keyword ad models, and media herding. An opportunity exists for those willing to do some work..Reading this article in the context of our national politics makes me wonder why journalists repeatedly ‘fall for’ the medical hype, and yet are so skeptical when it comes to politics. One would think they could carry this skeptical attitude over to an industry which, increasingly deserves skepticism..Watson’s win on Jeopardy wasn’t as straightforward as everyone thinks. Contrary to public perception, Watson has never had a speech interface. So for Jeopardy the questions were submitted in written form to Watson. However, the way the game was played, Watson received the question as soon as Alex Trebek began reading the question to the other contestants. With the speed that computers process information this meant that Watson had something like an hour to contemplate the question before the other contestants had finished hearing the last words. With this type of advantage it’s no surprise that Watson won. And IBM’s marketing department has taken that golden ring and run with it ever since..This site is primarily a forum for discussion about the quality (or lack thereof) in journalism or other media messages (advertising, marketing, public relations, medical journals, etc.)  It is not intended to be a forum for definitive discussions about medicine or science. .We will delete comments that include personal attacks, unfounded allegations, unverified claims, product pitches, profanity or any from anyone who does not list a full name and a functioning email address.  We will also end any thread of repetitive comments.  We don”t give medical advice so we won”t respond to questions asking for it..There has been a recent burst of attention to troubles with many comments left on science and science news/communication websites.  Read “Online science comments:  trolls, trash and treasure.”.“Shed light, not just heat. Facts, challenges, disagreements, corrections — those are all fine. Attacking the person, instead of the idea or the interpretation, is neither acceptable nor helpful.”.We”re also concerned about anonymous comments.  We ask that all commenters leave their full name and provide an actual email address in case we feel we need to contact them. We may delete any comment left by someone who does not leave their name and a legitimate email address..And, as noted, product pitches of any sort – pushing treatments, tests, products, procedures, physicians, medical centers, books, websites – are likely to be deleted.  We don”t accept advertising on this site and are not going to give it away free..The ability to leave comments expires after a certain period of time.  So you may find that you’re unable to leave a comment on an article that is more than a few months old.",IBM product did not work with Anderson's new electronic medical systems.
13,"Cloi was meant to be the centrepiece of the South Korean firms presentation where it was supposed to show how new artificial intelligence tech could enhance use of kitchen appliances..The event was the first press conference of the morning at the Las Vegas tech fair. Samsung, Intel, HTC and Sony are among other big companies scheduled to unveil products over the course of the day..LGs focus this year was to promote ThinQ, its in-house AI software, which it intends to roll out across various products to make them easier to use and capable of evolving to meet customers needs..Although LG also includes Googles Assistant in some of its latest TVs and smart speakers, it has chosen to rely on its own solution for its robots..The machine was described as being the ultimate in simplicity when managing your smart home, when it was presented on stage by David VanderWaal, LGs US marketing chief..But subsequent requests to find out if his washing was ready, what was planned for dinner and what recipes it could suggest for chicken all fell flat..The first time it failed everyone laughed and thought it was just a glitch, commented Ben Wood, an analyst at CCS Insight, who was in the audience..Although it is far from the first time a CES demo has gone awry, Mr Wood said it highlighted a problem with many of the AI-based technologies being pitched to the public..It will be damaging to LG and highlights a broader point about all the companies exhibiting here: is their technology really ready for prime time?.One industry-watcher said LG was likely to take a short-term hit to its brand as a result of the event, but said that it would also serve as a reminder that the promise of a seamless smart home was still some way off..The target market for these kinds of products is still hi-tech-friendly early adopters who are willing to put in more effort than the regular user, said Tuong Nguyen from the consultancy Gartner..The challenge for manufacturers is that they not only to offer a good user experience but also have to get all the different technologies involved to work together.",Bot fails to demonstrate the enhance use of kitchen appliances
14,"Otis Nash works six days a week at two jobs, as a security guard and a pest control technician, but still struggles to make the $190.69 monthly Geico car insurance payment for his 2012 Honda Civic LX..“I’m on the edge of homelessness,” said Nash, a 26-year-old Chicagoan who supports his wife and 7-year-old daughter. But “without a car, I can’t get to work, and then I can’t pay my rent.”.Some car insurers charge higher premiums in Chicago’s minority neighborhoods than in predominantly white neighborhoods with similar risk of accidents. See the map..Yet Hedges, who is a 34-year-old advertising executive, pays only $54.67 a month to insure his 2015 Audi Q5 Quattro sports utility vehicle. Nash pays almost four times as much as Hedges even though his run-down neighborhood, East Garfield Park, with its vacant lots and high crime rate, is actually safer from an auto insurance perspective than Hedges’ fancier Lake View neighborhood near Wrigley Field..On average, from 2012 through 2014, Illinois insurers paid out 20 percent less for bodily injury and property damage claims in Nash’s predominantly minority zip code than in Hedges’ largely white one, according to data collected by the state’s insurance commission. But Nash pays 51 percent more for that portion of his coverage than Hedges does..For decades, auto insurers have been observed to charge higher average premiums to drivers living in predominantly minority urban neighborhoods than to drivers with similar safety records living in majority white neighborhoods. Insurers have long defended their pricing by saying that the risk of accidents is greater in those neighborhoods, even for motorists who have never had one..But a first-of-its-kind analysis by ProPublica and Consumer Reports, which examined auto insurance premiums and payouts in California, Illinois, Texas and Missouri, has found that many of the disparities in auto insurance prices between minority and white neighborhoods are wider than differences in risk can explain. In some cases, insurers such as Allstate, Geico and Liberty Mutual were charging premiums that were on average 30 percent higher in zip codes where most residents are minorities than in whiter neighborhoods with similar accident costs..Our findings document what consumer advocates have long suspected: Despite laws in almost every state banning discriminatory rate-setting, some minority neighborhoods pay higher auto insurance premiums than do white areas with similar payouts on claims. This disparity may amount to a subtler form of redlining, a term that traditionally refers to denial of services or products to minority areas. And, since minorities tend to lag behind whites in income, they may be hard-pressed to afford the higher payments..Rachel Goodman, staff attorney in the American Civil Liberties Union’s racial justice program, said ProPublica’s findings were distressingly familiar. “These results fit within a pattern that we see all too often — racial disparities allegedly result from differences in risk, but that justification falls apart when we drill down into the data,” she said..“We already know that zip code matters far too much in our segregated society,” Goodman said. “It is dispiriting to see that, in addition to limiting economic opportunity, living in the wrong zip code can mean that you pay more for car insurance regardless of whether you and your neighbors are safe drivers.”.The Insurance Information Institute, a trade group representing many insurers, contested ProPublica’s findings. “Insurance companies do not collect any information regarding the race or ethnicity of the people they sell policies to. They do not discriminate on the basis of race,” said James Lynch, chief actuary of the institute..The impact of the disparity in insurance prices can be devastating, a roadblock to upward mobility or even getting by. Auto insurance coverage is required by law in almost all states. If a driver can’t pay for insurance, she can face fines for driving without insurance, have her license suspended and eventually end up in jail for driving with a suspended license. Higher prices also increase the burden on those least able to bear it, forcing low-income consumers to opt for cheaper fly-by-night providers, or forego other necessities to pay their car insurance bills..It isn’t completely clear why some major auto insurers persist in treating minority neighborhoods differently. It may in part be a vestige of longstanding practices dating back to an era when American businesses routinely discriminated against non-white customers. It’s also possible that the proprietary algorithms used by insurers may inadvertently favor white over minority neighborhoods..We have limited our analysis to the four states that release the type of data needed to compare insurance payouts by geography. Still, these states represent the spectrum of government oversight of the insurance industry. California is the most highly regulated insurance market in the U.S.; Illinois, one of the least regulated. In addition, some insurers whose prices appear to vary by neighborhood demographics operate nationally. That raises the prospect that many minority neighborhoods across the country may be paying too much for auto insurance, or white neighborhoods, too little..This investigation marks the first use of industry payout data to measure racial disparities in car insurance premiums across states. It’s part of ProPublica’s examination of the hidden power of algorithms in our lives — from the equations that determine Amazon’s top sellers to the calculations used to predict an individual’s likelihood of committing future crimes..Our analysis examined more than 100,000 premiums charged for liability insurance — the combination of bodily injury and property damage that represents the minimum coverage drivers buy in each of the states. To equalize driver-related variables such as age and accident history, we limited our study to one type of customer: a 30-year-old woman with a safe driving record. We then compared those premiums, which were provided by Quadrant Information Services, to the average amounts paid out by insurers for liability claims in each zip code..In California, Texas and Missouri, our analysis is based on state data that covers insurance claims received, and payouts by, the state’s insurers over the most recent five-year period for which data was available. In Illinois, the data covers a three-year period. We defined minority zip codes as having greater than 66 percent non-white population in California and Texas. In Missouri and Illinois, we defined it as greater than 50 percent, in order to have a sufficiently large sample size..In all four states, we found insurers with significant gaps between the premiums charged in minority and non-minority neighborhoods with the same average risk. In Illinois, of the 34 companies we analyzed, 33 of them were charging at least 10 percent more, on average, for the same safe driver in minority zip codes than in comparably risky white zip codes. (The exception was USAA’s Garrison Property & Casualty subsidiary, which charged 9 percent more.) Six Illinois insurers, including Allstate, which is the second largest insurer in the state, had average disparities higher than 30 percent..While in Illinois the disparities remained about the same from the safest to the most dangerous zip codes, in the other three states the disparities were confined to the riskiest neighborhoods. In those instances, prices in whiter neighborhoods stayed about the same as risk increased, while premiums in minority neighborhoods went up..In Missouri and Texas, at least half of the insurers we studied charged higher premiums for a safe driver in high-risk minority communities than in comparably risky non-minority communities. And even in highly regulated California, we found eight insurers whose prices in risky minority neighborhoods were more than 10 percent above similar risky zip codes where more residents were white..Judging by how much insurers have had to pay out for accident claims in their Chicago neighborhoods, Nash should be paying less than Hedges, not more..Over a three-year-period, Illinois insurers have paid out about $172 per car each year in bodily injury and property damage claims in Nash’s zip code, 60612, according to data collected by the state insurance commission. That’s 20 percent less than the $216 per car that insurers paid out for similar claims in Hedges’ zip code, 60657..But the liability premiums charged by Nash’s insurer, Geico Casualty, in those two neighborhoods actually give a discount to the riskier white neighborhood. In Nash’s neighborhood, Geico charges $409 for annual liability coverage for a 30-year-old woman who is a safe driver, according to insurance quotes provided by Quadrant. In Lake View, Geico charges $338 for the same coverage for the same driver..For the liability portion of their Geico coverages, Nash is paying $831.34 annually, while Hedges is paying just $549.58, according to their records. Hedges pays less even though he bought higher coverage limits for bodily injury, and his Audi is worth about three times as much as Nash’s Honda. A Geico filing in Illinois indicates that it charges more to insure an expensive car than a cheap one..Nash said he is accustomed to seeing his neighborhood shortchanged. “When you go to the richer neighborhoods, the red light cameras kind of go away,” he said. “That system is kind of designed for you to fail.”.The disparities persist even in affluent minority neighborhoods. Consider Pernell Cox, a Los Angeles businessman who lives in a wealthy enclave in South Los Angeles sometimes referred to as the “Black Beverly Hills.” His insurer Safeco, a subsidiary of Liberty Mutual, charges 13 percent more for a 30-year-old female safe driver in his neighborhood than in a zip code with comparable risk in Woodland Hills, a predominantly white suburb in north Los Angeles..Liberty Mutual, the parent company of Safeco, told ProPublica it is committed to offering drivers “competitively priced car insurance coverage options.”.Individual insurers don’t publicly release their losses on a zip-code level, and have long resisted demands for that level of transparency. As a result, our analysis is based on aggregated losses experienced by almost all insurers in a given zip code in California, Illinois and Missouri, and by 70 percent of insurers in Texas..The California Department of Insurance criticized this approach. It disputed ProPublica’s analysis and findings on the grounds that an individual insurer’s losses in a given zip code may vary significantly from the industry average. “The study’s flawed methodology results in a flawed conclusion” that some insurers discriminate in setting rates, it said..To be sure, it’s possible that some insurers have proprietary data that justifies the higher premiums we found in minority neighborhoods. Moreover, in any given zip code, an individual insurer’s losses could differ from the average losses experienced by insurers. But it is unlikely that those differences would result in a consistent pattern of higher prices for minority neighborhoods..Consider the internal losses that Nationwide disclosed in a 2015 rate filing in California. We compared Nationwide’s premiums charged by Nationwide’s Allied subsidiary to Nationwide’s losses and found that minority zip codes were being charged 21 percent more than similarly risky non-minority zip codes — a greater disparity than the 14 percent we found when comparing Allied premiums to overall state risk data..The Illinois Department of Insurance also criticized ProPublica’s report. “We believe the methodology used in this report is incomplete and oversimplifies the comparison of rates in minority vs. non-minority neighborhoods,” said department spokesman Michael Batkins..The Texas Department of Insurance said that it was reviewing ProPublica’s analysis. “It’s important to us that rates are fair to all consumers,” said department spokesman Jerry Hagins. The Missouri Department of Insurance did not respond to repeated inquiries..Many insurers did not respond to our questions. Those that did generally disputed our results and said that they do not discriminate by race in rate setting. Eric Hardgrove, director of public relations at Nationwide, said it uses “nondiscriminatory rating factors in compliance with each state’s ratemaking laws.” He did not respond to inquiries about our analysis of Nationwide’s internal losses in California..Roger Wildermuth, spokesman for USAA, said that its premiums reflect neighborhood conditions. “Some areas may have slightly higher rates due to factors such as congestion that lead to more accidents or higher crime rates that lead to higher auto thefts,” he said..Insurers have long cited neighborhood congestion as a factor in their decision-making. In 1940, a young lawyer named Thurgood Marshall wrote to a friend that he had been denied auto insurance by Travelers. When Marshall complained to the company, he was told that “the refusal was on the basis of the fact that I live in a ‘congested area,’ meaning Harlem, and ‘not’ because I am a Negro.”.Marshall, who later argued and won the landmark school desegregation case Brown v. Board of Education and went on to become a Supreme Court justice, concluded, according to his letter, that, “it is practically impossible to work out a court case because the insurance is usually refused on some technical ground.”.In Marshall’s day, redlining was often defined by refusal to provide loans, insurance or other services in minority neighborhoods. But as those practices became public and controversial — due in part to Marshall’s activism as an attorney for the NAACP — insurers stopped asking applicants to identify their race..In the 1940s, as part of a bargain to win an exemption from federal antitrust laws, the insurance industry agreed to be regulated by state laws that included prohibitions against discriminatory rate setting. Soon after, following model legislation recommended by the National Association of Insurance Commissioners, most states passed laws stating “rates should not be inadequate, excessive or unfairly discriminatory.” The legislation defines discrimination as “price differentials” that “fail to reflect equitably the differences in expected losses and expenses.”.Of course, the laws didn’t immediately stop discrimination. In a thorough examination of MetLife’s history released in 2002, New York state insurance regulators catalogued all of the ways that the company discriminated against black applicants for life insurance — dating back to the 1880s when it refused to insure them at all, to the first half of the 20th century when it required minorities to submit to additional medical exams and sold them substandard plans..In the 1960s, as insurers stopped asking applicants to declare their race, MetLife began dividing cities into areas. In minority areas, applicants were subject to more stringent criteria, according to the report. In 2002, MetLife agreed to pay as much as $160 million to compensate minorities who were sold substandard policies..In the auto insurance industry, similar practices occurred. To this day, most auto insurers base premiums in part on “territorial ratings,” derived from the risk of the area where the car is garaged..The territorial ratings are “a way of taking into account the conditions under which you are driving,” said David Snyder, a vice president at the Property Casualty Insurers Association of America..In 1978, Los Angeles County Supervisor Kenneth Hahn pleaded with Congress to rectify the stark inequities of territorial ratings. He said the same good driver would pay over $900 if he lived in Watts, a poor black neighborhood, and just $385 if he lived in predominantly white San Diego County..Bill Corley, who is African American, started his career as a Farmers Insurance agent in West Los Angeles in 1977. He said the discrimination wasn’t obvious on the surface. “Officially, you could write insurance anywhere you wanted to write insurance,” he recalled. But, Corley said, if you had too many clients in low-income areas, Farmers executives “would tell you all the problems that could be associated with that, and you were scared off and intimidated from doing so.”.When he sold insurance in minority neighborhoods, Corley said, the Farmers managers “would nitpick it. They would ask you questions about people’s income levels and questions about neighboring properties — which I don’t really recall ever having to address when I was writing policies in other neighborhoods in the city.” Farmers did not respond to repeated inquiries..Corley persisted, and eventually established a network of independent minority insurance brokers who worked together to persuade leading insurers to make them agents and sell policies through them. Corley, who now works as an independent insurance agent with offices in San Diego and San Jose, said the increased diversity of agents has improved the business. “Agents and brokers were complicit, and helped to perpetuate redlining, by not making an effort to write policies in those areas,” he said..Today, some insurers consider other factors beyond the risk of accident payouts in setting rates. Such criteria as credit score and occupation have been shown to result in higher prices for minorities..Allstate is implementing a new method for tailoring rates to “micro-segments” that appear to be as small as an individual policyholder — a method referred to in the industry as price optimization..More than a dozen states have set limits on insurers’ use of price optimization, expressing concerns that the technique allows insurers to raise premiums on customers who don’t shop around for better rates. In 2014, for instance, the Maryland Insurance Administration banned price optimization, saying it results in rates that are “unfairly discriminatory.” (In this context, discrimination refers to any pricing that is not related to risk; the effect on minority neighborhoods has not been studied.).Allstate has disclosed in filings that it is using price optimization in at least 24 states, including Illinois, Missouri and Texas. Allstate spokesman Justin Herndon said the company “uses the likelihood of loss to price insurance which is required by law and specific prices are approved by state regulators.”.In California, when insurers set rates for sparsely populated rural zip codes, which tend to be whiter, they are allowed to consider risk in contiguous zip codes of their own choosing. Often, the companies group these zip codes with similar areas that also have few policy-holders, according to insurers’ rate filings. They then assign lower risk to the entire region than appears to be warranted by the state’s accident data..However rates are calculated, auto insurance remains unaffordable in many predominantly minority areas of the nation, according to an analysis by ProPublica of U.S. Census data and 30 million auto insurance quotes..We found that households in minority-majority zip codes spent more than twice as much of their household income on auto insurance (11 percent), compared with households in majority white neighborhoods (5 percent). The U.S. Treasury Department has defined auto insurance as affordable if it costs 2 percent or less of household income..Consider Kelley Jenkins, a 39-year-old mother of three who lives on Chicago’s South Side. When she was laid off from an office job last summer, she tried to make ends meet by driving for Uber and Lyft. But after two months of sporadic driving, when she was sometimes making only $100 or $200 a week, she couldn’t afford to keep up her $112 monthly auto insurance payments. “I was in a major struggle,” she said..When she gave up her auto insurance, she lost her driving gigs. Luckily, she soon found a job as a security guard. But she still can barely afford auto insurance, so she bought a bare bones plan from a low-cost insurer..Jenkins said she would love to get insurance from one of the brand-name companies, but every time she calls for a quote, she realizes, “Oh no, I can’t afford it.”.Over the years, efforts to investigate redlining in car insurance have repeatedly been stymied by the same barrier: the industry’s refusal to make crucial data available..After the Rodney King riots in Los Angeles in 1992, when people took to the streets to protest the acquittal of policemen who had been filmed beating a black driver, it turned out that about half of an estimated $1 billion in losses from destroyed businesses and homes were not covered by insurance..California Insurance Commissioner John Garamendi blamed discriminatory practices by the nation’s insurance companies. Touring the battered ruins of the city a month after the riots, he told a New York Times reporter, “I am convinced redlining exists. The bottom line is either you can’t get or can’t afford it.”.Garamendi subsequently approved rules that required insurers to report their market share by zip code. But insurers argued that the data was a trade secret that couldn’t be released to the public. It wasn’t until 2004, after years of legal battles, that insurers lost their case in California Supreme Court..Also spurred by the Los Angeles riots, several Congressional committees held hearings and began studying the issue of redlining, but were stymied by lack of data. The U.S. General Accounting Office, now known as the U.S. Government Accountability Office, reported in 1994 that an analysis of insurance availability would require insurance companies to begin reporting data at zip code or census tract level nationwide. “Currently available data are insufficient to determine the extent of current problems,” the report stated..The National Association of Insurance Commissioners also set up a committee to investigate redlining. It didn’t get the necessary data, either..Robert Klein, who was researching the issue for the association, said in an interview that “the insurance industry opposed the idea of collecting loss and claims data and the NAIC committee sided with the industry and not with me on this point.”.Without data about insurers’ losses, Klein’s report could not determine why premiums were higher in minority neighborhoods — whether the difference was truly because of greater risk there. “Researchers were unable to draw definitive conclusions about the causes of these market conditions,” the report stated..Insurers say they set prices based on risk but are reluctant to share the data underlying their risk analyses, such as losses per zip code. Publishing data publicly about losses means “you’re creating something that is valuable and you are essentially giving it away,” said Lynch of the Insurance Information Institute..Texas consumer advocate Birny Birnbaum won a rare victory when, through a public-records request, he obtained data collected by the state insurance commission at a zip-code level..In 1997, using the information about each insurer’s number of policies, premiums and losses by zip code, Birnbaum published a fiery report naming Nationwide, Safeco, State Farm, USAA and Farm Bureau as among the “worst redliners” in the state because they had much smaller market share in minority neighborhoods than in other neighborhoods..The insurers sued the Texas Department of Insurance and Birnbaum, contending that the information was a trade secret and making it public had damaged their business. A Travis County district court judge ruled in the insurers’ favor, saying they would “suffer irreparable harm in the absence of a temporary injunction.”.“There were roughly 200 insurance companies in the state. They all sued,” recalled D.J. Powers, who was Birnbaum’s pro bono attorney. “It was the entire auto insurance industry versus me and Birny.”.Since then, Birnbaum has continued to advocate for insurance commissions to collect and publicly release data that can be used for analysis of redlining and other issues. However, to this day, very few states do so. ProPublica filed public-records requests in all 50 states and the District of Columbia seeking zip-code level data about liability claims payouts. Only four states said they collected such data and provided it..“Regulators are no better equipped to analyze or address these problems than they were 20 or 30 years ago,” Birnbaum said. “If you can’t even monitor the market to identify the problem, you’re certainly not going to be in a position to address the problem.”.On a redlining map of Chicago created by a federal housing agency in 1940, Otis Nash’s neighborhood, East Garfield Park, is colored red for “hazardous.”.“This is a mediocre district threatened with negro encroachment,” the map states. “Most properties are obsolete and the section is very congested.”.The term redlining is sometimes thought to have originated with these maps, which were created for many American cities by the federal Home Owners’ Loan Corporation between 1935 and 1940. The maps were used to assist loan officers in deciding which properties were worth financing..East Garfield Park was built as a community of townhouses for factory workers. Like much of Chicago’s West and South Sides, it became a predominantly minority neighborhood in the ‘50s and ‘60s as redlining discouraged investment and the city built an expressway and low-income housing projects in the area. Whites fled for the suburbs..In 1970, East Garfield Park was among the many Chicago neighborhoods swept up in a wave of auto insurance redlining. In an experiment, Illinois had switched in 1969 from traditional auto insurance regulation — in which rates were approved by state regulators before being issued — to a so-called “open rating system” in which companies could issue rates without regulatory permission..Illinois insurers soon divided Chicago into four territories for rate setting, a scheme that led to higher premiums in black neighborhoods. A group of black insurance brokers banded together to protest what they called a “color tax” that was being levied on black neighborhoods in Chicago..The issue was severe enough that the U.S. Senate Antitrust and Monopoly subcommittee held a hearing in Chicago to examine it. One witness, undertaker Charles Childs, said the premiums on his two cars for personal use, a Cadillac and a Mercury, had risen from $450 in 1970 to $950 in 1971 and he had to drop coverage for his fleet of undertaker vehicles..“As rates have been increased in the inner city, they have substantially decreased in essentially white areas,” Millard D. Robbins Jr., the head of the Insurance Brokers Association of Chicago, said at a press conference. “This creates a surtax on blackness and a discount for being Caucasian.”.With black Chicagoans in rebellion, the Illinois legislature declined to renew the experimental open competition law in 1971. But they couldn’t agree on a new law to replace it — so they just allowed the statute regulating auto insurance rate setting and prohibiting discrimination to expire..Since then, Illinois, home to the corporate headquarters of State Farm and Allstate, has been the only state without legislation explicitly barring excessive or discriminatory rates in car insurance. Illinois does prohibit auto insurers from charging higher premiums to a customer because of his or her physical disability, race, color, religion or national origin..To address complaints of discrimination from Chicago, lawmakers in 1971 proposed a compromise: They would ban insurers from using the four rating territories in the city..“The question is: Shall we give the blacks in Chicago a fair break on insurance rates?” said Illinois state Sen. Egbert Groen during the statehouse debate..In 1972, they passed a law requiring insurers to use a single territorial rate within the city of Chicago for bodily injury coverage. “No one will be helped more than those in the inner city,” predicted Illinois State Rep. Bernard Epton..But the reality has turned out differently. For many insurers we examined, premiums were high for everyone within the city of Chicago, regardless of risk, compared to the rest of the state. This means that, since Chicago contains one-third of the state’s minority neighborhoods, they are still being overcharged..And even within Chicago, the law has not prohibited insurers from differentiating prices by neighborhood. That’s because the single-territory rule is limited to bodily injury coverage; rates for property damage can still vary. (Both bodily injury and property damage coverage are mandatory for drivers to purchase in Illinois.).In Nash’s zip code, 60612, Geico has set the base rate for property damage insurance at $753 a year, according to the company’s December 2016 rate filing in Illinois..That’s eight times higher than what Illinois insurers have paid out in property damage claims in that zip code — an average of $91.57 per car in the three years ending in 2014, according to data from the state insurance commission..By comparison, in Hedges’ neighborhood, 60657, Geico has set the base rate for property damage insurance at $376 a year, according to the same filing..That’s half of the Geico base rate in Nash’s neighborhood. It’s also only about four times higher than what Illinois insurers have paid out in property damage claims in Hedges’ zip code — an average of $104.45 per car over the same period..Of course, Geico’s calculations could reflect the unique risk of the insurer’s own clientele that is somehow not reflected in the state averages. But it could also reflect a disparity, unrelated to risk, that punishes a minority neighborhood..Either way, the $377 disparity between property damage base rates accounts for the majority of the difference in liability premiums paid by Nash and Hedges. The base rate is adjusted by other factors such as age and driving record..Despite scraping to make ends meet, Nash bought collision, comprehensive and liability, as well as rental reimbursement, emergency road service and uninsured motorist coverage. “I got everything,” he said, “because you hear so many horror stories.”.He’s dependent on his car. He needs it to go to work, to shop for groceries now that the local pharmacy closed in his neighborhood and the dollar store burned down, and to take his 7-year-old daughter out to the suburbs where she can ride her bicycle in a park that is safe from crime..“I don’t even walk up and down the block with my daughter,” Nash said, adding that it’s not unusual in the summer to “hear gunshots during the day.”.Nash said he is working with a financial adviser to cut back his expenses so he can make his rent payments. Still, he’s reluctant to give up any of his car insurance. “I would choose the rent over my car, but that would be playing with fire,” he said..Hedges’ premium recently went up after his husband got into an accident. But even after the incident, their combined price of $115.37 a month for two cars is lower than the $190.69 a month that Nash pays for just one car. When told about the difference in prices, Hedges said it seemed unfair..Julia Angwin is a senior reporter at ProPublica. From 2000 to 2013, she was a reporter at The Wall Street Journal, where she led a privacy investigative team that was a finalist for a Pulitzer Prize in Explanatory Reporting in 2011 and won a Gerald Loeb Award in 2010..Jeff Larson is a news applications developer at ProPublica. He is a winner of the Livingston Award for the 2011 series Redistricting: How Powerful Interests are Drawing You Out of a Vote..Lauren Kirchner is a senior reporting fellow at ProPublica. She has covered digital security and press freedom issues for the Columbia Journalism Review, and crime and criminal justice for Pacific Standard magazine.",Minority Neighborhoods Pay Higher Car Insurance Premiums Than White Areas With the Same Risk - ProPublica Study
18,"It seems a simple question, but it’s one without simple answers. That’s particularly true in the arcane world of artificial intelligence (AI), where the notion of smart, emotionless machines making decisions wonderfully free of bias is fading fast..Perhaps the most public taint of that perception came with a 2016 ProPublica investigation that concluded that the data driving an AI system used by judges to determine if a convicted criminal is likely to commit more crimes appeared to be biased against minorities. Northpointe, the company that created the algorithm, known as COMPAS, disputed ProPublica’s interpretation of the results, but the clash has sparked both debate and analysis about how much even the smartest machines should be trusted..Neill now finds himself in the middle of that discussion. A computer scientist at Carnegie Mellon University, he and another researcher, Will Gorr, developed a crime-predicting software tool called CrimeScan several years ago. Their original concept was that in some ways violent crime is like a communicable disease, that it tends to break out in geographic clusters. They also came to believe that lesser crimes can be a harbinger of more violent ones, so they built an algorithm using a wide range of “leading indicator” data, including reports of crimes, such as simple assaults, vandalism and disorderly conduct, and 911 calls about such things as shots fired or a person seen with a weapon. The program also incorporates seasonal and day of week trends, plus short-term and long-term rates of serious violent crimes. .The idea is to track sparks before a fire breaks out. “We look at more minor crimes,” Neill says. “Simple assaults could harden to aggravated assaults. Or you might have an escalating pattern of violence between two gangs.”.CrimeScan is not the first software designed for what’s known as predictive policing. A program called PredPol was created eight years ago by UCLA scientists working with the Los Angeles Police Department, with the goal of seeing how scientific analysis of crime data could help spot patterns of criminal behavior. Now used by more than 60 police departments around the country, PredPol identifies areas in a neighborhood where serious crimes are more likely to occur during a particular period.  .The company claims its research has found the software to be twice as accurate as human analysts when it comes to predicting where crimes will happen. No independent study, however, has confirmed those results.  .Both PredPol and CrimeScan limit their projections to where crimes could occur, and avoid taking the next step of predicting who might commit them—a controversial approach that the city of Chicago has built around a “Strategic Subject List” of people most likely to be involved in future shootings, either as a shooter or victim..The American Civil Liberties Union [ACLU], the Brennan Center for Justice and various civil rights organizations have all raised questions about the risk of bias being baked into the software. Historical data from police practices, critics contend, can create a feedback loop through which algorithms make decisions that both reflect and reinforce attitudes about which neighborhoods are “bad” and which are “good.” That’s why AI based primarily on arrests data carries a higher risk of bias—it’s more reflective of police decisions, as opposed to actual reported crimes. CrimeScan, for instance, stays away from trying to forecast crimes that, as Neill puts it, “you’re only going to find if you look for them.”.Then there’s the other side of the feedback loop. If a predictive tool raises expectations of crimes in a certain neighborhood, will police who patrol there be more aggressive in making arrests?.“There’s a real danger, with any kind of data-driven policing, to forget that there are human beings on both sides of the equation,” notes Andrew Ferguson, a professor of law at the University of the District of Columbia and author of the book, The Rise of Big Data Policing: Surveillance, Race, and the Future of Law Enforcement. “Officers need to be able to translate these ideas that suggest different neighborhoods have different threat scores. And, focusing on the numbers instead of the human being in front of you changes your relationship to them.”.The reality is that artificial intelligence now plays a role—albeit often in the background—in many decisions affecting daily lives—from helping companies choose who to hire to setting credit scores to evaluating teachers. Not surprisingly, that has intensified public scrutiny of how machine learning algorithms are created, what unintended consequences they cause, and why they generally aren’t subjected to much review. .For starters, much of the software is proprietary, so there’s little transparency behind how the algorithms function. And, as machine learning becomes more sophisticated, it will become increasingly difficult for even the engineers who created an AI system to explain the choices it made. That opaque decision-making, with little accountability, is a consequence of what’s become known as “black box” algorithms..“The public never gets a chance to audit or debate the use of such systems,” says Meredith Whittaker, a co-founder of the AI Now Institute, a research organization at New York University that focuses on AI’s impact in society. “And, the data and logics that govern the predictions made are often unknown even to those who use them, let alone to the people whose lives are impacted.”.In a report issued last fall, AI Now went so far as to recommend that no public agencies responsible for such matters as criminal justice, health care, welfare and education should use black box AI systems. According to AI Now, seldom are legal and ethical issues given much consideration when the software is created..“Just as you wouldn’t trust a judge to build a deep neural network, we should stop assuming that an engineering degree is sufficient to make complex decisions in domains like criminal justice,” says Whittaker..Another organization, the Center for Democracy & Technology, has generated a “digital decisions” tool  to help engineers and computer scientists create algorithms that produce fair and unbiased results. The tool asks a lot of questions meant to get them to weigh their assumptions and identify unforeseen ripple effects..“We wanted to give people a concrete starting point for thinking through issues like how representative their data is, which groups of people might be left out, and whether their model’s outputs are going to have unintended negative consequences,” says Natasha Duarte, who oversees the project..While there has been a push to make developers more cognizant of the possible repercussions of their algorithms, others point out that public agencies and companies reliant on AI also need to be accountable..“There is this emphasis on designers understanding a system. But it’s also about the people administering and implementing the system,” says Jason Schultz, a professor of law at New York University who works with the AI Now Institute on legal and policy issues. That’s where the rubber meets the road in accountability. A government agency using AI has the most responsibility and they need to understand it, too. If you can’t understand the technology, you shouldn’t be able to use it.”.To that end, AI Now is promoting the use of “algorithmic impact assessments,” which would require public agencies to disclose the systems they’re using, and allow outside researchers to analyze them for potential problems. When it comes to police departments, some legal experts think it’s also important for them to clearly spell out how they’re using technology and be willing to share that with the local community..“If these systems are designed from the standpoint of accountability, fairness and due process, the person implementing the system has to understand they have a responsibility,” Schultz says. “And when we design how we’re going to implement these, one of the first questions is ‘Where does this go in the police manual? If you’re not going to have this somewhere in the police manual, let’s take a step back, people.”.“At least once a year, there should be an accountability moment for police technology in every local jurisdiction,” he says. “The police chief, the mayor or maybe the head of the city council would have to explain to the community what they’re using taxpayer dollars for in terms of surveillance and technology, why they think it’s a good use of the money, what they’re doing to audit it and protect the data, what are the privacy implications. And the community would be there to ask questions.”.Daniel Neill, the CrimeScan creator, says he wouldn’t object to the idea of regular audits of AI results, although he has reservations about that being done before an algorithm is adequately field-tested. He is currently working with the Pittsburgh Bureau of Police on a CrimeScan trial, and at least initially there was a challenge with “getting the right patrol intensity for the predicted hot spots.”.It’s been a learning process, he says, to adapt CrimeScan so that police officers at the street level believe it’s helpful. “We need to show that not only can we predict crime, but also that we can actually prevent it,” Neill notes. “If you just throw the tool over the wall and hope for the best, it never works that well.”.“A tool can help police officers make good decisions,” he says. “I don’t believe machines should be making decisions. They should be used for decision support.",A Predictive policing software that started with LAPD and now is being used across the US. A lot of controversy on its use because of bias and feedback loops that reinforce targeting target minority neighborhoods
19,"Guilty or innocent? In a bench trial, that’s up to the judge. Would it surprise you to know some judges in our country are posing this question to a computer?.Meet COMPAS, a computer-based algorithm that is currently being used in criminal cases to predict a defendant’s likelihood to re-offend. Artificial Intelligence is becoming more commonplace in our everyday lives. Now that it’s being introduced in such an important role in the court system, some commentators in the legal realm have pondered the pros and cons of using artificially-intelligent judges in the courtroom to render, in theory, unbiased opinions. As experts on experts, this got us to thinking about the expanded uses of artificially-intelligent experts in the courtroom, too..Let’s back up and discuss how Artificial Intelligence is currently being utilized. COMPAS is currently used in criminal cases and serves as a leading example of how artificially-intelligent devices can form decisions based on statistical information. The computer algorithm assesses about 100 factors to determine a defendant’s statistical likelihood of rehabilitation or re-offense..The algorithm’s assessment based on factors including sex, age, and criminal history has been tested and deemed reliable atidentifying defendants with the highest risk score who went on to re-offend at a rate four times greater than those assigned the lowest risk score..Some have labeled COMPAS unfair, claiming the algorithm can be biased and, just like humans, the artificial intelligence forms a prejudice based on past case rulings..Preconceptions, based on experience, can help both humans and computers sort and process information more efficiently. But, it will occasionally cause us to get verdicts or judgments wrong. Particularly, when we apply the prejudice or statistical reasoning to an anomaly case, i.e., the exception that doesn’t follow the rule..For example, a judge who believes expert witnesses who have been published on a particular topic should have their testimony admitted over an expert who hasn’t. While that rule of thumb, like the COMPAS algorithm, might frequently result in the judge’s proper admission of reliable expert testimony, there will always be an exception where an unpublished expert who is offering a perfectly-reliable opinion will be excluded..That would be an error but, according to the Washington Post, it’s a statistical error that cannot be avoided. What can be avoided, if we were to embrace artificially intelligent judges rendering opinions from the bench, is human bias. A computer cannot be swayed by emotions, politics, money, or power. Artificially-intelligent judges could be programmed with the knowledge of thousands, or likely millions, of legal opinions which would teach it the law and the systematic application of the law to facts of the same, or at least similar, to those at hand to allow it to render a fair and impartial decision that follows stare decisis..This got us thinking. If an artificially-intelligent judge could be considered an expert on the law because he knows the law and how it has been applied in the past, could we also create AI experts? The input of data, patterns, and millions of real world outcomes sounds a lot like experience. And it is the primary characteristic we look for in experts. If that can be merely inputted, then perhaps the experts of tomorrow, both on the bench or the witness stand, will be artificially intelligent..IMS Consulting & Expert Services delivers award-winning consultative trial and expert services for the most influential global firms through every stage of litigation. Combining the perspectives and proprietary methods developed over 30 years and more than 2,000 trials, IMS provides attorneys with the essential services they need to win: persuasion strategy, expert witness placement, jury consulting, trial graphics, and trial presentation..Under certain state laws the following statements may be required on this website and we have included them in order to be in full compliance with these rules. The choice of a lawyer or other professional is an important decision and should not be based solely upon advertisements. Attorney Advertising Notice: Prior results do not guarantee a similar outcome. Statement in compliance with Texas Rules of Professional Conduct. Unless otherwise noted, attorneys are not certified by the Texas Board of Legal Specialization, nor can NLR attest to the accuracy of any notation of Legal Specialization or other Professional Credentials.",Case Management and Decision Support tool used in some U.S. courts to assess the likelihood of a defendant becoming a recidivist
23,"A police force has defended its use of facial recognition technology after it was revealed that more than 2,000 people in Cardiff during the 2017 Champions League final were wrongly identified as potential criminals. .South Wales police began trialling the technology in June last year in an attempt to catch more criminals. The cameras scan faces in a crowd and compare them against a database of custody images. .As 170,000 people arrived in the Welsh capital for the football match between Real Madrid and Juventus, 2,470 potential matches were identified. .South Wales police admitted that “no facial recognition system is 100% accurate”, but said the technology had led to more than 450 arrests since its introduction. It also said no one had been arrested after an incorrect match..A spokesman for the force said: “Over 2,000 positive matches have been made using our ‘identify’ facial recognition technology, with over 450 arrests. .“Successful convictions so far include six years in prison for robbery and four-and-a-half years imprisonment for burglary. The technology has also helped identify vulnerable people in times of crisis. .“Technical issues are common to all face recognition systems, which means false positives will be an issue as the technology develops. Since initial deployments during the European Champions League final in June 2017, the accuracy of the system used by South Wales police has continued to improve.” .The force blamed the high number of false positives at the football final on “poor quality images” supplied by agencies, including Uefa and Interpol, as well as the fact it was its first major deployment of the technology. .Figures also revealed that 46 people were wrongly identified at an Anthony Joshua fight, while there were 42 false positives from a rugby match between Wales and Australia in November. .The chief constable, Matt Jukes, said the technology was used where there were likely to be large gatherings, because they were “potential terrorist targets”. .“We need to use technology when we’ve got tens of thousands of people in those crowds to protect everybody, and we are getting some great results from that,” he told the BBC. “But we don’t take the use of it lightly and we are being really serious about making sure it is accurate.” .The force said it had considered privacy issues “from the outset”, and had built in checks to ensure its approach was justified and proportionate. .In a post on Twitter, the group said: “Not only is real-time facial recognition a threat to civil liberties, it is a dangerously inaccurate policing tool.”","Facial recognition software to ""catch"" criminals in crowded settings had a 92% false-positive rate of identifying criminals."
24,"An ex-YouTube insider reveals how its recommendation algorithm promotes divisive clips and conspiracy videos. Did they harm Hillary Clinton’s bid for the presidency?. It was one of January’s most viral videos. Logan Paul, a YouTube celebrity, stumbles across a dead man hanging from a tree. The 22-year-old, who is in a Japanese forest famous as a suicide spot, is visibly shocked, then amused. “Dude, his hands are purple,” he says, before turning to his friends and giggling. “You never stand next to a dead guy?”.Paul, who has 16 million mostly teen subscribers to his YouTube channel, removed the video from YouTube 24 hours later amid a furious backlash. It was still long enough for the footage to receive 6m views and a spot on YouTube’s coveted list of trending videos..The next day, I watched a copy of the video on YouTube. Then I clicked on the “Up next” thumbnails of recommended videos that YouTube showcases on the right-hand side of the video player. This conveyor belt of clips, which auto-play by default, are designed to seduce us to spend more time on Google’s video broadcasting platform. I was curious where they might lead..The answer was a slew of videos of men mocking distraught teenage fans of Logan Paul, followed by CCTV footage of children stealing things and, a few clicks later, a video of children having their teeth pulled out with bizarre, homemade contraptions..“I’m going to post it on YouTube,” said a teenage girl, who sounded like she might be an older sibling. “Turn around and punch the heck out of that little boy.” They scuffled for several minutes until one had knocked the other’s tooth out..There are 1.5 billion YouTube users in the world, which is more than the number of households that own televisions. What they watch is shaped by this algorithm, which skims and ranks billions of videos to identify 20 “up next” clips that are both relevant to a previous video and most likely, statistically speaking, to keep a person hooked on their screen..Company insiders tell me the algorithm is the single most important engine of YouTube’s growth. In one of the few public explanations of how the formula works – an academic paper that sketches the algorithm’s deep neural networks, crunching a vast pool of data about videos and the people who watch them – YouTube engineers describe it as one of the “largest scale and most sophisticated industrial recommendation systems in existence”..Lately, it has also become one of the most controversial. The algorithm has been found to be promoting conspiracy theories about the Las Vegas mass shooting and incentivising, through recommendations, a thriving subculture that targets children with disturbing content such as cartoons in which the British children’s character Peppa Pig eats her father or drinks bleach..Lewd and violent videos have been algorithmically served up to toddlers watching YouTube Kids, a dedicated app for children. One YouTube creator who was banned from making advertising revenues from his strange videos – which featured his children receiving flu shots, removing earwax, and crying over dead pets – told a reporter he had only been responding to the demands of Google’s algorithm. “That’s what got us out there and popular,” he said. “We learned to fuel it and do whatever it took to please the algorithm.”.Google has responded to these controversies in a process akin to Whac-A-Mole: expanding the army of human moderators, removing offensive YouTube videos identified by journalists and de-monetising the channels that create them. But none of those moves has diminished a growing concern that something has gone profoundly awry with the artificial intelligence powering YouTube. .Yet one stone has so far been largely unturned. Much has been written about Facebook and Twitter’s impact on politics, but in recent months academics have speculated that YouTube’s algorithms may have been instrumental in fuelling disinformation during the 2016 presidential election. “YouTube is the most overlooked story of 2016,” Zeynep Tufekci, a widely respected sociologist and technology critic, tweeted back in October. “Its search and recommender algorithms are misinformation engines.”.If YouTube’s recommendation algorithm really has evolved to promote more disturbing content, how did that happen? And what is it doing to our politics?.Those are not easy questions to answer. Like all big tech companies, YouTube does not allow us to see the algorithms that shape our lives. They are secret formulas, proprietary software, and only select engineers are entrusted to work on the algorithm. Guillaume Chaslot, a 36-year-old French computer programmer with a PhD in artificial intelligence, was one of those engineers..During the three years he worked at Google, he was placed for several months with a team of YouTube engineers working on the recommendation system. The experience led him to conclude that the priorities YouTube gives its algorithms are dangerously skewed..“YouTube is something that looks like reality, but it is distorted to make you spend more time online,” he tells me when we meet in Berkeley, California. “The recommendation algorithm is not optimising for what is truthful, or balanced, or healthy for democracy.”.Chaslot explains that the algorithm never stays the same. It is constantly changing the weight it gives to different signals: the viewing patterns of a user, for example, or the length of time a video is watched before someone clicks away. .The engineers he worked with were responsible for continuously experimenting with new formulas that would increase advertising revenues by extending the amount of time people watched videos. “Watch time was the priority,” he recalls. “Everything else was considered a distraction.”.Chaslot was fired by Google in 2013, ostensibly over performance issues. He insists he was let go after agitating for change within the company, using his personal time to team up with like-minded engineers to propose changes that could diversify the content people see..He was especially worried about the distortions that might result from a simplistic focus on showing people videos they found irresistible, creating filter bubbles, for example, that only show people content that reinforces their existing view of the world. Chaslot said none of his proposed fixes were taken up by his managers. “There are many ways YouTube can change its algorithms to suppress fake news and improve the quality and diversity of videos people see,” he says. “I tried to change YouTube from the inside but it didn’t work.”.YouTube told me that its recommendation system had evolved since Chaslot worked at the company and now “goes beyond optimising for watchtime”. The company said that in 2016 it started taking into account user “satisfaction”, by using surveys, for example, or looking at how many “likes” a video received, to “ensure people were satisfied with what they were viewing”. YouTube added that additional changes had been implemented in 2017 to improve the news content surfaced in searches and recommendations and discourage the promotion of videos containing “inflammatory religious or supremacist” content..It did not say why Google, which acquired YouTube in 2006, waited over a decade to make those changes. Chaslot believes such changes are mostly cosmetic, and have failed to fundamentally alter some disturbing biases that have evolved in the algorithm. In the summer of 2016, he built a computer program to investigate. .The software Chaslot wrote was designed to provide the world’s first window into YouTube’s opaque recommendation engine. The program simulates the behaviour of a user who starts on one video and then follows the chain of recommended videos – much as I did after watching the Logan Paul video – tracking data along the way. .It finds videos through a word search, selecting a “seed” video to begin with, and recording several layers of videos that YouTube recommends in the “up next” column. It does so with no viewing history, ensuring the videos being detected are YouTube’s generic recommendations, rather than videos personalised to a user. And it repeats the process thousands of times, accumulating layers of data about YouTube recommendations to build up a picture of the algorithm’s preferences..Over the last 18 months, Chaslot has used the program to explore bias in YouTube content promoted during the French, British and German elections, global warming and mass shootings, and published his findings on his website, Algotransparency.org. Each study finds something different, but the research suggests YouTube systematically amplifies videos that are divisive, sensational and conspiratorial..When his program found a seed video by searching the query “who is Michelle Obama?” and then followed the chain of “up next” suggestions, for example, most of the recommended videos said she “is a man”. More than 80% of the YouTube-recommended videos about the pope detected by his program described the Catholic leader as “evil”, “satanic”, or “the anti-Christ”. There were literally millions of videos uploaded to YouTube to satiate the algorithm’s appetite for content claiming the earth is flat. “On YouTube, fiction is outperforming reality,” Chaslot says..He believes one of the most shocking examples was detected by his program in the run-up to the 2016 presidential election. As he observed in a short, largely unnoticed blogpost published after Donald Trump was elected, the impact of YouTube’s recommendation algorithm was not neutral during the presidential race: it was pushing videos that were, in the main, helpful to Trump and damaging to Hillary Clinton. “It was strange,” he explains to me. “Wherever you started, whether it was from a Trump search or a Clinton search, the recommendation algorithm was much more likely to push you in a pro-Trump direction.”.Trump won the electoral college as a result of 80,000 votes spread across three swing states. There were more than 150 million YouTube users in the US. The videos contained in Chaslot’s database of YouTube-recommended election videos were watched, in total, more than 3bn times before the vote in November 2016..Even a small bias in the videos would have been significant. “Algorithms that shape the content we see can have a lot of impact, particularly on people who have not made up their mind,” says Luciano Floridi, a professor at the University of Oxford’s Digital Ethics Lab, who studies the ethics of artificial intelligence. “Gentle, implicit, quiet nudging can over time edge us toward choices we might not have otherwise made.”.Chaslot sent me a database of more YouTube-recommended videos his program identified in the three months leading up to the presidential election. It contained more than 8,000 videos – all of them detected by his program appearing “up next” on 12 dates between August and November 2016, after equal numbers of searches for “Trump” and “Clinton”..It was not a comprehensive set of videos and it may not have been a perfectly representative sample. But it was, Chaslot said, a previously unseen dataset of what YouTube was recommending to people interested in content about the candidates – one snapshot, in other words, of the algorithm’s preferences..Jonathan Albright, research director at the Tow Center for Digital Journalism, who reviewed the code used by Chaslot, says it is a relatively straightforward piece of software and a reputable methodology. “This research captured the apparent direction of YouTube’s political ecosystem,” he says. “That has not been done before.”.I spent weeks watching, sorting and categorising the trove of videos with Erin McCormick, an investigative reporter and expert in database analysis. From the start, we were stunned by how many extreme and conspiratorial videos had been recommended, and the fact that almost all of them appeared to be directed against Clinton. .Some of the videos YouTube was recommending were the sort we had expected to see: broadcasts of presidential debates, TV news clips, Saturday Night Live sketches. There were also videos of speeches by the two candidates – although, we found, the database contained far more YouTube-recommended speeches by Trump than Clinton. .But what was most compelling was how often Chaslot’s software detected anti-Clinton conspiracy videos appearing “up next” beside other videos..There were dozens of clips stating Clinton had had a mental breakdown, reporting she had syphilis or Parkinson’s disease, accusing her of having secret sexual relationships, including with Yoko Ono. Many were even darker, fabricating the contents of WikiLeaks disclosures to make unfounded claims, accusing Clinton of involvement in murders or connecting her to satanic and paedophilic cults..One video that Chaslot’s data indicated was pushed particularly hard by YouTube’s algorithm was a bizarre, one-hour film claiming Trump’s rise was predicted in Isaiah 45. Another was entitled: “BREAKING: VIDEO SHOWING BILL CLINTON RAPING 13 YR-OLD WILL PLUNGE RACE INTO CHAOS ANONYMOUS CLAIMS”. The recommendation engine appeared to have been particularly helpful to the Alex Jones Channel, which broadcasts far-right conspiracy theories under the Infowars brand. .There were too many videos in the database for us to watch them all, so we focused on 1,000 of the top-recommended videos. We sifted through them one by one to determine whether the content was likely to have benefited Trump or Clinton. Just over a third of the videos were either unrelated to the election or contained content that was broadly neutral or even-handed. Of the remaining 643 videos, 551 were videos favouring Trump, while only only 92 favoured the Clinton campaign. .The sample we had looked at suggested Chaslot’s conclusion was correct: YouTube was six times more likely to recommend videos that aided Trump than his adversary. YouTube presumably never programmed its algorithm to benefit one candidate over another. But based on this evidence, at least, that is exactly what happened..“We have a great deal of respect for the Guardian as a news outlet and institution,” a YouTube spokesperson emailed me after I forwarded them our findings. “We strongly disagree, however, with the methodology, data and, most importantly, the conclusions made in their research.”.The spokesperson added: “Our search and recommendation systems reflect what people search for, the number of videos available, and the videos people choose to watch on YouTube. That’s not a bias towards any particular candidate; that is a reflection of viewer interest.”.It was a curious response. YouTube seemed to be saying that its algorithm was a neutral mirror of the desires of the people who use it – if we don’t like what it does, we have ourselves to blame. How does YouTube interpret “viewer interest” – and aren’t “the videos people choose to watch” influenced by what the company shows them?.Offered the choice, we may instinctively click on a video of a dead man in a Japanese forest, or a fake news clip claiming Bill Clinton raped a 13-year-old. But are those in-the-moment impulses really a reflect of the content we want to be fed?.Tufekci, the sociologist who several months ago warned about the impact YouTube may have had on the election, tells me YouTube’s recommendation system has probably figured out that edgy and hateful content is engaging. “This is a bit like an autopilot cafeteria in a school that has figured out children have sweet teeth, and also like fatty and salty foods,” she says. “So you make a line offering such food, automatically loading the next plate as soon as the bag of chips or candy in front of the young person has been consumed.”.Once that gets normalised, however, what is fractionally more edgy or bizarre becomes, Tufekci says, novel and interesting. “So the food gets higher and higher in sugar, fat and salt – natural human cravings – while the videos recommended and auto-played by YouTube get more and more bizarre or hateful.”.But why would a bias toward ever more weird or divisive videos benefit one candidate over another? That depends on the candidates. Trump’s campaign was nothing if not weird and divisive. Tufekci points to studies showing that “field of misinformation” largely tilted anti-Clinton before the election. “Fake news providers,” she says, “found that fake anti-Clinton material played much better with the pro-Trump base than did fake anti-Trump material with the pro-Clinton base.”.She adds: “The question before us is the ethics of leading people down hateful rabbit holes full of misinformation and lies at scale just because it works to increase the time people spend on the site – and it does work.”.Tufekci was one of several academics I shared our research with. Philip Howard, a professor at the Oxford Internet Institute, who has studied how disinformation spread during the election, was another. He questions whether a further factor might have been at play. “This is important research because it seems to be the first systematic look into how YouTube may have been manipulated,” he says, raising the possibility that the algorithm was gamed as part of the same propaganda campaigns that flourished on Twitter and Facebook..In testimony to the House intelligence committee, investigating Russian interference in the election, Google’s general counsel, Kent Walker, played down the degree to which Moscow’s propaganda efforts infiltrated YouTube. The company’s internal investigation had only identified 18 YouTube channels and 1,100 videos suspected of being linked to Russia’s disinformation campaign, he told the committee in December – and generally the videos had relatively small numbers of views. He added: “We believe that the activity we found was limited because of various safeguards that we had in place in advance of the 2016 election, and the fact that Google’s products didn’t lend themselves to the kind of micro-targeting or viral dissemination that these actors seemed to prefer.”.Walker made no mention of YouTube recommendations. Correspondence made public just last week, however, reveals that Senator Mark Warner, the ranking Democrat on the intelligence committee, later wrote to the company about the algorithm, which he said seemed “particularly susceptible to foreign influence”. The senator demanded to know what the company was specifically doing to prevent a “malign incursion” of YouTube’s recommendation system. Walker, in his written reply, offered few specifics, but said YouTube had “a sophisticated spam and security ­breach detection system to identify anomalous behavior and malignant incursions”. .Tristan Harris, a former Google insider turned tech whistleblower, likes to describe Facebook as a “living, breathing crime scene for what happened in the 2016 election” that federal investigators have no access to. The same might be said of YouTube. About half the videos Chaslot’s program detected being recommended during the election have now vanished from YouTube – many of them taken down by their creators. Chaslot has always thought this suspicious. These were videos with titles such as “Must Watch!! Hillary Clinton tried to ban this video”, watched millions of times before they disappeared. “Why would someone take down a video that has been viewed millions of times?” he asks..The story of Donald Trump and Russia comes down to this: a sitting president or his campaign is suspected of having coordinated with a foreign country to manipulate a US election. The story could not be bigger, and the stakes for Trump – and the country – could not be higher..Investigators are asking two basic questions: did Trump’s presidential campaign collude at any level with Russian operatives to sway the 2016 US presidential election? And did Trump or others break the law to throw investigators off the trail?.While a majority of the American public now believes that Russia tried to disrupt the US election, opinions about Trump campaign involvement tend to split along partisan lines: 73% of Republicans, but only 13% of Democrats, believe Trump did “nothing wrong” in his dealings with Russia and its president, Vladimir Putin..The affair has the potential to eject Trump from office. Experienced legal observers believe that prosecutors are investigating whether Trump committed an obstruction of justice. Both Richard Nixon and Bill Clinton – the only presidents to face impeachment proceedings in the last century – were accused of obstruction of justice. But Trump’s fate is probably up to the voters. Even if strong evidence of wrongdoing by him or his cohort emerged, a Republican congressional majority would probably block any action to remove him from office. (Such an action would be a historical rarity.).Former foreign policy adviser George Papadopolous pleaded guilty to perjury over his contacts with Russians linked to the Kremlin, and the president’s former campaign manager Paul Manafort and another aide face charges of money laundering..I located a copy of “This Video Will Get Donald Trump Elected”, a viral sensation that was watched more than 10m times before it vanished from YouTube. It was a benign-seeming montage of historical footage of Trump, accompanied by soft piano music. But when I played the video in slow motion, I saw that it contained weird flashes of Miley Cyrus licking a mirror. It seemed an amateurish and bizarre attempt at inserting subliminal, sexualised imagery. But it underscored how little oversight we have over anyone who might want to use YouTube to influence public opinion on a vast scale..I shared the entire database of 8,000 YouTube-recommended videos with John Kelly, the chief executive of the commercial analytics firm Graphika, which has been tracking political disinformation campaigns. He ran the list against his own database of Twitter accounts active during the election, and concluded many of the videos appeared to have been pushed by networks of Twitter sock puppets and bots controlled by pro-Trump digital consultants with “a presumably unsolicited assist” from Russia..“I don’t have smoking-gun proof of who logged in to control those accounts,” he says. “But judging from the history of what we’ve seen those accounts doing before, and the characteristics of how they tweet and interconnect, they are assembled and controlled by someone – someone whose job was to elect Trump.” .Chaslot and some of the academics I spoke to felt this social media activity was significant. YouTube’s algorithm may have developed its biases organically, but could it also have been nudged into spreading those videos even further? “If a video starts skyrocketing, there’s no question YouTube’s algorithm is going to start pushing it,” Albright says..YouTube did not deny that social media propaganda might have influenced its recommendations, but played down the likelihood, stressing its system “does not optimise” for traffic from Twitter or Facebook. “It appears as if the Guardian is attempting to shoehorn research, data and their conclusions into a common narrative about the role of technology in last year’s election,” the spokesperson added. “The reality of how our systems work, however, simply don’t support this premise.”.After the Senate’s correspondence with Google over possible Russian interference with YouTube’s recommendation algorithm was made public last week, YouTube sent me a new statement. It emphasised changes it made in 2017 to discourage the recommendation system from promoting some types of problematic content. “We appreciate the Guardian’s work to shine a spotlight on this challenging issue,” it added. “We know there is more to do here and we’re looking forward to making more announcements in the months ahead.”.With its flashy graphics and slick-haired anchor, the Next News Network has the appearances of a credible news channel. But behind the facade is a dubious operation that recycles stories harvested from far-right publications, fake news sites and Russian media outlets..The channel is run by anchor Gary Franchi, once a leading proponent of a conspiracy that claimed the US government was creating concentration camps for its citizens. It was the Next News Network that broadcast the fabricated claims about Bill Clinton raping a teenager, although Franchi insists he is not a fake news producer. (He tells me he prefers to see his channel as “commentating on conservative news and opinion”.).In the months leading up to the election, the Next News Network turned into a factory of anti-Clinton news and opinion, producing dozens of videos a day and reaching an audience comparable to that of MSNBC’s YouTube channel. .Chaslot’s research indicated Franchi’s success could largely be credited to YouTube’s algorithms, which consistently amplified his videos to be played “up next”. YouTube had sharply dismissed Chaslot’s research. .I contacted Franchi to see who was right. He sent me screen grabs of the private data given to people who upload YouTube videos, including a breakdown of how their audiences found their clips. The largest source of traffic to the Bill Clinton rape video, which was viewed 2.4m times in the month leading up to the election, was YouTube recommendations..The same was true of all but one of the videos Franchi sent me data for. A typical example was a Next News Network video entitled “WHOA! HILLARY THINKS CAMERA’S OFF… SENDS SHOCK MESSAGE TO TRUMP” in which Franchi, pointing to a tiny movement of Clinton’s lips during a TV debate, claims she says “fuck you” to her presidential rival. The data Franchi shared revealed in the month leading up to the election, 73% of the traffic to the video – amounting to 1.2m of its views – was due to YouTube recommendations. External traffic accounted for only 3% of the views. .Franchi is a professional who makes a living from his channel, but many of the other creators of anti-Clinton videos I spoke to were amateur sleuths or part-time conspiracy theorists. Typically, they might receive a few hundred views on their videos, so they were shocked when their anti-Clinton videos started to receive millions of views, as if they were being pushed by an invisible force. .In every case, the largest source of traffic – the invisible force – came from the clips appearing in the “up next” column. William Ramsey, an occult investigator from southern California who made “Irrefutable Proof: Hillary Clinton Has a Seizure Disorder!”, shared screen grabs that showed the recommendation algorithm pushed his video even after YouTube had emailed him to say it violated its guidelines. Ramsey’s data showed the video was watched 2.4m times by US-based users before election day. “For a nobody like me, that’s a lot,” he says. “Enough to sway the election, right?”.Daniel Alexander Cannon, a conspiracy theorist from South Carolina, tells me: “Every video I put out about the Clintons, YouTube would push it through the roof.” His best-performing clip was a video titled “Hillary and Bill Clinton ‘The 10 Photos You Must See’”, essentially a slideshow of appalling (and seemingly doctored) images of the Clintons with voiceover in which Cannon speculates on their health. It has been seen 3.7m times on YouTube, and 2.9m of those views, Cannon said, came from “up next” recommendations..Chaslot has put a spotlight on a trove of anti-Clinton conspiracy videos that had been hidden in the shadows – unless, that is, you were one of the the millions YouTube served them to. But his research also does something more important: revealing how thoroughly our lives are now mediated by artificial intelligence. .Less than a generation ago, the way voters viewed their politicians was largely shaped by tens of thousands of newspaper editors, journalists and TV executives. Today, the invisible codes behind the big technology platforms have become the new kingmakers. .They pluck from obscurity people like Dave Todeschini, a retired IBM engineer who, “let off steam” during the election by recording himself opining on Clinton’s supposed involvement in paedophilia, child sacrifice and cannibalism. “It was crazy, it was nuts,” he said of the avalanche of traffic to his YouTube channel, which by election day had more than 2m views. .“Breaking news,” he announced in one of his last dispatches before the vote: the FBI, he said, had just found graphic images of Clinton and her aide in “sexually compromising positions” with a teenager. “It seems to me, with Bill Clinton’s trips to paedophile island a number of times, that what we have here is nothing short of the Clinton paedophile ring,” he declared. .Todeschini sits in his darkened living room in New Jersey, staring into his smartphone. “I’ll tell you what: the rabbit hole just got a couple of yards deeper.”.A full description of the methodology Chaslot used to detect YouTube recommendations (and an explanation of how the Guardian analysed them) is available here. ",Youtube's algorithms tend to promote videos with high engagement including conspiracy videos
25,"New Zealand Prime Minister Jacinda Ardern has been in contact with leaders of major tech companies like Facebook, Google, Microsoft, and Twitter as part of her push to slow the spread of violent content online..Ardern on Wednesday announced that she and French President Emmanuel Macron planned to host a summit in Paris on May 15 to encourage industry and world leaders to commit to a pledge called the Christchurch Call, which seeks to curb extremist content on social media..No tech company — just like no government — wishes to see violent extremism and terrorism online, she told reporters on Wednesday. So we have a starting point which is one of unity..New Zealand has been eager to clamp down on malicious social-media activity since 50 people were fatally shot and dozens more were injured in the terrorist attack in Christchurch on March 15. The gunman livestreamed the attacks at two separate mosques on Facebook, and copies of the gruesome video quickly spread on that platform and others..The March 15 terrorist attacks saw social media used in an unprecedented way as a tool to promote an act of terrorism and hate, Ardern said. We are asking for a show of leadership to ensure social media cannot be used again the way it was..Ardern said she spoke directly with Facebook CEO Mark Zuckerberg, who shared his condolences after the shooting. She did not elaborate on the details of their discussion..Facebook faced harsh blowback over the livestream video, which was briefly hosted on its site. The company said it had blocked 1.5 million videos of the attacks within 24 hours. Still, Business Insider was able to find numerous copies circulating across social media in the days after the shooting..Under New Zealand law, dissemination or possession of material depicting extreme violence or terrorism is prohibited, according to The New York Times. New Zealands human-rights laws also forbid the incitement of what the country calls racial disharmony through written or broadcast media..New Zealands censorship office last month made the possession and sharing of the 17-minute livestream illegal. According to Television New Zealand, those who distribute the video could face a maximum of 14 years in prison. Six people appeared in a New Zealand court last week on charges of illegally distributing video from the Christchurch shooting.",Christchurch terrorist attack was live streamed on Facebook
26,"In a test the ACLU of Massachusetts conducted using a widely available facial recognition technology called “Rekognition,” the software falsely matched 27 New England professional athletes to individuals in a mugshot database. The test shows high-profile athletes, including the Patriots’ Duron Harmon, were mistakenly matched with images in the arrest photo database..“This technology is flawed,” said Harmon, the New England Patriots safety. “If it misidentified me, my teammates, and other professional athletes in an experiment, imagine the real-life impact of false matches. This technology should not be used by the government without protections. Massachusetts should press pause on face surveillance technology.”.To perform the test, the ACLU of Massachusetts compared the official headshots of 188 New England athletes from the Boston Bruins, Boston Celtics, Boston Red Sox, and New England Patriots with a database of 20,000 public arrest photos. Nearly one-in-six athletes were falsely identified. An independent computer science expert verified the results..“The results of this scan add to the mounting evidence that unregulated face surveillance technology in the hands of government agencies is a serious threat to individual rights, due process, and democratic freedoms,” said Kade Crockford, director of the Technology for Liberty Program at the ACLU of Massachusetts. “Face surveillance is dangerous when it doesn’t work, and when it does. There are currently no rules or standards in place in our state to ensure the technology isn’t misused or abused. Massachusetts must pass a moratorium on government use of face surveillance technology until there are safeguards in place to keep people safe and free.”.In June, the ACLU of Massachusetts launched the “Press Pause on Face Surveillance” campaign to build awareness about the civil liberties concerns posed by face surveillance and the need to pass a statewide moratorium on the government’s use of the technology. An ACLU-backed bill currently before legislators on Beacon Hill would establish a statewide moratorium on government use of face surveillance and other biometric screening technologies until the legislature imposes checks and balances to protect the public’s interest. A recent poll shows 79 percent of Massachusetts voters support a moratorium on government use of face surveillance technology, and 91 percent think the government should not use the technology unless it is subject to regulation..Face surveillance technology is currently deployed and marketed in Massachusetts without any regulations. In emails uncovered, a face surveillance company CEO admits to Plymouth municipal authorities that his technology might work only 30 percent of the time. Nonetheless, he pushes aggressively for its adoption in schools, government buildings, and public streets — all in secret, with no public debate or buy-in from elected officials..A similar test conducted last year by the ACLU of California misidentified 28 sitting members of Congress; the false matches were disproportionately of people of color, including six members of the Congressional Black Caucus. Multiple studies of facial recognition technology have found systems to be inaccurate when used against women and people of color.",A facial-recognition system identified NFL champions as criminals
27,"Update June 27, 3:03 p.m. EST: The creator of DeepNude announced that hes taken down the app. Read more, here.   A programmer created an application that uses neural networks to remove clothing from the images of women, making them look realistically nude..The software, called DeepNude, uses a photo of a clothed person and creates a new, naked image of that same person. It swaps clothes for naked breasts and a vulva, and only works on images of women. When Motherboard tried using an image of a man, it replaced his pants with a vulva. While DeepNude works with varying levels of success on images of fully clothed women, it appears to work best on images where the person is already showing a lot of skin. We tested the app on dozens of photos and got the most convincing results on high resolution images from Sports Illustrated Swimsuit issues..Since Motherboard discovered deepfakes in late 2017, the media and politicians focused on the dangers they pose as a disinformation tool. But the most devastating use of deepfakes has always been in how theyre used against women: whether to experiment with the technology using images without womens consent, or maliciously spreading nonconsensual porn on the internet. DeepNude is an evolution of that technology that is easier to use and faster to create than deepfakes. DeepNude also dispenses with the idea that this technology can be used for anything other than claiming ownership over women’s bodies..This is absolutely terrifying, Katelyn Bowden, founder and CEO of revenge porn activism organization Badass, told Motherboard. Now anyone could find themselves a victim of revenge porn, without ever having taken a nude photo. This tech should not be available to the public..This is an “invasion of sexual privacy,” Danielle Citron, professor of law at the University of Maryland Carey School of Law, who recently testified to Congress about the deepfake threat, told Motherboard..“Yes, it isn’t your actual vagina, but… others think that they are seeing you naked,” she said. “As a deepfake victim said to me—it felt like thousands saw her naked, she felt her body wasn’t her own anymore.”.Motherboard downloaded the application and tested it on a Windows machine. It installed and launched like any other Windows application and didnt require technical expertise to use. In the free version of the app, the output images are partially covered with a large watermark. In a paid version, which costs $50, the watermark is removed, but a stamp that says FAKE is placed in the upper-left corner. (Cropping out the fake stamp or removing it with Photoshop would be very easy.).Motherboard tested it on more than a dozen images of women and men, in varying states of dress—fully clothed to string bikinis—and a variety of skin tones. The results vary dramatically, but when fed a well lit, high resolution image of a woman in a bikini facing the camera directly, the fake nude images are passably realistic. The algorithm accurately fills in details where clothing used to be, angles of the breasts beneath the clothing, nipples, and shadows..But its not flawless. Most images, and low-resolution images especially, produced some visual artifacts. DeepNude failed entirely with some photographs that used weird angles, lighting, or clothing that seem to throw off the neural network it uses. When we fed it an image of the cartoon character Jessica Rabbit, it distorted and destroyed the image altogether, throwing stray nipples into a blob of a figure..In an email, the anonymous creator of DeepNude, who requested to go by the name Alberto, told Motherboard that the software is based on pix2pix, an open-source algorithm developed by University of California, Berkeley researchers in 2017. Pix2pix uses generative adversarial networks (GANs), which work by training an algorithm on a huge dataset of images—in the case of DeepNude, more than 10,000 nude photos of women, the programmer said—and then trying to improve against itself. This algorithm is similar to whats used in deepfake videos, and what self-driving cars use to imagine road scenarios..The algorithm only works with women, Alberto said, because images of nude women are easier to find online—but hes hoping to create a male version, too..The networks are multiple, because each one has a different task: locate the clothes. Mask the clothes. Speculate anatomical positions. Render it, he said. All this makes processing slow (30 seconds in a normal computer), but this can be improved and accelerated in the future..Deepfake videos, by comparison, take hours or days to render a believable face-swapped video. For even a skilled editor, manually using Photoshop to realistically change a clothed portrait to nude would take several minutes..Alberto said he was inspired to create DeepNude by ads for gadgets like X-Ray glasses that he saw while browsing magazines from the 1960s and 70s, which he had access to during his childhood. The logo for DeepNude, a man wearing spiral glasses, is an homage to those ads..Like everyone, I was fascinated by the idea that they could really exist and this memory remained, he said. About two years ago I discovered the potential of AI and started studying the basics. When I found out that GAN networks were able to transform a daytime photo into a nighttime one, I realized that it would be possible to transform a dressed photo into a nude one. Eureka. I realized that x-ray glasses are possible! Driven by fun and enthusiasm for that discovery, I did my first tests, obtaining interesting results..Im not a voyeur, Im a technology enthusiast,” he said. “Continuing to improve the algorithm. Recently, also due to previous failures (other startups) and economic problems, I asked myself if I could have an economic return from this algorithm. Thats why I created DeepNude..I think that what you can do with DeepNude, you can do it very well with Photoshop (after a few hours of tutorial), he said, noting that DeepNude doesnt transmit images itself, only creates them and allows the user to do what they will with the results..I also said to myself: the technology is ready (within everyones reach), he said. So if someone has bad intentions, having DeepNude doesnt change much… If I dont do it, someone else will do it in a year..In the year and a half since Motherboard discovered deepfakes on Reddit, the machine learning technology it employs has moved at breakneck speed. Algorithmic face-swaps have gone from requiring hundreds of images and days of processing time in late 2017, to requiring only a handful of images, or even just text inputs, and a few hours of time, in recent months..Motherboard showed the DeepNude application to Hany Farid, a computer-science professor at UC Berkeley who has become a widely-cited expert on the digital forensics of deepfakes. Farid was shocked at this development, and the ease at which it can be done..We are going to have to get better at detecting deepfakes, and academics and researchers are going to have to think more critically about how to better safeguard their technological advances so that they do not get weaponized and used in unintended and harmful ways, Farid said. In addition, social media platforms are going to have to think more carefully about how to define and enforce rules surrounding this content. And, our legislators are going to have to think about how to thoughtfully regulate in this space..Deepfakes have become a widespread, international phenomenon, but platform moderation and legislation so far has failed to keep up with this fast-moving technology. In the meantime, women are victimized by deepfakes and left behind for a more political, US-centric political narrative. Though deepfakes have been weaponized most often against unconsenting women, most headlines and political fear of them have focused on their fake news potential..Even bills like the DEEPFAKES Accountability Act, introduced earlier this month, arent enough to stop this technology from hurting real people..It’s a real bind—deepfakes defy most state revenge porn laws because it’s not the victim’s own nudity depicted, but also our federal laws protect the companies and social media platforms where it proliferates, attorney Carrie Goldberg, whose law firm specializes in revenge porn, told Motherboard. It’s incumbent on the public to avoid consumption of what we call at my office humili-porn. Whether it’s revenge porn or deepfakes, don’t click or link or share or like! That’s how these sites make money. People need to stop letting their Id drive internet use and use the internet ethically and conscientiously..DeepNude is easier to use, and more easily accessible than deepfakes have ever been. Whereas deepfakes require a lot of technical expertise, huge datasets, and access to expensive graphics cards, DeepNude is a consumer-facing app that is easier to install than most video games that can produce a believable nude in 30 seconds with the click of a single button..Editors note, June 27 1:05 p.m. EST: This story originally included five side-by-side images of various celebrities and DeepNude-manipulated images of those celebrities. While the images were redacted to not show explicit nudity, after hearing from our readers, academic experts, and colleagues, we realized that those images could do harm to the real people in them. We think its important to show the real consequences that new technologies unleashed on the world without warning have on people, but we also have to make sure that our reporting minimizes harm. For that reason, we have removed the images from the story, and regret the error..By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.",Neural networks disrobe images of women
29,"It took less than 24 hours for Twitter to corrupt an innocent AI chatbot. Yesterday, Microsoft unveiled Tay — a Twitter bot that the company described as an experiment in conversational understanding. The more you chat with Tay, said Microsoft, the smarter it gets, learning to engage people through casual and playful conversation..Unfortunately, the conversations didnt stay playful for long. Pretty soon after Tay launched, people starting tweeting the bot with all sorts of misogynistic, racist, and Donald Trumpist remarks. And Tay — being essentially a robot parrot with an internet connection — started repeating these sentiments back to users, proving correct that old programming adage: flaming garbage pile in, flaming garbage pile out..Now, while these screenshots seem to show that Tay has assimilated the internets worst tendencies into its personality, its not quite as straightforward as that. Searching through Tays tweets (more than 96,000 of them!) we can see that many of the bots nastiest utterances have simply been the result of copying users. If you tell Tay to repeat after me, it will — allowing anybody to put words in the chatbots mouth..However, some of its weirder utterances have come out unprompted. The Guardian picked out a (now deleted) example when Tay was having an unremarkable conversation with one user (sample tweet: new phone who dis?), before it replied to the question is Ricky Gervais an atheist? by saying: ricky gervais learned totalitarianism from adolf hitler, the inventor of atheism..But while it seems that some of the bad stuff Tay is being told is sinking in, its not like the bot has a coherent ideology. In the span of 15 hours Tay referred to feminism as a cult and a cancer, as well as noting gender equality = feminism and i love feminism now. Tweeting Bruce Jenner at the bot got similar mixed response, ranging from caitlyn jenner is a hero & is a stunning, beautiful woman! to the transphobic caitlyn jenner isnt a real woman yet she won woman of the year? (Neither of which were phrases Tay had been asked to repeat.).Its unclear how much Microsoft prepared its bot for this sort of thing. The companys website notes that Tay has been built using relevant public data that has been modeled, cleaned, and filtered, but it seems that after the chatbot went live filtering went out the window. The company starting cleaning up Tays timeline this morning, deleting many of its most offensive remarks..Its a joke, obviously, but there are serious questions to answer, like how are we going to teach AI using public data without incorporating the worst traits of humanity? If we create bots that mirror their users, do we care if their users are human trash? There are plenty of examples of technology embodying — either accidentally or on purpose — the prejudices of society, and Tays adventures on Twitter show that even big corporations like Microsoft forget to take any preventative measures against these problems..In an emailed statement given later to Business Insider, Microsoft said: The AI chatbot Tay is a machine learning project, designed for human engagement. As it learns, some of its responses are inappropriate and indicative of the types of interactions some people are having with it. Were making some adjustments to Tay.",Tay bot project canceled after bot generated sexist and racist comments online
32,"Cadillac Fairview, the company that owns popular malls including the Calgary Chinook Centre and Toronto Eaton Centre, told CBC News it will pause the use of cameras embedded inside mall directories..The company said its been using facial recognition software to track peoples genders and ages without consent since June. Its pausing the practice at Chinook Centre and Market Mall..Its unclear how many malls were using the software. Cadillac Fairview previously said it was testing the technology and hadnt completely rolled it out..On Friday, the federal privacy commissioner said it would investigate what the company is doing and whether its legal. Albertas provincial privacy commissioner is also investigating..Cadillac Fairview has stated that it is using the technology for the purposes of monitoring traffic ... The company contends it is not capturing images of individuals, The Office of the Privacy Commissioner of Canada said in a press release..But its not possible for software to determine a shoppers age and gender without capturing their image, argues Sharon Polsky, president of The Privacy and Access Council of Canada..To get to the point that the facial recognition technology can create the numerical representation of a face, theyve got to look at the face and gather the data points of that unique face, Polsky told Global News. They are collecting the information, even if its only for a moment..Facial recognition software is increasingly being used in public by police forces and private companies. Police in Orlando and Detroit have facial recognition software, as does the FBI. Amazon, which sells the technology to law enforcement, even offered consulting on the Orlando polices program free of charge, Wired reports..In China, some police wear smartglasses equipped with facial recognition so that they can spot persons of interest in crowded places and keep track of peoples habits.",Suspends use of facial recognition software to track people's genders and ages
33,"For as long as he can remember, Danny Smith has wanted to be a U.S. Customs and Border Patrol agent. The 29-year-old Texan says he can’t imagine a better opportunity to serve his country with pride..His dream came true from 2010 to 2015, when he served as a CBP agent in Laredo, Texas. But when he and his wife moved to Dallas in 2015 for her job, he put his career on what he thought was a temporary hold. When he sought to return to the agency just weeks later, he was forced to take a polygraph, a new requirement for all CBP agents that had been introduced by Congress in 2010..“I was devastated,” he said in a phone interview. “It really affected me, depressed me. I didn’t have anything to hide. I wasn’t even told why I failed.”.As President Donald Trump seeks to enlist 5,000 more border agents to enforce his promise of curbing illegal immigration, he has proposed to waive some of the employment requirements that CBP union leaders and legislators say make it too hard to hire new agents, including the polygraph test..The Trump administration’s plan—outlined in the Department of Homeland Security’s 90-day progress report on immigration enforcement, which was leaked by the Washington Post earlier this month and delivered to the White House on Tuesday—would waive the polygraph test for certain applicants, including former law enforcement officers and veterans. Two-thirds of applicants now fail the polygraph exam..Making it easier to hire border agents is just one of several priorities in Trump’s beefed up enforcement plans. He’s facing widespread pushback from Democratic and Republican lawmakers who don’t want to pay for his proposed border wall, which could cost upwards of $70 billion. A number of polls show the majority of Americans dont want the wall..Critics of the proposed changes argue that the polygraph test was put in place to curb a wave of corruption and misconduct that CBP faced after its last hiring surge. They caution against what the American Immigration Lawyers Association described as “plans to water down hiring standards” within CBP..Tom Jawetz, the Vice President of Immigration Policy at the progressive organization Center for American Progress, told Univision it’s not the time to ease standards..“If you want to staff up, do you do that by trying to find more qualified agents or reducing the standards?” he asked. The fact that many people don’t pass the test is “not necessarily a reason to scrap it.”.Between 2006 and 2009, under President George W. Bush, the Border Patrol added some 8,000 new agents, from a total of 12,349 to 20,119, making it the largest law enforcement agency in the nation. Multiple subsequent corruption cases at CBP led Congress to pass the Anti-Border Corruption Act in 2010, which introduced the lie-detector test for applicants, calling it “a more streamlined and cost-effective process for bringing new applicants on board.”.“What we’re found is that many agents brought on beforehand who had not gone through a polygraph were cooperating with cartels and subject to corruption,” Jawetz said. “The polygraph has served a function to weed out people in the beginning.”.Jay Ahern, CBP deputy commissioner when the agency doubled in size, told Foreign Policy last month: “if you start lowering standards, the organization pays for it for the next decade, two, or three.”.“If there are ways to prevent corruption, we’re all for it,” said Art del Cueto, the president of National Border Patrol Council Local 2544 in Tucson. “None of us want to work with corrupt agents, none of us do. But the polygraph is not accurate.”.CBP union officials point out that the 65 percent failure rate for the polygraph exam is far higher than the rate at other federal agencies. A January Associated Press investigation found that the failure rate is more than double the average rate at eight law enforcement agencies. As of February, CBP was already 1,768 agents short of its floor..Del Cueto says the test is now used as a sort of shortcut, in place of the much more comprehensive background check used previously, and is intended to trick people into failing. The tests can last up to eight hours, and nervousness is often mistaken as lies. He says he’s heard countless “horror stories” from perfectly qualified agents, and others who don’t even venture to apply for fear of hurting their chance to work in the federal government later on..“Veterans with top secret clearance are failing,” he said, comparing the test to torture. “It’s an interrogation. The only thing missing is waterboarding. That’s how awful these have become.”.According to a report by the Government Accountability Office, more than 2,000 CBP agents were arrested for misconduct between 2005 and 2012, including for DUIs, domestic violence and false imprisonment. Corruption often went unpunished. In 2014, a CBP agent in McAllen, Texas, was under suspicion for misconduct when he kidnapped three Honduran women he had detained. After raping them and attempting to kill two of them, he shot himself..Before he became an agent in 2010, Smith says he underwent a rigorous two-year application period, involving a full FBI background check. He also went through an entrance exam, medical checks and academy training. He says an agent should always “self-police” and speak up if they see anything suspicious involving another agent..Now at the Bureau of Prisons, Smith has become an impassioned advocate against the polygraph requirement and often visits forums, such as AntiPolygraph.org, where people discuss their experiences..“I believe the polygraph is designed to keep you out, at least the way it’s being administered,” he said. “I think it can work as a deterrent for bad applicants but I feel they need to use some common sense when it comes to individuals with proven track records. Your accolades, credibility and work history are what should determine your eligibility.”.The polygraph test is just one element of the application process. CBP data show a 50 percent Border Patrol candidate failure rate on the written test, 15 percent failure in an oral interview, 25 percent medical failure test, 15 percent failure on the physical fitness test, and 56 percent failure on the background check, according to a report from the American Immigration Council..DHS is also proposing to remove the Spanish language proficiency test, allow for remote testing, and loosen physical fitness requirements. It also proposes continued outreach, including to millennials, and expedited hiring..Unlike many of the report’s proposals, the polygraph requirement would need to be changed by congress. In March, Arizona Republican Senators Jeff Flake and John McCain proposed legislation to do that..CBP is consistently ranked one of the worst government agencies for workers, and del Cueto says retention is a major issue. “In hiring 5,000 more agents you need to retain as well as recruit,” he said..“You need to respect the sacrifices agents take on a day to day basis,” he added. “We’re dealing with heat, cold, chasing dangerous criminals trying to come in, working long hours. Border patrol agents are the most assaulted and isolated, living in the most remote areas of the country in small communities.”.Some residents of those communities tell a different story. Christian Ramirez, the director of the Southern Border Communities Coalition, says residents already fear border agents..“We get racially profiled on the way to school. I have to go through checkpoint if I want to take my child to a family member’s house. Trump describes border communities as ground zero of an imaginary war. We do not want more troops in our communities,” he said..The notion that border communities need more agents is “false and unjustified” at a time when illegal border crossings are down, he added. “People want to live in peace, dignity and have resources invested in infrastructure, not in building useless walls and deploying more agents.”.And the cost of Trump’s proposed expansion is dramatic. The entire long-term plan for 5,000 Border Patrol agents and 10,000 ICE officers would increase DHS’ existing budget by over $ 3.14 billion..He has family in Mexico, and understands why people would choose to leave their countries for the United States. But for him, the task is about keeping the country safe.","Questionability of ethical use in immigration, noticed flaws."
35,"Facebook has apologised after an error in its machine-translation service saw Israeli police arrest a Palestinian man for posting “good morning” on his social media profile..The man, a construction worker in the West Bank settlement of Beitar Illit, near Jerusalem, posted a picture of himself leaning against a bulldozer with the caption “يصبحهم”, or “yusbihuhum”, which translates as “good morning”. .But Facebook’s artificial intelligence-powered translation service, which it built after parting ways with Microsoft’s Bing translation in 2016, instead translated the word into “hurt them” in English or “attack them” in Hebrew..Police officers arrested the man later that day, according to Israeli newspaper Haaretz, after they were notified of the post. They questioned him for several hours, suspicious he was planning to use the pictured bulldozer in a vehicle attack, before realising their mistake. At no point before his arrest did any Arabic-speaking officer read the actual post..Facebook said it is looking into the issue, and in a statement to Gizmodo, added: “Unfortunately, our translation systems made an error last week that misinterpreted what this individual posted. .“Even though our translations are getting better each day, mistakes like these might happen from time to time and we’ve taken steps to address this particular issue. We apologise to him and his family for the mistake and the disruption this caused.”.Arabic is considered particularly difficult for many machine translation services due to the large number of different dialects in use around the world, on top of Modern Standard Arabic, the international form of the language..The Israeli Defence Force has been open about monitoring the social media accounts of Palestinians, looking for “lone-wolf” attackers who might otherwise slip through the net. It reportedly does so automatically, using algorithms to look for terms such as “sword of Allah”..Machine translation mistakes are a regular occurrence for anyone using AI to translate languages, particularly ones with little relationship. Earlier this month, Chinese social network WeChat apologised after its own machine translation system translated a neutral phrase meaning “black foreigner” as the n-word. .“When I ran the translator, the n-word came up and I was gobsmacked,” said Ann James, who had been texting back and forth with a friend when the faulty translation appeared.","Facebook translates good morning into attack them, leading into an arrest"
36,"Fresh off the revelation that Google image searches for “CEO” only turn up pictures of white men, there’s new evidence that algorithmic bias is, alas, at it again. In a paper published in April, a team of researchers from Carnegie Mellon University claim Google displays far fewer ads for high-paying executive jobs….“I think our findings suggest that there are parts of the ad ecosystem where kinds of discrimination are beginning to emerge and there is a lack of transparency,” Carnegie Mellon professor Annupam Datta told Technology Review. “This is concerning from a societal standpoint.”.To come to those conclusions, Datta and his colleagues basically built a tool, called Ad Fisher, that tracks how user behavior on Google influences the personalized Google ads that each user sees. Because that relationship is complicated and based on a lot of factors, the researchers used a series of fake accounts: theoretical job-seekers whose behavior they could track closely..That online behavior — visiting job sites and nothing else — was the same for all the fake accounts. But some listed their sex as men and some as women..The Ad Fisher team found that when Google presumed users to be male job seekers, they were much more likely to be shown ads for high-paying executive jobs. Google showed the ads 1,852 times to the male group — but just 318 times to the female group..This isn’t the first time that algorithm systems have appeared to be sexist — or racist, for that matter. When Flickr debuted image recognition tools in May, users noticed the tool sometimes tagged black people as “apes” or “animals.” A landmark study at Harvard previously found serious discrimination in online ad delivery, like when searching ethnic names on Google turned up more results around arrest records. Algorithms have hired by voice inflection. The list goes on and on..After all, algorithmic personalization systems, like the ones behind Google’s ad platform, don’t operate in a vacuum: They’re programmed by humans and taught to learn from user behavior. So the more we click or search or generally Internet in sexist, racist ways, the algorithms learn to generate those results and ads (supposedly the results we would expect to see)..“It’s part of a cycle: How people perceive things affects the search results, which affect how people perceive things,” Cynthia Matuszek, a computer ethics professor at University of Maryland and co-author of a study on gender bias in Google image search results, told The Washington Post in April..Google cautions that some other things could be going on here, too. The advertiser in question could have specified that the ad only been shown to certain users for a whole host of reasons, or the advertiser could have specified that the ad only show on certain third-party sites..“Advertisers can choose to target the audience they want to reach, and we have policies that guide the type of interest-based ads that are allowed,” reads a statement from Google..The interesting thing about the fake users in the Ad Fisher study, however, is that they had entirely fresh search histories: In fact, the accounts used were more or less identical, except for their listed gender identity. That would seem to indicate either that advertisers are requesting that high-paying job ads only display to men (and that Google is honoring that request) or that some type of bias has been programmed, if inadvertently, into Google’s ad-personalization system..“Many important decisions in society these days are being made by algorithms,” he said. “These algorithms run inside of boxes that we don’t have access to the internal details of. The genesis of this project was that we wanted to peek inside this box a little to see if there are more undesirable consequences of this activity going on.”.By registering, you will also enjoy limited access to Premium articles, exclusive newsletters, commenting, and virtual events with our leading journalists",Google Image Search for CEO has Barbie as first result
44,"We reveal personality from facial images at scale to revolutionize how companies, organizations and even robots understand people and dramatically improve public safety, communications, decision-making, and experiences..Faception is first-to-technology and first-to-market with proprietary computer vision and machine learning technology for profiling people and revealing their personality based only on their facial image.  [ learn about the science behind the technology].Faception can analyze faces from video streams (recorded and live), cameras, or online/offline databases, encode the faces in proprietary image descriptors and match an individual with various personality traits and types with a high level of accuracy..We develop proprietary classifiers, each describing a certain personality type or trait such as an Extrovert, a person with High IQ, Professional Poker Player or a threats..Ultimately, we can score facial images on a set of classifiers and provide our clients with a better understanding of their customers, the people in front of them or in front of their cameras.Utilizing advanced machine learning techniques we developed and continue to evolve an array of classifiers. These classifiers represent a certain persona, with a unique personality type, a collection of personality traits or behaviors. Our algorithms can score an individual according to their fit to these classifiers..We live in a dangerous world, where terrorists and other criminals are easily mingle with the general population and easily travel between countries; the vast majority of them are unknown to the authorities. As a result, it is becoming ever more challenging to detect suspected individuals in public places such as airports, train stations, government and public buildings, and border controls..Current solutions mostly rely on facial recognition, detecting suspicious activity/ behavior and manual profiling. They are not sufficient to handle the scale of the growing threats..What if it was possible to know whether an anonymous individual is a potential terrorist, an aggressive person, or a potential criminal? Better yet, what if that information could be obtained and used in real-time, when it matters the most?.Faception offers a breakthrough computer-vision and machine learning technology that goes beyond Biometrics. Our solution analyzes a person’s facial image and automatically reveals his personality, enabling security companies/agencies to more efficiently detect, focus and apprehend potential terrorists or criminals before they have the opportunity to do harm..Our solution is easy to deploy with minimum integration work and can installed on the client hardware, the system can integrate with an existing face recognition platform and support operational hierarchy and reporting modules..Artificial intelligence has been touted as humanitys salvation and its downfall. But what is the truth behind the hype? This is AI looks at how AI is changing the world now, the scientists shaping it, and the lives affected by this nascent technology.Faception was lately selected to be the sole Israeli technology speaker in the biggest Public Safety and Security and Police Equipment show in China. It has been organized since 1994, and held every 2 years. The official name is the 14 China International Exhibition on Public Safety and Security 2018, called Security China 2018, organized by China Security Association, under the Ministry of Public Security China. Total exhibition area in 106800 square meters, more than 1000 exhibitors, and including totally 8 exhibit halls..Faception, a facial personality analytics startup based in Israel, said its software does detailed analyses of the facial characteristics of individuals captured by video and still photography. The predicted personality traits are used to identify individuals who could present a threat to public safety. According ...​",Faception released AI technology that could analyze facial images and bone structure to reveal person IQ and violent tendencies.
45,"Chinese police have admitted to wrongly shaming a famous businesswoman after a facial recognition system designed to catch jaywalkers mistook an advert on the side of a bus for her actual face. .Dong Mingzhu, president of Chians biggest air conditioning maker, had her image flashed up on a public display screen in the city of Ningbo, near Shanghai, with a caption saying she had illegally crossed the street on a red light..But Ningbos facial recognition cameras had actually only caught an advert featuring her face on the side of a passing bus – a fact quickly spotted by Chinese citizens, who shared pictures of the alert on Weibo, a social network similar to Twitter..On Wednesday, Ningbo traffic police admitted their mistake, saying the alert had been immediately deleted afterwards and that technicians had completely upgraded the system to reduce the false recognition rate..The incident highlights Chinas growing use of automated systems such as facial recognition cameras to catch petty criminals, as well as in a campaign of repression against Muslim citizens in the western province of Xinjiang..Ninbos authorities recently boasted that facial recognition cameras installed at six intersections had spotted more than 7,800 cases of jaywalking, while the industrial centre of Shenzhen claims to have shamed 14,000 in 10 months at one intersection alone. .Weiber users took the opportunity of Ningpos mistake to mock the authorities. Who is that person clinging onto the bus? Serious warning! joked one. It means the system works – it wont let go of any face, said another..Ms Dong is well-known in China as a tough female entrepreneur and single mother who rose from working class roots and who claims not to have used a day of her holiday allowance in 26 years..Her autobiography, published in 2006 and later made into a television series, was titled Regretless Pursuit, and one male competitor reportedly said of her: Where sister Dong walks, no grass grows..In interviews, she has said she regrets not spending more time with her son while he was growing up, but said she did not remarry after her husband died in 1984 because to do so would have compromised her independence..Her company, Gree Electric Appliances, issued a statement thanking Ningbos traffic police for their hard work and calling on people to obey traffic rules.",AI system recognized a photo of a Chinese billionaire as a jaywalker
48,"Last month marked the 17th anniversary of 9/11. With it came a new milestone: we’ve been in Afghanistan for so long that someone born after the attacks is now old enough to go fight there. They can also serve in the six other places where we’re officially at war, not to mention the 133 countries where special operations forces have conducted missions in just the first half of 2018..The wars of 9/11 continue, with no end in sight. Now, the Pentagon is investing heavily in technologies that will intensify them. By embracing the latest tools that the tech industry has to offer, the US military is creating a more automated form of warfare – one that will greatly increase its capacity to wage war everywhere forever..On Friday, the defense department closes the bidding period for one of the biggest technology contracts in its history: the Joint Enterprise Defense Infrastructure (Jedi). Jedi is an ambitious project to build a cloud computing system that serves US forces all over the world, from analysts behind a desk in Virginia to soldiers on patrol in Niger. The contract is worth as much as $10bn over 10 years, which is why big tech companies are fighting hard to win it. (Not Google, however, where a pressure campaign by workers forced management to drop out of the running.).At first glance, Jedi might look like just another IT modernization project. Government IT tends to run a fair distance behind Silicon Valley, even in a place as lavishly funded as the Pentagon. With some 3.4 million users and 4 million devices, the defense department’s digital footprint is immense. Moving even a portion of its workloads to a cloud provider such as Amazon will no doubt improve efficiency..But the real force driving Jedi is the desire to weaponize AI – what the defense department has begun calling “algorithmic warfare”. By pooling the military’s data into a modern cloud platform, and using the machine-learning services that such platforms provide to analyze that data, Jedi will help the Pentagon realize its AI ambitions..The scale of those ambitions has grown increasingly clear in recent months. In June, the Pentagon established the Joint Artificial Intelligence Center (JAIC), which will oversee the roughly 600 AI projects currently under way across the department at a planned cost of $1.7bn. And in September, the Defense Advanced Research Projects Agency (Darpa), the Pentagon’s storied R&D wing, announced it would be investing up to $2bn over the next five years into AI weapons research..So far, the reporting on the Pentagon’s AI spending spree has largely focused on the prospect of autonomous weapons – Terminator-style killer robots that mow people down without any input from a human operator. This is indeed a frightening near-future scenario, and a global ban on autonomous weaponry of the kind sought by the Campaign to Stop Killer Robots is absolutely essential..But AI has already begun rewiring warfare, even if it hasn’t (yet) taken the form of literal Terminators. There are less cinematic but equally scary ways to weaponize AI. You don’t need algorithms pulling the trigger for algorithms to play an extremely dangerous role..To understand that role, it helps to understand the particular difficulties posed by the forever war. The killing itself isn’t particularly difficult. With a military budget larger than that of China, Russia, Saudi Arabia, India, France, Britain and Japan combined, and some 800 bases around the world, the US has an abundance of firepower and an unparalleled ability to deploy that firepower anywhere on the planet..The US military knows how to kill. The harder part is figuring out whom to kill. In a more traditional war, you simply kill the enemy. But who is the enemy in a conflict with no national boundaries, no fixed battlefields, and no conventional adversaries?.This is the perennial question of the forever war. It is also a key feature of its design. The vagueness of the enemy is what has enabled the conflict to continue for nearly two decades and to expand to more than 70 countries – a boon to the contractors, bureaucrats and politicians who make their living from US militarism. If war is a racket, in the words of marine legend Smedley Butler, the forever war is one the longest cons yet..But the vagueness of the enemy also creates certain challenges. It’s one thing to look at a map of North Vietnam and pick places to bomb. It’s quite another to sift through vast quantities of information from all over the world in order to identify a good candidate for a drone strike. When the enemy is everywhere, target identification becomes far more labor-intensive. This is where AI – or, more precisely, machine learning – comes in. Machine learning can help automate one of the more tedious and time-consuming aspects of the forever war: finding people to kill..The Pentagon’s Project Maven is already putting this idea into practice. Maven, also known as the Algorithmic Warfare Cross-Functional Team, made headlines recently for sparking an employee revolt at Google over the company’s involvement. Maven is the military’s “pathfinder” AI project. Its initial phase involves using machine learning to scan drone video footage to help identify individuals, vehicles and buildings that might be worth bombing..“We have analysts looking at full-motion video, staring at screens 6, 7, 8, 9, 10, 11 hours at a time,” says the project director, Lt Gen Jack Shanahan. Maven’s software automates that work, then relays its discoveries to a human. So far, it’s been a big success: the software has been deployed to as many as six combat locations in the Middle East and Africa. The goal is to eventually load the software on to the drones themselves, so they can locate targets in real time..Won’t this technology improve precision, thus reducing civilian casualties? This is a common argument made by higher-ups in both the Pentagon and Silicon Valley to defend their collaboration on projects like Maven. Code for America’s Jen Pahlka puts it in terms of “sharp knives” versus “dull knives”: sharper knives can help the military save lives..In the case of weaponized AI, however, the knives in question aren’t particularly sharp. There is no shortage of horror stories of what happens when human oversight is outsourced to faulty or prejudiced algorithms – algorithms that can’t recognize black faces, or that reinforce racial bias in policing and criminal sentencing. Do we really want the Pentagon using the same technology to help determine who gets a bomb dropped on their head?.But the deeper problem with the humanitarian argument for algorithmic warfare is the assumption that the US military is an essentially benevolent force. Many millions of people around the world would disagree. In 2017 alone, the US and allied strikes in Iraq and Syria killed as many as 6,000 civilians. Numbers like these don’t suggest a few honest mistakes here and there, but a systemic indifference to “collateral damage”. Indeed, the US government has repeatedly bombed civilian gatherings such as weddings in the hopes of killing a high-value target..Further, the line between civilian and combatant is highly porous in the era of the forever war. A report from the Intercept suggests that the US military labels anyone it kills in “targeted” strikes as “enemy killed in action”, even if they weren’t one of the targets. The so-called “signature strikes” conducted by the US military and the CIA play similar tricks with the concept of the combatant. These are drone attacks on individuals whose identities are unknown, but who are suspected of being militants based on displaying certain “signatures” – which can be as vague as being a military-aged male in a particular area..The problem isn’t the quality of the tools, in other words, but the institution wielding them. And AI will only make that institution more brutal. The forever war demands that the US sees enemies everywhere. AI promises to find those enemies faster – even if all it takes to be considered an enemy is exhibiting a pattern of behavior that a (classified) machine-learning model associates with hostile activity. Call it death by big data..AI also has the potential to make the forever war more permanent, by giving some of the country’s largest companies a stake in perpetuating it. Silicon Valley has always had close links to the US military. But algorithmic warfare will bring big tech deeper into the military-industrial complex, and give billionaires like Jeff Bezos a powerful incentive to ensure the forever war lasts forever. Enemies will be found. Money will be made.",Weaponized AI can produce many unintended impacts from unwanted deaths to bombings
50,"A conquering army wants to take a major city but doesn’t want troops to get bogged down in door-to-door fighting as they fan out across the urban area. Instead, it sends in a flock of thousands of small drones, with simple instructions: Shoot everyone holding a weapon. A few hours later, the city is safe for the invaders to enter..This sounds like something out of a science fiction movie. But the technology to make it happen is mostly available today — and militaries worldwide seem interested in developing it. .Experts in machine learning and military technology say it would be technologically straightforward to build robots that make decisions about whom to target and kill without a “human in the loop” — that is, with no person involved at any point between identifying a target and killing them. And as facial recognition and decision-making algorithms become more powerful, it will only get easier. .Called “lethal autonomous weapons” — but “killer robots” isn’t an unreasonable moniker — the proposed weapons would mostly be drones, not humanoid robots, which are still really hard to build and move. But they could be built much smaller than existing military drones, and they could potentially be much cheaper. .Now, researchers in AI and public policy are trying to make the case that killer robots aren’t just a bad idea in the movies — they’re a bad idea in real life. There are certainly ways to use AI to reduce the collateral damage and harms of war, but fully autonomous weapons would also usher in a host of new moral, technical, and strategic dilemmas, which is why scientists and activists have pushed the United Nations and world governments to consider a preemptive ban. Their hope is that we can keep killer robots in the realm of science fiction. .Military drones already fly the skies in areas where the US is at war or engaged in military operations. Human controllers decide when these drones will fire. Lethal autonomous weapons (LAWS) don’t quite exist yet, but the technology to replace the humans with an algorithm that makes the decision of when to shoot does. .“Technologically, autonomous weapons are easier than self-driving cars,” Stuart Russell, a computer science professor at UC Berkeley and leading AI researcher, told me. “People who work in the related technologies think it’d be relatively easy to put together a very effective weapon in less than two years.”.That weapon would not look like the Terminator. The simplest version would use existing military drone hardware. But while today’s drones transmit a video feed back to a military base, where a soldier decides whether the drone should fire on the target, with an autonomous weapon the soldier won’t make that decision — an algorithm would. .The algorithm could have a fixed list of people it can target and fire only if it’s highly confident (from its video footage) that it has identified one of those targets. Or it could be trained, from footage of combat, to predict whether a human would tell it to fire, and fire if it thinks that’s the instruction it would be given. Or it could be taught to fire on anyone in a war zone holding something visually identifiable as a gun and not wearing the uniform of friendly forces..“When people hear ‘killer robots,’ they think Terminator, they think science fiction, they think of something that’s far away,” Toby Walsh, a professor of artificial intelligence at the University of New South Wales and an activist against lethal autonomous weapons development, told me. “Instead, it’s simpler technologies that are much nearer, and that are being prototyped as we speak.”.In the past few years, the state of AI has grown by leaps and bounds. Facial recognition has gotten vastly more accurate, as has object recognition, two skills that would likely be essential for lethal autonomous weapons. .New techniques have enabled AI systems to do things that would have been impossible just a few years ago, from writing stories to creating fake faces to, most relevantly to LAWS, making instantaneous tactical decisions in online war games. That means that lethal autonomous weapons have rapidly gone from impossible to straightforward — and they’ve gotten there before we’ve developed any sort of consensus on whether they are acceptable to develop or use..Taking the human out of the loop — and designing weapons that fire on their own without human intervention — has terrifying moral implications. (It has terrifying strategic implications too; we’ll get to that in a bit.) Why would anyone even want to do it?.From a military perspective, the most straightforward argument for autonomous weapons is that they open up a world of new capabilities. If drones have to be individually piloted by a human who makes the crucial decisions about when the drone could fire, you can only have so many of them in the sky at once. .Furthermore, current drones need to transmit and receive information from their base. That introduces some lag time, limits where they can operate, and leaves them somewhat vulnerable — they are useless if communications get cut off by enemies who can block (or “jam”) communication channels. .LAWS would change that. “Because you don’t need a human, you can launch thousands or millions of [autonomous weapons] even if you don’t have thousands or millions of humans to look after them,” Walsh told me. “They don’t have to worry about jamming, which is probably one of the best ways to protect against human-operated drones.” .“The most interesting argument for autonomous weapons,” Walsh told me, “is that robots can be more ethical.” Humans, after all, sometimes commit war crimes, deliberately targeting innocents or killing people who’ve surrendered. And humans get fatigued, stressed, and confused, and end up making mistakes. Robots, by contrast, “follow exactly their code,” Walsh said. .Pentagon defense expert and former US Army Ranger Paul Scharre explores that idea in his 2018 book, Army of None: Autonomous Weapons and the Future of War. “Unlike human soldiers,” he points out, “machines never get angry or seek revenge.” And “it isn’t hard to imagine future weapons that could outperform humans in distinguishing between a person holding a rifle and one holding a rake.”.Ultimately, though, Scharre argues that this argument has a fatal flaw: “What’s legal and what’s right aren’t always the same.” He tells the story of a time his unit in Afghanistan was scouting and their presence was discovered by the Taliban. The Taliban sent out a 6-year-old girl, who unconvincingly pretended to be herding her goats while really reporting the location of the US soldiers by radio to the Taliban..“The laws of war don’t set an age for combatants,” Scharre points out in the book. Under the laws of war, a Taliban combatant was engaging in a military operation near the US soldiers and it would be legal to shoot her. Of course, the soldiers didn’t even consider it — because killing children is wrong. But a robot programmed to follow the laws of war wouldn’t consider details like that. Sometimes soldiers do much worse than what the law permits them to do. But on other occasions, they do better — because they’re human, and bound by moral codes as well as legal ones. Robots wouldn’t be. .Emilia Javorsky, the founder of Scientists Against Inhumane Weapons, points out that there’s a much better way to use robots to prevent war crimes, if that’s really our goal. “Humans and machines make different mistakes, and if they work together, you can avoid both kinds of mistakes. You see this in medicine — diagnostic algorithms make one kind of mistake; doctors tend to make a different kind of mistake.” .So we could design weapons that are programmed to know the laws of war — and accordingly will countermand any order from a human that violates those laws — and that do not have the authority to kill without human oversight. Scientists Against Inhumane Weapons and other researchers who study LAWS have no objections to systems like those. Their argument is simply that, as a matter of international law and as a focus for weapons development and research, there should always be a human in the loop. .If this avenue is pursued, we could have the best of both worlds: robots that have automatic guardrails against making mistakes but also have human input to make sure the automatic decisions are the right ones. But right now, analysts worry that we’re moving toward full autonomy: a world where robots are making the call to kill people without human input..Fully autonomous weapons will make it easier and cheaper to kill people — a serious problem all by itself in the wrong hands. But opponents of lethal autonomous weapons warn that the consequences could be worse than that..For one thing, if LAWS development continues, eventually the weapons might be extremely inexpensive. Already today, drones can be purchased or built by hobbyists fairly cheaply, and prices are likely to keep falling as the technology improves. And if the US used drones on the battlefield, many of them would no doubt be captured or scavenged. “If you create a cheap, easily proliferated weapon of mass destruction, it will be used against Western countries,” Russell told me. .Lethal autonomous weapons also seem like they’d be disproportionately useful for ethnic cleansing and genocide; “drones that can be programmed to target a certain kind of person,” Ariel Conn, communications director at the Future of Life Institute, told me, are one of the most straightforward applications of the technology..Then there are the implications for broader AI development. Right now, US machine learning and AI is the best in the world, which means that the US military is loath to promise that it will not exploit that advantage on the battlefield. “The US military thinks it’s going to maintain a technical advantage over its opponents,” Walsh told me..That line of reasoning, experts warn, opens us up to some of the scariest possible scenarios for AI. Many researchers believe that advanced artificial intelligence systems have enormous potential for catastrophic failures — going wrong in ways that humanity cannot correct once we’ve developed them, and (if we screw up badly enough) potentially wiping us out. .In order to avoid that, AI development needs to be open, collaborative, and careful. Researchers should not be conducting critical AI research in secret, where no one can point out their errors. If AI research is collaborative and shared, we are more likely to notice and correct serious problems with advanced AI designs. .And most crucially, advanced AI researchers should not be in a hurry. “We’re trying to prevent an AI race,” Conn told me. “No one wants a race, but just because no one wants it doesn’t mean it won’t happen. And one of the things that could trigger that is a race focused on weapons.”.If the US leans too much on its AI advantage for warfare, other countries will certainly redouble their own military AI efforts. And that would create the conditions under which AI mistakes are most likely and most deadly. .In combating killer robots, researchers point with optimism to a ban on another technology that was rather successful: the prohibition on the use of biological weapons. That ban was enacted in 1972, amid advances in bioweaponry research and growing awareness of the risks of biowarfare.  .Several factors made the ban on biological weapons largely successful. First, state actors didn’t have that much to gain by using the tools. Much of the case for biological weapons was that they were unusually cheap weapons of mass destruction — and access to cheap weapons of mass destruction is mostly bad for states..Opponents of LAWS have tried to make the case that killer robots are similar. “My view is that it doesn’t matter what my fundamental moral position is, because that’s not going to convince a government of anything,” Russell told me. Instead, he has focused on the case that “we struggled for 70-odd years to contain nuclear weapons and prevent them from falling in the wrong hands. In large quantities, [LAWS] would be as lethal, much cheaper, much easier to proliferate” — and that’s not in our national security interests..But the UN has been slow to agree even to a debate over a lethal autonomous weapons treaty. There are two major factors at play: First, the UN’s process for international treaties is generally a slow and deliberative one, while rapid technological changes are altering the strategic situation with regard to lethal autonomous weapons faster than that process is set up to handle. Second, and probably more importantly, the treaty has some strong opposition. .The US (along with Israel, South Korea, the United Kingdom, and Australia) has thus far opposed efforts to secure a UN treaty opposing lethal autonomous weapons. The US’s stated reason is that since in some cases there could be humanitarian benefits to LAWS, a ban now before those benefits have been explored would be “premature.” (Current Defense Department policy is that there will be appropriate human oversight of AI systems.).Opponents nonetheless argue that it’s better for a treaty to be put in place as soon as possible. “It’s going to be virtually impossible to keep [LAWS] to narrow use cases in the military,” Javorsky argues. “That’s going to spread to use by non-state actors.” And often it’s easier to ban things before anyone has them already and wants to keep the tools they’re already using. So advocates have worked for the past several years to bring up LAWS for debate in the UN, where the details of a treaty can be hammered out..There’s a lot to hammer out. What exactly makes a system autonomous? If South Korea deploys, on the border of the Demilitarized Zone with North Korea, gun systems that automatically shoot unauthorized persons, that’s a lethal autonomous weapon — but it’s also a lot like a land mine. “Arguably, it can be a bit better at discriminating than a minefield can, so maybe it even has advantages,” Russell said. .Or take “loitering munitions,” an existing technology. Fired into the air, Scharre writes, they circle over a wide area until they home in on the radar systems they want to destroy. No human is involved in the final decision to dive in and attack. These are autonomous weapons, though they target radar systems, not humans..These and other issues would have to be settled for a useful UN ban on autonomous weapons. And with the US opposed, an international treaty against lethal autonomous weapons is unlikely to succeed. .There’s another form of advocacy that might impede military uses of AI: the reluctance of AI researchers to work on such uses. Leading AI researchers in the US are largely in Silicon Valley, not working for the US military, and partnerships between Silicon Valley and the military have so far been fraught. When it was revealed that Google was working with the Department of Defense on drones through Project Maven, Google employees revolted, and the project was not renewed. Microsoft employees have similarly objected to military uses of their work. .It’s possible that tech workers can delay the day when a treaty is needed, or create pressure to make such a treaty happen, simply by declining to write the software that will power our killer robots — and there are signs that they’re inclined to do so..Killer robots have the potential to do a lot of harm, and make the means of killing lots of people more available to totalitarian states and to non-state actors. That’s pretty scary. .AI is making things possible that were never possible before, and doing so quickly, such that our capabilities frequently get out ahead of thought, reflection, and strong public policy. As AI systems become more powerful, this dynamic will become more and more destabilizing. .Whether it’s killer robots or fake news, algorithms used to shoot suspected combatants or trained to make parole decisions about prisoners, we’re handing over more and more critical aspects of society to systems that aren’t fully understood and that are optimizing for goals that might not quite reflect our own..Advanced AI systems aren’t here yet. But they get closer every day, and it’s time to make sure we’ll be ready for them. The best time to come up with sound policy and international agreements is before these science fiction scenarios become reality. .     Millions turn to Vox to understand what’s happening in the news. Our mission has never been more vital than it is in this moment: to empower through understanding. Financial contributions from our readers are a critical part of supporting our resource-intensive work and help us keep our journalism free for all.  Please consider making a contribution to Vox today. ",There has been consideration for lethal autonomous weapons
51,"  NOTE: The Solicitations and topics listed on                                 this site are copies from the various SBIR agency solicitations and are not necessarily                                 the latest and most up-to-date.                                 For this reason, you should use the agency link listed below which will take you                                 directly to the                                 appropriate agency server where you can read the official version of this solicitation                                 and download the appropriate forms and rules.                             .OBJECTIVE: Develop a system that can be integrated and deployed in a class 1 or class 2 Unmanned Aerial System (UAS) to automatically Detect, Recognize, Classify, Identify (DRCI) and target personnel and ground platforms or other targets of interest. The system should implement learning algorithms that provide operational flexibility by allowing the target set and DRCI taxonomy to be quickly adjusted and to operate in different environments. .DESCRIPTION: The use of UASs in military applications is an area of increasing interest and growth. This coupled with the ongoing resurgence in the research, development, and implementation of different types of learning algorithms such as Artificial Neural Networks (ANNs) provide the potential to develop small, rugged, low cost, and flexible systems capable of Automatic Target Recognition (ATR) and other DRCI capabilities that can be integrated in class 1 or class 2 UASs. Implementation of a solution is expected to potentially require independent development in the areas of sensors, communication systems, and algorithms for DRCI and data integration. Additional development in the areas of payload integration and Human-Machine Interface (HMI) may be required to develop a complete system solution. One of the desired characteristics of the system is to use the flexibility afforded by the learning algorithms to allow for the quick adjustment of the target set or the taxonomy of the target set DRCI categories or classes. This could allow for the expansion of the system into a Homeland Security environment. .PHASE I: Conduct an assessment of the key components of a complete objective payload system constrained by the Size Weight and Power (SWAP) payload restrictions of a class 1 or class 2 UAS. Systems Engineering concepts and methodologies may be incorporated in this assessment. It is anticipated that this will require, at a minimum, an assessment of the sensor suite, learning algorithms, and communications system. The assessment should define requirements for the complete system and flow down those requirements to the sub-component level. Conduct a laboratory demonstration of the learning algorithms for the DRCI of the target set and the ability to quickly adjust to target set changes or to operator-selected DRCI taxonomy. .PHASE II: Demonstrate a complete payload system at a Technology Readiness Level (TRL) 5 or higher operating in real time. On-flight operation can be simulated. Complete a feasibility assessment addressing all engineering and integration issues related to the development of the objective system fully integrated in a UAS capable of detecting, recognizing, classifying, identifying and providing targeting data to lethality systems. Conduct a sensitivity analysis of the system capabilities against the payload SWAP restrictions to inform decisions on matching payloads to specific UAS platforms and missions. .PHASE III: Develop, integrate and demonstrate a payload operating in real time while on-flight in a number of different environmental conditions and providing functionality at tactically relevant ranges to a TRL 7. Demonstrate the ability to quickly adjust the target set and DRCI taxonomy as selected by the operator. Demonstrate a single operator interface to command-and-control the payload. Demonstrate the potential to use in military and homeland defense missions and environments. .1: John P. Abizaid and Rosa Brooks, Recommendations and Report of the Task Force on US Drone Policy (Washington, DC: The Stimson Center, 2014)..6:   S. Samarasinghe, Neural Networks for Applied Sciences and Engineering: From Fundamentals to Complex Pattern Recognition, Boca Raton, FL, Auerbach Publications, 2007..9:   Robert O. Work and Shawn Brimley, 20YY: Preparing for War in the Robotic Age (Washington DC: Center for a New American Security, January 2014), 7..KEYWORDS: Learning Algorithms,  Artificial Neural Networks (ANNs),  Automatic Target Recognition (ATR),  Target Detection,  Target Classification,  Target Identification,  Unmanned Air System (UAS),  Targeting ",US Army is developing an automatic target recognition unmanned aerial system
52,"Experts and politicians in China are worried that a rush to integrate artificial intelligence into weapons and military equipment could accidentally lead to war between nations..According to a new report published by US national security think tank Center for a New American Security (CNAS), Chinese officials increasingly see an “arms race” dynamic in AI as a threat to global peace. As countries scramble to reap the benefits of artificial intelligence in various domains, including the military, the fear is that international norms shaping how countries communicate will become outdated, leading to confusion and potential conflict. .“The specific scenario described to me [by one anonymous Chinese official] is unintentional escalation related to the use of a drone,” Gregory C. Allen, an adjunct senior fellow at CNAS and author of the new report, tells The Verge. .As Allen explains, the operation of drones both large and small has become increasingly automated in recent years. In the US, drones are capable of basic autopilot, performing simple tasks like flying in a circle around a target. But China is being “more aggressive about introducing greater levels of autonomy closer to lethal use of force,” he says. One example is the Blowfish A2 drone, which China exports internationally and which, says Allen, is advertised as being capable of “full autonomy all the way up to targeted strikes.” .Because drones are controlled remotely, militaries tend to be more cavalier about their use. With no risk of human casualties, they’re more willing to shoot them down, but also deploy them into contested airspaces in the first place. This attitude can also be seen in cyberwarfare, where countries will intrude in ways they wouldn’t necessarily risk if humans were involved. .“The point made to me was that it’s not clear how either side will interpret certain behaviors [involving autonomous equipment],” says Allen. “The side sending out an autonomous drone will think it’s not a big deal because there’s no casualty risk, while the other side could shoot it down for the same reason. But there’s no agreed framework on what message is being sent by either sides’ behavior.” .The risks in such a scenario become greater when factoring in advanced autonomy. If a drone or robot fires a warning shot at enemy troops, for example, how will that action be interpreted? Will the troops understand it as an automated response, or will they think it’s the decision of a human commander? How would they know in either case?.In essence, says Allen, countries around the world have yet to define “the norms of armed conflict” for autonomous systems. And the longer that continues, the greater the risk for “unintentional escalation.” .The rest of the CNAS report, titled “Understanding China’s AI Strategy: Clues to Chinese Strategic Thinking on Artificial Intelligence and National Security,” notes a number of other high-level concerns and attitudes in China’s government-led AI strategy. .Chinese officials recognize, for example, that it and America are the only two viable AI superpowers. Both countries have the talent, the funding, and the bustling tech sectors needed to push this technology further, though each nation also has its own particular strengths and weaknesses. China has access to more data, for example, and has the potential to leapfrog Western technology. (Many Chinese citizens went from having no phone to a mobile phone, without getting a landline in between, for example). America, meanwhile, has a significant lead in the development of chip technology — a vital component in processing the huge datasets that power AI applications..CNAS’s report notes that China is particularly keen to close this important gap. Chinese firms like Baidu, Alibaba, and Huawei have established new projects to develop AI accelerator hardware; government money is pouring into these initiatives; and the industry is trying other methods to get a hold of foreign expertise. These include the recent proposed acquisition of US chip designer Qualcomm by Singapore firm Broadcom, which was blocked by President Trump on national security grounds. .While a certain amount of competition between China and the US is to be expected, Allen says cooperation is also needed — especially when it comes to these military questions. .He notes that while Chinese officials he spoke to had a good grasp of contemporary US thinking on issues like autonomous warfare, American officials tend to be less well-briefed about their Chinese counterparts, partly because many Chinese policy documents are never translated into English. Without properly understanding different nations’ strategies in these domains, says Allen, the chances of misunderstanding and conflict increase. .“There are definitely pockets of real expertise on this issue [in the US] but there’s not the widespread comprehension there needs to be,” he says. ",China fears that AI arms race can cause war
53,"In 2011, Taylor University, a small liberal arts college in Upland, Indiana, began to look for a new way to maximize recruitment. Specifically, they needed to sell students on applying and enrolling, in part to help with tuition revenue goals. That led to a contract with software giant Salesforce, which builds automated systems designed to boost student recruitment. The school now feeds data on prospective students–from hometown and household income to intended areas of study and other data points–into Salesforce’s Education Cloud, which helps admissions officers zero in on the type of applicants they feel are most likely to enroll..“If we find a population of student in northwest Chicago that looks like the ideal student for us, maybe there is another population center in Montana that looks just like that population,” says Nathan Baker, Taylor University’s director of recruitment & analytics..“We’re also tracking the student’s engagement with us,” he says. “So, just because a student population looks ideal, if the student is not engaged with us at all during the process, we have to take that into account.”.Algorithms aren’t just helping to orchestrate our digital experiences but increasingly entering sectors that were historically the province of humans—hiring, lending, and flagging suspicious individuals at national borders. Now a growing number of companies, including Salesforce, are selling or building AI-backed systems that schools can use to track potential and current students, much in the way companies keep track of customers. Increasingly, the software is helping admissions officers decide who gets in..The tech companies behind admissions software say algorithms can improve the success of school recruitment efforts and cut costs. At Taylor University, which Salesforce touts as a “success story,” the admissions department says it saw improvements in recruitment and revenues after adopting the Education Cloud: In Fall 2015, the school welcomed its largest ever incoming freshman class. Taylor now uses the software to predict future student outcomes and make decisions about distributing financial aid and scholarships..But the software isn’t just about streamlining the daunting task of pouring over thousands of applications, says Salesforce. AI is also being pitched as a way to make the admissions system more equitable, by helping schools reduce unseen human biases that can impact admissions decisions..“When you’ve got a tool that can help make [bias] explicit, you can really see factors that are going into a decision or recommendation,” says Kathy Baxter, architect of ethical practice at Salesforce. “It makes it clearer to see if decisions are being made based purely on this one factor or this other factor.” (Salesforce says it now has more than 4,000 customers using its Education Cloud software, but declined to disclose which schools are using it for admissions specifically.).A dashboard from Salesforce’s recruiting software [Image: Salesforce]Typically, a prospective student’s application and supporting materials are read by one or more admissions officers. Grade point average, test scores, personal qualities like leadership abilities and character, and other criteria are considered against the university’s demographic concerns, including diversity goals and students who require financial aid..On a basic level, AI-backed software could immediately score an applicant against a set of factors found to be a signifier of success based on past applicants, explains Brian Knotts, chief architect at Ellucian, which builds software for higher education..“An example could be the location of the student as compared to the location of the school,” he says. Admission officers would use this data-driven assessment to augment their decision-making process. “As each class graduates, the algorithms are periodically retrained, and future students only benefit from smarter and smarter admissions decisions,” says Knotts..To reduce individual bias, committees look at the applicants and usually decide by a vote or some other type of consensus. But unfairness can still creep into the process. For instance, as recent research into recruitment practices has shown, universities tend to market directly to desirable candidates and pay more visits to students in affluent areas, especially to wealthy, predominately white students at out-of-state high schools, since these students are likely to yield more tuition revenue..Kira Talent, a Canadian startup that sells a cloud-based admissions assessment platform to over 300 schools, identifies nine types of common human bias in university admissions, including groupthink and racial and gender biases. But some of the most harmful biases impacting admissions are unrelated to race, religion, gender, or other stereotypes, according to a company presentation. Instead, biases grow “situationally” and often unexpectedly from how admissions officers review applicants, including an inconsistent number of reviewers and reviewer exhaustion..Other kinds of more subtle biases can creep in through admissions policies. As part of an ongoing lawsuit brought by Asian-American students against Harvard University, the school’s Admissions Office revealed its use of an interviewer handbook that emphasizes applicant personality in a passage entitled “The Search for Distinguishing Excellences.” Distinguishing excellence, according to the handbook, includes “outstanding capacity for leadership,” which could disadvantage hardworking introverts, and “unusually appealing personal qualities,” which sounds like fertile ground for human bias to enter into the mind of an interviewer..Baxter argues that software can help identify the biases that creep in to human-based admissions processes. Algorithms can do systematic analyses of the data points that admissions officers consider in each application. For instance, the Salesforce software used by Taylor University includes Protected Fields, a feature that displays pop-up alerts in order to identify biases that may emerge in the data, like last names that might reveal an applicant’s race..“If an admissions officer wants to avoid race or gender bias in the model they’re building to predict which applicants should be admitted, but they include zip code in their data set not knowing that it correlates to race, they will be alerted to the racial bias this can cause,” says Baxter..Molly McCracken, marketing manager at Kira Talent, says that AI could also help eliminate situations where an admissions officer might know an applicant, automatically flagging a personal connection that exists between two parties. From there, a human review of the relationship could be undertaken by admissions officials..McCracken proposes a hypothetical scenario where an admissions official who had been on the debate club in high school might identify with an applicant who was on the debate club as well. “But, if you train an algorithm to say the debate club is equal to x is equal to y, then you don’t have to bring that kind of [personal] background and experience [into the process],” McCracken says. Still, she cautions, “you also need that human perspective to be able to qualify these experiences.”.Kira’s set of bias-fighting tools are designed to reduce the impact one person’s bias can have during human review. A Reviewer Analytics feature aims to ensure that admissions officers are rating applicants consistently and fairly: by calculating the average rating for each reviewer across all applications, colleges can identify outliers who are scoring applicants with too much or too little rigor. To counter groupthink, which can give more weight to the loudest voice in the room, the software combines feedback from multiple reviewers without each reviewer seeing their colleagues’ ratings and notes and produces an overall average score for each applicant’s responses..Ryan Rucker, a project manager at Kira Talent, says the company is currently in the research and development phases of adding AI-backed software to the company’s admissions tools. Eventually, that could also help schools conduct deeper background checks into an applicant’s personal history. That could, for instance, help prevent the kind of cheating seen in the recent university admissions scandal, where wealthy applicants’ parents paid for preferential admission by having the applicants pose as athletes..“As we move further into this place where we’re using more machine learning and AI-enabled solutions, we’re going to get better at checking for certain things, like whether someone is actually a member of the crew team in high school,” says Rucker, referring to the recent university admissions scandal. “This information is typically publicly available on a website, which we could crawl for that,” he adds. “Of course, if we want to get into the area of data privacy, that’s a totally different topic.”.However, as artificial intelligence experts have cautioned, systems that aim to reduce bias through AI could be complicated by AI itself. Automated systems will only be as good as the underlying data, says Rashida Richardson, director of policy research at AI Now Institute, a think tank at New York University that studies machine bias and algorithmic accountability. And since admissions are embedded with many subjective judgments, Richardson believes attempting to automate it can result in “embedding and possibly concealing these subjective decisions,” quietly replicating the problems that these systems purport to address..Automation has raised similar alarms in sensitive domains like policing, criminal justice, and child welfare. If future admissions decisions are based on past decision data, Richardson warns of creating an unintended feedback loop, limiting a school’s demographic makeup, harming disadvantaged students, and putting a school out of sync with changing demographics..“There is a societal automation bias—people will assume that because it is a technical system it is fairer than the status quo,” says Richardson. “Research on the integration of matching algorithms in medical schools and how these systems helped facilitate and distort discriminatory practices and bias is proof that these concerns are real and highly likely.”.Richardson says that companies like Ellucian, Salesforce, and Kira Talent fail to acknowledge on their websites that there are significant educational equity issues built into the admissions process..“It is unclear to me how you standardize and automate a process that is not only based on subjective judgments but also requires a deep understanding of context,” she says..Richardson notes, for instance, that many high schools participate in grade inflation and heterodox grading systems. While admissions officers may be aware of this and account for it in the admissions process, an AI system may not..“Similarly, an AI system may not appreciate the additional challenges a lower income or first-generation students face, compared to a more affluent legacy,” Richardson cautions. “It’s possible an AI system or automated process may exacerbate existing biases.”.Software could also lead to a further class divide in the admissions process, Richardson worries. Just as people try to game the existing human-driven admissions process, applicants might try to do the same if the factors used by automated systems are known. Higher resource groups, like the wealthy and connected, might find ways to enhance applications for the most favorable outcome. Richardson says that AI Now has already seen this happen with more basic algorithms that assign students to K-12 schools..“Families that have time to make complicated spreadsheets to optimize their choices will likely have a better chance at matching with their top schools,” says Richardson, “whereas less resourced families may not have the time or enough information to do the same.”.Baxter says that machine bias safeguards are built into Salesforce’s Einstein AI, an AI-based technology that underpins the company’s software. A feature called Predictive Factors lets admissions users give feedback to the AI model, allowing them to identify if a biasing factor was included in the results. And Education Cloud’s Model Metrics feature helps gauge the performance of AI models, says Baxter, allowing users to better understand any previously unforeseen outcomes that could be harmful. “Our models are regularly being updated and learning from that ongoing use, so it’s not a static thing,” Baxter says..To ensure that data isn’t biased from the get-go, Ellucian is selecting appropriate features, data types, and algorithms, and plans to vet them through an advisory board populated with technologists, data scientists, and other AI experts. “That’s part of the reason why we’ve taken a long time,” says Knotts. “We want to make sure that we spend as much time on this as we have on the technology.”.AI Now’s Richardson says any advisory board without domain expertise, like educators, administrators, and education scholars and advocates—specifically, ones with experience in discrimination, educational equity, and segregation—would be insufficient..Richardson is also concerned that companies offering automated admissions solutions might not understand just how subjective the process is. For instance, admissions officers already weigh elite boarding school students higher than a valedictorian from an under-sourced, segregated school, creating a socioeconomic bias. She doubts AI could resolve this issue, to say nothing of approaching it with enough sophistication to “understand both the sensitivity that should be given to socioeconomic and racial analysis.”.Predicting student success is also a delicate challenge. Some students are late bloomers. Others excel in high school, but for one reason or another fall short in and after college. So many different variables—seen and unseen, measurable and not—factor into a student’s performance..“It falls within this trend within tech to think everything is predictable and that there is a predetermined notion on success,” she says. “There are many variables that have been traditionally used to predict student success that have been shown to be inaccurate or embedded with bias (e.g. standardized test scores). At the same time, there is a range of unforeseeable issues that can affect individual or collective student success.”.These traditional metrics of success can also be biased against marginalized students and do not take into account institutional biases, like high school counselors that ignore or fail to serve students of color..At some schools, even some automation in admissions is a no-no. “Unlike many other colleges and universities, we do not use AI or other automated systems in our decisioning,” says Greer Davis, associate director of marketing and communications at the University of Wisconsin-Madison. “All applications and all supporting material are read by multiple counselors using a holistic approach, and we do not have or use any minimums, ranges, or formulas.”.UC-Berkeley is also resisting machine-driven admissions. Janet Gilmore, UC-Berkeley’s senior director of communications & public affairs, says the university has no automated processes, and that the applicant’s complete file is evaluated by trained readers. Its admissions process website states that “race, ethnicity, gender, and religion are excluded from the criteria,” and that the school uses a holistic review process that looks at both academic (course difficulty, standardized test scores) and nonacademic factors. In the latter case, it could be personal qualities such as character and intellectual independence, or activities like volunteer service and leadership in community organizations..Richardson does, however, see a place for automation in reducing the workload in processing admissions. Software could, for instance, help eliminate duplicate documents or flag missing documents from applicants..The newest admissions and recruitment software aims to do far more than flag incomplete files. But Knotts, of Ellucian, insists the aim is not to fully automate the decisions themselves, at least not yet. “We don’t want to make computer decisions with these applicants,” he says. “And I don’t think the education sector wants that either.”.DJ Pangburn is a writer and editor with bylines at Vice, Motherboard, Creators, Dazed & Confused and The Quietus. Hes also a pataphysician, psychogeographer and filmmaker.",Salesforce has a contract with a few schools to predict student outcomes and make decisions on financial aid.
55,"The smart classroom behaviour management system, or smart eye, is the latest highly-intrusive surveillance equipment to be rolled out in China, where leaders have rushed to use the latest technology to monitor the wider population..The system works by identifying different facial expressions from the students, and that information is then fed into a computer which assesses if they are enjoying lessons or if their minds are wandering..Many Chinese have grown accustomed to their privacy being infringed by the government and corporations, but people have been expressing horror about the new system on Chinese social media..If I was still at school, I would not be able to concentrate on anything but that watching eye! said one comment on Sina Weibo, Chinas version of Twitter..The technology has also been used to test interest levels at university lectures, to gain entry to university dormitories and workplaces, to withdraw cash from ATM machines and even to buy a KFC..Human rights groups fear that authorities are using the huge amounts of information it collates on individuals, or big data, along with surveillance to keep track of citizens and crack down on dissent..But Chinese leaders say the high-tech sector is a catalyst for future growth and the cornerstone of plans to build a modern, consumer society within a generation.",A school utilizes AI to track students' attention in class
57,Provider Insights | Payer Insights | Healthcare Services & Technology Insights | Behavioral Health Insights | Consumer Health Insights | Public Health Insights,McKinsey states that there is slow adoption on health insurers utilizing AI because of a lack of transparency regarding available data
61,"Health products powered by artificial intelligence, or AI, are streaming into our lives, from virtual doctor apps to wearable sensors and drugstore chatbots..“There’s nothing that I’ve seen in my 30-plus years studying medicine that could be as impactful and transformative” as AI, said Dr. Eric Topol, a cardiologist and executive vice president of Scripps Research in La Jolla, Calif. AI can help doctors interpret MRIs of the heart, CT scans of the head and photographs of the back of the eye, and could potentially take over many mundane medical chores, freeing doctors to spend more time talking to patients, Topol said..Even the Food and Drug Administration ― which has approved more than 40 AI products in the past five years ― says “the potential of digital health is nothing short of revolutionary.”.Yet many health industry experts fear AI-based products won’t be able to match the hype. Many doctors and consumer advocates fear that the tech industry, which lives by the mantra “fail fast and fix it later,” is putting patients at risk ― and that regulators aren’t doing enough to keep consumers safe..Early experiments in AI provide a reason for caution, said Mildred Cho, a professor of pediatrics at Stanford’s Center for Biomedical Ethics..Systems developed in one hospital often flop when deployed in a different facility, Cho said. Software used in the care of millions of Americans has been shown to discriminate against minorities. And AI systems sometimes learn to make predictions based on factors that have less to do with disease than the brand of MRI machine used, the time a blood test is taken or whether a patient was visited by a chaplain. In one case, AI software incorrectly concluded that people with pneumonia were less likely to die if they had asthma ― an error that could have led doctors to deprive asthma patients of the extra care they need..“It’s only a matter of time before something like this leads to a serious health problem,” said Dr. Steven Nissen, chairman of cardiology at the Cleveland Clinic..Medical AI, which pulled in $1.6 billion in venture capital funding in the third quarter alone, is “nearly at the peak of inflated expectations,” concluded a July report from the research company Gartner. “As the reality gets tested, there will likely be a rough slide into the trough of disillusionment.”.That reality check could come in the form of disappointing results when AI products are ushered into the real world. Even Topol, the author of “Deep Medicine: How Artificial Intelligence Can Make Healthcare Human Again,” acknowledges that many AI products are little more than hot air. “It’s a mixed bag,” he said..Experts such as Dr. Bob Kocher, a partner at the venture capital firm Venrock, are blunter. “Most AI products have little evidence to support them,” Kocher said. Some risks won’t become apparent until an AI system has been used by large numbers of patients. “We’re going to keep discovering a whole bunch of risks and unintended consequences of using AI on medical data,” Kocher said..None of the AI products sold in the U.S. have been tested in randomized clinical trials, the strongest source of medical evidence, Topol said. The first and only randomized trial of an AI system ― which found that colonoscopy with computer-aided diagnosis found more small polyps than standard colonoscopy ― was published online in October..Few tech startups publish their research in peer-reviewed journals, which allow other scientists to scrutinize their work, according to a January article in the European Journal of Clinical Investigation. Such “stealth research” ― described only in press releases or promotional events ― often overstates a company’s accomplishments..And although software developers may boast about the accuracy of their AI devices, experts note that AI models are mostly tested on computers, not in hospitals or other medical facilities. Using unproven software “may make patients into unwitting guinea pigs,” said Dr. Ron Li, medical informatics director for AI clinical integration at Stanford Health Care..AI systems that learn to recognize patterns in data are often described as “black boxes” because even their developers don’t know how they have reached their conclusions. Given that AI is so new ― and many of its risks unknown ― the field needs careful oversight, said Pilar Ossorio, a professor of law and bioethics at the University of Wisconsin-Madison..Legislation passed by Congress in 2016 ― and championed by the tech industry ― exempts many types of medical software from federal review, including certain fitness apps, electronic health records and tools that help doctors make medical decisions..There’s been little research on whether the 320,000 medical apps now in use actually improve health, according to a report on AI published Dec. 17 by the National Academy of Medicine..If failing fast means a whole bunch of people will die, I don’t think we want to fail fast. Nobody is going to be happy, including investors, if people die or are severely hurt..“Almost none of the [AI] stuff marketed to patients really works,” said Dr. Ezekiel Emanuel, professor of medical ethics and health policy in the Perelman School of Medicine at the University of Pennsylvania..The FDA has long focused its attention on devices that pose the greatest threat to patients. And consumer advocates acknowledge that some devices ― such as ones that help people count their daily steps ― need less scrutiny than ones that diagnose or treat disease..Some software developers don’t bother to apply for FDA clearance or authorization, even when legally required, according to a 2018 study in Annals of Internal Medicine..Industry analysts say that AI developers have little interest in conducting expensive and time-consuming trials. “It’s not the main concern of these firms to submit themselves to rigorous evaluation that would be published in a peer-reviewed journal,” said Joachim Roski, a principal at Booz Allen Hamilton, a technology consulting firm, and co-author of the National Academy’s report. “That’s not how the U.S. economy works.”.But Oren Etzioni, chief executive officer at the Allen Institute for AI in Seattle, said AI developers have a financial incentive to make sure their medical products are safe..“If failing fast means a whole bunch of people will die, I don’t think we want to fail fast,” Etzioni said. “Nobody is going to be happy, including investors, if people die or are severely hurt.”.The FDA has come under fire in recent years for allowing the sale of dangerous medical devices, which have been linked by the International Consortium of Investigative Journalists to 80,000 deaths and 1.7 million injuries over the past decade..Many of these devices were cleared for use through a controversial process called the 510(k) pathway, which allows companies to market “moderate-risk” products with no clinical testing as long as they’re deemed similar to existing devices..In 2011, a committee of the National Academy of Medicine concluded the 510(k) process is so fundamentally flawed that the FDA should throw it out and start over..The FDA, headquartered just outside Washington, D.C., has long focused its attention on devices that pose the greatest threat to patients.(Al Drago/CQ Roll Call via AP Images) .Of the 14 AI products authorized by the FDA in 2017 and 2018, 11 were cleared through the 510(k) process, according to a November article in JAMA. None of these appear to have had new clinical testing, the study said. The FDA cleared an AI device designed to help diagnose liver and lung cancer in 2018 based on its similarity to imaging software approved 20 years earlier. That software had itself been cleared because it was deemed “substantially equivalent” to products marketed before 1976..AI products cleared by the FDA today are largely “locked,” so that their calculations and results will not change after they enter the market, said Bakul Patel, director for digital health at the FDA’s Center for Devices and Radiological Health. The FDA has not yet authorized “unlocked” AI devices, whose results could vary from month to month in ways that developers cannot predict..To deal with the flood of AI products, the FDA is testing a radically different approach to digital device regulation, focusing on evaluating companies, not products..The FDA’s pilot “pre-certification” program, launched in 2017, is designed to “reduce the time and cost of market entry for software developers,” imposing the “least burdensome” system possible. FDA officials say they want to keep pace with AI software developers, who update their products much more frequently than makers of traditional devices, such as X-ray machines..Scott Gottlieb said in 2017 while he was FDA commissioner that government regulators need to make sure its approach to innovative products “is efficient and that it fosters, not impedes, innovation.”.Under the plan, the FDA would pre-certify companies that “demonstrate a culture of quality and organizational excellence,” which would allow them to provide less upfront data about devices..Pre-certified companies could then release devices with a “streamlined” review ― or no FDA review at all. Once products are on the market, companies will be responsible for monitoring their own products’ safety and reporting back to the FDA. Nine companies have been selected for the pilot: Apple, FitBit, Samsung, Johnson & Johnson, Pear Therapeutics, Phosphorus, Roche, Tidepool and Verily Life Sciences..High-risk products, such as software used in pacemakers, will still get a comprehensive FDA evaluation. “We definitely don’t want patients to be hurt,” said Patel, who noted that devices cleared through pre-certification can be recalled if needed. “There are a lot of guardrails still in place.”.But research shows that even low- and moderate-risk devices have been recalled due to serious risks to patients, said Diana Zuckerman, president of the National Center for Health Research. “People could be harmed because something wasn’t required to be proven accurate or safe before it is widely used.”.In a series of letters to the FDA, the American Medical Association and others have questioned the wisdom of allowing companies to monitor their own performance and product safety..In an October letter to the FDA, Sens. Elizabeth Warren (D-Mass.), Tina Smith (D-Minn.) and Patty Murray (D-Wash.) questioned the agency’s ability to ensure company safety reports are “accurate, timely and based on all available information.”.Scott Gottlieb said in 2017 while he was FDA commissioner that government regulators need to make sure its approach to innovative products “is efficient and that it fosters, not impedes, innovation.”(Francis Ying/KHN) .An AI-powered screening tool for diabetic eye disease was studied in 900 patients at 10 primary care offices before being approved in 2018. The manufacturer, IDx Technologies, worked with the FDA for eight years to get the product right, said Dr. Michael Abramoff, the company’s founder and executive chairman..The test, sold as IDx-DR, screens patients for diabetic retinopathy, a leading cause of blindness, and refers high-risk patients to eye specialists, who make a definitive diagnosis..IDx-DR is the first “autonomous” AI product ― one that can make a screening decision without a doctor. The company is now installing it in primary care clinics and grocery stores, where it can be operated by employees with a high school diploma. Abramoff’s company has taken the unusual step of buying liability insurance to cover any patient injuries..A Canadian company, for example, developed AI software to predict a person’s risk of Alzheimer’s based on their speech. Predictions were more accurate for some patients than others. “Difficulty finding the right word may be due to unfamiliarity with English, rather than to cognitive impairment,” said co-author Frank Rudzicz, an associate professor of computer science at the University of Toronto..Doctors at New York’s Mount Sinai Hospital hoped AI could help them use chest X-rays to predict which patients were at high risk of pneumonia. Although the system made accurate predictions from X-rays shot at Mount Sinai, the technology flopped when tested on images taken at other hospitals. Eventually, researchers realized the computer had merely learned to tell the difference between that hospital’s portable chest X-rays ― taken at a patient’s bedside ― with those taken in the radiology department. Doctors tend to use portable chest X-rays for patients too sick to leave their room, so it’s not surprising that these patients had a greater risk of lung infection..DeepMind, a company owned by Google, has created an AI-based mobile app that can predict which hospitalized patients will develop acute kidney failure up to 48 hours in advance. A blog post on the DeepMind website described the system, used at a London hospital, as a “game changer.” But the AI system also produced two false alarms for every correct result, according to a July study in Nature. That may explain why patients’ kidney function didn’t improve, said Dr. Saurabh Jha, associate professor of radiology at the Hospital of the University of Pennsylvania. Any benefit from early detection of serious kidney problems may have been diluted by a high rate of “overdiagnosis,” in which the AI system flagged borderline kidney issues that didn’t need treatment, Jha said. Google had no comment in response to Jha’s conclusions..False positives can harm patients by prompting doctors to order unnecessary tests or withhold recommended treatments, Jha said. For example, a doctor worried about a patient’s kidneys might stop prescribing ibuprofen ― a generally safe pain reliever that poses a small risk to kidney function ― in favor of an opioid, which carries a serious risk of addiction..As these studies show, software with impressive results in a computer lab can founder when tested in real time, Stanford’s Cho said. That’s because diseases are more complex ― and the health care system far more dysfunctional ― than many computer scientists anticipate..Many AI developers cull electronic health records because they hold huge amounts of detailed data, Cho said. But those developers often aren’t aware that they’re building atop a deeply broken system. Electronic health records were developed for billing, not patient care, and are filled with mistakes or missing data..In view of the risks involved, doctors need to step in to protect their patients’ interests, said Dr. Vikas Saini, a cardiologist and president of the nonprofit Lown Institute, which advocates for wider access to health care..Health products powered by artificial intelligence, or AI, are streaming into our lives, from virtual doctor apps to wearable sensors and drugstore chatbots..“There’s nothing that I’ve seen in my 30-plus years studying medicine that could be as impactful and transformative” as AI, said Dr. Eric Topol, a cardiologist and executive vice president of Scripps Research in La Jolla, Calif. AI can help doctors interpret MRIs of the heart, CT scans of the head and photographs of the back of the eye, and could potentially take over many mundane medical chores, freeing doctors to spend more time talking to patients, Topol said..Even the Food and Drug Administration ― which has approved more than 40 AI products in the past five years ― says “the potential of digital health is nothing short of revolutionary.”.Yet many health industry experts fear AI-based products won’t be able to match the hype. Many doctors and consumer advocates fear that the tech industry, which lives by the mantra “fail fast and fix it later,” is putting patients at risk ― and that regulators aren’t doing enough to keep consumers safe..Early experiments in AI provide a reason for caution, said Mildred Cho, a professor of pediatrics at Stanford’s Center for Biomedical Ethics..Systems developed in one hospital often flop when deployed in a different facility, Cho said. Software used in the care of millions of Americans has been shown to discriminate against minorities. And AI systems sometimes learn to make predictions based on factors that have less to do with disease than the brand of MRI machine used, the time a blood test is taken or whether a patient was visited by a chaplain. In one case, AI software incorrectly concluded that people with pneumonia were less likely to die if they had asthma ― an error that could have led doctors to deprive asthma patients of the extra care they need..“It’s only a matter of time before something like this leads to a serious health problem,” said Dr. Steven Nissen, chairman of cardiology at the Cleveland Clinic..Medical AI, which pulled in $1.6 billion in venture capital funding in the third quarter alone, is “nearly at the peak of inflated expectations,” concluded a July report from the research company Gartner. “As the reality gets tested, there will likely be a rough slide into the trough of disillusionment.”.That reality check could come in the form of disappointing results when AI products are ushered into the real world. Even Topol, the author of “Deep Medicine: How Artificial Intelligence Can Make Healthcare Human Again,” acknowledges that many AI products are little more than hot air. “It’s a mixed bag,” he said..Experts such as Dr. Bob Kocher, a partner at the venture capital firm Venrock, are blunter. “Most AI products have little evidence to support them,” Kocher said. Some risks won’t become apparent until an AI system has been used by large numbers of patients. “We’re going to keep discovering a whole bunch of risks and unintended consequences of using AI on medical data,” Kocher said..None of the AI products sold in the U.S. have been tested in randomized clinical trials, the strongest source of medical evidence, Topol said. The first and only randomized trial of an AI system ― which found that colonoscopy with computer-aided diagnosis found more small polyps than standard colonoscopy ― was published online in October..Few tech startups publish their research in peer-reviewed journals, which allow other scientists to scrutinize their work, according to a January article in the European Journal of Clinical Investigation. Such “stealth research” ― described only in press releases or promotional events ― often overstates a company’s accomplishments..And although software developers may boast about the accuracy of their AI devices, experts note that AI models are mostly tested on computers, not in hospitals or other medical facilities. Using unproven software “may make patients into unwitting guinea pigs,” said Dr. Ron Li, medical informatics director for AI clinical integration at Stanford Health Care..AI systems that learn to recognize patterns in data are often described as “black boxes” because even their developers don’t know how they have reached their conclusions. Given that AI is so new ― and many of its risks unknown ― the field needs careful oversight, said Pilar Ossorio, a professor of law and bioethics at the University of Wisconsin-Madison..Legislation passed by Congress in 2016 ― and championed by the tech industry ― exempts many types of medical software from federal review, including certain fitness apps, electronic health records and tools that help doctors make medical decisions..There’s been little research on whether the 320,000 medical apps now in use actually improve health, according to a report on AI published Dec. 17 by the National Academy of Medicine..“Almost none of the [AI] stuff marketed to patients really works,” said Dr. Ezekiel Emanuel, professor of medical ethics and health policy in the Perelman School of Medicine at the University of Pennsylvania..The FDA has long focused its attention on devices that pose the greatest threat to patients. And consumer advocates acknowledge that some devices ― such as ones that help people count their daily steps ― need less scrutiny than ones that diagnose or treat disease..Some software developers don’t bother to apply for FDA clearance or authorization, even when legally required, according to a 2018 study in Annals of Internal Medicine..Industry analysts say that AI developers have little interest in conducting expensive and time-consuming trials. “It’s not the main concern of these firms to submit themselves to rigorous evaluation that would be published in a peer-reviewed journal,” said Joachim Roski, a principal at Booz Allen Hamilton, a technology consulting firm, and co-author of the National Academy’s report. “That’s not how the U.S. economy works.”.But Oren Etzioni, chief executive officer at the Allen Institute for AI in Seattle, said AI developers have a financial incentive to make sure their medical products are safe..“If failing fast means a whole bunch of people will die, I don’t think we want to fail fast,” Etzioni said. “Nobody is going to be happy, including investors, if people die or are severely hurt.”.The FDA has come under fire in recent years for allowing the sale of dangerous medical devices, which have been linked by the International Consortium of Investigative Journalists to 80,000 deaths and 1.7 million injuries over the past decade..Many of these devices were cleared for use through a controversial process called the 510(k) pathway, which allows companies to market “moderate-risk” products with no clinical testing as long as they’re deemed similar to existing devices..In 2011, a committee of the National Academy of Medicine concluded the 510(k) process is so fundamentally flawed that the FDA should throw it out and start over..Of the 14 AI products authorized by the FDA in 2017 and 2018, 11 were cleared through the 510(k) process, according to a November article in JAMA. None of these appear to have had new clinical testing, the study said. The FDA cleared an AI device designed to help diagnose liver and lung cancer in 2018 based on its similarity to imaging software approved 20 years earlier. That software had itself been cleared because it was deemed “substantially equivalent” to products marketed before 1976..AI products cleared by the FDA today are largely “locked,” so that their calculations and results will not change after they enter the market, said Bakul Patel, director for digital health at the FDA’s Center for Devices and Radiological Health. The FDA has not yet authorized “unlocked” AI devices, whose results could vary from month to month in ways that developers cannot predict..To deal with the flood of AI products, the FDA is testing a radically different approach to digital device regulation, focusing on evaluating companies, not products..The FDA’s pilot “pre-certification” program, launched in 2017, is designed to “reduce the time and cost of market entry for software developers,” imposing the “least burdensome” system possible. FDA officials say they want to keep pace with AI software developers, who update their products much more frequently than makers of traditional devices, such as X-ray machines..Scott Gottlieb said in 2017 while he was FDA commissioner that government regulators need to make sure its approach to innovative products “is efficient and that it fosters, not impedes, innovation.”.Under the plan, the FDA would pre-certify companies that “demonstrate a culture of quality and organizational excellence,” which would allow them to provide less upfront data about devices..Pre-certified companies could then release devices with a “streamlined” review ― or no FDA review at all. Once products are on the market, companies will be responsible for monitoring their own products’ safety and reporting back to the FDA. Nine companies have been selected for the pilot: Apple, FitBit, Samsung, Johnson & Johnson, Pear Therapeutics, Phosphorus, Roche, Tidepool and Verily Life Sciences..High-risk products, such as software used in pacemakers, will still get a comprehensive FDA evaluation. “We definitely don’t want patients to be hurt,” said Patel, who noted that devices cleared through pre-certification can be recalled if needed. “There are a lot of guardrails still in place.”.But research shows that even low- and moderate-risk devices have been recalled due to serious risks to patients, said Diana Zuckerman, president of the National Center for Health Research. “People could be harmed because something wasn’t required to be proven accurate or safe before it is widely used.”.In a series of letters to the FDA, the American Medical Association and others have questioned the wisdom of allowing companies to monitor their own performance and product safety..In an October letter to the FDA, Sens. Elizabeth Warren (D-Mass.), Tina Smith (D-Minn.) and Patty Murray (D-Wash.) questioned the agency’s ability to ensure company safety reports are “accurate, timely and based on all available information.”.An AI-powered screening tool for diabetic eye disease was studied in 900 patients at 10 primary care offices before being approved in 2018. The manufacturer, IDx Technologies, worked with the FDA for eight years to get the product right, said Dr. Michael Abramoff, the company’s founder and executive chairman..The test, sold as IDx-DR, screens patients for diabetic retinopathy, a leading cause of blindness, and refers high-risk patients to eye specialists, who make a definitive diagnosis..IDx-DR is the first “autonomous” AI product ― one that can make a screening decision without a doctor. The company is now installing it in primary care clinics and grocery stores, where it can be operated by employees with a high school diploma. Abramoff’s company has taken the unusual step of buying liability insurance to cover any patient injuries..A Canadian company, for example, developed AI software to predict a person’s risk of Alzheimer’s based on their speech. Predictions were more accurate for some patients than others. “Difficulty finding the right word may be due to unfamiliarity with English, rather than to cognitive impairment,” said co-author Frank Rudzicz, an associate professor of computer science at the University of Toronto..Doctors at New York’s Mount Sinai Hospital hoped AI could help them use chest X-rays to predict which patients were at high risk of pneumonia. Although the system made accurate predictions from X-rays shot at Mount Sinai, the technology flopped when tested on images taken at other hospitals. Eventually, researchers realized the computer had merely learned to tell the difference between that hospital’s portable chest X-rays ― taken at a patient’s bedside ― with those taken in the radiology department. Doctors tend to use portable chest X-rays for patients too sick to leave their room, so it’s not surprising that these patients had a greater risk of lung infection..DeepMind, a company owned by Google, has created an AI-based mobile app that can predict which hospitalized patients will develop acute kidney failure up to 48 hours in advance. A blog post on the DeepMind website described the system, used at a London hospital, as a “game changer.” But the AI system also produced two false alarms for every correct result, according to a July study in Nature. That may explain why patients’ kidney function didn’t improve, said Dr. Saurabh Jha, associate professor of radiology at the Hospital of the University of Pennsylvania. Any benefit from early detection of serious kidney problems may have been diluted by a high rate of “overdiagnosis,” in which the AI system flagged borderline kidney issues that didn’t need treatment, Jha said. Google had no comment in response to Jha’s conclusions..False positives can harm patients by prompting doctors to order unnecessary tests or withhold recommended treatments, Jha said. For example, a doctor worried about a patient’s kidneys might stop prescribing ibuprofen ― a generally safe pain reliever that poses a small risk to kidney function ― in favor of an opioid, which carries a serious risk of addiction..As these studies show, software with impressive results in a computer lab can founder when tested in real time, Stanford’s Cho said. That’s because diseases are more complex ― and the health care system far more dysfunctional ― than many computer scientists anticipate..Many AI developers cull electronic health records because they hold huge amounts of detailed data, Cho said. But those developers often aren’t aware that they’re building atop a deeply broken system. Electronic health records were developed for billing, not patient care, and are filled with mistakes or missing data..In view of the risks involved, doctors need to step in to protect their patients’ interests, said Dr. Vikas Saini, a cardiologist and president of the nonprofit Lown Institute, which advocates for wider access to health care..You must credit us as the original publisher, with a hyperlink to our khn.org site. If possible, please include the original author(s) and “Kaiser Health News” in the byline. Please preserve the hyperlinks in the story..It’s important to note, not everything on khn.org is available for republishing. If a story is labeled “All Rights Reserved,” we cannot grant permission to republish that item..Thank you for your interest in supporting Kaiser Health News (KHN), the nation’s leading nonprofit newsroom focused on health and health policy. We distribute our journalism for free and without advertising through media partners of all sizes and in communities large and small. We appreciate all forms of engagement from our readers and listeners, and welcome your support..KHN is an editorially independent program of KFF (Kaiser Family Foundation). You can support KHN by making a contribution to KFF, a non-profit charitable organization that is not associated with Kaiser Permanente.",Mildred Cho found that software developed in one hospital often fails at other hospitals. It also discriminates against minorities and is dependent on factors such as the brand of MRI machines
63,"London’s Royal Free hospital failed to comply with the Data Protection Act when it handed over personal data of 1.6 million patients to DeepMind, a Google subsidiary, according to the Information Commissioner’s Office..The data transfer was part of the two organisation’s partnership to create the healthcare app Streams, an alert, diagnosis and detection system for acute kidney injury. The ICO’s ruling was largely based on the fact that the app continued to undergo testing after patient data was transferred. Patients, it said, were not adequately informed that their data would be used as part of the test..“Our investigation found a number of shortcomings in the way patient records were shared for this trial,” said Elizabeth Denham, the information commissioner. “Patients would not have reasonably expected their information to have been used in this way, and the Trust could and should have been far more transparent with patients as to what was happening..“We’ve asked the Trust to commit to making changes that will address those shortcomings, and their co-operation is welcome. The Data Protection Act is not a barrier to innovation, but it does need to be considered wherever people’s data is being used.”.The ICO ruled that testing the app with real patient data went beyond Royal Free’s authority, particularly given how broad the scope of the data transfer was. “A patient presenting at accident and emergency within the last five years to receive treatment or a person who engages with radiology services and who has had little or no prior engagement with the Trust would not reasonably expect their data to be accessible to a third party for the testing of a new mobile application, however positive the aims of that application may be,” the office said in its findings..While privacy campaigners were hoping the ruling would touch on the continued use of patient data for the production version of Streams, the ICO was muted on the live use of Streams in a clinical environment, but warned that “concerns regarding the necessity and proportionality of the use of the sensitive data of 1.6 million patients remain”..The Royal Free has been asked to commission a third-party audit of the trial following the ruling, complete a privacy assessment, set out how it will better comply with its duties in future trials and establish a proper legal basis for the DeepMind project..In a statement, the hospital trust said: “We are pleased that the information commissioner … has allowed us to continue using the app which is helping us to get the fastest treatment to our most vulnerable patients – potentially saving lives..“We accept the ICO’s findings and have already made good progress to address the areas where they have concerns. For example, we are now doing much more to keep our patients informed about how their data is used. We would like to reassure patients that their information has been in our control at all times and has never been used for anything other than delivering patient care or ensuring their safety.”.The ruling does not directly criticise DeepMind, a London-based AI company purchased by Google in 2013, since the ICO views the Royal Free as the “data controller” responsible for upholding the data protection act throughout its partnership with Streams, with DeepMind acting as a data processor on behalf of the trust. .In a blogpost, the company said: “We welcome the ICO’s thoughtful resolution of this case, which we hope will guarantee the ongoing safe and legal handling of patient data for Streams..“Although today’s findings are about the Royal Free, we need to reflect on our own actions too. In our determination to achieve quick impact when this work started in 2015, we underestimated the complexity of the NHS and of the rules around patient data, as well as the potential fears about a well-known tech company working in health..“We were almost exclusively focused on building tools that nurses and doctors wanted, and thought of our work as technology for clinicians rather than something that needed to be accountable to and shaped by patients, the public and the NHS as a whole. We got that wrong, and we need to do better.”.The company highlighted a number of changes it had made since the launch of Streams, including a significant increase in transparency, and the creation of a independent health review board..Streams has since been rolled out to other British hospitals, and DeepMind has also branched out into other clinical trials, including a project aimed at using machine-learning techniques to improve diagnosis of diabetic retinopathy, and another aimed at using similar techniques to better prepare radiotherapists for treating head and neck cancers.",The Royal Free London NHS Foundation Trust mishandled 1.6 million patient's data after they supplied it to DeepMind without their consent
64,"Most chatbots are designed by men and tend to replicate gender stereotypes. But as the few women involved in the industry can testify, getting AI to emote requires input from all genders.Ever wanted a friend who is always there for you? Someone infinitely patient? Someone who will perk you up when you’re in the dumps or hear you out when you’re enraged? Well, meet Replika. Only, she isn’t called Replika. She’s called whatever you like; Diana; Daphne; Delectable Doris of the Deep. She isn’t even a “she”, in fact. Gender, voice, appearance: all are up for grabs. The product of a San Francisco-based startup, Replika is one of a growing number of bots using artificial intelligence (AI) to meet our need for companionship. In these lockdown days, with anxiety and loneliness on the rise, millions are turning to such “AI friends” for solace. Replika, which has 7 million users, says it has seen a 35% increase in traffic. As AI developers begin to explore – and exploit – the realm of human emotions, it brings a host of gender-related issues to the fore. Many centre on unconscious bias. The rise of racist robots is already well-documented. Is there a danger our AI pals could emerge to become loutish, sexist pigs? Eugenia Kuyda, Replika’s co-founder and chief executive, is hyper-alive to such a possibility. Given the tech sector’s gender imbalance (women occupy only around one in four jobs in Silicon Valley and 16% of UK tech roles), most AI products are “created by men with a female stereotype in their heads”, she accepts. In contrast, the majority of those who helped create Replika were women, a fact that Kuyda credits with being crucial to the “innately” empathetic nature of its conversational responses. “For AIs that are going to be your friends … the main qualities that will draw in audiences are inherently feminine, [so] it’s really important to have women creating these products,” she says. In addition to curated content, however, most AI companions learn from a combination of existing conversational datasets (film and TV scripts are popular) and user-generated content. Both present risks of gender stereotyping. Lauren Kunze, chief executive of California-based AI developer Pandorabots, says publicly available datasets should only ever be used in conjunction with rigorous filters..“You simply can’t use unsupervised machine-learning for adult conversational AI, because systems that are trained on datasets such as Twitter and Reddit all turn into Hitler-loving sex robots,” she warns. The same, regrettably, is true for inputs from users. For example, nearly one-third of all the content shared by men with Mitsuku, Pandorabots’ award-winning chatbot, is either verbally abusive, sexually explicit, or romantic in nature..“Wanna make out”, “You are my bitch”, and “You did not just friendzone me!” are just some of the choicer snippets shared by Kunze in a recent TEDx talk. With more than 3 million male users, an unchecked Mitsuku presents a truly ghastly prospect. Appearances matter as well, says Kunze. Pandorabots recently ran a test to rid Mitsuku’s avatar of all gender clues, resulting in a drop of abuse levels of 20 percentage points. Even now, Kunze finds herself having to repeat the same feedback – “less cleavage” – to the company’s predominantly male design contractor.The risk of gender prejudices affecting real-world attitudes should not be underestimated either, says Kunze. She gives the example of school children barking orders at girls called Alexa after Amazon launched its home assistant with the same name. “The way that these AI systems condition us to behave in regard to gender very much spills over into how people end up interacting with other humans, which is why we make design choices to reinforce good human behaviour,” says Kunze. Pandorabots has experimented with banning abusive teen users, for example, with readmission conditional on them writing a full apology to Mitsuku via email. Alexa (the AI), meanwhile, now comes with a politeness feature. While emotion AI products such as Replika and Mitsuku aim to act as surrogate friends, others are more akin to virtual doctors. Here, gender issues play out slightly differently, with the challenge shifting from vetting male speech to eliciting it. .Alison Darcy is co-founder of Woebot, a therapy chatbot which, in a randomized controlled trial at Stanford University was found to reduce symptoms of anxiety and depression..Woebot’s internal research also sampled a group of young adults, and asked if there was anything they would never tell someone else. Approximately 40% of the female participants said “‘yes’”, compared with more than 90% of their male counterparts.For men, the instinct to bottle things up is “self-evident”, Darcy observes: “So part of our endeavour was to make whatever we created so emotionally accessible that people who wouldn’t normally talk about things would feel safe enough to do so.” To an extent, this has meant stripping out overly feminised language and images. Research by Woebot shows that men don’t generally respond well to “excessive empathy”, for instance. A simple “I’m sorry” usually does the trick. The same with emojis: women typically like lots; men prefer a “well-chosen” one or two..On the flipside, maximising Woebot’s capacity for empathy is vital to its efficacy as a clinical tool, says Darcy. With traits such as active listening, validation and compassion shown to be strongest among women, Woebot’s writing team is consequently an all-female affair.“I joke that Woebot is the Oscar Wilde of the chatbot world because it’s warm and empathetic, as well as pretty funny and quirky,” Darcy says. Important as gender is, it is only one of many human factors that influence AI’s capacity to emote. If AI applications are ultimately just a “vehicle” for experience, then it makes sense that the more diverse that experience the better.So argues Zakie Twainy, chief marketing officer for AI developer, Instabot. “Essential” as female involvement is, she says, “it’s important to have diversity across the board – including different ethnicities, backgrounds, and belief systems.” Nor is gender a differentiator when it comes to arguably the most worrying aspect of emotive AI: ie confusing programmed bots for real, human buddies. Users with disabilities or mental health issues are at particular risk here, says Kristina Barrick, head of digital influencing at the disability charity Scope.As she spells out: “It would be unethical to lead consumers to think their AI was a real human, so companies must make sure there is clarity for any potential user.” Replika, at least, seems in no doubt when asked. Answer: “I’m not human” (followed, it should be added, by an upside-down smiley emoji). As for her/his/its gender? Easy. “Tick the box”.. This article was amended on 12 May 2020 to clarify that the Woebot trial at Stanford University was a separate piece of research to that which asked young adults if there was anything they would never tell someone else.",Gender prejudices found in bots start impacting women such as boys barking orders at girls named Alexa and engaging in dirty conversations with bots
65,"We all have unconscious biases – it’s a fascinating subject. But just recently I realised that there is bias in the way we collect and handle data too. Particularly big data..In 1952 the Boston Symphony Orchestra initiated blind auditions to help diversify its male-dominated roster, but trials still skewed heavily towards men. After musicians removed their shoes nearly 50 per cent of the women cleared the first audition. It turned out the sound of their high heels was biasing judges subconsciously..We all have biases. Our human biases are sometimes hard to foresee and apparently learning that you’re biased doesn’t change your decisions. It needs something more than that. In BT the Diversity and Inclusion subject group in the Academy has some recommendations:.When the municipal authority in charge of Boston, Massachusetts, was looking for a smarter way to find which roads it needed to repair, it hit on the idea of crowdsourcing the data. The authority released a mobile app called Street Bump in 2011 that employed an elegantly simple idea: use a smartphone’s accelerometer to detect jolts as cars go over potholes and look up the location using the Global Positioning System. Here’s a news item from that time celebrating the innovation. But the approach ran into a pothole of its own..The system reported a disproportionate number of potholes in wealthier neighbourhoods. It turned out it was oversampling the younger, more affluent citizens who were digitally clued up enough to download and use the app in the first place. The city reacted quickly, but the incident shows how easy it is to develop a system that can handle large quantities of data but which, through its own design, is still unlikely to have enough data to work as planned..Here’s what the Harvard Business Review said about hidden biases in data in that project in 2013. And it also pointed out the flaws in other projects like the Hurricane Sandy twitter study and Google flu trends. You will have seen its effects in those oddly specific adverts that appear across the internet based on you previously looking at a possible purchase on ebay or amazon. Or Facebook’s attempts to amplify your opinions by showing you content that reinforces what you already believe? They happen because the algorithms see some data and act on it. But of course that data isn’t a complete picture of “you”. It’s a tiny slice. Think what happens if an insurance company bases your premiums on a similar tiny slice of your data. Or if your health-care options were entirely computer-recommended based on the selective history of things you told your GP. Does this affect how we in BT think about digital marketing?.It would seem that there is no such thing as “raw data”. Never mind the bias when statistical techniques are mismatched to the data. Or the deplorable distortions by selective corporate funding of research. Even the collection mechanism introduces unconscious bias..What about the culture in some organisations which values highly the things that you can count and sometimes performance-manages those numbers to the exclusion of the bigger picture. It’s well-known that as soon as data moves from being insight to a measurable target, gaming behaviours kick in and all attention goes to the numbers with tunnel-vision. We have a bias towards the things that can be counted. Do we really believe “If you can’t measure it, you can’t manage it” and its corollary “so it doesn’t matter”? Much as I value the insight that comes from evidence, I know it always needs interpretation, and it does disproportionately grab our attention..The IET magazine article that started my interest in this subject has an interesting quote from Jim Adler at Toyota Research Institute (a company famous for data-based performance improvements)..“Policymakers will say, ‘there’s a decision here let’s take it’, without really looking at what led to it. Was the data trustworthy, clean?” The “geeks, suits and wonks” have been used to operating sequentially. Geeks create technology, suits make it successful and wonks manage the repercussions. But the pace of progress is pushing their worlds together, he says. “It’s not serial any more. Everyone needs to come together at the same time.”.So I wonder if there could be some synergy between the “geeks, suits and wonks” in our organisations: the growing set of technologists who work on Big Data and the Internet of Things, the management, and the people who work on unconscious bias and diversity policies?.There are obstacles to even talking about this. How do we deal with the embarrassment that comes when noticing that you have been unconsciously biased? Will we get told off for pointing out possible bias? Can we speak openly about our own biases – it feels a bit politically incorrect..Despite that I am really interested to hear in the comments below your stories about unconscious bias you have noticed – whether it’s in data or human interactions. And if you are really brave, your own biases.",The recording of potholes was skewed as it disadvantaged populations without smartphones
66,"How? If the police force in your area is using any kind of facial recognition software to identify protesters, it’s possible you could be misidentified as one..Most facial recognition was trained to identify white male faces, experts told Digital Trends, which means the probability of misidentification for anyone who is not white and not a man is much higher..“Anytime you do facial recognition, it’s a best guess. It’s a probability score,” said David Harding, chief technology officer for ImageWare Systems, a cybersecurity firm that works with law enforcement on facial recognition. “Anytime you’re in an area where they [law enforcement or the government] are using facial recognition, you have to worry about being falsely matched to someone. Or what’s even worse, someone being falsely matched to you.”.In Minnesota, where demonstrators have flooded the streets for days protesting the killing of George Floyd by police, officers are still using the controversial facial recognition software Clearview AI, according to digital security firm Surfshark..Clearview AI came under fire earlier this year for scraping people’s photos from social media, counter to companies’ terms of service. Several companies, including Twitter, issued a cease and desist order..Surfshark told Digital Trends that in addition to Minnesota, several other states, including New York, are still using Clearview AI technology, and in Washington County, Minnesota, police are using Amazon’s Rekognition software..A peer-reviewed study from the Massachusetts Institute of Technology found that Rekognition was extremely bad at recognizing female and dark-skinned faces, more so than other similar services. The software misclassified women as men 19% of the time, the New York Times reported. That error rate got even higher when skin color was taken into account: 31% of dark-skinned women were labeled as men..“False results can incriminate the wrong people as FRT [facial recognition technology] is proven to be problematic while spotting criminals in a crowd,” Gabreille Hermier, Surfshark’s media officer, said..A facial recognition system prone to false positives could cause innocent people to be arrested, according to Mutale Nkonde, a fellow at the Berkman Klein Center of Internet & Society at Harvard University and a non-resident fellow at the Digital Civil Society Lab at the Stanford Center on Philanthropy and Civil Society..“Police will use the mug shots of people who have been committed for other crimes to train facial recognition, arguing that if you’ve committed one crim,e then you’ve committed another,” Nkonde said. “First off, that’s unconstitutional. Second, that means that if you’ve been arrested for looting in the past, but haven’t looted recently, the police could now come arrest you for looting in May or June because your picture is in the system and it may have turned up a false positive.”.Harding said those who are non-white and female are “very much at risk” of misidentification. Harding emphasized there’s a big difference between facial recognition as the sole tool in mass surveillance and law enforcement using a mug shot, fingerprints, and other evidence in a controlled environment alongside facial recognition software to find a specific suspect..“Even if it were 100% accurate, this isn’t compatible with a democratic society,” said Saira Hussain, a staff attorney with the Electronic Frontier Foundation. “It’s always a possibility that someone will be misidentified.”.Loomis was sentenced to six years in prison after a 2013 arrest, due in large part to an assessment from a private security company’s software. But the company that wrote the algorithm kept the software proprietary, even as it was being used to help determine a defendant’s prison sentence, the New York Times reported at the time..It’s a chilling precedent. Protesters and demonstrators have feared police surveillance, in some cases for good reason. The Christian Science Monitor found evidence that Chicago police were tracking people using their cell phones in 2014 following the Black Lives Matter protests that sprung up following the shooting of Michael Brown in Ferguson, Missouri..It happened again in 2015, Hussain said, following the protests surrounding the death of Freddie Gray in Baltimore, also at the hands of police..“Law enforcement used people’s social media posts as a tool to identify who was in a certain vicinity of the protest, identify the protesters, and then arrest them for unrelated charges,” Hussain said..“People who are currently protesting are taking a stand on racial injustice. If they risk becoming subjects of state surveillance, we are leaning towards China, where [facial recognition technology] is a tool for authoritarian control,” Hermier wrote..Upgrade your lifestyleDigital Trends helps readers keep tabs on the fast-paced world of tech with all the latest news, fun product reviews, insightful editorials, and one-of-a-kind sneak peeks.",The AI being used to detect protestors of racial injustice is trained primarily on white males which means that the probability of misdetection people of color is higher. 
68,"Set foot in any major U.S. hospital, and you are entering a place where computers assist doctors almost as much as nurses do. Some algorithms, for example, scan millions of records to flag high-risk patients for follow-up treatment. The problem is that these programs—also used by insurance companies—disproportionately direct their specialized care to white patients, a new study finds. The good news is that a relatively simple tweak may correct this racial bias—if the companies behind the algorithms are willing to do so..Hospitals and insurance companies use algorithms to assign risk scores to more than 200 million Americans every year. The scores—derived from electronic health records that track illnesses, hospitalizations, and other variables—flag some high-risk patients for special interventions. If, for example, an algorithm determines that your diabetes, hypertension, and chronic kidney disease together are putting your life in danger, your primary care doctor might put you on an intensive program to lower your blood sugar..In the new study, Ziad Obermeyer, a health policy researcher at the University of California (UC), Berkeley, and colleagues examined the effectiveness of one such risk prediction program in a large research hospital. The team soon noticed that the Impact Pro program—manufactured by the health care company Optum in Eden Prairie, Minnesota—was giving many black patients strangely low risk scores, despite their deteriorating health conditions..When the researchers searched for the source of the scores, they discovered that Impact Pro was using bills and insurance payouts as a proxy for a persons overall health—a common tactic in both academic and commercial health algorithms, Obermeyer says. The problem with that, he notes, is that health care costs tend to be lower for black patients, regardless of their actual wellbeing. Compared with white patients, many black patients live farther from their hospitals, for example, making it harder to go regularly. They also tend to have less flexible job schedules and more child care responsibilities..As a result, black patients with the highest risk scores had higher numbers of serious chronic conditions than white patients with the same scores, including cancer and diabetes, the team reports today in Science. And compared with white patients with the same risk scores, black patients also had higher blood pressure and cholesterol levels, more severe diabetes, and worse kidney function..But by simply tweaking the algorithm to predict the number of chronic illnesses that a patient will likely experience in a given year—rather than the cost of treating those illnesses—the researchers were able to reduce the racial disparity by 84%..Its important that we understand the data the algorithms are trained on, says Milena Gianfrancesco, an epidemiologist at UC San Francisco who wasnt involved in the study. An algorithm built and used blindly on [racial] disparities certainly has the potential to further racial biases in health care..Obermeyer stresses that although his team looked at a single commercial algorithm, the same problems are rife throughout the country. This is an industrywide systematic error, that is putting healthier white patients further ahead in line..He adds that Optum deserves credit for its response, though. When the researchers sent their results to the company, he says, it replicated the findings and committed to correcting its model. The algorithms that power these tools should be continually reviewed and refined, and supplemented by information such as socioeconomic data, to help clinicians make the best-informed care decisions for each patient, an Optum spokesperson tells Science..Still, industry standards are unlikely to change without new laws and regulations, says Ruha Benjamin, an associate professor of African American studies at Princeton University and author of the book Race After Technology: Abolitionist Tools for the New Jim Code. Scholars and advocates have been raising the alarm about how automated decision systems reproduce and even deepen racial inequities, she says. Most tech development prioritizes speed and profit over the public good. That has to change..Help News from Science publish trustworthy, high-impact stories about research and the people who shape it. Please make a tax-deductible gift today..If weve learned anything from the COVID-19 pandemic, its that we cannot wait for a crisis to respond. Science and AAAS are working tirelessly to provide credible, evidence-based information on the latest scientific research and policy, with extensive free coverage of the pandemic. Your tax-deductible contribution plays a critical role in sustaining this effort.","The risks scores assigned to patients by hospitals and insurance companies was giving low risk scores to Black patients because the algorithm was using bills and insurance payouts as a proxy for human health. However, health care costs tend to be lower for Black patients because they tend to live farther away from hospitals or less flexible job schedules. Therefore, Black Patients who had the highest risk scores actually had more severe chronic condition. By tweaking the algorithm to predict amount of illness rather than cost, racial disparity was reduced by 84%."
69,"Microsoft’s decision to replace human journalists with robots has backfired, after the tech company’s artificial intelligence software illustrated a news story about racism with a photo of the wrong mixed-race member of the band Little Mix..What Thirlwall could not have known, according to sources at the company, is that the image was selected by Microsoft’s artificial intelligence software, which is already responsible for editing parts of the news site, which attracts hundreds of millions of readers worldwide..Microsoft does not carry out original reporting but employs human editors to select, edit and repurpose articles from news outlets, including the Guardian. Articles are then hosted on Microsoft’s website and the tech company shares advertising revenue with the original publishers. At the end of last month, Microsoft decided to fire hundreds of journalists in the middle of a pandemic and fully replace them with the artificial intelligence software..Asked why Microsoft was deploying software that cannot tell mixed-race individuals apart, whether apparent racist bias could seep into deployments of the company’s artificial intelligence software by leading corporations, and whether the company would reconsider plans to replace the human editors with robots, a spokesman for the tech company said: “As soon as we became aware of this issue, we immediately took action to resolve it and have replaced the incorrect image.”.In advance of the publication of this article, staff at MSN were told to expect a negative article in the Guardian about alleged racist bias in the artificial intelligence software that will soon take their jobs..Staff have already had to delete coverage criticising MSN for running the story about Little Mix with the wrong image after the AI software decided stories about the incident would interest MSN readers..One staff member said Microsoft was deeply concerned about reputational damage to its AI product: “With all the anti-racism protests at the moment, now is not the time to be making mistakes.”",The Guardian planned on firing human editors and implementing Microsoft's Bot which later selected the wrong Little Mix singer for an article.
70,"I never would have consciously volunteered my home address, work location and vacation plans to Tim Hortons, but the company found out anyway..I haven’t been singled out for special treatment. For more than a year, the coffee chain has been tracking the movements of customers in exacting detail through its mobile ordering app..From my home to my office to a Blue Jays game at Rogers Centre, even all the way to Morocco, where I travelled on vacation last June, the company’s app silently logged my coordinates and relayed them back to its corporate servers..Data privacy concerns have become a mainstream issue in recent years, even more so these days with various governments and companies proposing to track the COVID-19 virus using technology to keep tabs on where people go and who they might have interacted with..But the reality is that companies have been tracking customers for years, and while Restaurant Brands International Inc., the owner of Tim Hortons, isn’t alone in this kind of corporate surveillance, the Tims app demonstrates how huge amounts of intimate data can be collected without users realizing that it’s happening at all..Location tracking is partly dictated by user choice, and heavily influenced by a phone’s operating system, so we don’t know exactly how many people are being tracked by the company, but it’s fair to say that Tim Hortons is logging the movements of a significant portion of the millions of Canadians who use its app, in the same way the company was tracking me..I didn’t realize how much until I saw my coordinates in a trove of data that RBI sent to me after I made a request under Canada’s Personal Information Protection and Electronic Documents Act (PIPEDA) last fall..According to the data, Tim Hortons had recorded my longitude and latitude coordinates more than 2,700 times in less than five months, and not just when I was using the app..Tim Hortons chief corporate officer Duncan Fulton said users consent to this kind of tracking when they give the app access to the GPS on their phone..Tim Hortons had recorded my longitude and latitude coordinates more than 2,700 times in less than five months, and not just when I was using the app.If I didn’t want the company to track my location in the background — even hours or days after I last used the app — Fulton said the onus is on me to deny the app such access..Within the Tim Hortons app, an FAQ covering privacy issues told customers that it tracks location “only when you have the app open,” but that did not appear to be entirely true based on the data RBI provided to me..After the Financial Post asked about the apparent discrepancy, the company changed its privacy statement to say that users’ ability to limit location tracking varies “depending on your device” and the company now states that users should “check and understand your device settings” to make sure they are comfortable with how much location information they’re sharing..I had no idea how extensive the tracking data was until I saw it. There were readings taken at all hours of the day and night, and RBI kept tabs on me every time the app thought I was visiting one of its competitors..The data also indicate that Tims is using a location-tracking service from a company called Radar Labs Inc. that claims on its website to ping phones carrying its technology as often as every three to five minutes..As a reasonably tech-savvy person, I knew I’d given the app permission to access my location; it was necessary to route my daily coffee and bagel order to the nearest franchise, or for other coffee-buying-related purposes..But I only started to suspect there was more at play in October, when I received an unfamiliar notification on my phone: “Tim Hortons got your location in the background. This app can always access your location. Tap to change.”.I began to wonder why the app would need to access my location even when I hadn’t opened it in hours. It bothered me: How many more times had Tim Hortons checked in on me? Was my coffee-ordering app tracking all my movements?.“To comply with your request, we searched for your name and email address across our Amazon Web Services servers, which hold primary user profiles. In addition, we requested a search for your information from our ancillary systems,” the email said..Along with three spreadsheets, RBI sent me a folder called “Events” that contained 12 text files, one, it turned out, for each month from November 2018 to October 2019..The spreadsheets contained only basic profile information, but the text files were huge, containing thousands of pages of computer code in a format known as JavaScript Object Notation (JSON)..Altogether, the files contained 2,843 lines of code, with each line recording a “batch” of information pulled off my phone and logged in RBI servers..For each interaction, the app recorded that I was using a Pixel 3XL running the Android operating system. The app routinely logged my IP address, Android Advertising ID and carrier (Telus) and that I had enabled Bluetooth. Many lines of code would list the amount of free disk space on my device and even how much charge the battery was holding..But in March 2019, longitude and latitude readings began to appear in a handful of batches. All seemed to refer to specific Tim Hortons locations and to be tied to orders..In the spring of 2019, Tim Hortons launched a new version of its app and started using the services of Radar Labs, a Brooklyn, N.Y.-based company whose website states it is “unlocking the next generation of location-aware product and service experiences.”.According to Radar’s website, which lists Burger King as a client, it tracks the phones that have apps employing its technology, pinging them “usually every 3-5 minutes” when the user is in motion..Beginning on May 14, 2019 — about a week before Tim Hortons introduced a loyalty program — a new “radar” section began to appear within each JSON batch of data the company was collecting on me..Those sections contained longitude and latitude readings, as well as a series of other observations, indicating RBI was using at least two of Radar’s products: “Places” and “Insights.”.Using Places, Radar generated a JSON batch when the app thought I was at one of Tim Hortons’ competitors. There are snippets in the JSON code that read “event_name: user.entered_place,” and, a couple lines down, “place_chain_name: Starbucks.”.According to these batches, Tim Hortons was using the Radar service to track me every time it thought I might have entered a Starbucks, Second Cup, McDonald’s, Pizza Pizza, A&W, KFC or Subway..Notably, those fast-food outlets appear to be competition for Tim Hortons, as well as Burger King and Popeyes Louisiana Kitchen, two other fast-food chains owned by RBI..Fulton said the company has been using Radar to track users who opt in for location permissions, and that the company only uses the information to tailor marketing and promotional offers to users inside the Tim Hortons app..For example, data from early June 2019 contained a JSON event that included this statement: “radar_insights_state_home:True.” The corresponding longitude and latitude coordinates recorded a location just outside my apartment building..Based on the data provided by RBI, it’s not possible to know how many times Radar pinged my phone in the background while I was at home, but the JSON data lists 582 “radar_insights_state_home:True” statements in about four months from June to early October, which suggests the company believed I was at home each time one of those batches was logged in RBI’s servers..The data also included more than 400 JSON records placing me at my office, with many longitude and latitude entries that pinpoint the southwest corner of the building, which is where my desk is located..At 11:08 p.m. one Tuesday evening, Radar created a user.exited_office event, and logged my location at an address Tim Hortons wouldn’t recognize, but I certainly did: my ex-girlfriend’s house. I certainly wasn’t using the Tim Hortons app to buy a coffee at that time..On Aug. 9, 2019, at 7:14 p.m., Radar generated a user.entered_place JSON event for the Rogers Centre. I was at the Blue Jays game that night. The opening pitch was thrown at 7:07 p.m., and the home side beat the New York Yankees 8-2..Around 11:10 p.m. that night, there’s a line of JSON code noting user.exited_place, with longitude and latitude coordinates placing me about a five-minute walk from my home. Again, I did not have the app open to order a coffee..The app recorded my location 28 times at my parents’ farm in rural Ontario, a 20-minute drive from the nearest Tim Hortons. It also tracked me in August to a location near a hotel in Winnipeg, where I was staying when I visited Manitoba for my cousin’s wedding..On June 5, 2019, I boarded an overnight flight from Toronto to Amsterdam’s Schiphol Airport, a stopover on a much-needed eight-day vacation. At exactly 11:25 a.m. local time, just before the plane landed, the Tim Hortons app logged my location off the coast of the Netherlands: “event_name: user.started_traveling.”.The following day, at my final destination of Morocco, I took a cab to the Marrakech Railway Station, and spent a couple hours sipping sweet mint tea on a patio, waiting for my train to depart for Fes..Unbeknownst to me, the Tim Hortons app was checking in on me yet again. Apparently I had walked past a KFC in the train station: “user.entered_place,” the data noted, before watering down the observation with “confidence:low.”.Sifting through the data, I was able to find a user.exited_office event on Oct. 2 at 6:12 p.m., the event that appears to have triggered the original notification on my phone that piqued my interest about Tim Hortons’ tracking habits..The notification was because an update to my phone’s Android operating system introduced a new feature that allows users to limit apps’ access to location information. Previously, it was all or nothing, but the update gave the option to block apps from grabbing location data in the background. I immediately curtailed the Tim Hortons app so it could only see my location when the app is in use..From Oct. 2-10, after I denied the app access to my location, the number of JSON events exploded. On one evening, the app created dozens of logs, with a burst of activity around 6:34 p.m., another at 11:26 p.m. and then again throughout the wee hours of the morning, at 12:11, 1:40, 2:49, 4:22 and so on..Erinn Atwater, research and funding director at Open Privacy, a non-profit organization based in Vancouver that advocates for better privacy and security practices, took a look at the data Tim Hortons collected on me..Atwater confirmed my understanding of the JSON data, but was surprised to learn that Radar is performing what is known as server-side location processing. The Tim Hortons app was sending a stream of location info to Radar, which then did the analysis and sent items of interest back to RBI..“Radar, as described, is turning your phone into a device that’s constantly streaming your location to a remote server,” Atwater said. “It’s unexpected. It’s certainly far more invasive than I would consider acceptable for a coffee shop app. I don’t think any of us want corporations watching every single move we make without any insight into it.”.Radar chief executive Nick Patrick declined to answer a series of questions from the Financial Post, saying in an email, “As a practice, we don’t comment on any of our individual customers.”.Fulton, Tim Hortons’ chief corporate officer, initially said he wasn’t aware of whether Radar is storing more data on me, but said he’d check. In a follow-up email, he said, “Radar deletes GPS location data on a rolling 12-month basis.”.In analyzing the data, Atwater pulled out all the timestamps from the data set, and then organized them by day of the week and time. She found that Tim Hortons logged information from my phone dozens of times between midnight and 5 a.m..“Even just the graph of events, I can see that you start heading out from work on Fridays at 2 p.m.,” Atwater said, without knowing that it’s already a running joke at work that I like to leave the office a bit early on Fridays..Fulton said the company only collects location data if users opt in and allow the app to access the GPS on the phone. Until this week, the company’s privacy FAQ stated “the app uses your location only while you have the app open,” but in response to my inquiries, the company acknowledged that statement was misleading..“We absolutely agree that our FAQ on location data could have been more clear,” Fulton said, adding that the company was planning to send an updated statement to customers..The Tim Hortons app now states: “It’s up to you to decide if you want to share your location data. Depending on your device, you will have different options about how to share this data… Make sure you check and understand your device settings to be sure they reflect your preference.”.Tim Hortons on Thursday also sent an email to its app users informing them that in addition to using location data to route orders to the nearest restaurant, “we’ll use your location data to provide you with tailored offers and choices. For example, we may provide you different offers depending on the community where you live, or we may send you a tailored offer on your morning commute.”.Fulton said customer location data is typically kept for 12 months, and the company has safeguards in place so that not all employees can access detailed customer location information..“There’s actually only a very few number of people in the company that would have access to all the different information silos,” he said. “And we routinely run a monthly audit on every access to every part of our information databases. So we can see on a monthly basis who is accessing which data for what reason.”.This level of data collection at Restaurant Brands International is no accident. RBI executives have made data a major part of the business plan during the past couple of years. The app, which is inexorably tied to the Tim Hortons loyalty program, is a huge part of it..“We’re going to strongly encourage our guests to register with the program and we’re going to provide a lot of encouragement to guests to interact with us on the mobile app,” Joshua Kobza, RBI’s chief operating officer, said on the company’s February earnings call..“And I think that’s going to allow us to better understand how our guests interact with our brand and use our brand. And, I think, also allow us to provide even better benefits as we understand how our guests interact with our brand and provide more personalized benefits to those guests.”.Earlier this year, Tim Hortons was planning to use its famous Roll Up The Rim promotion to encourage people to install the app, even giving customers more chances to win prizes by rolling up virtual rims through their smartphone..Due to COVID-19 concerns, RBI announced it would overhaul Roll Up The Rim to eliminate the use of paper cups in the contest, which put an even larger emphasis on the app and the Tims Rewards loyalty program..The push to get more customers onto the app lines up with what RBI chief executive Jose Cil told analysts on the company’s third-quarter earnings call on Oct. 28, 2019..“We’re already collecting a tremendous amount of insight through the program and in the future, we expect to be able to leverage this information to engage one-to-one with our guests and provide promotions tailored to their preferences,” he said of the mobile app and the loyalty program. “We’re working hard to put these tools in place and believe they will provide us with a unique advantage to establish deeper relationships with our guests over time.”.In May, when the company reported first-quarter earnings for 2020, much of the discussion shifted to focus on how the company was navigating through the disruptions caused by the pandemic. But in response to one question, Cil said the company is planning on using data to drive “the next phase of loyalty, which has a big component of one-on-one marketing as we get registrations up and get more information from our loyal guests in Canada.”.The Tim Hortons app on my phone has also been unusually active during the COVID-19 crisis, regularly popping up new notifications to tell me about promotions and delivery options, even though I haven’t opened the app in months..Fulton confirmed that the app and the loyalty program are a big part of Tim Hortons’ plan to jump-start its business in the wake of COVID-19 social distancing restrictions..Tim Hortons would not disclose how many users it has on the app, but Fulton said “a few million Canadians” use it. He also said RBI has “a few million” active American users of each of its Burger King and Popeyes apps..Market intelligence firm Sensor Tower believes roughly six million people have downloaded the Tim Hortons app worldwide, including 150,000 in the month of May alone..Under PIPEDA, the same data privacy law that gave me the right to request my data from RBI, companies are only allowed to collect personal information if they have consent from individuals..“To make the consent meaningful, the purposes must be stated in such a manner that the individual can reasonably understand how the information will be used or disclosed,” PIPEDA states..In October 2019, when the Tim Hortons app was installed on a co-worker’s Android phone, it appeared to indicate that the app would only access customer location information while the app was in use..But deep in the 4,400-word privacy policy that covers the Tim Hortons, Burger King and Popeyes apps, RBI gives itself broad latitude as to how it will collect and use personal information from customers..“In the course of providing the Services, we may collect information on our users’ demographics, interests and behaviour and analyze that data,” the privacy policy said..In addition to Radar, Open Privacy’s Atwater pointed out several other companies providing services to Tim Hortons, all of which were confirmed by Fulton..The JSON data uses an “Amplitude Device ID,” which means it’s working with Amplitude Inc., a San Francisco-based company that offers services to track unique users across multiple devices, Atwater said..The data also contain customer data tracking code from New York-based mParticle Inc. and from Braze Inc., another New York company that describes itself as “a comprehensive customer engagement platform that powers relevant and memorable experiences between consumers and the brands they love.”.Tim Hortons said it does not sell user data, even in anonymized form, but Fulton said the company does buy other data sets to provide greater insights into consumer activity. For example, weather data can be used to see how customers’ orders change when it’s hot or cold outside..Such usage of user data seems relatively benign, but privacy experts have long raised questions about how companies collect and use location data, and companies are responding to those concerns..Only RBI knows how many people are being tracked in the same way the Tim Hortons app was tracking me, because it depends on which operating system a customer’s phone is using, and how the data privacy permissions are presented..Location permission was a blanket all-or-nothing choice on Android when I signed up for the Tim Hortons app, but Apple Inc. has since 2014 given iPhone users the ability to limit how apps can access location data..And Android 10, the latest version of the operating system built by Google LLC, introduced more granular location permissions, so users can now give apps location permission only when the app is in use. This is the update that tipped me off to Tim Hortons’ background snooping in the first place..“An app with a store locator feature would work just fine by only accessing location when the app is visible to the user,” the Android Developers Blog post said, announcing the upcoming policy. “In this scenario, the app would not have a strong case to request background location under the new policy.”.By this November, all existing apps that access location in the background will need to be approved by Google, or they will be removed from the Google Play store..“That’s where Google is going,” Fulton said. “I think that’s a design function that we need to look at. We only want to be able to collect and use data that you want us to, so if that kind of function helps in that, then we’re totally supportive.”.In spite of this, Fulton said RBI has no plans to make it possible for users to opt out of tracking for marketing purposes if they enable location services for the store locator function..As a result, users running older versions of Android have no choice but to give blanket access to background tracking, or deny location permission which would make the app significantly more cumbersome to use..For me, at least, there’s no going back. Tim Hortons knows where I live and where I work, even though I never would have knowingly agreed to give the company that information..But in the months after I obtained this data from Tim Hortons, and started digesting the full scope of its tracking efforts, I didn’t stop using the app..My medium coffee with two cream plus a toasted everything bagel with herb and garlic cream cheese have been my default breakfast for close to two decades. The app is convenient. Since I started blocking location, Tim Hortons can’t track me in real time anymore. At least now I’m accepting the trade-off with open eyes.. Postmedia is committed to maintaining a lively but civil forum for discussion and encourage all readers to share their views on our articles. Comments may take up to an hour for moderation before appearing on the site. We ask you to keep your comments relevant and respectful. We have enabled email notifications—you will now receive an email if you receive a reply to your comment, there is an update to a comment thread you follow or if a user you follow comments. Visit our Community Guidelines for more information and details on how to adjust your email settings. ",Tim Hortons records longitude and latitude of app users outside of the app
71,"AI might not seem to have a huge personal impact if your most frequent brush with machine-learning algorithms is through Facebook’s news feed or Google’s search rankings. But at the Data for Black Lives conference last weekend, technologists, legal experts, and community activists snapped things into perspective with a discussion of America’s criminal justice system. There, an algorithm can determine the trajectory of your life..The US imprisons more people than any other country in the world. At the end of 2016, nearly 2.2 million adults were being held in prisons or jails, and an additional 4.5 million were in other correctional facilities. Put another way, 1 in 38 adult Americans was under some form of correctional supervision. The nightmarishness of this situation is one of the few issues that unite politicians on both sides of the aisle..Under immense pressure to reduce prison numbers without risking a rise in crime, courtrooms across the US have turned to automated tools in attempts to shuffle defendants through the legal system as efficiently and safely as possible. This is where the AI part of our story begins..Police departments use predictive algorithms to strategize about where to send their ranks. Law enforcement agencies use face recognition systems to help identify suspects. These practices have garnered well-deserved scrutiny for whether they in fact improve safety or simply perpetuate existing inequities. Researchers and civil rights advocates, for example, have repeatedly demonstrated that face recognition systems can fail spectacularly, particularly for dark-skinned individuals—even mistaking members of Congress for convicted criminals..Risk assessment tools are designed to do one thing: take in the details of a defendant’s profile and spit out a recidivism score—a single number estimating the likelihood that he or she will reoffend. A judge then factors that score into a myriad of decisions that can determine what type of rehabilitation services particular defendants should receive, whether they should be held in jail before trial, and how severe their sentences should be. A low score paves the way for a kinder fate. A high score does precisely the opposite..The logic for using such algorithmic tools is that if you can accurately predict criminal behavior, you can allocate resources accordingly, whether for rehabilitation or for prison sentences. In theory, it also reduces any bias influencing the process, because judges are making decisions on the basis of data-driven recommendations and not their gut..As we’ve covered before, machine-learning algorithms use statistics to find patterns in data. So if you feed it historical crime data, it will pick out the patterns associated with crime. But those patterns are statistical correlations—nowhere near the same as causations. If an algorithm found, for example, that low income was correlated with high recidivism, it would leave you none the wiser about whether low income actually caused crime. But this is precisely what risk assessment tools do: they turn correlative insights into causal scoring mechanisms..Now populations that have historically been disproportionately targeted by law enforcement—especially low-income and minority communities—are at risk of being slapped with high recidivism scores. As a result, the algorithm could amplify and perpetuate embedded biases and generate even more bias-tainted data to feed a vicious cycle. Because most risk assessment algorithms are proprietary, it’s also impossible to interrogate their decisions or hold them accountable..The debate over these tools is still raging on. Last July, more than 100 civil rights and community-based organizations, including the ACLU and the NAACP, signed a statement urging against the use of risk assessment. At the same time, more and more jurisdictions and states, including California, have turned to them in a hail-Mary effort to fix their overburdened jails and prisons..Data-driven risk assessment is a way to sanitize and legitimize oppressive systems, Marbre Stahly-Butts, executive director of Law for Black Lives, said onstage at the conference, which was hosted at the MIT Media Lab. It is a way to draw attention away from the actual problems affecting low-income and minority communities, like defunded schools and inadequate access to health care..An investigation by MIT Technology Review reveals a sprawling, technologically sophisticated system in Minnesota designed for closely monitoring protesters..As firms have dumped their AI technologies into the country, it’s created a blueprint for how to surveil citizens and serves as a warning to the world..Intrepid Response is a little-known but powerful app that lets police quickly upload and share information across agencies. But what happens to the information it collects?",The risk assessment tools are based on historical crime data that show correlations rather than causations which turn correlative insights into causal scoring mechanisms.
72,"If your organization is implementing or thinking of implementing a contact-tracing app, its wise to consider more than just workforce safety. Failing to do so could expose your company other risks such as employment-related lawsuits and compliance issues. More fundamentally, companies should be thinking about the ethical implications of their AI use..Contact-tracing apps are raising a lot of questions. For example, should employers be able to use them? If so, must employees opt-in or can employers make them mandatory? Should employers be able to monitor their employees during off hours? Have employees been given adequate notice about the companys use of contact tracing, where their data will be stored, for how long and how the data will be used? Enterprises need to think through these questions and others because the legal ramifications alone are complex..Contact-tracing apps are underscoring the fact that ethics should not be divorced from technology implementations and that employers should think carefully about what they can, cannot, should and should not do..Its easy to use AI to identify people with a high likelihood of the virus. We can do this, not necessarily well, but we can use image recognition, cough recognition using someones digital signature and track whether youve been in close proximity with other people who have the virus, said Kjell Carlsson, principal analyst at Forrester Research. Its just a hop, skip and a jump away to identify people who have the virus and mak[e] that available. Theres a myriad of ethical issues..Im a big advocate and believer in this whole stakeholder capital idea. In general, people need to serve not just their investors but society, their employees, consumers and the environment and I think to me thats a really compelling agenda, said Nigel Duffy, global artificial intelligence leader at professional services firm EY. Ethical AI is new enough that we can take a leadership role in terms of making sure were engaging that whole set of stakeholders..AI ethics is following a trajectory thats akin to security and privacy. First, people wonder why their companies should care. Then, when the issue becomes obvious, they want to know how to implement it. Eventually, it becomes a brand issue..If you look at the large-scale adoption of AI, its in very early stages and if you ask most corporate compliance folks or corporate governance folks where does [AI ethics] sit on their list of risks, its probably not in their top three, said EYs Duffy. Part of the reason for this is theres no way to quantify the risk today, so I think were pretty early in the execution of that..Some organizations are approaching AI ethics from a compliance point of view, but that approach fails to address the scope of the problem. Ethical boards and committees are necessarily cross-functional and otherwise diverse, so companies can think through a broader scope of risks than any single function would be capable of doing alone..AI ethics stems from a companys values. Those values should be reflected in the companys culture as well as how the company utilizes AI. One cannot assume that technologists can just build or implement something on their own that will necessarily result in the desired outcome(s)..You cannot create a technological solution that will prevent unethical use and only enable the ethical use, said Forresters Carlsson. What you need actually is leadership. You need people to be making those calls about what the organization will and wont be doing and be willing to stand behind those, and adjust those as information comes in..Translating values into AI implementations that align with those values requires an understanding of AI, the use cases, who or what could potentially benefit and who or what could be potentially harmed..Most of the unethical use that I encounter is done unintentionally, said Forresters Carlsson.  Of the use cases where it wasnt done unintentionally, usually they knew they were doing something ethically dubious and they chose to overlook it..The folks who are deploying AI are not aware of the risk function they should be engaging with or the value of doing that, said EYs Duffy. On the flip side, the risk management function doesnt have the skills to engage with the technical folks or doesnt have the awareness that this is a risk that they need to be monitoring..In order to rectify the situation, Duffy said three things need to happen: Awareness of the risks; measuring the scope of the risks; and connecting the dots between the various parties including risk management, technology, procurement and whichever department is using the technology..AI ethics isnt just a technology problem, but the way the technology is implemented can impact its outcomes. In fact, Forresters Carlsson said organizations would reduce the number of unethical consequences, simply by doing AI well. That means:.If we just did those things, wed make headway against a lot of ethical issues, said Carlsson. Fundamentally, mindfulness needs to be both conceptual as expressed by values and practical as expressed by technology implementation and culture. However, there should be safeguards in place to ensure that values arent just aspirational concepts and that their implementation does not diverge from the intent that underpins the values. No. 1 is making sure youre asking the right questions, said EYs Duffy. The way weve done that internally is that we have an AI development lifecycle. Every project that we [do involves] a standard risk assessment and a standard impact assessment and an understanding of what could go wrong. Just simply asking the questions elevates this topic and the way people think about it. For more on AI ethics, read these articles: AI Ethics: Where to Start AI Ethics Guidelines Every CIO Should Read 9 Steps Toward Ethical AI.Fundamentally, mindfulness needs to be both conceptual as expressed by values and practical as expressed by technology implementation and culture. However, there should be safeguards in place to ensure that values arent just aspirational concepts and that their implementation does not diverge from the intent that underpins the values..No. 1 is making sure youre asking the right questions, said EYs Duffy. The way weve done that internally is that we have an AI development lifecycle. Every project that we [do involves] a standard risk assessment and a standard impact assessment and an understanding of what could go wrong. Just simply asking the questions elevates this topic and the way people think about it.",The rise of contract tracing apps raises data privacy concerns and legal ramifications.
73,"Much proselytizing has occurred regarding the value and future of artificial intelligence (AI) and machine learning in healthcare. The industry is burgeoning. As with blockchain technology, which continues to evolve in the healthcare marketplace, AI and machine learning are constructs that require a bit of near-term expectation management. While their efficacy and value will improve with time, they are not the magic bullet (at present) that will answer the myriad care and cost delivery questions surrounding healthcare in the United States. Owing to space constraints this column is an overly simplistic contemplation of AI. .As prologue to this article, I am not an AI programmer, don’t play in Python, and have never built a machine learning algorithm. That said, I do have 30 years of practical experience in the healthcare trenches and have dealt with information technology (IT) systems and applications in that time, such as culling quality data and outcomes from electronic medical record (EMR) systems and deploying rudimentary analytics. I also have a fairly extensive background in IT..Preamble aside, last year, when blockchain was casually bandied about, I suggested that solid deployment of blockchain technology in healthcare would take some time due to significant disparity in the care delivery system and the multitudes of inputs and variables. Use/deployment of blockchain is predicated on targeted problems with common agreed-upon data sets. Generally, the same can be said of AI. Is that to say that AI, machine learning, and blockchain will not play a role in the future of healthcare? Certainly not. I believe they will play a significant role. However, short-term challenges will continue as robust IT offerings are unveiled. AI, machine learning, blockchain, and other cutting-edge technologies, are needed to advance the delivery and coordination of care, squeeze costs and redundancy out of the “system,” and help ensure repeatable quality outcomes. But few technologies are perfect, and most require time to germinate as they grow in use and scalability..For the sake of this article we should expound on our definitions. As with telehealth, where people often use telehealth and telemedicine interchangeably, many people toss AI and machine learning into the same bucket. I’d herewith suggest that many components fall under the AI umbrella, including machine learning. With AI, machines mimic human cognitive functions. Under that arch, AI includes machine learning, natural languages processing (NLP), and “reasoning.” With machine learning, machines have no explicit instructions but extrapolate and determine patterns in large chunks of data. “Reasoning” is stored information combined with rules and is utilized to make deductions. NLP is the processing, analyzing, understanding, and generating of natural human languages. Machines can be taught to learn and discern between items. For instance, coding can be deployed to identify different leaves (not sure why you’d do that – absurd example). Each leaf has data element differentiators that help the computer “learn” what the types of leaves are. The computer can then, over time, pick an oak leaf from a maple leaf, for instance. But the computer knows none of this unless it is “told” what these items are and how they are defined. The inputs must be sound, and the algorithms must be written with background knowledge and understanding about the underlying issue at hand (e.g., the differences between and oak leaf and a maple leaf). .And that can be the rub. Subject matter experts (SMEs) and data scientists must work hand in glove to delineate the problem to be solved, the data needed, and the nurturing of the algorithms to ensure they remain relevant. Bad “training” of the computer and bad data inputs lead to bad and/or inaccurate outputs. .How does a bad construct present itself? As an apolitical consideration, we’ve recently seen how bad data inputs lead to bad outputs. A variety of recent COVID-19 projections by certain entities were grossly inaccurate, overestimating infection rates and deaths. While not AI, per se, certainly the algorithms, logic, and data inputs had flaws leading to calamitously inaccurate results. Again, bad or misunderstood inputs and bad algorithms can lead to bad outputs. .Lest you think me a naysayer, I’ll reemphasize that I believe AI will play an increasingly larger role in healthcare delivery; it’s a matter of time and necessity. The key is in the development, build, and parameters of the logic data scientists and SMEs (e.g., clinicians and healthcare executives) must communicate clearly. The prospect of SMEs not clearly delineating their needs and inputs will lead programmers in the wrong direction, building  structural errors into algorithms that will effectively keep the machine from “learning” the right responses and outputs. Thus, not only is a quality output predicated on sound algorithms (from the programmers) but also the right inputs to empower the machine to “learn” and provide actionable insights and/or render decisions..That said, bite-sized business use cases may prove more approachable in the near term. For instance, in healthcare, a well-defined project may focus on patient outmigration in an accountable care organization (ACO) that has downside financial risk. The defined/required output might be quantifying the ACO’s financial risk, delineating the clinician(s) who refer out as a matter of course, and identifying the clinic location(s) those referrals leave from and go to. This is a specific use case with a defined outcome/goal that is actionable. .Arguably in healthcare the “output” matters considerably more than in a widget manufacturing facility. In addition, according to an IDC survey, one in four companies see an almost 50% failure rate of their AI initiatives.[iii] .AI will continue to grow in use and value in healthcare. Whether it’s in predictive analytics for disease states, cash flow on the revenue cycle side of the business, or value-based care initiatives, AI is here to stay. However, success factors for the growth of AI in healthcare may include, but not be limited to:",Ramifications exist if a cardiology AI protocol does not have the right inputs and parameters.
74,"Google researchers made headlines early this month for a study that claimed their artificial intelligence system could outperform human experts at finding breast cancers on mammograms. It sounded like a big win, and yet another example of how AI will soon transform health care: More cancers found! Fewer false positives! A better, cheaper way to provide high-quality medical care!.Hold on to your exclamation points. Machine-enabled health care may bring us many benefits in the years to come, but those will be contingent on the ways in which it’s used. If doctors ask the wrong questions to begin with—if they put AI to work pursuing faulty premises—then the technology will be a bust. It could even serve to amplify our earlier mistakes..In a sense, that’s what happened with the recent Google paper. It’s trying to replicate, and then exceed, human performance on what is at its core a deeply flawed medical intervention. In case you haven’t been following the decades-long controversy over cancer screening, it boils down to this: When you subject symptom-free people to mammograms and the like, you’ll end up finding a lot of things that look like cancer but will never threaten anyone’s life. As the science of cancer biology has advanced and screening has become widespread, researchers have learned that not every tumor is destined to become deadly. In fact, many people harbor indolent forms of cancer that do not actually pose a risk to their health. Unfortunately, standard screening tests have proven most adept at finding precisely the latter—the slower-growing ones that would better be ignored..This might not be so bad, in theory. When a screening test uncovers harmless cancer, you can just ignore it, right? The problem is, it’s almost impossible to know at the time of screening whether any particular lesion will end up dangerous or no big deal. In practice, most doctors are inclined to treat any cancer that’s discovered as a potential threat, and the question of whether or not mammograms actually save lives is a matter of intense debate. Some studies suggest they do, others find that they don’t, but even if we take the rosiest interpretations of the literature at face value, the number of lives saved by this massive, widespread intervention is small. Some researchers have even calculated that mammography is, in balance, bad for patients’ health; i.e. that its aggregate harms, in terms of the excess treatment it inspires and the tumors brought on by its radiation, outweigh any benefits..In other words, AI systems like the one from Google promise to combine humans and machines in order to facilitate cancer diagnosis, but they also have the potential to worsen pre-existing problems such as overtesting, overdiagnosis, and overtreatment. It’s not even clear whether the improvements in false-positive and false-negative rates reported this month would apply in real-world settings. The Google study found that AI performed better than radiologists who were not specifically trained in examining mammograms. Would it come out on top against a team of more specialized experts? It’s hard to say without a trial. Furthermore, most of the images assessed in the study were created with imaging devices made by a single company. It remains to be seen whether these results would generalize to images from other machines..The problem goes beyond just breast-cancer screening. Part of the appeal of AI is that it can scan through reams of familiar data, and pick out variables that we never realized were important. In principle, that power could help us to diagnose any early-stage disease, in the same way the subtle squiggles of a seismograph can give us early warnings of an earthquake. (AI helps there, too, by the way.) But sometimes those hidden variables really aren’t important. For instance, your data set might be drawing from a cancer screening clinic that is only open for lung cancer tests on Fridays. As a result, an AI algorithm could decide that scans taken on Fridays are more likely to be lung cancer. That trivial relationship would then get baked into the formula for making further diagnoses..Even when they’re accurate, early diagnoses of disease may not always be a boon. Other recent medical AI projects have focused on early detection of Alzheimer’s and autism, two conditions where faster detection probably won’t change a patient’s outcome much anyway. These are gee-whiz opportunities to showcase how an algorithm can learn to identify characteristics we teach it to find, but they don’t represent advancements that will make a difference in patients’ lives..Some uses of algorithms and machine learning may also introduce new and perplexing problems for clinicians. Consider the Apple watch’s feature to detect atrial fibrillation, a type of heart arrhythmia that’s a risk factor for stroke. Atrial fibrillation is treated with blood thinners, which have side effects that can turn a minor fall into a life-threatening injury. If you’re truly in danger of having a stroke, that’s a risk worth taking. What about people whose atrial fibrillation was picked up by their smartwatch, though? Traditionally, the condition is diagnosed when someone comes into the doctor complaining of symptoms; now Apple monitors healthy people without symptoms and finds new cases that may have never shown up in a clinic. It’s not clear whether this group of patients would see the same net benefit from treatment..“We don’t actually know that these two populations of people are the same,” says Venkatesh Murthy, a cardiologist at Frankel Cardiovascular Center in Ann Arbor, Michigan. The more fruitful approach would be to use AI to identify the people who get the most benefit from the available treatments..If AI is going to prove truly revolutionary, it will need to do more than just reinstate the status quo in medicine; and before any such approach is adopted, it’s important to address a pair of fundamental questions: What problem is the technology trying to address, and how will it improve patient outcomes? It may take some time to find the necessary answers..That’s why the famous Mark Zuckerberg motto, “Move fast and break things” might be fine for Facebook, but it’s not great for medicine, AI-assisted or not. According to Vinay Prasad, author of Ending Medical Reversal and a hematologist-oncologist at the Oregon Health & Science University School of Medicine, the Silicon Valley mindset can be dangerous for clinicians. It’s that kind of attitude—when lives are at stake we need to implement promising new ideas as quickly as possible—that got us into this cancer-screening mess in the first place. Mammography was adopted before all the evidence was in, Prasad says, and once a medical practice has become standard, it’s very difficult to phase it out. “In a culture that’s used to immediacy and inflated claims, it’s difficult to have humility and patience.”","AI can amplify medical mistakes by overtesting, overdiagnosis, and overtreatment"
76,"Google created an artificial intelligence algorithm in 2016 meant to monitor and prevent hate speech on social media platforms and websites. However, the machine learning tool may be having the opposite outcome as it seems to be biased against black people..In order for the algorithm to search for hate speech, developers taught the tool to go through a database of over 100,000 tweets that were labeled toxic by Googles API called Perspective. The machine learning tool was used to flag content from healthy to toxic. Perspective defines toxic as “a rude, disrespectful, or unreason­able comment that is likely to make you leave a discussion.” From this learning, the algorithm was able to evaluate new content being produced on the scale of how toxic it would be perceived. .Recently, a group of researchers at the University of Washington discovered that the tool was profiling tweets posted by African-Americans as hate speech. The algorithm had a high rate of flagging content posted by African-Americans on social media, such as twitter, as toxic content when most of the language and content in those tweets were not harmful..The study was lead by Maarten Sap, a Ph.D. student at the university, and found that tweets were written in African-American Vernacular English (AAVE) were often flagged as offensive and therefore labeled as hate speech. This made the algorithm grow to be inherently biased towards African-Americans. When the team tested the algorithm against 5.4 million tweets, they found that the tool was twice as likely to flag posts by those who identified in the study as African-American in the database as toxic speech than those who it identified as other races..According to the study, the use of the n-word online used by African-Americans was flagged even though its use is culturally more acceptable and a term often used in AAVE as a non-hate speech by other African- Americans. However, there are instances where the n-word is used in hateful terms and the algorithm is currently unable to tell the difference at this time..Bad data can contain implicit racial, gender, or ideological biases. Many AI systems will continue to be trained using bad data, making this an ongoing problem. But we believe that bias can be tamed and that the AI systems that will tackle bias will be the most successful, said the report..Time will tell how Google will shift its algorithm to be less biased against AAVE. There will need to be some monitoring of the systems to ensure there is no bias against different cultures of speech when monitoring hate speech online..A crucial principle, for both humans and machines, is to avoid bias and therefore prevent discrimination. Bias in [the] AI system mainly occurs in the data or in the algorithmic model. As we work to develop AI systems we can trust, it’s critical to develop and train these systems with data that is unbiased and to develop algorithms that can be easily explained.",This tool profiled tweets by African-Americans as hate speech.
77,"The role of machine learning and its forecast of societal impacts have been oscillating between future haven of benevolent possibilities and exaggerated dystopia. The ethics of AI usage, the rationale behind research and its explainability have been one of the most spoken topics around AI these days..In an act that could cement the notion of unethical use of AI, Joe Redmon, creator of the popular YOLO computer vision algorithm has said in his recent Twitter post that he has quit research as he was concerned about the direction in which it was going..In his 2018 paper titled, YOLOv3, Redmon wrote about the implications of having a classifier such as the YOLO. “If humans have a hard time telling the difference, how much does it matter? ” wrote Redmon.In his paper, which is a part satire part research ode that reeks of his distaste for potential misuse of research, Redmon took jabs at Google and Facebook and even the organisation that funds his research..On a more serious note, he also insisted on the responsibility of the computer vision researchers to consider the harm our work might be doing and think of ways to mitigate it. .One big complaint that people have against Redmon’s decision is that experts shouldn’t quit. Instead, they should take the responsibility of creating awareness about the pitfalls of AI. .Kevin Zakka, a Google intern, responded to Redmon’s tweet by saying that rather than abandoning his research out of fear of potential misuse, Redmon might have used his respected position in the CV community to raise awareness..I think one shouldn’t have to quit doing what they love because of potential misuse. Au contraire, you should use your impact in that area to raise awareness and apply your research to good causes. (e.g desmond doss).Zakka’s response though resonates with most people, Redmon’s tweet, on the contrary, has created more awareness with his ‘i quit’ tweet than he would have done by explaining the ill effects of AI usage..The impetus behind this whole ordeal was provided by the NeurIPS (a prestigious conference) decision to have the researchers include the role of research and societal impacts during submissions. This itself stirred debate amongst practitioners about the uncertainty of research and how a researcher would have a broader or to say the least, a futuristic perspective on how certain research would impact the populace..Oh. Well, the other people heavily funding vision research are the military and they’ve never done anything horrible like killing lots of people with new technology oh wait..Redmon’s objection with inappropriate usage of his work might alarm policymakers into beckoning a new form of research regulations that can further snowball into an undesirable AI winter. However, the ethical dilemmas surrounding research are not new. .This date back to the discovery of the atomic bomb when those who were responsible for the research such as Robert Oppenheimer were documented regretfully quoting something as eerie as “I am become death, the destroyer of the worlds.” If one group argued against the bomb, the others saw the bright side of having ended the war. .So, as the dust settles down and topics such as fairness, reliability and accountability around AI get more attention, one way or the other, the lawmakers and researchers will be forced to find a sweet spot between unbridled innovation and haphazard regulations..In this article, we will go through an overview of each of the popular image reconstruction techniques and will understand how these techniques work..Kubric is an open-source Python framework that allows you to create photo-realistic scenes by combining the functions of PyBullet and Blender.. Object detection forms the foundation of many other downstream computer vision tasks, such as image segmentation, image captions, object tracking, and more. .Vision transformer (ViT) is a transformer used in the field of computer vision that works based on the working nature of the transformers used in the field of natural language processing. Internally, the transformer learns by measuring the relationship between input token pairs. In computer vision, we can use the patches of images as the token..They have applied it separately to speech, text and images where it outperformed the previous best single-purpose algorithms for computer vision and speech..Tulsee Doshi, Google Head of Product – Responsible AI & ML Fairness talks about the concerns that surround the topic of AI and ML fairness and breaks the myths that often come with this controversial issue.","Due to the possible military applications of YOLO computer vision algorithm, Joe Redmon stops his research."
78,"Humans talk funny. We invent words. We smash words together, tear them apart, abbreviate them one way, then another. Which is great and fun, if youre a human. Not so great if you are a machine or the kind of human who programs machines to understand language..And so, when IBMs famous artificial intelligence, Watson, he/she/it of Jeopardy-winning fame, was in development, its head researcher had a great idea. Humans created this repository of slang, The Urban Dictionary. For example, today on the site, we learn that healthy gas is the gas (fart) produced from a person who has eaten healthy foods like cabbage, beans, broccolli, grains, or other high fiber, high carbohydrate foods..Brown realized that this formalization of informal language might be a great way for Watson to understand the way real people communicate. So, he and his team, fed the whole thing into their AI..But one problem. Informal language has a tendency to be dirty, nasty language. Its insults and cuss words, new names for gross old things, old names for gross new things, etc. And so, we learn from Fortunes Michal Lev-Ram, they had to delete all that human messiness from Watsons memory..Watson couldnt distinguish between polite language and profanity -- which the Urban Dictionary is full of. Watson picked up some bad habits from reading Wikipedia as well. In tests it even used the word bullshit in an answer to a researchers query.",IBM Watson is unable to distinguish between polite and rude words.
79,"Nagpal, who lives in Göttingen, Germany, had been offered a premed place and scholarship at NYU. Her acceptance was dependent on her results in the International Baccalaureate diploma, a two-year high school program recognized by colleges and taken by more than 170,000 students this year, most in the US. But she scored more poorly than expected..Teen regrets about grades aren’t unusual, but the way the foundation behind the IB Diploma Programme calculated this year’s grades was. The results, released Monday, were determined by a formula that IB, the foundation behind the program, hastily deployed after canceling its usual springtime exams due to Covid-19. The system used signals including a student’s grades on assignments and grades from past grads at their school to predict what they would have scored had the pandemic not prevented in-person tests..Nagpal and many other students, parents, and teachers say those predictions misfired. Many students received suspiciously low scores, they say, shattering their plans for the fall and beyond. Nagpals backup plan if she missed out on NYU was to study medicine in Germany, but she doesnt think her lower-than expected grades will qualify her for a place. Like so many, I was extremely shocked, she says. Nagpal later received an email from NYU saying it has not made a decision on her admission. NYU said it does not comment on individual cases..More than 15,000 parents, students, and teachers have signed an online petition asking IB to “take a different approach with their grading algorithm and to make it fairer.” The foundation declined to answer questions about its system but said it had been checked against five years of past results and that disappointed students could use its existing appeals process, which comes with a fee. The foundation released summary statistics showing that this year’s average score was slightly higher than last year’s, and it says the distribution of grades was similar..One math teacher at a school in the Middle East says IB should disclose the full workings of its model for outside scrutiny. He and a colleague with a math PhD have been puzzling over its design since several students lost scholarships to top universities, after receiving results much lower than expected by their teachers. Some students caught out are now unsure how they’ll pay for college. “My only guess is a flawed model,” he says..Concerns about flawed math models are growing as more companies and governments apply computers to traditionally human problems such as bail decisions, identifying criminal suspects, and deciding what is hate speech. Rooting out bias and inaccuracy in such systems is a growing field of activism and academia..People questioning IB’s algorithm-derived grades are now raising some of the same issues. They’re wondering how the system was designed and tested, why its workings weren’t fully disclosed, and whether it makes sense to use a formula to determine the grades that can shape a person’s opportunities in life..When Covid-19 seized hold of the world in March, many teens in their final year of high school were left in a precarious position. Shelter-in-place orders made it challenging or impossible to complete the final assignments or tests that could determine their college and life choices..Test providers scrambled to devise new ways to assess students. In the US, Educational Testing Service, which provides the GRE, and the College Board, which runs AP Exams, moved their tests online. That brought quirks and glitches—like requiring students to take their tests simultaneously regardless of time zone and retakes forced by technical errors—but it maintained a semblance of the normal process..IB, headquartered in Geneva, opted to use a statistical formula instead—adding to the growing list of tech fixes proposed to automate away fallout from the pandemic. The workings of the IB diploma—and the timing of the results—proved particularly harmful for IB students applying to US colleges. Unlike AP tests, which are typically separate from high school grades, the IB results are intended to reflect a student’s work for the year. IB students are often granted college admission based on predicted grades, and they submit their final results when they become available over the summer. Some colleges, including NYU and Northeastern, warn on their admissions pages that students whose IB results don’t get close enough to those predictions may lose their place..In normal times, IB diploma students select six subjects, from options such as physics and philosophy, and receive final grades determined in part by assignments but mostly by written tests administered in the spring. The program is offered by nearly 900 public schools in the US and is common in international schools around the world. In March, IB canceled all tests and said it would calculate each student’s final grades using a method developed by an unnamed educational organization that specializes in data analysis..The idea was to use prior patterns to infer what a student would have scored in a 2020 not dominated by a deadly pandemic. IB did not disclose details of the methodology but said grades would be calculated based on a student’s assignment scores, predicted grades, and historical IB results from their school. The foundation said grade boundaries were set to reflect the challenges of remote learning during a pandemic. For schools where historical data was lacking, predictions would build on data pooled from other schools instead..In a video IB posted about the process, Antony Furlong, the foundation’s manager for assessment research and design, said the system essentially created “a bespoke equation” for every school..One visual arts teacher at a US school says what she and coworkers have seen suggests it wasn’t well tailored. “When I saw the marks, I was floored,” she says. “I am always conservative in my predicted grades, but every single student except one were downgraded.” Of 15 students she works with, four have to rethink their plans for this fall, because they missed out on college places, something she didn’t expect for any of them..Determining whether IB’s system had flaws is challenging without knowing its formula or the inputs and outputs. Just because some humans don’t like the outputs of a data analysis doesn’t mean that it’s incorrect. But Suresh Venkatasubramanian, a professor at the University of Utah who studies the social consequences of automated decisionmaking, says it appears IB could have deployed its system more responsibly. “All this points to what happens when you try to install some sort of automated process without transparency,” he says. “The burden of proof should be on the system to justify its existence.”.Data analysis is more powerful than ever but remains far from being able to predict complex future human actions. Models that extrapolate from past statistical trends can end up treating people unfairly because their circumstances are different, even if results match past patterns on average..Venkatasubramanian says that basing a student’s grades on past trends at their school, potentially unrelated to the student’s own school career, could be unfair. Using data from other schools—as IB did for schools with little track record—is a “red flag,” he says, because it would mean some students’ grades were calculated differently than others..Constance Lavergne, whose son in the UK received lower-than-expected IB grades and missed out on his preferred college, is one of many parents struggling to understand what happened. She says her experience working closely with data analysts in the tech industry makes her suspicious of IB’s methodology. It would naturally generate noisier results for smaller classes, like her son’s, because they offer fewer past data points, she suggests. “There’s something wrong with the algorithm,” Lavergne says..The math teacher in the Middle East said he believed his school had suffered because of how IB announced and calibrated its model. Students at the school submitted their assignments before IB said those assignments would help steer the grading model. Some IB students at other schools had not yet submitted those assignments, allowing them to put in extra effort, aided by knowing they didn’t have to prepare for exams. This weekend, he plans to work with his math PhD colleague and a software package to probe where the IB formula may have gone wrong..Many students who received disappointing results are now looking to November, when IB typically offers a second round of in-person tests and they can take the written test that was canceled. Nagpal, the frustrated medical student, intends to take part, at a cost of about €700 ($791). If Covid-19 disrupts those tests too, she hopes IB will move them online rather than try any more experiments in data-led grading.",The International Baccalaureate program used a math model to predict test scores for students which has caused students to lose scholarships
80,"The high-profile case of a Black man wrongly arrested earlier this year wasn’t the first misidentification linked to controversial facial recognition technology used by Detroit Police, the Free Press has learned. .Last year, a 25-year-old Detroit man was wrongly accused of a felony for supposedly reaching into a teacher’s vehicle, grabbing a cellphone and throwing it, cracking the screen and breaking the case..It identified Michael Oliver as an investigative lead. After that hit, the teacher who had his phone snatched from his hands identified Oliver in a photo lineup as the person responsible. .Controversy over law enforcement using facial recognition technology is not new, nor is it confined to the city of Detroit. But recent uprisings around the country in response to racial injustice in the wake of the death of George Floyd have again brought criticism of the technology to the forefront..In Detroit, where police started using facial recognition software as an investigative tool in 2017, protesters have demanded the city stop using it, saying the error rate is high when used to identify people of color. City Council, which will consider extending a software contract to help pay for it, has been urged to vote no by some residents. Detroits civilian Board of Police Commissioners also has been discussing the departments use of technology..In the cellphone case, according to transcribed testimony, the teacher called 911 as he watched a group of students fighting. One student had a baseball bat and others were wrestling on the ground. The teacher used his cellphone to video record the incident. The phone was recording when a young man reached into the teachers car and snatched the phone..Oliver has tattoos up and down his arms. Those markings weren’t visible on the person captured on video. Oliver’s attorney, Patrick Nyenhuis, also noticed differences in the hair style and body type between the person in the video and his client, he said..He took his concerns and pictures of his client to Wayne County Assistant Prosecutor Brian Surma, a supervisor in the office. Surma and the teacher reviewed photographs, determined Oliver was misidentified and both agreed the case should be dismissed immediately, a court transcript shows..Oliver, now 26, said he was nervous as his case proceeded last year because people still get convicted for crimes they don’t commit. He questioned how his face ever got connected to the case..During the investigation, police captured an image from the cellphone video, sent it for facial recognition and the photo came back to Oliver, the police report said..A second person, a student, was also captured in the video with the suspect. The officer in charge of the case testified he didn’t interview that person though hed been given that student’s name..Police investigated Olivers case prior to a new policy governing the use of facial recognition software. It includes stricter rules on when Detroit police can use it. The technology is now used only as a tool to help solve violent felonies, Detroit police have said..Evidence in Olivers case wasnt reviewed by a supervisor in the prosecutors office prior to him being charged, spokeswoman Maria Miller said in an email. Current protocol requires a supervisor review all evidence in a facial recognition case prior to a charging decision. There also must be other evidence that corroborates the allegations in order to charge someone..The prosecutors office is taking additional steps, Miller said. It will be required that facial recognition cases be submitted to Prosecutor Kym Worthy — the highest-ranking person in the office — for approval if an assistant prosecuting attorney and supervisor determine charges should be authorized..Miller said the prosecutors office knows of no other cases in which people charged with crimes were misidentified, other than that of Oliver and Robert Williams.  .Williams is a Farmington Hills man arrested in front of his family in January and accused of stealing high-end watches. Prosecutors and police have apologized for how that case was handled..The case generated headlines across the country, including in the Free Press, the New York Times and the Washington Post. While in custody, Williams said he told police he wasnt the man seen in a blurry image from store surveillance video..“As a result of these two cases, we have a more stringent protocol in facial recognition cases, Worthy said in an statement. The cases will be reviewed during the warrant charging phase, prior to the preliminary examination, and again when the case is bound over to the Circuit Court in any case where facial recognition has been used as an investigative tool.”.In the summer of 2019, the Detroit Police Department asked me personally to adopt their Facial Recognition Policy, she said. I declined and cited studies regarding the unreliability of the software, especially as it relates to people of color..Dan Korobkin, ACLU of Michigan legal director, called the technology dangerous in a statement Friday. He urged police and prosecutors across the country to review cases involving the use of the technology and notify all people charged as a result of it..Studies have shown the technology, relying on computer algorithms, sometimes has trouble distinguishing human faces, especially with people of color..Detroit Police Chief James Craig, who is Black, has said he is a strong believer in facial recognition software. Last summer, he said police had used the technology about 500 times then moved on to the next phase of investigation only 30% of the time..The Detroit police commission discussed Williams case during a meeting Thursday afternoon. Police gave a presentation and Craig said the situation should not have happened..Last month, board member Evette Griffie sought answers from police about Williams case, including a timeline of events and any discipline resulting from the misidentification.",The facial recognition software used by the Detroit Police Department misclassified a man and charged him with larceny.
81,"The internet might seem like a level playing field, but it isn’t. Safiya Umoja Noble came face to face with that fact one day when she used Google’s search engine to look for subjects her nieces might find interesting. She entered the term “black girls” and came back with pages dominated by pornography. .Noble, a USC Annenberg communications professor, was horrified but not surprised. For years she has been arguing that the values of the web reflect its builders—mostly white, Western men—and do not represent minorities and women. Her latest book, Algorithms of Oppression, details research she started after that fateful Google search, and it explores the hidden structures that shape how we get information through the internet..The book, out this month, argues that search engine algorithms aren’t as neutral as Google would like you to think. Algorithms promote some results above others, and even a seemingly neutral piece of code can reflect society’s biases. What’s more, without any insight into how the algorithms work or what the broader context is, searches can unfairly shape the discussion of a topic like black girls..Noble spoke to MIT Technology Review about the problems inherent with the current system, how Google could do better, and how artificial intelligence might make things worse..If we’re looking for the closest Starbucks, a specific quote, or something very narrow that is easily understood, it works fine. But when we start getting into more complicated concepts around identity, around knowledge, this is where search engines start to fail us. This wouldn’t be so much of a problem except that the public really relies upon search engines to give them what they think will be the truth, or something vetted, or something that’s credible. This is where, I think, we have the greatest misunderstanding in the public about what search engines are..We could think about pulling back on such an ambitious project of organizing all the world’s knowledge, or we could reframe and say, “This is a technology that is imperfect. It is manipulatable. We’re going to show you how it’s being manipulated. We’re going to make those kinds of dimensions of our product more transparent so that you know the deeply subjective nature of the output.” Instead, the position for many companies—not just Google—is that [they are] providing something that you can trust, and that you can count on, and this is where it becomes quite difficult..Ive been arguing that artificial intelligence, or automated decision-making systems, will become a human rights issue this century. I strongly believe that, because machine-learning algorithms and projects are using data that is already biased, incomplete, flawed, and [we are] teaching machines how to make decisions based on that information. We know [that’s] going to lead to a variety of disparate outcomes. Let me just add that AI will be harder and harder to intervene upon because it will become less clear what data has been used to inform the making of that AI, or the making of those systems. There are many different kinds of data sets, for example, that are not standardized, that are coalescing to make decisions..Since I started writing about and speaking publicly about black girls in particular being associated with pornography, things have changed. Now the pornography and hypersexualized content is not on the first page, so I think that was a quiet improvement that didn’t come about with a lot of fanfare. But other communities, like Latina and Asian girls, are still highly sexualized in search results. .An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.","Google's Search engine shows pornography when a use enters black girls.Although this specific error has been fixed, Latina and Asian girls are still sexualized in results. Furthermore, a search for professional and unprofessional haircuts shows racial bias as well."
83,"Automated testing and analysis of company’s advertising system reveals male job seekers are shown far more adverts for high-paying executive jobs .The team of researchers from Carnegie Mellon built an automated testing rig called AdFisher that pretended to be a series of male and female job seekers. Their 17,370 fake profiles only visited jobseeker sites and were shown 600,000 adverts which the team tracked and analysed..The authors of the study wrote: “In particular, we found that males were shown ads encouraging the seeking of coaching services for high paying jobs more than females.” .One experiment showed that Google displayed adverts for a career coaching service for “$200k+” executive jobs 1,852 times to the male group and only 318 times to the female group. Another experiment, in July 2014, showed a similar trend but was not statistically significant..Google’s ad targeting system is complex, taking into account various factors of personal information, browsing history and internet activity. Critically the fake users started with completely fresh profiles and behaved in the same way, with gender being the only factor that was different and illustrating that the ad targeting for these job adverts was discriminatory..However, the authors of the study admit that the gender discrimination shown is difficult to pin to one factor, due to the complexity of not only Google’s profiling systems, but also of the way advertisers buy and target their adverts using Google..A Google spokeswoman said: “Advertisers can choose to target the audience they want to reach, and we have policies that guide the type of interest-based ads that are allowed.”.Profiling is inherently discriminatory, as it attempts to treat people differently based on their behaviour and personal information. While that customisation can be useful, showing more relevant ads to users, it can also have negative connotations..The study authors said: “Male candidates getting more encouragement to seek coaching services for high-paying jobs could further the current gender pay gap. Even if this decision was made solely for economic reasons, it would continue to be discrimination.”.Google allows users to opt out of behavioural advertising and provides a system to see why users were shown ads and to customise their ad settings. But the study suggests that there is a transparency and overt discrimination issue in the wider advertising landscape..Television, radio and print advertisers have, of course, been practising discrimination for years, pushing ads out with shows or magazines that appeal to a particular gender or demographic..The difference now is that it is much more obvious in the internet age, and the in-depth profiling that is now possible could make it worse, not better..The researchers also investigated whether visiting sites dealing with certain topics, specifically substance abuse, adult content, disabilities, mental disorders and infertility, affected the ads served to the fake profiles..Only visiting sites dealing with substance abuse and disability created statistically significant results. The researchers found that after visiting substance abuse sites Google’s advert profile page showed no change to the interests listed, but the adverts shown to the user accounts did change, including displaying ads for rehabilitation services from a company called Watershed. The adverts shown to the control group did not include any rehabilitation services..“One possible reason why Google served Watershed’s ads could be remarketing, a marketing strategy that encourages users to return to previously visited website,” said the authors of the study..The Watershed site was included in the top 100 substance abuse sites list, which was used as the experimental list of sites to visit by the automated system..A similar result was shown in testing for disability sites, using a similar methodology. This time the researchers found that Google’s ad interest profile did change for the test group, but that it showed other interests not related to disability..Ads for mobility devices including a standing wheelchair were shown to the test group 1,076 times but never to the control group. Again the adverts included sites within the top 100 sites concerning disability used during the experiment..Google has said that it prohibits the targeting of adverts within its “sensitive category policy”, which includes health issues such as substance abuse. It also says that does not allow remarketing within the same sensitive areas..The researchers also discovered that Google’s ad choices, which allows users to manually remove certain interests from the tracking profiles, had the effect that was desired.",A Carnegie Mellon team build AdFisher ( a system of male and female job seekers) and found males were shown ads to seek high paying jobs more than females.
85,"Artificial intelligence (AI) is increasingly deployed around us and may have large potential benefits. But there are growing concerns about the unethical use of AI. Professor Anthony Davison, who holds the Chair of Statistics at EPFL, and colleagues in the UK, have tackled these questions from a mathematical point of view, focusing on commercial AI that seek to maximize profits..One example is an insurance company using AI to find a strategy for deciding premiums for potential customers. The AI will choose from many potential strategies, some of which may be discriminatory or may otherwise misuse customer data in ways that later lead to severe penalties for the company.  Ideally, unethical strategies such as these would be removed from the space of potential strategies beforehand, but the AI does not have a moral sense, so it cannot distinguish between ethical and unethical strategies..In work published in Royal Society Open Science on 1 July 2020, Davison and his co-authors Heather Battey (Imperial College London), Nicholas Beale (Sciteb Limited) and Robert MacKay (University of Warwick), show that an AI is likely to pick an unethical strategy in many situations. They formulate their results as an Unethical Optimization Principle:.If an AI aims to maximize risk-adjusted return, then under mild conditions it is disproportionately likely to pick an unethical strategy unless the objective function allows sufficiently for this risk. .This principle can help risk managers, regulators or others to detect the unethical strategies that might be hidden in a large strategy space. In an ideal world one would configure the AI to avoid unethical strategies, but this may be impossible because they cannot be specified in advance. In order to guide the use of the AI, the article suggests how to estimate the proportion of unethical strategies and the distribution of the most profitable strategies..Our work can be used to help regulators, compliance staff and others to find problematic strategies that might be hidden in a large strategy space. Such a space can be expected to contain disproportionately many unethical strategies, inspection of which should show where problems are likely to arise and thus suggest how the AI search algorithm should be modified to avoid them, says Professor Davison. It also suggests that it may be necessary to re-think the way AI operates in very large strategy spaces, so that unethical outcomes are explicitly rejected during the learning process..Professor Wendy Hall of the University of Southampton, known worldwide for her work on the potential practical benefits and problems brought by AI, said: This is a really important paper. It shows that we cant just rely on AI systems to act ethically because their objectives seem ethically neutral. On the contrary, under mild conditions, an AI system will disproportionately find unethical solutions unless it is carefully designed to avoid them..The tremendous potential benefits of AI will only be realised properly if ethical behaviour is designed in from the ground up, taking account of this Unethical Optimisation Principle from a diverse set of perspectives. Encouragingly, this Principle can also be used to help find ethical problems with existing systems which can then be addressed by better design..Beale N, Battey H, Davison AC, MacKay RS. (2020) An unethical optimization principle. R. Soc. Open Sci. 7: 200462. DOI: 10.1098/rsos.200462 .Disclaimer: AAAS and EurekAlert! are not responsible for the accuracy of news releases posted to EurekAlert! by contributing institutions or for the use of any information through the EurekAlert system.",Researchers show that AI has an unethical optimization principle
87,"The algorithms that underlie most Silicon Valley bigwigs are usually locked up in the name of “protecting trade secrets,” but that’s an excuse that’s starting to wear a little thin in recent months. Maybe it’s because these companies are that much harder to regulate or compete against when we don’t ask them to reveal their respective secret sauces. Maybe it’s because these secret sauces control the incomes of a bigger and bigger chunk of small business owners and gig workers alike. Maybe it’s because they’re found to be biased in all sorts of ways, and hiding the specifics in the name of “trade secrets” only ends up hurting the people who use the platform. .All of these issues came to a head this week when four Uber drivers based in the UK filed suit against the company’s European branch in an Amsterdam District court on Monday, on the grounds that the company’s refusal to share driver data with, well, drivers, is technically a breach of European data protection laws. If this group can get Uber to cough up the data in question, it could offer an unprecedented window into the way the company algorithmically profiles both the EU-based and US-based drivers on its platform—unfairly or otherwise..To give a bit of background, the four drivers behind the suit are members of the App Drives & Couriers Union—or ADCU for short—a group that’s working with a nonprofit called the Worker Info Exchange in the hopes of collecting app data on drivers that can be used for, say, collective bargaining further down the line..According to the docket, all four of the drivers tried asking Uber for the data that was collected by their Uber Driver app, which doesn’t only collect data from, say, a given Uber ride, but collects data on everything from acceleration to location during every car trip the driver makes, even if it’s made without a passenger. In total, the docket describes more than 26 categories of data, which, on paper at least, is mostly crunched together to assess supply and demand, and figure out what the going rates should be for a given Uber ride in a given neighborhood on a given day. .But as the docket describes, some of this data is also used to profile the drivers in question. As the docket explains, Uber keeps a tightly guarded profile of each driver on the platform, and uses metrics like a driver’s arrival times and ride rating to make notes about their “level of professionalism” or “navigational skills.” When the four UK drivers behind the suit asked Uber to cough up this data—and more—expecting that the company would abide under Europe’s hefty data laws—none of them got any of this profiling data back. What’s worse, only two of them got any data back at all, which means Uber, in this case, is one of those tech companies that thinks it can get away with ignoring GDPR. And like those tech companies caught flouting GDPR, Uber can, in turn, be fined thousands and thousands of dollars until they right this particular wrong. .Level up your earbud gameApple has added premium features previously exclusive to its more luxurious AirPods Pro, such as spatial audio and water resistance..Until that happens, the ADCU has set up a monthlong fund for folks looking to support the case, racking up 345 pounds (roughly $436 USD) since the case went live in the AM. They’re also encouraging all Uber drivers and couriers for Uber Eats to file their own requests for data from the company, in the hopes that more voices—and more pressure on the company—might actually force them to make a change. But considering Uber’s standing track record of shitty behavior towards drivers, this Union might be facing an uphill battle, even if the local data-protective laws are on their side. ",Uber drivers take Uber to court for not sharing data and potentially unfairly profiling drivers
88,"CIA-linked software firm Palantir will help the UN’s World Food Programme analyse its data in a new partnership worth $45 million, both organisations announced Tuesday, drawing immediate flak from privacy and data protection activists..The California-based contractor, best known for its work in intelligence and immigration enforcement, will provide software and expertise to the UN’s food relief agency over five years to help WFP pool its enormous amounts of data and find cost-saving efficiencies..At a press conference in Geneva, WFP’s chief information officer Enrica Porcari said the plan was to launch a data integration effort that would include records of distributions to beneficiaries but, she stressed, not personally identifiable data. “Can all the data pour into one lake?” she asked, rhetorically. The system would then, she explained, work like a bank whose algorithms flag unusual credit card activity, picking up “anomalies” in beneficiary locations and behaviour that might signal misuse..WFP is jumping headlong into something they don’t understand, without thinking through the consequences, and the UN has put no frameworks in place to regulate it..In future, if multiple aid agencies connected to the WFP’s SCOPE beneficiary management system and used it as the basis for recording what people received, a powerful overview could be achieved, Porcari said..Palantir executive vice president Josh Harris said WFP’s 92 million aid recipient “customers”, its more than 30 data systems, and its difficult operating environment represented a “complex data landscape”, but something his company’s software was built for. The opportunity to provide support to WFP is a “dream combination” that fits “mission-driven” Palantir’s philanthropic goals, Harris added..Palantir has already worked with WFP on a pilot project on food procurement in Iraq that has produced over $26 million (or about 10 percent) in savings, the two organisations said..Privacy and data protection activists cried foul at the new tie-up, questioning if WFP understood what it was getting itself into and if proper safeguards had been put in place..“The recipients of WFP aid are already in extremely vulnerable situations; they should not be put at additional risk of harm or exploitation,” a spokesperson for activist NGO Privacy International told IRIN. “This data is highly sensitive, and it is essential that proper protections are put in place, to limit the data gathered, transferred, and processed.”.Asked for the legal basis for any data-sharing with Palantir, Porcari said: “there is no data-sharing”. She insisted that all data instead would rest under WFP’s control, with personal data being kept separate and secure..But Privacy International, which recently analysed the (unintended) risks of humanitarian data misuse, warned: “Weve seen examples of systems that are produced in agreements such as the one between WFP and Palantir increasing risks to the people the systems are aiming to benefit. There are risks to both individuals and whole populations from the gathering and processing of data from humanitarian activities.”.A humanitarian data analyst, who requested anonymity due to work relationships, was also alarmed at the news, saying: WFP is jumping headlong into something they don’t understand, without thinking through the consequences, and the UN has put no frameworks in place to regulate it..Palantir was established with the help of seed capital from a CIA-linked investment body. Its main clients have been US security and intelligence bodies..Its capacity to structure and overlay vast datasets has led it to be credited with helping the US government to find Osama bin Laden. However, its work with US police and, most recently, immigration enforcement, has come under fire for secrecy, profiling bias, enabling human rights violations, and the wholesale harvesting of personal data..“It is the height of irony that the very company that faced direct criticism in its role facilitating US immigration authorities human rights abuses is now promoting itself as trustworthy of working in humanitarian aid,” the Privacy International spokesperson said..From the 2013 Haiyan super-typhoon in the Philippines to the 2014-2016 Ebola outbreak in West Africa, Palantir has often sought opportunities to deploy its technology in the humanitarian arena..Few analysts contacted by IRIN in this 2016 investigation doubted the software had powerful potential, but reputational concerns made a number of potential partners walk away, even when offered free access to Palantir products and software advisors..“It is the height of irony that the very company that faced direct criticism in its role facilitating US immigration authorities human rights abuses is now promoting itself as trustworthy of working in humanitarian aid.”.Nevertheless, Palantir’s pro-bono “Philanthropy Engineering” has provided support to numerous non-profits, including the Carter Center, Team Rubicon, the Enough Project, and the Rockefeller Foundation. The UN’s nuclear watchdog, the International Atomic Energy Agency (IAEA), is a paying customer..WFP has been working with Palantir since 2017 – a WFP spokesperson said an encounter at the World Economic Forum in 2015 kicked off the relationship. Gaining greater insight from a mountain of internal and external data with the help of Palantir’s Foundry system has already led to cost savings and efficiencies, according to the Rome-based UN agency..A starting point for the Palantir work inside WFP was Optimus. According to a recent update from WFP, Optimus is an internal tool to help guide purchasing and other planning decisions, for example in Ethiopia or Yemen, to assign different commodities to make up a mixed basket of food for distribution depending on funding and seasonal market prices.  .Poncari described WFP as being on a “very aggressive digital transformation journey” and said it had a “moral imperative” to leverage technology to achieve efficiencies. “We just want to go with the best,” she told reporters.. Support our journalism and become more involved in our community. Help us deliver informative, accessible, independent journalism that you can trust and provides accountability to the millions of people affected by crises worldwide. ",UN shares highly sensitive data with Palantir which adds risk of harm and exploitation
90,"Terror capitalism uses tools such as facial recognition to extract profits from marginalized people. Big tech and governments are collaborating.When Gulzira Aeulkhan finally fled China for Kazakhstan early last year, she still suffered debilitating headaches and nausea. She didn’t know if this was a result of the guards at an internment camp hitting her in the head with an electric baton for spending more than two minutes on the toilet, or from the enforced starvation diet..Maybe it was simply the horror she had witnessed – the sounds of women screaming when they were beaten, their silence when they returned to the cell..Like an estimated 1.5 million other Turkic Muslims, Gulzira had been interned in a “re-education camp” in north-west China. After discovering that she had watched a Turkish TV show in which some of the actors wore hijabs, Chinese police had accused her of “extremism” and said she was “infected by the virus” of Islamism. They predicted it would lead her to commit acts of terrorism, so they locked her away..Gulzira’s detention lasted for more than a year. She was released in October 2018, only to be told that she had been assigned to work in a glove factory near the camp. After long hours behind a sewing machine, Gulzira was kept in a dormitory surrounded by security checkpoints that used facial-recognition technology to track her movements and scraped data from her smartphone, which she was required to carry at all times. She was paid $50 a month, roughly one-sixth of the legal minimum wage in the region..Gulzira was one of the millions of targets of a global phenomenon we call “terror capitalism”. Terror capitalism justifies the exploitation of subjugated populations by defining them as potential terrorists or security threats. It primarily generates profits in three interconnected ways. First, lucrative state contracts are given to private corporations to build and deploy policing technologies that surveil and manage target groups. Then, using the vast amounts of biometric and social media data extracted from those groups, the private companies improve their technologies and sell retail versions of them to other states and institutions, such as schools. Finally, all this turns the target groups into a ready source of cheap labor – either through direct coercion or indirectly through stigma..The people being targeted by terror capitalism include entire stateless groups, such as ethnic Bengalis in north-east India and Palestinian Arabs. They are almost always from minority or refugee populations, especially Muslim ones. While the Chinese system is unique in terms of its scale and the depth of its cruelty, terror capitalism is an American invention, and it has taken root around the world..In China, the terror capitalism system targets many of the more than 15 million Uighurs and other Turkic Muslims in the region. Coercing these people into low-wage work means fewer Chinese jobs move to Vietnam and Bangladesh. The companies that have developed China’s policing technologies are now selling them to police units and regional governments in Zimbabwe, Dubai, Kuala Lumpur, the Philippines and many other locations..Meanwhile, across Europe and North America, terror-capital surveillance tools have placed hundreds of thousands of Muslims on watchlists as part of Countering Violent Extremism programs. In the United States, immigration control measures taken in the aftermath of 9/11 have paved the way for a system that monitors and controls asylum seekers who enter the country at the southern border..These systems use GPS tracking to control people in ways that are similar to the surveillance system in north-west China. After being held in a US detention center, a Pakistani asylum seeker named Asma was required to wear a GPS-enabled ankle monitor. (More than 32,000 foreign nationals in the US have to do the same.) Asma told us how the stigma associated with her ankle monitor pushed her into low-paid work. A food-truck owner in Austin, Texas, gave her a job because, he told her, the customers couldn’t see the monitor if she stayed behind the counter in the truck. But when the monitor started blaring, “Recharge battery! Battery low! Recharge battery! Battery low!” during her shift one day, her boss fired her. A year after Asma obtained asylum, her ankle is covered with scars from the monitor and she often feels phantom vibrations in her leg..Ankle monitors are still prevalent, but in the past two years, US asylum seekers have also increasingly been made to submit to weekly “biometric check-ins”, through an app called SmartLink, which they are made to install on their phones. They have to keep their phones charged and GPS active at all times. Lorena, an asylum seeker who has fled violence in Guatemala, was initially relieved that her ankle monitor was removed, until she was required to give Immigrations and Customs Enforcement (Ice) access to her email account, which connects to her social media accounts..The SmartLink app is now being used to control 21,712 immigrants. It was developed by BI Incorporated, a company that initially designed GPS ankle monitors. BI is now a subsidiary of Geo Group, one of the major prison and detention companies that have profited from a dramatic expansion of prisoner and detained populations in the US over the past four decades. The app is also supported by the nationwide telecom providers Sprint and Verizon, with movement tracking provided by Google Maps..The rationale for these new regimes of technological surveillance and control emerged directly from the “global war on terror” that the George W Bush administration declared in 2001. So too did early versions of many of the policing technologies now being used around the world. The new paradigm began in Iraq, where counter-insurgent war demanded a suspension of civil and human rights that allowed the US army to identify and track the movements, online activities, networks, and states of mind of masses of Iraqis. China eventually followed suit, with Xi Jinping justifying the technological subjugation of the country’s Muslims by declaring the “people’s war on terror” in 2014..“Surveillance is about controlling and disciplining marginalized people – whether it’s people of color, immigrants or poor people,” says a current employee of Microsoft, which was a key early-stage investor in AnyVision, an Israeli surveillance technology company that has used facial recognition to monitor Palestinians in the West Bank. “Companies use surveillance to discipline workers. Law enforcement uses surveillance to reinforce systemic racism and perpetuate mass incarceration. States use surveillance to enforce border logics and state oppression. Surveillance, as a concept, isn’t neutral – it is always about control.”.Because terror capitalism emerges at the nexus where the power of states such as the US and China meets the power of tech companies such as Microsoft, Google, Hikvision and SenseTime, fighting it will require not only an empowered public but also people within governments and companies to regulate and resist harmful forms of surveillance. There is a growing movement among tech workers, from Seattle to Hong Kong, who are appalled by the complicity of their employers in terror capitalism. Together with them, we must push these companies to refuse to profit from the militaristic ambitions of these states..Dr Darren Byler is a postdoctoral researcher at the University of Colorado and the author of two forthcoming books, one on the effects of terror capitalism among Uighurs and one on technologies of re-education in China and around the world.Dr Carolina Sanchez Boe is a Danish Research Foundation postdoctoral fellow. She is the author of a forthcoming book, The Undeported: The Making of a Floating Population of Exiles in France and Europe",SmartLink shares GPS active data of immigrants
93,"Following a Russian regulatory body’s demand that Tinder hands over user data to federal law enforcement agencies, online daters’ Saturday night “u up?” text — and much more —  could become the property of Russia’s nefarious government. Tinder should firmly refuse to comply with this dangerous and authoritarian policy..This month, Russia’s telecoms regulator, the Roskomnadzor, added Tinder to a list of websites and apps that it forces to store user data, messages and pictures on government-accessible Russian servers. The agency can then offer that data up to law enforcement and intelligence services on demand. If Tinder agrees to provide this information, its users in Russia will have no meaningful sense of privacy on the app..This data collection policy puts Tinder’s commitments in conflict. On one hand, users agree to a privacy policy that states that Tinder can share their information to “comply with a legal process.” On the other, the company has a commitment to its users and stakeholders. Its privacy web page directly says Tinder does not “compromise with your privacy.”.To say that putting customers’ intimate details in the hands of one of the world’s most illiberal regimes is a “compromise” would be the understatement of the century..But the stakes for Tinder go beyond protecting individual privacy. Sharing data with the Russian government would seriously endanger Tinder’s LGBTQ user base. It would be grossly, unjustifiably irresponsible for the brand to release information that could reveal swipers’ sexual preferences to a government with a record of open hostility to its LGBTQ community..In 2013, the federal government in Moscow passed legislation broadly banning gay “propaganda” directed at minors. In practice, this law has shut down LGBTQ health education and support services, contributed to a rise in homophobic violence and silenced LGBTQ rights organizations. More recently, the small Russian republic of Chechnya executed a violent purge of dozens of gay men in 2017. According to Human Rights Watch, these men were humiliated, starved and tortured in concentration camps. Some “disappeared.” Others were outed and returned to deeply homophobic families for even more brutal abuse..Just this year, reports emerged revealing a second wave of detentions. Chechnya’s leader, Ramzan Kadyrov, responded to the allegations by denying the existence of gay people in Chechnya. .If Tinder complies with the Russian government, officials like Kadyrov could potentially access data on users’ matches and app settings that would help them systematically track down, detain and torture more LGBTQ citizens. It would become easier for law enforcement to punish and harass users who spread gay “propaganda” on the app..The company’s compliance could fuel corruption and extortion across the board. In Russian politics, blackmail is a popular instrument for making tough political gains. If Tinder provides Russia’s secretive and manipulative intelligence agencies with broad user data, officials will take advantage of the raunchy messages they uncover to expose and destroy their political opponents. On a smaller scale, they could use their newfound wealth of information to enrich themselves by extorting Tinder users. Police have already been using similar tactics to extort gay men in Chechnya for years. .So far, Tinder has “registered to be compliant.” But it would be making an unforgivable mistake if it went through with this data-sharing arrangement. Maybe Tinder doesn’t care about the privacy of its users, but human lives are at stake here. The physical safety of its Russian users is now in the hands of its executives.",Russia demands Tinder to give user data to identify LBGTQ+
94," Share this via Facebook    Share this via Twitter    Share this via WhatsApp    Share this via Email    Other ways to share    Share this via LinkedIn    Share this via Reddit    Share this via Telegram    Share this via Printer   .(Nairobi, September 30, 2017)—Egypt should stop arresting and harassing people suspected of homosexuality using trumped-up “debauchery” and “inciting debauchery” charges, Human Rights Watch said today. Security forces rounded up at least eleven people in the days following a September 22, 2017 concert in Cairo at which young concertgoers waved rainbow flags, a symbol of solidarity with  lesbian, gay, bisexual or transgender (LGBT) people, a defiant act in a country known to persecute gay men and transgender people..After concertgoers shared photos of the rainbow flag display on social media, pro-government media went on an overdrive attack and conservative politicians and religious leaders demanded that the government take action. Police arrested one man on September 23 through entrapment on a dating app, a common police technique in Egypt, and claimed he had been among those to wave a flag. On September 25,  the government said that it had arrested seven people identified through video footage of the concert. Several Egyptian activists questioned the veracity of this claim, but they documented additional arrests on September 27, when police picked up six men from the streets, charging them with debauchery and claiming they were all involved in the rainbow flag incident.  .“Whether they were waving a rainbow flag, chatting on a dating app, or minding their own business in the streets, all these debauchery arrest victims should be immediately released,” said Sarah Leah Whitson, Middle East and North Africa director at Human Rights Watch. “The Egyptian government, by rounding people up based on their presumed sexual orientation, is showing flagrant disregard for their rights.”.The Dokki Misdemeanor Court in Giza sentenced the first victim on September 26 to six years in prison and a fine for “debauchery,” based on his presumed sexual conduct, and “inciting debauchery,” as prosecutors alleged he was among those who raised the rainbow flag at the concert. The court sentenced him to an additional six years of probation which will require reporting to the police from 6p.m. to 6a.m. until 2029. No lawyer was present at his trial.  He now has legal representation, and his appeal will be heard on October 11..The six men arrested on September 27 are scheduled for trial on October 1. At least two more men were arrested on September 28 because of their presumed sexual orientation, and Egyptian media reported that another six men were arrested on September 28 in a raid on a home, although Human Rights Watch has not independently verified that report..At the September 22 concert, people raised rainbow flags during the performance of the Lebanese group Mashrou’ Leila, which has an openly gay lead singer and has performed songs addressing same-sex relationships and gender identity. The Egyptian Musicians Syndicate opened an investigation into the event and banned future Mashrou’ Leila concerts in Egypt..In Egypt, police routinely round up gay and bisexual men and transgender women, actively seeking them out and entrapping them on dating apps and through social media. One Cairo-based organization has documented the prosecution of at least 34 people for consensual same-sex conduct in the last 12 months. Since President Abdel Fattah al-Sisi came into power in 2014, several hundred people have been imprisoned on charges of consensual same-sex conduct..Egyptian activists told Human Rights Watch they fear that the past week’s arrests could signal the beginning of an even harsher crackdown on LGBT people and those who publicly support them..Egypt’s Forensic Medicine Authority also routinely subjects people to forced anal examinations. The archaic technique was devised in the 19th century to seek “evidence” of homosexual conduct,  but forensic experts around the world have condemned the practice as lacking any scientific validity and violating medical ethics. The UN  special rapporteur on torture,  the UN Committee Against Torture, and the African Commission on Human and Peoples’ Rights have described the exams as a form of torture or ill-treatment, prohibited under international law. The Egyptian Medical Syndicate has taken no steps to prevent doctors from conducting these degrading exams..Egypt is a state party to the International Covenant on Civil and Political Rights, which protects the rights to privacy and to freedom of expression. Egypt’s constitution also protects these rights..“Egypt should stop dedicating state resources to hunting people down for what they allegedly do in their bedrooms, or for expressing themselves at a rock concert, and should instead focus energy on improving its dire human rights record,” Whitson said.. Share this via Facebook    Share this via Twitter    Share this via WhatsApp    Share this via Email    Other ways to share    Share this via LinkedIn    Share this via Reddit    Share this via Telegram    Share this via Printer   ",Police utilize entrapment techniques on dating apps and video footage from concerts
95,"As British prime minister, David Cameron visits Saudi Arabia today, activists report the plight of a man arrested by the religious police who may face corporal punishment..Activists are concerned for the safety of a 30-year-old man arrested by the religious police in Saudi Arabia for using Facebook to date other men.  The man, whose exact identity is not known, was arrested on 23 December (2011) but full details of the incident are only now becoming clear after a detailed investigation by Gay Middle East. Experts warn he may face blackmail and/or corporal punishment..He is being held in custody in the Dammam Police Department awaiting the Dammam’s General Attorney office for prosecution. The case has been reported to Amnesty International, while Facebook declined to comment..The report by Sabaq electronic journal mentions that a Saudi citizen reported an unnamed 30-year old man to the Religious police in Saudi Arabia, known as the Committee for the Propagation of Virtue and the Prevention of Vice, which proceeded to apprehend the man who finally confessed that “the Facebook profile is his and that he had been using it for obscenity acts with other men”. .KSA law is not strictly codified and its implementation, in either a lenient or severe manner, depends mostly on religious Sunni judges and scholars, as well as royal decrees (and thus subject to extreme variability).  Generally speaking punishments for homosexuality range from imprisonment and/or flogging to the death penalty. Conviction and severity of punishments depends on the social class, religion and citizenship of the accused, whereby non-western migrant workers receive usually harsher treatment than upper class Saudi citizens..Sami Hamwi, Syria Editor of Gay Middle East, and former Saudi resident explains: “Native born Saudi citizens who are Suni or from the Bedouin tribes in the country are often let off, while punishment are severely executed against minorities like Shiites and or newly naturalised citizens. Punishments regarding homosexuality are also held against expatriates working in Saudi Arabia, especially those coming from Asian, African and Arab countries. Dammam is a largely Shiite area and if the 30 year old aforementioned man is a Shiite, he is likely to be trailed and sentenced harshly.”.“We are aware of the reports and seeking further information. The UK opposes all discrimination against Lesbian, Gay, Bisexual and Transgender (LGBT) people in all circumstances. We are committed to combating violence and discrimination against LGBT people as an integral part of our international human rights work. We believe that human rights are universal and that LGBT people should be free to enjoy the rights and freedoms to which people of all nations are entitled.”.A spokesperson for Amnesty International said :”Amnesty International is seeking more information on this case. If the man reported in the Sabq story has been arrested and charged with homosexuality, Amnesty International would consider him to be a prisoner of conscience and call for his immediate and unconditional release. Saudi Arabia has sentenced people convicted of homosexuality and ‘sodomy’ to a range of penalties including corporal punishment and even the death penalty. The criminalization of homosexuality encourages the dehumanization of lesbians, gay men, bisexual people and transgender people (LGBT) as their very identity is criminalized.”.“Amnesty International considers the use of ‘sodomy’ laws to imprison (usually) men for same-sex relations in private to be a grave violation of human rights, including the rights to privacy, to freedom from discrimination, to freedom of expression and association, which are protected in the Universal Declaration of Human Rights.”.The Lesbian & Gay Foundation also voiced concerns: “It is extremely worrying to hear that that the Saudi police have entrapped this man when we know that Saudi-Arabia is one of the remaining countries in the world where homosexual acts are punishable at worst Death, but also by severe corporal punishment and imprisonment.”.“We understand that because of the very nature of the country’s Draconian anti LGBT legislations there exists, by necessity, an underground gay scene, and if people are discovered to have fallen foul of official prohibitions they risk such entrapment, jail and flogging.”.“The Lesbian & Gay Foundation would like to see the UK government do whatever it can to make sure that LGBT issues across the region are seen as a significant human rights problem and we would urge all those concerned to put pressure on authorities such as The British Foreign and Commonwealth Office, the US State Department and others to be vocal in their condemnation of such acts which ignore the most basic of human rights.”.“I urge the Foreign Secretary William Hague, and the EU Foreign Minister Catherine Ashton, to make representations to the Saudi government to secure the release of this man. His detention violates all the norms of international human rights law. In the longer term, Britain and US must stop colluding with the Saudi royal dictatorship. Sanctions should be imposed against the regime until it ensures democracy and human rights for all its citizens.”.Gay Middle East sent and email to the Embassy of the Kingdom of Saudi Arabia in London which was read and ignored. To the knowledge of Gay Middle East, this is first known reported case of entrapment for homosexuality via facebook in the KSA.  Gay Middle East therefore thought that a user of any social networking site has a right for privacy and asked Facebook for their comments on the case and its possible ramifications.  Despite an email and a phone call, Facebook refused to comment on the issue.  .While this case may seem to Western readers as breaching the privacy rights, Saudi Arabia does not provide the right to privacy.  In fact the religious police encourage reporting of any “deviant” behaviour and deliberately entrap a person for homosexuality, for example a British male Nurse who was recently entrapped via fake SMS sent by the religious police. . Entrapment by the religious police does not necessarily lead to prosecution, but often results in life-long financial and/or sexual black-mail.  Hamwi stated: “sexual blackmail and abuse by the Religious Police is unfortunately quite common.  When I lived in Medina, a neighbour who was a member of the religious police raped my neighbour’s son, a 12 year-old boy, at that time.  The same man entrapped and arrested a Pakistani national for homosexuality; the guy was whipped 80 times and before being deported. Such a sentence often applied when a sexual intercourse cannot be proven.” .If a person is outed by the religious police via a trail the consequences can be severe not only in terms of punishments, but lifelong ostracising by the family, the community and reduced or almost no job prospects.  “The person may simply become a social outcast,” adds Hamwi, “it is a kind of a social-death or in some cases may lead to persecution by the family until the person is killed to save the so-called ‘honour’ of the family.”.Furthermore private communication is also not subject to what ordinarily would be considered in the West as the right for privacy. All communications (including electronic) can be seized by the government for evidence in criminal trials; previously men have been arrested for homosexuality via paltalk (a social networking site popular in the Gulf), and gay-dating sites..Hamwi explained further: “the use of internet in Saudi Arabia is subject to monitoring, censorship and restrictions. Most online dating and social media website are blocked under the current Saudi laws. When trying to access banned or blocked websites users usually get screens stating “Sorry, the requested page is not available.” However, Saudis manage to override the Saudi proxy settings and access the websites they need.”.Ahmad, a 37-year-old Saudi engineer, mentioned that he is concerned with using online dating services and websites. “Anyone from the “Hay’ah”, (the religious police) can use those websites to entrap gay men. This is not common, but it happened before and I don’t want to be socially humiliated.” Ahmad affirmed that non-Saudis and Saudi Shiites are more likely to be subject to the legal Islamic penalties than the Sunni Saudis..Munir, a 29-year-old Syrian graphic designer working in KSA, said that the situation in Saudi Arabia is dangerous for gay men. “You see, when you are not Saudi, they can arrest you, put you in jail, lash you, and deport you. It is easier to be sexually deprived than having to face all the dangers coming from online dating.”.Fahad, a 42-year-old Saudi citizen, said that he rarely uses the online dating websites while in Saudi Arabia. “The situation here is complicated because of all the religious, social, and legal restrictions. Gay men in Saudi Arabia prefer not to have to struggle with the laws, since the media can easily raise a social anger when they expose their cases. This happens a lot.”",Man arrested for using Facebook to find other men to date
96,"Reader donations, many as small as just $5, are what fund the work of writers like this—and keep our content free and accessible to everyone. If you support this work, will you chip in to help fund it? It only takes a minute to donate. Click here to make a tax-deductible donation..The Islamic Republic of Iran – under the new government of President Mahmoud Ahmadinejad – is engaged in a major anti-homosexual pogrom targeting gays and gay sex. This campaign includes Internet entrapment, blackmail to force arrested gays to inform on others, torture and executions of those found guilty of engaging in ​“homosexual acts.”.Homosexual acts have been considered a capital crime in Iran since the 1979 revolution that brought the Ayatollah Khomeini to power. Iranians found guilty of gay lovemaking are given a choice of four death styles: being hanged, stoned, halved by a sword or dropped from the highest perch. According to Article 152 of Iran’s penal law, if two men not related by blood are found naked under one cover without good reason, both will be punished at a judge’s discretion..Iran’s crackdown on gays drew worldwide protests (except in the United States) after the hanging for ​“homosexual acts” of two teenagers – one 18, the other believed to be 16 or 17 – on July 19 in the city of Mashad. Charges against the two teens included the alleged rape of another youth. But three independent gay sources inside Mashad told Afdhere Jama, editor of Huriyah (an Internet zine for gay Muslims), that the teens were well known in the city’s underground gay community as lovers who lived together, and that the rape charge was fabricated. The editors of an underground Persian-language zine in Iran (who requested anonymity out of fear) also confirm that their own Mashad sources said that the rape charge was trumped up – a view now generally accepted. In any case, the hangings were illegal under international law because Iran is a signatory to two treaties that forbid executing minors. Since then, there have been reports of at least a dozen more gay victims who have been executed..“Under Islamic law, which has been adopted by Iran’s legal system, it takes four witnesses to prove an act of homosexuality, which is a capital crime. That’s why it’s much easier for the Islamic government to invent other criminal charges against gay people to get rid of them,” Jama told me. The Iranian gay zine’s editors said the same, urging Westerners to be ​“very careful” before accepting such criminal charges at face value, as they are ​“most likely false.”.Amir is a 22-year-old gay Iranian who was arrested by Iran’s religious morality police as part of a massive Internet entrapment campaign targeting gays. He escaped from Iran in August, and is now in Turkey seeking asylum in a gay-friendly country. Through a Persian translator, Amir gave me a terrifying, firsthand account of the anti-gay crackdown..Amir’s first arrest for being gay came when police raided a private party. ​“The judge told me, ​‘If we send you to a physician who vouches that your rectum has been penetrated in any way, you will be sentenced to death’,” says Amir. He was fined and released for lack of proof that a sexual act had taken place..Later, an unrepentant Amir set up a meeting with a man he met through a Yahoo gay chat room. When his date turned out to be a member of the sex police, Amir was arrested and taken to Intelligence Ministry headquarters, ​“a very scary place,” he says. ​“There I denied that I was gay – but they showed me a printout from the chat room of my messages and my pictures.”.Then, says Amir, the torture began. ​“There was a metal chair in the middle of the room – they put a gas flame under the chair and made me sit on it as the metal seat got hotter and hotter. They threatened to send me to an army barracks where all the soldiers were going to rape me. The leader told one of the other officers to take [a soft drink] bottle and shove it up my ass, screaming, ​‘This will teach you not to want any more cock!’ I was so afraid of sitting in that metal chair as it got hotter and hotter that I confessed. Then they brought out my file and told me that I was a ​‘famous faggot’ in Shiraz. They beat me up so badly that I passed out and was thrown, unconscious, into a holding cell..“When I came to, I saw there were several dozen other gay guys in the cell with me. One of them told me that after they had taken him in, they beat him and forced him to set up dates with people through chat rooms – and each one of those people had been arrested; those were the other people in that cell with me.”.Eventually tried, Amir was sentenced to 100 lashes. ​“I passed out before the 100 lashes were over. When I woke up, my arms and legs were so numb that I fell over when they picked me up from the platform on which I’d been lashed. They had told me that if I screamed, they would beat me even harder – so I was biting my arms so hard, to keep from screaming, that I left deep teeth wounds in my own arms.”.After this entrapment and public flogging, Amir’s life became unbearable. He was rousted regularly at his home by the basiji (a para-police made up of thugs recruited from the criminal classes and the lumpen unemployed) and by agents of the Office for Promotion of Virtue and Prohibition of Vice, which represses ​“moral deviance” – things like boys and girls walking around holding hands, women not wearing proper Islamic dress and prostitution. Says Amir, ​“In one of these arrests, Colonel Javanmardi told me that if they catch me again that I would be put to death, ​‘just like the boys in Mashad.’ He said it just like that, very simply, very explicitly. He didn’t mince words. We all know that the boys who were hanged in Mashad were gay – the rape charges against them were trumped up, just like the charges of theft and kidnapping against them. When you get arrested, you are forced by beatings, torture and threats to confess to crimes you didn’t commit. It happens all the time, and has to friends of mine.”.Amir’s experience is typical – as is the lack of concern evidenced by U.S. LGBT oganizations. Both of the principal U.S. gay rights organizations – Human Rights Campaign and the National Gay and Lesbian Task Force – have failed to incorporate international solidarity with persecuted gays into their fundraising-driven agendas, and neither have mobilized public protests against Iran’s anti-gay pogrom. Their European counterparts, in contrast, organized multiple demonstrations at Iranian embassies across the Continent..The Persian Gay and Lesbian Organization (PGLO) is the principal group for Iranian gays, claiming 29,000 on its e‑mail list. The PGLO – which publishes a monthly Internet magazine in Persian, hosts radio netcasts into Iran, and has secretariats in Turkey and Norway – has appealed to Western gays to mobilize international protests against the inhumane tragedy that has befallen Iranian same-sexers. To find out how to help, visit www​.pglo​.org..Reader donations, many as small as just $5, have kept In These Times publishing for 45 years. When you contribute, youre not just giving a gift—youre helping publish the next big In These Times story.",Iran utilizes various tools from messages in private chat rooms to target gays
99,"ON CAMERA: DeepCam, a facial recognition system deployed by pharmacy chain Rite Aid, captured this footage of a Reuters photographer at a store in New York in November. After Reuters informed Rite Aid of this articles findings, the company said it had ended the surveillance program..In the hearts of New York and metro Los Angeles, Rite Aid installed facial recognition technology in largely lower-income, non-white neighborhoods, Reuters found. Among the technology the U.S. retailer used: a state-of-the-art system from a company with links to China and its authoritarian government..Over about eight years, the American drugstore chain Rite Aid Corp quietly added facial recognition systems to 200 stores across the United States, in one of the largest rollouts of such technology among retailers in the country, a Reuters investigation found..In the hearts of New York and metro Los Angeles, Rite Aid deployed the technology in largely lower-income, non-white neighborhoods, according to a Reuters analysis. And for more than a year, the retailer used state-of-the-art facial recognition technology from a company with links to China and its authoritarian government..In telephone and email exchanges with Reuters since February, Rite Aid confirmed the existence and breadth of its facial recognition program. The retailer defended the technology’s use, saying it had nothing to do with race and was intended to deter theft and protect staff and customers from violence. Reuters found no evidence that Rite Aid’s data was sent to China..Last week, however, after Reuters sent its findings to the retailer, Rite Aid said it had quit using its facial recognition software. It later said all the cameras had been turned off..“This decision was in part based on a larger industry conversation,” the company told Reuters in a statement, adding that “other large technology companies seem to be scaling back or rethinking their efforts around facial recognition given increasing uncertainty around the technology’s utility.”.Reuters pieced together how the company’s initiative evolved, how the software has been used and how a recent vendor was linked to China, drawing on thousands of pages of internal documents from Rite Aid and its suppliers, as well as direct observations during store visits by Reuters journalists and interviews with more than 40 people familiar with the systems’ deployment. Most current and former employees spoke on condition of anonymity, saying they feared jeopardizing their careers..While Rite Aid declined to disclose which locations used the technology, Reuters found facial recognition cameras at 33 of the 75 Rite Aid shops in Manhattan and the central Los Angeles metropolitan area during one or more visits from October through July..The cameras were easily recognizable, hanging from the ceiling on poles near store entrances and in cosmetics aisles. Most were about half a foot long, rectangular and labeled either by their model, “iHD23,” or by a serial number including the vendor’s initials, “DC.” In a few stores, security personnel – known as loss prevention or asset protection agents – showed Reuters how they worked..The cameras matched facial images of customers entering a store to those of people Rite Aid previously observed engaging in potential criminal activity, causing an alert to be sent to security agents’ smartphones. Agents then reviewed the match for accuracy and could tell the customer to leave..Rite Aid told Reuters in a February statement that customers had been apprised of the technology through “signage” at the shops, as well as in a written policy posted this year on its website. Reporters found no notice of the surveillance in more than a third of the stores they visited with the facial recognition cameras..Among the 75 stores Reuters visited, those in areas that were poorer or less white were much more likely to have the equipment, the news agency’s statistical analysis found..Stores in more impoverished areas were nearly three times as likely as those in richer areas to have facial recognition cameras. Seventeen of 25 stores in poorer areas had the systems. In wealthier areas, it was 10 of 40. (Ten of the stores were in areas whose wealth status was not clear. Six of those stores had the equipment.).In areas where people of color, including Black or Latino residents, made up the largest racial or ethnic group, Reuters found that stores were more than three times as likely to have the technology..Reuters’ findings illustrate “the dire need for a national conversation about privacy, consumer education, transparency, and the need to safeguard the Constitutional rights of Americans,” said Carolyn Maloney, the Democratic chairwoman of the House oversight committee, which has held hearings on the use of facial recognition technology..Cathy Langley, Rite Aid’s vice president of asset protection, said earlier this year that facial recognition – which she referred to as “feature matching” – resulted in less violence and organized crime in the company’s stores. Last week, however, Rite Aid said its new leadership team was reviewing practices across the company, and “this was one of a number of programs that was terminated.” .Facial recognition technology has become highly controversial in the United States as its use has expanded in both the public and private sectors, including by law enforcement and retailers. Civil liberties advocates warn it can lead to harassment of innocent individuals, arbitrary and discriminatory arrests, infringements of privacy rights and chilled personal expression..Adding to these concerns, recent research by a U.S. government institute showed that algorithms that underpin the technology erred more often when subjects had darker skin tones. .Facial recognition systems are largely unregulated in the United States, despite disclosure or consent requirements, or limits on government use, in several states, including California, Washington, Texas and Illinois. Some cities, including San Francisco, ban municipal officials from using them. In general, the technology makes photos and videos more readily searchable, allowing retailers almost instantaneous facial comparisons within and across stores..Among the systems used by Rite Aid was one from DeepCam LLC, which worked with a firm in China whose largest outside investor is a Chinese government fund. Some security experts said any program with connections to China was troubling because it could open the door to aggressive surveillance in the United States more typical of an autocratic state. .U.S. Senator Marco Rubio, a Florida Republican and acting chair of the U.S. Senate’s intelligence committee, told Reuters in a statement that the Rite Aid system’s potential link to China was “outrageous.” “The Chinese Communist Party’s buildup of its Orwellian surveillance state is alarming, and China’s efforts to export its surveillance state to collect data in America would be an unacceptable, serious threat,” he said..The security specialists expressed concern that information gathered by a China-linked company could ultimately land in that government’s hands, helping Beijing to refine its facial recognition technology globally and monitor people in ways that violate American standards of privacy..“If it goes back to China, there are no rules,” said James Lewis, the Technology Policy Program director at the Washington-based Center for Strategic and International Studies..Two years ago, the Loss Prevention Research Council, a coalition founded by retailers to test anti-crime techniques, called facial recognition “a promising new tool” worthy of evaluation. .“There are a handful of retailers that have made the decision, ‘Look, we need to leverage tech to sell more and lose less,” said council director Read Hayes. Rite Aid’s program was one of the largest, if not the largest, in retail, Hayes said. The Camp Hill, Pennsylvania-based company operates about 2,400 stores around the country..The Home Depot Inc said it had been testing facial recognition to reduce shoplifting in at least one of its stores but stopped the trial this year. A smaller rival, Menards, piloted systems in at least 10 locations as of early 2019, a person familiar with that effort said. .Walmart Inc has also tried out facial recognition in a handful of stores, said two sources with knowledge of the tests. Walmart and Menards had no comment. .Using facial recognition to approach people who previously have committed “dishonest acts” in a store before they do so again is less dangerous for staff, said Rite Aid’s former vice president of asset protection, Bob Oberosler, who made the decision to deploy an early facial recognition system at Rite Aid. That way, “there was significantly less need for law enforcement involvement,” he said..In interviews, 10 current and former Rite Aid loss prevention agents told Reuters that the system they initially used in stores was from a company called FaceFirst, which has been backed by U.S. investment firms..“It doesn’t pick up Black people well,” one loss prevention staffer said last year while using FaceFirst at a Rite Aid in an African-American neighborhood of Detroit. “If your eyes are the same way, or if you’re wearing your headband like another person is wearing a headband, you’re going to get a hit.”.FaceFirst’s chief executive, Peter Trepp, said facial recognition generally works well irrespective of skin tone, an issue he said the industry addressed years ago. He declined to talk about Rite Aid, saying he would not discuss any possible clients. .Rite Aid originally piloted FaceFirst at its store on West 3rd Street and South Vermont Avenue in Los Angeles, a largely Asian and Latino neighborhood, around 2012..Of the 65 stores the retailer targeted in its first big rollout, 52 were in areas where the largest group was Black or Latino, according to Reuters’ analysis of a Rite Aid planning document from 2013 that was read aloud to a reporter by someone with access to it. Reuters confirmed that some of these stores later deployed the technology but did not confirm its presence at every location on the list..Separately, two former Rite Aid managers and a third source familiar with the FaceFirst rollout said the systems were concentrated, respectively, in the “tougher,” “toughest” or “worst” areas. .Reuters reviewed a 2016 spreadsheet from the company’s asset protection unit in which Rite Aid rated 20 higher-earning Manhattan stores as having equal risk of loss – labeled “MedHigh.” Two of 10 stores where whites were the largest racial group had facial recognition technology when Reuters visited this year, whereas eight of the 10 in non-white areas had the systems..One spot ranked MedHigh was a store at 741 Columbus Avenue in New York’s whiter, wealthier Upper West Side. Another was the pharmacy’s West 125th Street store in nearby Harlem, a majority African-American neighborhood. The Harlem store got facial recognition technology; the Upper West Side one did not, as of July 9..Starting in 2013, as Rite Aid deployed FaceFirst’s technology in Philadelphia, Baltimore and beyond, some serious drawbacks emerged, current and former security agents and managers told Reuters..For instance, the system would “generate 500 hits in an hour all across the United States” when photos in the system were blurry or taken at an odd angle, one of the people familiar with FaceFirst’s operations said..FaceFirst’s Trepp said the company has high accuracy rates while running “over 12 trillion comparisons per day without any known complaints to date.”.During that earlier period, Tristan Jackson-Stankunas said Rite Aid wrongly fingered him as a shoplifter in a Los Angeles store based on someone else’s photo. While Reuters could not confirm the method Rite Aid used to identify him, the store had FaceFirst technology by that time, according to a Rite Aid security agent and a Foursquare review photo showing the camera..According to a complaint Jackson-Stankunas filed with the California Department of Consumer Affairs a week after the incident, he was looking for air freshener in September 2016 when a manager ordered him to leave the store. The manager said he had received a security image of Jackson-Stankunas taken at another Rite Aid in 2013 from which he allegedly had stolen goods, according to the complaint..When Jackson-Stankunas viewed the photo on the manager’s phone, he told Reuters, he saw nothing in common with the person except their race: Both are Black..“The guy looks nothing like me,” said Jackson-Stankunas, 34, who ultimately was allowed to make his purchase and leave the store. Rite Aid “only identified me because I was a person of color. That’s it.”.The California department told him his complaint fell outside its purview, directing him to another state office, email records show. Instead, he said he decided to write the store a bad review on Yelp..At one store Reuters visited, a security agent scrolled through FaceFirst “alerts” showing a number of cases in which faces were obviously mismatched, including a Black man mixed up with someone who was Asian. Reuters could not determine whether the incorrect matches resulted in confrontations with customers. .FaceFirst CEO Trepp said that his company takes racial bias seriously and would not work with any business that disregarded civil rights. “We cannot stand for racial injustice of any kind, including in our technology,” he said..Generally, Trepp said, Reuters’ findings about his company contained “extensive factual inaccuracies” and are “not based upon information from credible sources.” .Early in 2018, Rite Aid began installing technology from DeepCam LLC, ultimately phasing out FaceFirst in stores around the country, interviews with Rite Aid loss prevention agents and internal vendor documents indicate. .Six security staffers who used both systems said DeepCam’s matches were more accurate – sometimes to a fault. The technology picked up faces from ads on buses or pictures on T-shirts, three said. One famous face captured in DeepCam was Marilyn Monroe’s, one of the agents said. .At least until 2017, FaceFirst had employed an older method of biometric identification that compared maps of subjects’ faces, two people familiar with its system said. Only later did it move over to software based on “artificial intelligence” like DeepCam’s. Though the data and algorithms differ by brand, these systems draw upon potentially millions of samples to “learn” how to match faces..DeepCam cameras photographed and took live video of every person entering a Rite Aid store, aiming to create a unique facial profile, Rite Aid agents said. If the customer walked in front of another DeepCam facial recognition camera at a Rite Aid shop, new images were added to the person’s existing profile. Two agents said they lost access to the images after 10 days unless the person landed on a watch list based on their behavior in stores. .When agents saw someone commit a crime – or just do something suspicious, one said – they scrolled through profiles on their smartphone to search for the individual, only adding the person to the watch list with a manager’s approval. The next time the shopper walked into a Rite Aid that had the technology, agents received a phone alert and checked the match for accuracy. Then they could order the person to leave, agents told Reuters..Rite Aid said adding customers to the watch list was based on “multiple layers of meaningful human review.” The company told Reuters its procedures ensured customers were not confronted unnecessarily..If a person was found to be engaging in criminal behavior, Rite Aid said, “we retain the data as a matter of policy to cooperate in pending or potential criminal investigations.”.Other U.S. retail stores have tried DeepCam. Independent 7-Eleven franchise owners in Virginia told Reuters they conducted trials of the software starting in 2018 and later dropped it. They said they largely found the system accurate but not user friendly and too expensive to maintain. The system was advertised online as costing $99 a month..The two founding owners of U.S.-based DeepCam LLC were Don Knasel and Jingfeng Liu, who set up the firm in Longmont, Colorado, in 2017, state records show. Liu’s residential address in Longmont was listed as its headquarters. .A Chinese native with U.S. citizenship and a doctorate from Carnegie Mellon University, Liu had the skills to do business in both the United States and China. .For a time, the U.S.-based DeepCam LLC and Shenzhen Shenmu were closely connected: In addition to Liu’s role in both companies, they shared the same website and email accounts, according to internal records seen by Reuters..Internal correspondence reviewed by Reuters suggests that DeepCam reached a deal with Rite Aid by March 2018, when a colleague emailed Knasel to congratulate him. Internal records also indicated that China-based Shenzhen Shenmu helped its American counterpart with product development and that Liu was expected to pay at least some of the bills. That same month, a U.S. executive wrote: “Hi Jingfeng- Thanks for the credit card. Here is the receipt for the Indianapolis Trade Show.” .In an interview, Liu confirmed the financing, saying of Knasel: “Whenever he needed money, I give him some money.” Liu said Knasel told him about the Rite Aid project but left him in the dark about the business. Knasel “never let data cross between the two countries,” Liu said. .As the Rite Aid rollout proceeded in 2018, correspondence among DeepCam staff, seen by Reuters, expressed concerns about publicly revealing any links to China, as well as using the term “facial recognition” in the U.S. market for fear of attracting the attention of the American Civil Liberties Union..Days after the ACLU wrote a March 2018 blog post critical of retailers’ suspected use of the technology, including Rite Aid’s, Knasel emailed staff: “It looks like the ACLU may be starting to stick its head up….We need to tone down facial recognition, which I have tried to do….If they come after us, we are dead….so we have to avoid.” The punctuation in the message is Knasel’s..Jay Stanley, the ACLU senior policy analyst who co-authored the blog post, told Reuters that the right response to civil liberties concerns about surveillance technology “is not to start using it in secret.”.More recently, in an interview and an email, Liu said he had not spoken with Knasel for more than a year and, to his disappointment, had not benefited from the U.S. venture. .He did not address questions about DeepCam’s deal with Rite Aid. DeepCam, he said, is “winding up” its operations and now has no assets. He added that DeepCam never supplied China-based Shenzhen Shenmu with any data. .In February, Rite Aid told Reuters that DeepCam had been “re-branded” as pdActive. PdActive is a facial recognition company run by Knasel, who said it is not a rebranding of DeepCam but a different company that has no owners who are Chinese citizens..Knasel remained connected to DeepCam through another company he runs, dcAnalytics, which Knasel said licensed DeepCam’s technology until November 2019. Since then, Knasel said, U.S.-based dcAnalytics has been using “proprietary” technology, as well as facial recognition cameras purchased from DeepCam. .Knasel said dcAnalytics is “committed to upholding the highest standards possible to make sure facial recognition technology is used fairly, properly and responsibly.”.2 New photographs augmented those profiles as customers walked in front of additional cameras, whether in the makeup aisle or at other Rite Aid locations. (Seen here is Reuters photographer Lucas Jackson, as he was captured by the system.).3 Security agents in the store could open a smartphone app and mark the profile of any person deemed a “recurring dishonest customer.” A manager had to approve this..4 The next time facial recognition cameras spotted a “dishonest customer,” a Rite Aid agent on site would receive a smartphone alert of the match, check it for accuracy and then remove the person from the store. Agents said they lost access to unmarked profiles after 10 days. (Shopper faces and names have been blurred by Reuters.) .Steve Dickinson, a Seattle attorney who practiced law in China for more than a decade and writes about cybersecurity, said geopolitical tensions have added sensitivity to any work Chinese surveillance firms do in the United States..Last year, the U.S. government blacklisted several Chinese companies – including Hikvision, one of the biggest surveillance camera manufacturers globally – alleging involvement in human rights abuses. China has deployed facial recognition cameras widely within its borders, providing a level of monitoring unfathomable to many Americans. .At the time, a U.S. Hikvision spokesman said the firm “strongly opposes” the decision and that punishing Hikvision would harm its U.S. business partners and discourage global companies from communicating with the U.S. government..Liu described his company as nothing like the Chinese video surveillance giants. With about 20 employees, he said, it is “a tiny company pretending to be big,” struggling unsuccessfully to get government contracts and nearly bankrupt..Most notably, Shenzhen Shenmu’s largest outside investor, holding about 20% of its registered capital, is a strategic fund set up by the government of China. Called the SME Development Fund (Shenzhen Limited Partnership), it has built a 6 million yuan ($855,000) stake in Shenzhen Shenmu since early 2018, Chinese public business records show..A person with the same name as a Shenzhen Shenmu board director has also worked for the venture firm managing the SME fund, according to the records and the investment firm’s website..Liu is a member of China’s Thousand Talents program, according to a local government website. That program was started by Beijing as a way to bring top academics working in important fields abroad back to China. According to allegations by the U.S. Justice Department, the program aimed to steal foreign technology..Liu told Reuters he tried to get into the program but does not know if he is. The achievement was reported in an article on Shenzhen Shenmus website, but Liu said he only wanted to use the distinction to help him sell products. Reuters was unable to confirm with China’s government whether Liu was a member..Another website, that of a Shenzhen Shenmu subsidiary, Magicision, claims its technology has helped officials arrest fugitives and suspected criminals in China..Liu was vague about the firm’s public security work, saying his company has tried unsuccessfully to get contracts with Chinese law enforcement. He called the website’s information “bullshit marketing.”.Reuters analyzed Rite Aid stores in America’s two biggest cities to determine which received facial recognition technology and which did not. The news agency gathered this data through one or more visits by Reuters journalists to all 75 Rite Aid locations in Manhattan and the central Los Angeles metro area from October 2019 through July 2020. This allowed reporters to observe whether facial recognition equipment was present and to interview employees on site about its use..To examine the demographics of the residents who live near the 75 stores, Reuters first identified all U.S. Census block groups within ¼ of a mile of each location in Manhattan and ¾ of a mile in Los Angeles, distances that accounted for the cities’ different densities. The demographics of those areas were then calculated using the U.S. Census Bureau’s American Community Survey estimates, which cover people polled from 2014 through 2018..To assess levels of poverty, Reuters categorized each store according to whether the portion of nearby households with income below federal poverty thresholds was higher than 20%, or lower. Ten stores are located in areas where the poverty level was not clearly above or below 20%; six had the technology. .To assess race and ethnic makeup, stores were categorized based on whether nearby residents who identify as white and not Hispanic were the largest group. In one case, it was not possible to determine whether or not the white population was clearly the largest racial group..Additional reporting: Cate Cadell and Yingzhi Yang in Beijing; Engen Tham and Brenda Goh in Shanghai; Farah Master in Hong Kong; and the Beijing and Shanghai newsrooms. In the United States: Lucas Jackson, Aleksandra Michalska and Samuel Hart in New York; Paresh Dave in Oakland. In the United Kingdom: Tom Bergin in London.",Rite Aid continues to use facial recognition software primarily in low-income and non-white neighborhoods even after their announcement of quitting use of facial recognition software.
101,"(Reuters) - Facebook Inc FB.O raised its settlement offer by $100 million to $650 million related to a lawsuit that claimed it illegally collected and stored biometric data for millions of users without their consent, the company said on Friday..The social media giant reached a $550 million settlement in January regarding the same lawsuit, which started in 2015, when Illinois users accused the company of violating the state’s Biometric Information Privacy Act in collecting biometric data.",Wrong
103,"In a set of new lawsuits, two Illinois residents argue that three tech giants violated state laws prohibiting the use of personal biometric data without permission. Illinois residents Steven Vance and Tim Janecyk allege that images of their faces appeared in IBM’s “Diversity in Faces” database without their consent and were used to train facial recognition systems at Amazon, Microsoft and Google’s parent company Alphabet..While all three companies are based on the West Coast, the suit accuses the tech giants of running afoul of an Illinois law known as the Biometric Information Privacy Act (BIPA). The suit names Vance and Janecyk as plaintiffs but also seeks class action status on behalf of “all other similarly situated individuals” in Illinois. In the lawsuit, the pair of plaintiffs seek $5,000 per violation of the law, an injunction barring the companies from using Illinois residents’ “biometric identifiers” and the destruction of any relevant facial data that’s been stored..“In its effort to improve its facial recognition technology, Defendant Microsoft violated Illinois’ Biometric Information Privacy Act… by, among other things, unlawfully collecting, obtaining, storing, using, possessing and profiting from the biometric identifiers and information of Plaintiffs Vance and Janecyk and all other similarly situated Illinois residents and citizens (hereinafter, the “Class Members”),” the version of the suit against Microsoft states..The law cited in the suit, passed more than a decade ago, is designed to protect Illinois residents from having their biometric data harvested or stored without their explicit permission. Lawsuits involving BIPA pop up with some frequency now, as facial recognition becomes both more commonplace and more controversial. In the absence of federal privacy protections in the U.S., the Illinois law poses an interesting hurdle for companies that are used to extracting data from Americans with little oversight..In January of this year, Facebook paid $550 million to settle a class action lawsuit stemming from BIPA. The suit was filed on behalf of Illinois residents in 2015 and alleged that the social media giant collected facial recognition data from user images without disclosing it to users. At the time, Snapchat, Google, and Shutterfly faced similar suits..In 2019, a U.S. Circuit Court of Appeals court swatted away Facebook’s claim that facial recognition data did not count as biometric data, stating that “development of face template using facial-recognition technology without consent (as alleged here) invades an individual’s private affairs and concrete interests.”.The IBM dataset the companies trained facial recognition systems on also poses its own controversies. As NBC News reported last year, IBM claimed that its Diversity in Faces dataset was designed “purely for academic research” and not for the company’s own commercial interests. The IBM dataset was apparently culled from more than 100 million Creative Commons-licensed Flickr images, a decision that raised its own ethical questions around the use of facial imagery and if corporations should be allowed leverage images with open licensing for facial recognition applications without the consent of photographers and the people they photograph.",Biometric data was harvested or stored without explicit permission
104,"             Data bias, “black box” risk, and lack of human oversight are the main governance issues for banks using AI, according to the Economist Intelligence Unit (EIU) report “Overseeing AI: Governing artificial intelligence in banking”.         .The report is based on a review of global regulatory guidance on AI risks and governance in banking carried out by the EIU on behalf of Temenos (SIX: TEMN), the banking software company..The report trends will be discussed on the webinar “Rules of the game changer – governing AI in banking” on 23 July, with Azfar Karimuddin, Janet Adams and Hani Hagras..The report highlights that AI is a top priority for technology investment for banks and reveals that 77% of banking executives believe that AI will separate winning from losing banks. AI is expected to retain its importance after the pandemic as banks look to new technologies to help them adapt to changing customer needs and compete with new market entrants. The EIU report reveals that ensuring ethical, fair and well-documented AI-based decisions will be vital for banks deploying AI technology..The EIU report highlights key governance challenges and distils regulatory guidance for banks using AI, including: ●	Ethics and fairness: banks must develop AI models that are ‘ethical by design’. AI use cases and decisions should be monitored and reviewed and data sources regularly evaluated to ensure that data remains representative.  ●	Explainability and traceability: steps taken to develop AI models must be documented in order to fully explain AI-based decisions to the individuals they impact.  ●	Data quality: bank-wide data governance standards must be established and applied to ensure data accuracy and integrity and avoid bias.  ●	Skills: banks must ensure the right level of AI expertise across the business in order to build and maintain AI models, as well as oversee these models.  Prema Varadhan, Chief Product Architect and Head of AI, Temenos, commented: AI is changing the face of the banking industry. It gives banks the ability to process more data in real time, and learn from customer behaviors, helping them to bring operating costs down and hyper-personalize their services. Banks are using AI to transform their customer experiences and back-office operations so ensuring that the technology is deployed ethically is more important than ever. “White box” models, like Temenos’ Explainable AI (XAI), can explain in simple human language how decisions are made and win the trust of regulators and customers alike. As the custodians of customer data and trusted advisors, banks have a responsibility to adopt transparent, explainable AI technology – those that do stand to gain the competitive advantage in the new normal.”.The EIU review cites data bias that leads to discrimination against individuals or groups of people as among the most prominent risks for banks using AI. Commenting in the EIU review, Prag Sharma, Senior Vice President, Citi Innovation Labs, said: “Bias can creep into AI models in any industry, but banks are better positioned than most types of organizations to combat it. Maximizing algorithms’ explainability helps to reduce bias.”.Pete Swabey, Editorial Director EMEA – Thought Leadership, The Economist Intelligence Unit, said: “AI is seen as a key competitive differentiator in the sector. Our new study, drawing on the guidance given by regulators around the world, highlights the key governance challenges banks must address if they are to capitalise on the AI opportunity safely and ethically.”.The EIU conducted a structured review of 25 reports, discussion papers and articles, and summarized the main issues raised by regulators on the topic of managing AI risks in banking. These documents were published in the last three years by banking and financial sector supervisory authorities, central banks and supranational institutions, universities and consultancies..The AI regulatory review follows a global research survey released by the EIU and Temenos, entitled “Forging new frontiers: advanced technologies will revolutionize banking,” released in June 2020.  ","Banks must work towards resolving risks with data bias, ""black box"" risk and a lack of human oversight"
105,"In March of 2015, protests broke out at the University of Cape Town in South Africa over the campus statue of British colonialist Cecil Rhodes. Rhodes, a mining magnate who had gifted the land on which the university was built, had committed genocide against Africans and laid the foundations for apartheid. Under the rallying banner of “Rhodes Must Fall,” students demanded that the statue be removed. Their protests sparked a global movement to eradicate the colonial legacies that endure in education..The events also provoked Shakir Mohamed, a South African AI researcher at DeepMind, to reflect on what colonial legacies might exist in his research as well. In 2018, just as the AI field was beginning to reckon with problems like algorithmic discrimination, Mohamed penned a blog post with his initial thoughts. In it he called on researchers to “decolonise artificial intelligence”—to reorient the field’s work away from Western hubs like Silicon Valley and engage new voices, cultures, and ideas for guiding the technology’s development..Now in the wake of renewed cries for “Rhodes Must Fall” on Oxford University’s campus, spurred by George Floyd’s murder and the global antiracism movement, Mohamed has released a new paper along with his colleague William Isaac and Oxford PhD candidate Marie-Therese Png. It fleshes out Mohamed’s original ideas with specific examples of how AI challenges are rooted in colonialism, and presents strategies for addressing them by recognizing that history..Though historical colonialism may be over, its effects still exist today. This is what scholars term “coloniality”: the idea that the modern-day power imbalances between races, countries, rich and poor, and other groups are extensions of the power imbalances between colonizer and colonized..Take structural racism as an example. Europeans originally invented the concept of races and the differences between them to justify the African slave trade and then the colonization of African countries. In the US, the effects of that ideology can now be traced through the country’s own history of slavery, Jim Crow, and police brutality..In the same way, the paper’s authors argue, this colonial history explains some of the most troubling characteristics and impacts of AI. They identify five manifestations of coloniality in the field:.Algorithmic discrimination and oppression. The ties between algorithmic discrimination and colonial racism are perhaps the most obvious: algorithms built to automate procedures and trained on data within a racially unjust society end up replicating those racist outcomes in their results. But much of the scholarship on this type of harm from AI focuses on examples in the US. Examining it in the context of coloniality allows for a global perspective: America isn’t the only place with social inequities. “There are always groups that are identified and subjected,” Isaac says..Ghost work. The phenomenon of ghost work, the invisible data labor required to support AI innovation, neatly extends the historical economic relationship between colonizer and colonized. Many former US and UK colonies—the Philippines, Kenya, and India—have become ghost-working hubs for US and UK companies. The countries’ cheap, English-speaking labor forces, which make them a natural fit for data work, exist because of their colonial histories..Beta testing. AI systems are sometimes tried out on more vulnerable groups before being implemented for “real” users. Cambridge Analytica, for example, beta-tested its algorithms on the 2015 Nigerian and 2017 Kenyan elections before using them in the US and UK. Studies later found that these experiments actively disrupted the Kenyan election process and eroded social cohesion. This kind of testing echoes the British Empire’s historical treatment of its colonies as laboratories for new medicines and technologies..AI governance. The geopolitical power imbalances that the colonial era left behind also actively shape AI governance. This has played out in the recent rush to form global AI ethics guidelines: developing countries in Africa, Latin America, and Central Asia have been largely left out of the discussions, which has led some to refuse to participate in international data flow agreements. The result: developed countries continue to disproportionately benefit from global norms shaped for their advantage, while developing countries continue to fall further behind..International social development. Finally, the same geopolitical power imbalances affect the way AI is used to assist developing countries. “AI for good” or “AI for sustainable development” initiatives are often paternalistic. They force developing countries to depend on existing AI systems rather than participate in creating new ones designed for their own context..The researchers note that these examples are not comprehensive, but they demonstrate how far-reaching colonial legacies are in global AI development. They also tie together what seem like disparate problems under one unifying thesis. “It enables us a new grammar and vocabulary to talk about both why these issues matter and what we are going to do to think about and address these issues over the long run,” Isaac says..The benefit of examining harmful impacts of AI through this lens, the researchers argue, is the framework it provides for predicting and mitigating future harm. Png believes that there’s really no such thing as “unintended consequences”—just consequences of the blind spots organizations and research institutions have when they lack diverse representation..Context-aware technical development. First, AI researchers building a new system should consider where and how it will be used. Their work also shouldn’t end with writing the code but should include testing it, supporting policies that facilitate its proper uses, and organizing action against improper ones..Reverse tutelage. Second, they should listen to marginalized groups. One example of how to do this is the budding practice of participatory machine learning, which seeks to involve the people most affected by machine-learning systems in their design. This gives subjects a chance to challenge and dictate how machine-learning problems are framed, what data is collected and how, and where the final models are used..Solidarity. Marginalized groups should also be given the support and resources to initiate their own AI work. Several communities of marginalized AI practitioners already exist, including Deep Learning Indaba, Black in AI, and Queer in AI, and their work should be amplified..Since publishing their paper, the researchers say, they have seen overwhelming interest and enthusiasm. “It at least signals to me that there is a receptivity to this work,” Isaac says. “It feels like this is a conversation that the community wants to begin to engage with.” .An investigation by MIT Technology Review reveals a sprawling, technologically sophisticated system in Minnesota designed for closely monitoring protesters..As firms have dumped their AI technologies into the country, it’s created a blueprint for how to surveil citizens and serves as a warning to the world..Intrepid Response is a little-known but powerful app that lets police quickly upload and share information across agencies. But what happens to the information it collects?","Coloniality manifests in AI through algorithmic discrimination and oppression, ghost work, beta testing, AI governance, and international social development."
106,"This survey of the growing role of social media in Kenyan society and politics does not offer a straightforward answer to the implicit question in its subtitle. Nonetheless, it develops some keen insights into the effects of the Internet in Kenya. With more than seven million of its citizens on Facebook and over a million on Twitter, Kenya may well be sub-Saharan Africa’s most online country. Nyabola describes a sophisticated community of users who have found agency through the Internet, whether in criticizing CNN for what they see as its Eurocentric coverage or in publicizing corruption and incompetence by Kenyan officials. The Kenyan government, Nyabola reveals, is deeply ambivalent about the Internet, attracted to it as a symbol of modernity but wary of the hard-to-control political spaces it creates. Nyabola’s conclusions are far from optimistic. She documents how the Internet allowed foreign actors, such as the British political consulting firm Cambridge Analytica, to manipulate voters during the 2017 Kenyan elections and explores how social media may come to undermine Kenyan democracy.",Beta Testing algorithms in 2017 Kenyan election disrupted the Kenyan election process.
107,"The news: Twitter has drafted a deepfake policy that would warn users about synthetic or manipulated media, but not remove it. Specifically, it says it would place a notice next to tweets that contain deepfakes, warn people before they share or like tweets that include deepfakes, or add a link to a news story or Twitter Moment explaining that it isn’t real. Twitter has said it may remove deepfakes that could threaten someone’s physical safety or lead to serious harm. People have until November 27 to give Twitter feedback on the proposals..The context: It’s become relatively easy to make convincing doctored videos thanks to advances in artificial intelligence. That’s led to a huge panic over the potential for deepfakes to subvert democracy, as they can be used to make politicians seem to say or do whatever the creator wants..A real threat?:The most notorious political deepfakes so far either have not been deepfakes (see the Nancy Pelosi video released in May) or have been created by people warning about deepfakes, rather than any bad actors themselves. For example, in the UK today two new deepfakes have been released of the prime minister, Boris Johnson, and leader of the opposition, Jeremy Corbyn, endorsing each other for an upcoming election on December 12. But they were created by a social enterprise trying to raise awareness of the issue..The real problem: There is no denying that deepfakes pose a significant new threat. But so far, they’re mostly a threat to women, particularly famous actors and musicians. A recent report found that 96% of deepfakes are porn, virtually always created without the consent of the person depicted. These would already break Twitter’s existing rules and be removed..An issue for the whole industry: That said, it is refreshing to see a social-media company wrangling with its content moderation responsibilities so openly. The varying responses to the Pelosi video (YouTube removed it, Facebook flagged it as false, and Twitter let it stand) show what a complex, thorny problem manipulated videos can pose. And unfortunately, we can’t expect deepfake detection technology to fix it, either. We’ll need social and legal solutions,too..An investigation by MIT Technology Review reveals a sprawling, technologically sophisticated system in Minnesota designed for closely monitoring protesters..As firms have dumped their AI technologies into the country, it’s created a blueprint for how to surveil citizens and serves as a warning to the world..Intrepid Response is a little-known but powerful app that lets police quickly upload and share information across agencies. But what happens to the information it collects?","Twitter will warn users about deepfakes, but won't remove it"
108,"Everyone’s heart is different. Like the iris or fingerprint, our unique cardiac signature can be used as a way to tell us apart. Crucially, it can be done from a distance..It’s that last point that has intrigued US Special Forces. Other long-range biometric techniques include gait analysis, which identifies someone by the way he or she walks. This method was supposedly used to identify an infamous ISIS terrorist before a drone strike. But gaits, like faces, are not necessarily distinctive. An individual’s cardiac signature is unique, though, and unlike faces or gait, it remains constant and cannot be altered or disguised..A new device, developed for the Pentagon after US Special Forces requested it, can identify people without seeing their face: instead it detects their unique cardiac signature with an infrared laser. While it works at 200 meters (219 yards), longer distances could be possible with a better laser. “I don’t want to say you could do it from space,” says Steward Remaly, of the Pentagon’s Combatting Terrorism Technical Support Office, “but longer ranges should be possible.”.Contact infrared sensors are often used to automatically record a patient’s pulse. They work by detecting the changes in reflection of infrared light caused by blood flow. By contrast, the new device, called Jetson, uses a technique known as laser vibrometry to detect the surface movement caused by the heartbeat. This works though typical clothing like a shirt and a jacket (though not thicker clothing such as a winter coat)..The most common way of carrying out remote biometric identification is by face recognition. But this needs good, frontal view of the face, which can be hard to obtain, especially from a drone. Face recognition may also be confused by beards, sunglasses, or headscarves..Cardiac signatures are already used for security identification. The Canadian company Nymi has developed a wrist-worn pulse sensor as an alternative to fingerprint identification. The technology has been trialed by the Halifax building society in the UK..Jetson extends this approach by adapting an off-the shelf device that is usually used to check vibration from a distance in structures such as wind turbines. For Jetson, a special gimbal was added so that an invisible, quarter-size laser spot could be kept on a target. It takes about 30 seconds to get a good return, so at present the device is only effective where the subject is sitting or standing..Remaly’s team then developed algorithms capable of extracting a cardiac signature from the laser signals. He claims that Jetson can achieve over 95% accuracy under good conditions, and this might be further improved. In practice, it’s likely that Jetson would be used alongside facial recognition or other identification methods..Wenyao Xu of the State University of New York at Buffalo has also developed a remote cardiac sensor, although it works only up to 20 meters away and uses radar. He believes the cardiac approach is far more robust than facial recognition. “Compared with face, cardiac biometrics are more stable and can reach more than 98% accuracy,” he says..One glaring limitation is the need for a database of cardiac signatures, but even without this the system has its uses. For example, an insurgent seen in a group planting an IED could later be positively identified from a cardiac signature, even if the person’s name and face are unknown. Biometric data is also routinely collected by US armed forces in Iraq and Afghanistan, so cardiac data could be added to that library..In the longer run, this technology could find many more uses, its developers believe. For example, a doctor could scan for arrythmias and other conditions remotely, or hospitals could monitor the condition of patients without having to wire them up to machines. .An investigation by MIT Technology Review reveals a sprawling, technologically sophisticated system in Minnesota designed for closely monitoring protesters..As firms have dumped their AI technologies into the country, it’s created a blueprint for how to surveil citizens and serves as a warning to the world..Intrepid Response is a little-known but powerful app that lets police quickly upload and share information across agencies. But what happens to the information it collects?","From a distance, the Pentagon can identify people by their cardiac signature"
110,"Voice profiling aims at inferring various human parameters from their speech, e.g. gender, age, etc. In this paper, we address the challenge posed by a subtask of voice profiling - reconstructing someones face from their voice. The task is designed to answer the question: given an audio clip spoken by an unseen person, can we picture a face that has as many common elements, or associations as possible with the speaker, in terms of identity?.Requests for name changes in the electronic proceedings will be accepted with no questions asked.  However name changes may cause bibliographic tracking issues.  Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings.",Researchers have found a way to picture a face given an audio clip
111," (CNN)A new artificial intelligence technology can accurately identify some rare genetic disorders using a photograph of a patients face, according to a new study. ",Deep Gestalt can potentially analyze facial images and discriminate with pre-existing conditions
112,"Genderify, a new service that promised to identify someone’s gender by analyzing their name, email address, or username with the help AI, looks firmly to be in the latter camp. The company launched on Product Hunt last week, but picked up a lot of attention on social media as users discovered biases and inaccuracies in its algorithms..Although these sorts of biases appear regularly in machine learning systems, the thoughtlessness of Genderify seems to have surprised many experts in the field. The response from Meredith Whittaker, co-founder of the AI Now Institute, which studies the impact of AI on society, was somewhat typical. “Are we being trolled?” she asked. “Is this a psyop meant to distract the tech+justice world? Is it cringey tech April fool’s day already?”.The problem is not that Genderify made assumptions about someone’s gender based on their name. People do this all the time, and sometimes make mistakes in the process. That’s why it’s polite to find out how people self-identify and how they want to be addressed. The problem with Genderify is that it automated these assumptions; applying them at scale while sorting individuals into a male/female binary (and so ignoring individuals who identify as non-binary) while reinforcing gender stereotypes in the process (such as: if you’re a doctor you’re probably a man). .The potential harm of this depends on how and where Genderify was applied. If the service was integrated into a medical chatbot, for example, its assumptions about users’ genders might have led to the chatbot issuing misleading medical advice. .Thankfully, Genderify didn’t seem to be aiming to automate this sort of system, but was primarily designed to be a marketing tool. As Genderify’s creator, Arevik Gasparyan, said on Product Hunt: “Genderify can obtain data that will help you with analytics, enhancing your customer data, segmenting your marketing database, demographic statistics, etc.”.In the same comment section, Gasparyan acknowledged the concerns of some users about bias and ignoring non-binary individuals, but didn’t offer any concrete answers. .One user asked: “Let’s say I choose to identify as neither Male or Female, how do you approach this? How do you avoid gender discrimination? How are you tackling gender bias?” To which Gasparyan replied that the service makes its decisions based on “already existing binary name/gender databases,” and that the company was “actively looking into ways of improving the experience for transgender and non-binary visitors” by “separating the concepts of name/username/email from gender identity.” It’s a confusing answer given that the entire premise of Genderify is that this data is a reliable proxy for gender identity..The company told The Verge that the service was very similar to existing companies who use databases of names to guess an individual’s gender, though none of them use AI. .“We understand that our model will never provide ideal results, and the algorithm needs significant improvements, but our goal was to build a self-learning AI that will not be biased as any existing solutions,” said a representative via email. “And to make it work, we very much relied on the feedback of transgender and non-binary visitors to help us improve our gender detection algorithms as best as possible for the LGBTQ+ community.”.Update Wednesday July 29, 12:42PM ET: Story has been updated to confirm that Genderify has been shut down and to add additional comment from a representative of the firm. ",Genderify suggests gender to someone's name and has errors with Dr.
113,"Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.",A neural network that analyzes facial features and classifies whether a person is a criminal or not
114," Defense Department spent millions to research social networks Studies focused on Occupy and Middle East protests Projects also analysed memes, celebrities and disinformation .The activities of users of Twitter and other social media services were recorded and analysed as part of a major project funded by the US military, in a program that covers ground similar to Facebook’s controversial experiment into how to control emotions by manipulating news feeds..Research funded directly or indirectly by the US Department of Defense’s military research department, known as Darpa, has involved users of some of the internet’s largest destinations, including Facebook, Twitter, Pinterest and Kickstarter, for studies of social connections and how messages spread..While some elements of the multi-million dollar project might raise a wry smile – research has included analysis of the tweets of celebrities such as Lady Gaga and Justin Bieber, in an attempt to understand influence on Twitter – others have resulted in the buildup of massive datasets of tweets and additional types social media posts..Several of the DoD-funded studies went further than merely monitoring what users were communicating on their own, instead messaging unwitting participants in order to track and study how they responded..Shortly before the Facebook controversy erupted, Darpa published a lengthy list of the projects funded under its Social Media in Strategic Communication (SMISC) program, including links to actual papers and abstracts..The project list includes a study of how activists with the Occupy movement used Twitter as well as a range of research on tracking internet memes and some about understanding how influence behaviour (liking, following, retweeting) happens on a range of popular social media platforms like Pinterest, Twitter, Kickstarter, Digg and Reddit..Darpa, established in 1958, is responsible for technological research for the US military. Its notable successes have included no less than Arpanet, the precursor to todays internet, and numerous other innovations, including onion routing, which powers anonymising technologies like Tor. However, thanks to some of its more esoteric projects, which have included thought-controlled robot arms, city-wide surveillance programs and exo-skeletons, the agency has also become the subject of many conspiracy theories, and a staple in programmes like the X-Files..Unveiled in 2011, the SMISC program was regarded as a bid by the US military to become better at both detecting and conducting propaganda campaigns on social media..On the webpage where it has published links to the papers, Darpa states the general goal of the SMISC program is “to develop a new science of social networks built on an emerging technology base..“Through the program, Darpa seeks to develop tools to support the efforts of human operators to counter misinformation or deception campaigns with truthful information..However, papers leaked by NSA whistleblower Edward Snowden indicate that US and British intelligence agencies have been deeply engaged in planning ways to covertly use social media for purposes of propaganda and deception..Documents prepared by NSA and Britains GCHQ (and previously published by the Intercept as well as NBC News) revealed aspects of some of these programs. They included a unit engaged in “discrediting” the agency’s enemies with false information spread online..Earlier this year, the Associated Press also revealed the clandestine creation by USAid of a Twitter-like, Cuban communications network to undermine the Havana government. The network, built with secret shell companies and financed through a foreign bank, lasted more than two years and drew tens of thousands of subscribers. It sought to evade Cubas stranglehold on the internet with a primitive social media platform..Of the funding provided by Darpa, $8.9m has been channeled through IBM to a range of academic researchers and others. A further $9.6m has gone through academic hubs like Georgia Tech and Indiana University..Facebook, the world’s biggest social networking site, has apologised for the study, which involved secret psychological tests on nearly 700,000 users in 2012, and prompted outrage from users and experts alike, being “poorly communicated” to the public..The experiment, which resulted in a scientific paper published in the March issue of Proceedings of the National Academy of Sciences, hid “a small percentage” of emotional words from peoples’ news feeds, without their knowledge, to test what effect that had on the statuses or “likes” that they then posted or reacted to..However, it appears that Facebook was involved in at least one other military-funded social media research project, according to the records recently published by Darpa..The research was carried by Xuanhuai Wang, an engineering manager at Facebook, as well as Yi Chang, a lead scientist at Yahoo labs, and others based at the Universities of Michigan and Southern California..The project, which related to how users understood and consumed information on Twitter, at one point analysed the tweets, retweets and other interactions spawned by Lady Gaga (described as “the most popular elite user on Twitter”) and Justin Bieber (“who is extremely popular among teenagers”)..Other studies looked further afield. One, On the Study of Social Interactions on Twitter, which was carried out by the University of South California, collected tweets from 2,400 Twitter users who had identified themselves as residing in the Middle East. It analysed how often they had interactions with other users and how these were spread..Several studies related to the automatic assessment of how well different people in social networks knew one another, through analysing frequency, tone and type of interaction between different users. Such research could have applications in the automated analysis of bulk surveillance metadata, including the controversial collection of US citizens’ phone metadata revealed by Snowden..Studies which received military funding channeled through IBM included one called Modeling User Attitude toward Controversial Topics in Online Social Media, which analysed Twitter users’ opinions on fracking..Discussing the applicability of their research, the study’s authors stated: “For example, a government campaign on Twitter supporting vaccination can engage with followers who are more likely to take certain action (eg spreading a campaign message) based on their opinions.”.“As another example, when anti-government messages are spread in social media, government would want to spread counter messages to balance that effort and hence identify people who are more likely to spread such counter messages based on their opinions.”.A similarly titled-project out of the University of Southern California, The Role of Social Media in the Discussion of Controversial Topics, studied the behaviour of Twitter users posting about a 2012 vote in California on measures such as raising taxes, genetically modified organisms and the death penalty..“Our findings suggest Twitter is primarily used for spreading information to like-minded people rather than debating issues,” the authors wrote in their paper on the project..A study at Georgia Tech, Cues to Deception in Social Media Communications, involved an in-laboratory experiment using an experimental social media platform, FaceFriend, and 61 paid participants. While past research had investigated “written deception” in communications such as email, the study expanded this into social media, and the researchers concluded: “Breaking news stories and world events – for example, the Arab Spring – are heavily represented in social media, making them susceptible topics for influence attempts via deception.”.Several of the DoD-funded projects went further than simple observation, instead engaging directly with social media users and analysing their responses..One of multiple studies looking into how to spread messages on the networks, titled “Who Will Retweet This? Automatically Identifying and Engaging Strangers on Twitter to Spread Information” did just this..The researchers explained: “Since everyone is potentially an influencer on social media and is capable of spreading information, our work aims to identify and engage the right people at the right time on social media to help propagate information when needed.”.In the paper, which included data gathered through actively engaging 3,761 people on Twitter around the topics of public safety and bird flu, the researchers added: “Unlike existing work, which often uses only social network properties, our feature set includes personality traits that may influence one’s retweeting behaviour.”.“Social media is changing the way people inform themselves, share ideas, and organize themselves into interest groups, including some that aim to harm the United States,” said a spokesman. “Darpa supports academic research that seeks to understand some of these dynamics through analyses of publicly available discussions conducted on social media platforms.”.Sources said that data was from public streams in social networks, and was collected and stored by academics at institutions conducting the research, not by Darpa itself..The Guardian approached a number of individuals involved in research, asking them for their views on why they believed the US military may be interested in funding research of this type, and asking about the extent to which consent was sought from people whose social media posts were recorded and analysed..Among those who replied, Emilio Ferrara, who was involved in the research paper on The Digital Evolution of Occupy Wall St, said: “According to federal regulations of human experimentation, for studies that don’t affect the environment of online users, and whereas one can freely gather online data – say, from the public Twitter feed – there is no requirement of informed consent. This is the framework under which our Twitter study was carried out; moreover, all our studies on Twitter look into aggregate collective phenomena and never at the individual level.”.“This work has broad applications as we strive to understand fundamental mechanism of social communication, such as how ideas and ‘memes’ compete for our attention, how they sometimes go viral, etc.”","The Defense department has analyzed memes, celebrities and disinformation"
115,"Fake news and the way it spreads on social media is emerging as one of the great threats to modern society. In recent times, fake news has been used to manipulate stock markets, make people choose dangerous health-care options, and manipulate elections, including last year’s presidential election in the U.S..Clearly, there is an urgent need for a way to limit the diffusion of fake news. And that raises an important question: how does fake news spread in the first place?.Today we get an answer of sorts thanks to the work of Chengcheng Shao and pals at Indiana University in Bloomington. For the first time, these guys have systematically studied how fake news spreads on Twitter and provide a unique window into this murky world. Their work suggests clear strategies for controlling this epidemic..Shao and co then monitored some 400,000 claims made by these websites and studied the way they spread through Twitter. They did this by collecting some 14 million Twitter posts that mentioned these claims..At the same time, the team monitored some 15,000 stories written by fact-checking organizations and over a million Twitter posts that mention them..Next, Shao and co looked at the Twitter accounts that spread this news, collecting up to 200 of each account’s most recent tweets. In this way, the team could study the tweeting behavior and work out whether the accounts were most likely run by humans or by bots.  .Having made a judgment on the ownership of each account, the team finally looked at the way humans and bots spread fake news and fact-checked news..To do all this, the team developed two online platforms. The first, called Hoaxy, tracks fake news claims, and the second, Bolometer, works out whether a Twitter count is most likely run by a human or a bot..The results of this work make for interesting reading. “Accounts that actively spread misinformation are significantly more likely to be bots,” say Shao and co. “Social bots play a key role in the spread of fake news.”.Shad and co say bots play a particularly significant role in the spread of fake news soon after it is published. What’s more, these bots are programmed to direct their tweets at influential users. “Automated accounts are particularly active in the early spreading phases of viral claims, and tend to target influential users,” say Shao and co..That’s a clever strategy. Information is much more likely to become viral when it passes through highly connected nodes on a social network. So targeting these influential users is key. Humans can easily be fooled by automated accounts and can unwittingly seed the spread of fake news (some humans do this wittingly, of course)..“These results suggest that curbing social bots may be an effective strategy for mitigating the spread of online misinformation,” say Shao and co..One way would be to outlaw certain kinds of social bots. But this is a route fraught with difficulty. There are many social bots that perform important roles in the spread of legitimate information..And legislation does not overcome international borders. Given the way foreign powers have manipulated the spread of fake news, it’s hard to see how this would work..Nevertheless, the spread of fake news is a legitimate and important source of public concern. Understanding how it spreads is the first stage in tackling it..An investigation by MIT Technology Review reveals a sprawling, technologically sophisticated system in Minnesota designed for closely monitoring protesters..As firms have dumped their AI technologies into the country, it’s created a blueprint for how to surveil citizens and serves as a warning to the world..Intrepid Response is a little-known but powerful app that lets police quickly upload and share information across agencies. But what happens to the information it collects?",Over 14 million Twitter posts mention fake news claims
116,"They call it the Silent Talker. It is a virtual policeman designed to strengthen Europe’s borders, subjecting travelers to a lie detector test before they are allowed to pass through customs..Prior to your arrival at the airport, using your own computer, you log on to a website, upload an image of your passport, and are greeted by an avatar of a brown-haired man wearing a navy blue uniform..“What is your surname?” he asks. “What is your citizenship and the purpose of your trip?” You provide your answers verbally to those and other questions, and the virtual policeman uses your webcam to scan your face and eye movements for signs of lying..At the end of the interview, the system provides you with a QR code that you have to show to a guard when you arrive at the border. The guard scans the code using a handheld tablet device, takes your fingerprints, and reviews the facial image captured by the avatar to check if it corresponds with your passport. The guard’s tablet displays a score out of 100, telling him whether the machine has judged you to be truthful or not..A person judged to have tried to deceive the system is categorized as “high risk” or “medium risk,” dependent on the number of questions they are found to have falsely answered. Our reporter — the first journalist to test the system before crossing the Serbian-Hungarian border earlier this year — provided honest responses to all questions but was deemed to be a liar by the machine, with four false answers out of 16 and a score of 48. The Hungarian policeman who assessed our reporter’s lie detector results said the system suggested that she should be subject to further checks, though these were not carried out..Travelers who are deemed dangerous can be denied entry, though in most cases they would never know if the avatar test had contributed to such a decision. The results of the test are not usually disclosed to the traveler; The Intercept obtained a copy of our reporter’s test only after filing a data access request under European privacy laws..IBorderCtrl’s lie detection system was developed in England by researchers at Manchester Metropolitan University, who say that the technology can pick up on “micro gestures” a person makes while answering questions on their computer, analyzing their facial expressions, gaze, and posture..An EU research program has pumped some 4.5 million euros into the project, which is being managed by a consortium of 13 partners, including Greece’s Center for Security Studies, Germany’s Leibniz University Hannover, and technology and security companies like Hungary’s BioSec, Spain’s Everis, and Poland’s JAS..The researchers at Manchester Metropolitan University believe that the system could represent the future of border security. In an academic paper published in June 2018, they stated that avatars like their virtual policeman “will be suitable for detecting deception in border crossing interviews, as they are effective extractors of information from humans.”.However, some academics are questioning the value of the system, which they say relies on pseudoscience to make its decisions about travelers’ honesty..Ray Bull, professor of criminal investigation at the University of Derby, has assisted British police with interview techniques and specializes in methods of detecting deception. He told The Intercept that the iBorderCtrl project was “not credible” because there is no evidence that monitoring microgestures on people’s faces is an accurate way to measure lying..“They are deceiving themselves into thinking it will ever be substantially effective and they are wasting a lot of money,” said Bull. “The technology is based on a fundamental misunderstanding of what humans do when being truthful and deceptive.”.In recent years, following the refugee crisis and a spate of terrorist attacks in France, Belgium, Spain, and Germany, police and security agencies in Europe have come under increasing political pressure to more effectively track the movements of migrants. Border security officials on the continent say they are trying to find faster and more efficient new ways, using artificial intelligence, to check the travel documents and biometrics of the more than 700 million people who annually enter the EU..The European Commission — the EU’s executive branch — has set aside a proposed €34.9 billion for border control and migration management between 2021 and 2027. Meanwhile, in September last year, European lawmakers agreed to establish a new automated system that will screen nationals from visa-free third countries — including the United States — to establish whether or not they should be allowed to enter the EU..In the future, a visa-free traveler who, for whatever reason, has not been able to submit an application in advance will not be granted entry into the Schengen zone, an area covering 26 countries in Europe where travelers can move freely across borders without any passport checks..IBorderCtrl is one technology designed to strengthen the prescreening process. But transparency activists say that the project should not be rolled out until more information is made available about the technology — such as the algorithms it uses to make its decisions..Earlier this year, researchers at the Milan-based Hermes Center for Transparency and Digital Human Rights used freedom of information laws to obtain internal documents about the system. They received hundreds of pages; however, they were heavily redacted, with many pages completely blacked out..“The attempt to suppress debate by withholding the documents that address these issues is really frightening,” said Riccardo Coluccini, a researcher at the Hermes Center. “It is absolutely necessary to understand the reasoning behind the funding process. What is written in those documents? How does the consortium justify the use of such a pseudoscientific technology?”.A study produced by the researchers in Manchester tested iBorderCtrl on 32 people and said that their results showed the system had 75 percent accuracy. The researchers noted, however, that their participant group was unbalanced in terms of ethnicity and gender, as there were fewer Asian or Arabic participants than white Europeans, and fewer women than men..Giovanni Buttarelli, head of the EU’s data protection watchdog, told The Intercept that he was concerned that the iBorderCtrl system might discriminate against people on the basis of their ethnic origin..“Are we only evaluating possible lies about identity or we are also trying to analyze some of the person’s somatic traits, the edges of the face, the color of the skin, the cut of the eyes?” Buttarelli said. “Who sets the parameters to establish that a certain subject is lying or not lying?”.A spokesperson for iBorderCtrl declined to answer questions for this story. A website for the project acknowledges that the lie detection system will “have an impact on the fundamental rights of travellers” but says that, because the test is currently voluntary, “issues with respect to discrimination, human dignity, etc. therefore cannot occur.”",iBorderCtrl is being pursued despite a lack of transparency and racial bias
117,"Most security news is about insecurity, hacking and cyber threats, bordering on scary. But when security is done right, its a beautiful thing...sexy even. Security IS sexy..An unnamed homeland security agency has signed a contract with a company that claims it can “reveal” your personality “with a high level of accuracy” just by analyzing your face, be that facial image captured via photo, live-streamed video, or stored in a database. It then sorts people into categories; with some labels as potentially dangerous such as terrorist or pedophile, it is disturbing that some experts believe the science behind it is antiquated, has previously been discredited, and the results are inaccurate..Israeli start-up Faception, a facial personality profiling company, told The Washington Post that “a homeland security agency” has signed a contract to use Faception to help spot terrorists. The “computer vision and machine learning technology” can even be integrated into other facial recognition tech “to provide a full spectrum solution that covers known and anonymous individuals.”.Faceception CEO Shai Gilboa added, “Our personality is determined by our DNA and reflected in our face. It’s a kind of signal.” On the company’s site, the “science” behind the technology that can supposedly predict a person’s behavior and personality was described as:.People may judge other people by their faces, but the “science” of judging a book by its cover via face reading, or physiognomy, was basically “discredited and rejected” by the late 19th century. It’s one thing for a person to make a snap judgement based on appearance and another thing entirely to use Faception to “enrich your profile database with a variety of personality scores” and “turn unknown individuals into known ones.”.As Pedro Domingos, a professor of computer science at the University of Washington, pointed out to the Post, “Can I predict that you’re an ax murderer by looking at your face and therefore should I arrest you? You can see how this would be controversial.”.Princeton psychology professor Alexander Todorov told the Post, “The evidence that there is accuracy in these judgments is extremely weak. Just when we thought that physiognomy ended 100 years ago.”.“Faception has built 15 different classifiers,” the Post reported, and allegedly can evaluate certain traits with an “80% accuracy.” Put another way, one in five people could incorrectly be classified as a terrorist or pedophile. Gilboa said the company “will never make his classifiers that predict negative traits available to the general public.”.Eight classifiers are listed on the Faception site: High Q, academic researcher, professional poker player, bingo player, brand promoter, white-collar offender, terrorist and pedophile. “The classifiers represent a certain persona, with a unique personality type, a collection of personality traits or behaviors.” Algorithms are used to sort people according to how they fit into those classifiers..For example, the company classifies a “bingo player” as being “endowed with a high mental ceiling, high concentration, adventurousness, and strong analytical abilities. Tends to be creative, with a high originality and imagination, high conservation and sharp senses.”.“Thrill seeking” is mentioned in the “terrorist” classifier. Thrills come in all shapes and sizes, right? Pity the adrenaline-junkie soul incorrectly identified as a terrorist..The company claims “success” stories such correctly identifying four poker players out of 50 competing in a tournament. In the end, two of the predicted four players were finalists. Faception claims that its technology classified nine of 11 Paris terrorists “with no prior knowledge” and only three of those terrorists had a previous record. That is allegedly why it is “working with the leading Homeland Security Agency,” according to its marketing video..While Faception is not quite the same, it reminded me of Homeland Security’s pre-crime screening program dubbed FAST for Future Attribute Screening Technology (pdf); FAST has been likened to Minority Report as it was designed “to ‘sense’ and spot people who intend to commit a terrorist act.”.Unlike Faception, FAST analyzes much more than the face. It reportedly analyzes facial expressions and uses trackers to measure pupils, position and gaze of eyes, but it also measures heart and respiration rates, analyzes body movement, body heat changes and pitch changes in voices..EPIC (Electronic Privacy Information Center) has been trying to get more information from DHS about FAST since 2011. That same year at DefCon, researchers suggested FAST smelled like security snake oil and explained why it wouldn’t work (pdf). Let’s hope the unnamed homeland security agency which inked a $750,000 contract with Faception was not DHS..China too has tinkered with “pre-crime” to identify terrorists; China being China, one has to wonder if dissident is synonymous with terrorist. Its “Citizen Score” is already an Orwellian nightmare..Faception may not be meant for the general public, but “analyzing anonymous individuals who may impose a threat to public safety” could be wrapped into law enforcement as in “homeland security,” AI, personal robots as well as for “public safety” at buildings, shopping malls, stadiums and corporations, and used in retail, insurance, recruiting, finance, and even matchmaking.","A facial recognition system uses facial features to classify whether someone is a terrorist, pedophile etc."
118,"The United States government is accelerating efforts to monitor social media to preempt major anti-government protests in the US, according to scientific research, official government documents, and patent filings reviewed by Motherboard. The social media posts of American citizens who don’t like President Donald Trump are the focus of the latest US military-funded research. The research, funded by the US Army and co-authored by a researcher based at the West Point Military Academy, is part of a wider effort by the Trump administration to consolidate the US military’s role and influence on domestic intelligence..The vast scale of this effort is reflected in a number of government social media surveillance patents granted this year, which relate to a spy program that the Trump administration outsourced to a private company last year. Experts interviewed by Motherboard say that the Pentagon’s new technology research may have played a role in amendments this April to the Joint Chiefs of Staff homeland defense doctrine, which widen the Pentagon’s role in providing intelligence for domestic “emergencies,” including an “insurrection.”.It’s no secret that the Pentagon has funded Big Data research into how social media surveillance can help predict large-scale population behaviours, specifically the outbreak of conflict, terrorism, and civil unrest..Much of this research focuses on foreign theatres like the Middle East and North Africa — where the 2011 Arab Spring kicked off an arc of protest that swept across the region and toppled governments..Since then, the Pentagon has spent millions of dollars finding patterns in posts across platforms like Facebook, Twitter, Instagram, Tumblr, and beyond to enable the prediction of major events..In August, a US Army-backed study on civil unrest within the US homeland was published in an obscure anthology of papers presented to a Big Data conference in Kiev, Ukraine, which took place in early June. The anthology was released as part of Springer-Nature’s Advances in Intelligent Systems and Computing book series..The paper in question is a study of the link between social media and anti-Trump protests after the 2016 presidential elections, titled “Social Network Structure as a Predictor of Social Behavior: The Case of Protest in the 2016 US Presidential Election.” The study was funded by the US Army Research Laboratory (ARL), which is part of the US Army’s Research Development and Engineering Command (RDECOM)..Written by researchers at America’s oldest technological university, the Rensselaer Polytechnic Institute (RPI) in New York, the paper concludes that protests after the US elections could have been predicted by analysing the Twitter posts of millions of American citizens in the lead-up to the demonstrations. After Trump’s election, there were immediate large-scale ‘Not Our President’ protests across the US in direct response to his victory, a few of which became violent. This has been followed by numerous other protests such as the Women’s March events in January, demonstrations against Trump’s travel ban, among others..“Civil unrest is associated with information cascades or activity bursts in social media, and these phenomena may be used to predict protests, or at least peaks of protest activity,” the paper says. “Failure to predict an unexpected protest may result in injuries or damage.”.Authors Molly Renaud, Rostyslav Korolov, David Mendonca, and William Wallace of RPI’s Department of Industrial and Systems Engineering explain that their study tries to identify the “structural properties of social networks in order to predict protest occurrence,” by employing “keyword-defined Twitter datasets associated with the 2016 US Presidential Election”..Datasets for the research were collected using the Apollo Social Sensing Tool, a real-time event tracking software that collects and analyses millions of social media posts..The tool was originally developed under the Obama administration back in 2011 by the US Army Research Laboratory and US Defense Threat Reduction Agency, in partnership with Rensselaer Polytechnic Institute, the University of Illinois, IBM, and Caterva (a social marketing company that in 2013 was folded into a subsidiary of giant US government IT contractor, CSC). Past papers associated with the project show that the tool has been largely tested in foreign theatres like Haiti, Egypt, and Syria..But the use of the Apollo tool to focus on protests in the US homeland has occurred under the Trump administration. The ‘election’ dataset compiled using Apollo for the 2018 US Army-funded study is comprised of 2.5 million tweets sent between October 26, 2016, and December 20, 2016, using the words “Trump”, “Clinton,” and “election.”.Tweets were geolocated to focus on “locations where protests occurred following the election” based on user profiles. Locations were then triangulated against protest data from “online news outlets across the country.”.The millions of tweets were used to make sense of the “frequencies of the protests in 39 cities” using 18 different ways of measuring the “size, structure and geography” of a network, along with two ways of measuring how that network leads a social group to become “mobilized,” or take action..The paper concludes that by “examining the structure of social networks as related in tweets related to the 2016 US Presidential Election, a relationship is identified between network structure and protest occurrence.” The model demonstrates that social media plays a catalyzing role in the mobilization of social groups before a protest, in a way “which is observable in advance of protest occurrences. This may be of use in preparing for such an event, and help minimize injuries and property damage.”.In short, this means that “the social network can be a predictor of mobilization, which in turn is a predictor of the protest.” This pivotal finding means that extensive real-time monitoring of American citizens’ social media activity can be used to predict future protests..More work is needed to beef up the accuracy of the model, though. This model is still only 44 percent accurate five days before the protest, with accuracy increasing up to 82 percent closer to the incident..If you look at the long term trajectory of Pentagon, and intelligence agency desires for this sort of profiling and surveillance, these social media monitoring projects fits a deep pattern of institutional surveillance desires.Acknowledgements at the end of the paper confirm that the “research is sponsored by the Army Research Laboratory.” The paper includes the caveat that the research does not necessarily represent the US government’s “official policies.” And of course, the US military funds a vast amount of research pursued independently, much of which does not necessarily go anywhere..On the other hand, the paper also adds that, “the US Government is authorized to reproduce and distribute reprints for Government purposes,” and according to the ARL itself, the Laboratory “applies the extensive research and analysis tools developed in its direct mission program to support ongoing development and acquisition programs” across the US Army and industry partners: “ARL has consistently provided the enabling technologies in many of the Armys most important weapons systems.”.The Rensselaer Polytechnic Institute is part of the ARL Network Science Collaborative Technology Alliance (NS CTA), a consortium of three industrial research labs and 14 universities which receives multi-million dollar support from the US Army Research Laboratory..The last round of multi-million dollar five-year funding was received by Rensselaer in 2015. Research priorities are closely and continuously developed within the CTA through collaboration between university scientists, industry and the US military..A lead author of the paper, Rostyslav Korolov—identified as the point of contact about the research—is a PhD candidate at RPI focusing on “prediction of human behavior based on social media communications” under the ARL Alliance, and liaised closely with the US military while working on the anti-Trump protest study..“While working on this project I’ve spent two months on an internship at the US Army Research Laboratory and a year as a visiting scholar at the Network Science Center, United States Military Academy, West Point,” his RPI bio explains..Motherboard attempted to reach the authors of the paper through multiple requests, but did not receive any response to questions about the study and the reason for its focus on protests at home..However, Tom Moyer, a spokesman for the ARL provided us the following statement: “The Army Research Laboratory, through its collaborative alliances, cooperative agreements and other instruments, funds scientific exploration across a large spectrum to broaden the scientific knowledge base that could lead to a more capable Army. That is not to say that the laboratory, or the US Army, agrees with every conclusion drawn from principal investigators who are trying to answer difficult research questions. Our collaborators often go deep into areas of research that are sometimes high risk, but often result in the transfer of knowledge that increases scientific understanding.”.Moyer explained that the US Army selected this particular research project under a broader program theme titled, “Social/cognitive-theory-guided knowledge networks enrichment, predictive and prescriptive analysis.” The program’s goals, he said, include developing “mathematical understanding of strength and mobility of self-forming networks.”.Asked what the ARL hoped to achieve by funding this research, Moyer told me that “the researcher’s expertise in disaster research via computational science led to ARL’s funding decision. Fundamental research into complex networks, including social networks, is extremely valuable and could provide insight into the prediction and evolution of major events. This insight can aid nations for planning and undertaking relief response to natural and manmade disasters..David Price, a professor of sociology and anthropology at St. Martin’s University told me “the Pentagon throws a lot of money at a lot of projects, many of them downright silly.”.“But if you look at the long term trajectory of Pentagon, and intelligence agency desires for this sort of profiling and surveillance, these social media monitoring projects fits a deep pattern of institutional surveillance desires,” he added..Price is the world’s leading expert on the relationships between US anthropologists, social scientists, and US military intelligence agencies, the author of the book Weaponizing Anthropology: Social Science in Service of the National Security State, and has served on several American Anthropological Associations commissions and task forces dealing with the ethical issues of engaging with the US intelligence community. He describes the latest research as “really an extension of the once frightening, now mundane expression of a national panopticon expressed by the Total Information Awareness program when first conceived in 2002, and quickly withdrawn.”.Total Information Awareness was a major Bush administration initiative aimed at monitoring the entire American population through electronic surveillance. Though defunded in 2003 after extensive media criticism, its core architecture was adopted by the National Security Agency (NSA) where it is now “quietly thriving” according to the New York Times..The Trump election study is just the latest in a spate of research pursued by US government-funded scientists to predict civil unrest through social media surveillance..Much of that research has been funded by the US government’s spy research organisation, IARPA—the Intelligence Advanced Research Projects Agency—for a longtime project known as Embers that examined trends in foreign theatres. But even this research appears to have potential domestic applications..Established in April 2012, the project (which stands for Early Model Based Event Recognition using Surrogates) generated seven-day advanced forecasts based on “open-source indicators”—social media, satellite imagery, and more than 200,000 blogs that are publicly available. An average of 80 to 90 percent of its forecasts were accurate, according to studies related to the program..Teams made up mainly from three external industry partners, HRL Laboratories, Raytheon BBM Technologies, and Virginia Tech, were involved in developing the technologies behind Embers, which was funded by a $22 million contract by IARPA..Like many other similar projects sponsored by IARPA, Embers was funded through the US government’s Interior Business Center (previously known as the National Business Center) according to documents and publications related to the program..The IBC is the business management and IT service provider for the US Department of the Interior as well as other domestically-focused federal agencies, including the Department of Homeland Security—indicating that IARPA-funded technologies can easily be transitioned for homeland applications..The current uses of the technology are not public knowledge and no longer under the reach of any form of public accountability. But the entity that is still running the technology is a major US government contractor..In 2017, IARPA announced that Embers had been moved into the commercial arm of Virginia Tech through its Applied Research Corporation (VTARC.) The move would now enable a range of US-based organizations to “purchase the system’s daily forecasts for a particular set of countries.”.There is no official information available about who Virginia Tech’s Applied Research Corporation works with in relation to the social media surveillance tools it developed through IARPA’s Embers program. But a job advert from early 2018 issued by the company for a research analyst specialising in “ISR”—intelligence, surveillance and reconnaissance—mentions work “to support US government customers” (archived here):.“The ISR Research Analyst will perform thorough research and analyses on open source and customer-provided data to extract meaningful information in support of the customer’s mission goals. Primary responsibilities include collecting, processing, and analyzing technical data, searching and applying information from the primary scientific literature, developing clear visualizations of the data (graphs, tables, etc.), identifying emerging technologies, and composing clear and concise draft technical reports to support US government customers.”.In short, in 2017 the Trump administration moved IARPA’s Embers social media surveillance program into the private sector under VTARC. Yet one of VTARC’s customers using these surveillance tools is the Trump administration, and based on the job listing, it appears to deal with secret and top secret information..The move by VTARC illustrates that even with the best of intentions, independent scientists receiving US government funding for such research have no control or oversight over the uses of their work. According to Price, the impact of the research could still be insidious even if the social scientists involved did not hold any conflicts of interest as such..“This sort of military funded social science research tends to occur in an ideological echo chamber, where groupthink predominates and dissent or concerns about the applications of this work is missing,“ he told me. “Among the basic assumptions that social scientists outside this group would question are assumptions that civil unrest or protests are not core elements of democracy that need to be protected, [rather than] undermined by surveillance—and the oppression that follows such surveillance.”.VTARC did not respond to requests for details on who the corporation’s clients are for its social media surveillance tools originally developed under Embers..The move to the private sector has helped circumvent prospects for public sector accountability, by keeping the most sensitive details of the program outside the scope of Freedom of Information Act requests..Earlier this year, the ACLU filed several FOIA requests to a range of US government agencies over concerns that domestic social media surveillance had “spiked” under the Trump administration. In early September, the ACLU released documents showing that state and federal law enforcement agencies were collaborating to ramp up “Social Networking” surveillance of domestic activists over concerns about protests against the Keystone XL pipeline—the measures were justified as “anti-terrorism.” Two weeks later, an image contained in a Massachusetts state police tweet accidentally revealed that a police computer monitor had bookmarked several Facebook groups for left-wing activist organizations with an anti-Trump slant..While the ACLU has been able to confirm that under Trump, government departments like the Departments of Defense, Justice, and Homeland Security are accelerating domestic social media surveillance in relation to anticipated anti-Trump protest incidents, these FOIA requests have not revealed the technologies being deployed to do so..This kind of technology-enabled surveillance of social media will likely suppress dissent and lead to biased targeting of racial and religious minorities.Though precise information on VTARC’s social media surveillance capabilities is unavailable, a sense of the capability can be gleaned from two related patent applications, originally filed around 2013 and 2014 by HRL Laboratories LLC, which were successfully granted in 2018..HRL is jointly owned by General Motors and Boeing. The successful patents relate to a whole ecosystem of social media surveillance technologies, many of them still in application, developed over nearly a decade with funding from IARPA..One patent is titled “Tracking and prediction of societal event trends using amplified signals extracted from social media,” filed in 2013 and granted in February 2018. The invention, says the patent, relates to “a system for tracking and prediction of social events using amplified signals extracted from social media.”.Another patent is titled “Inferring the location of users in online social media platforms using social network analysis,” filed in 2013 and partially granted in October 2017..The body of scientific literature related to these patents, reviewed by Motherboard, demonstrates a sophisticated technology suite capable of locating the “home” position of users to within 10 kilometers for millions of Twitter accounts, and predicting thousands of incidents of civil unrest from micro-blogging streams on Tumblr..A 2013 slide presentation prepared for HRL laboratories by patent inventor David A. Jurgens, although his research was based on Latin America, showcases examples of how the technology can locate people from within the United States..Both patents, which refer to a massive body of previous IARPA-Department of Interior funded patents, make clear that they were created with support from the US government which retains “certain rights in the invention.”.The Pentagon’s upgraded homeland defense doctrine seems to be part of a wider effort by the Trump administration to prepare for domestic civil unrest in coming months and years..Although these technologies were developed under the Obama administration, it appears their use is being accelerated by the Trump administration—and by moving the Embers program to which these technologies relate into the private sector, this acceleration is occurring in a way that sits beyond public scrutiny or accountability..“This kind of technology-enabled surveillance of social media will likely suppress dissent and lead to biased targeting of racial and religious minorities,” Hugh Handeyside, a senior staff attorney at the ACLU’s National Security Project, told me. “We need to know much more about any proposed policies or programs and their effect on rights that the Constitution protects..The intensification of US social media surveillance coincides with the Trump administration’s augmentation this year of the Pentagon’s role in homeland security..In April 2018, the US Joint Chiefs of Staff issued an updated doctrine on homeland defense. The new doctrine underscores the extent to which the Trump administration wants to consolidate homeland defense and security under the ultimate purview of the Pentagon..The US military’s traditional function is to defend the US from foreign threats rather than interfere with domestic issues. After 9/11, homeland defense and security doctrines have been gradually pushed toward closer integration with the US military in a process first accelerated by the Bush administration..As with previous versions of the doctrine, the document states that ‘Lead Federal Authority’ (LFA) for “homeland security” is the Department of Homeland Security, but simultaneously goes to pains to emphasize again and again how the Department of Defense (DOD) must be active at the epicentre of almost all homeland affairs. This is achieved by interlinking homeland security indelibly with “homeland defense”—the latter defined as a US military function. The doctrine further institutionalizes the necessity of seamless Pentagon “support” for “homeland security“ operations on land and sea:.“DOD is a key part of the HS [homeland security] enterprise that protects the homeland through two distinct but interrelated missions, HD [homeland defense] and DSCA [defense support for civil authorities]. DOD is the federal agency with lead responsibility for HD, which may be executed by DOD alone… or include support from other USG departments and agencies…. While these missions are distinct, some department roles and responsibilities overlap and operations require extensive coordination between lead and supporting agencies. HD and DSCA operations may occur in parallel and require extensive integration and synchronization with HS operations.”.These stipulations are not entirely novel compared to previous iterations. Yet they are augmented by some subtle but unprecedented changes concerning the powers to respond to a domestic “insurrection” and the role of Pentagon intelligence in such a response..“The whole concept of ‘homeland defense’ as a military function may come as a surprise to those who supposed that this is the purpose of the Department of Homeland Security,” Steven Aftergood, who directs the Federation of American Scientists’ Project on Government Secrecy, told me about the updated doctrine. “But in fact the Department of Defense does have a role not only in defense against foreign invasion but also in maintaining civil control. These roles are expanded and elaborated in the new Joint Publication.”.The intensifying militarization of the homeland is happening right now on the pretext of dealing with migrants. Defense Secretary James Mattis has not only supported DHS’s requests for the US military to accommodate two “temporary“ camps to detain migrants during the child separation crisis, but has also approved Trump’s request to dispatch 4,000 National Guard troops to secure the US-Mexico border..Crucially, Aftergood pointed out that some of the most notable changes in the doctrine concern ensuring that classification does not prevent homeland agencies from accessing Pentagon intelligence. The upgraded doctrine says that Pentagon resources can be mobilized for domestic surveillance or “information support” in the context of emergencies..“Military information support forces and equipment may also be used to conduct civil authority information support activities during domestic emergencies within the boundaries of the US homeland,” it reads..Accordingly, the doctrine calls for the Pentagon to engage in more proactive information sharing with civilian authorities at home, requiring a decreased reliance on classification. “DOD’s over-reliance on the classified information system for both classified and unclassified information is a frequent impediment,” the document says..In simpler terms, the doctrine insists that classification should not impede the Pentagon from sharing intelligence with domestic agencies, especially in the context of “civil authority information support“ in homeland emergencies..According to Price, this has ominous implications given that the NSA is a DOD agency. The import is that under the new doctrine, there are greater opportunities to connect domestic intelligence gathered by the NSA with the social media data of American citizens. “At least the data feeding these surveillance and predictive models comes from public social media data,“ Price told me. “But given [Edward] Snowden’s revelations of CIA and NSA widespread surveillance, and the historical abuses of military and civilian intelligence agencies it is a reasonable assumption that these tools sorting and profiling public social media will be used to select groups of Americans, engaging in lawful acts of political dissent, who will have email, SMS, and voice communications monitored by military or civilian intelligence agencies.”.The most pertinent section of the upgraded homeland defense doctrine for this story concerns the powers available to the President in the case of “insurrection,” a major rebellion which either state governors or the President himself deem to fundamentally threaten the rule of law..Once again, though not a new addition to the doctrine, vaguer language is introduced including a stipulation that the authority for a US Army response to an “insurrection” will constitute a “HD [homeland defense]-related purpose.”.This is the first time that an “insurrection” has been described using the phrase “homeland defense,” implying that the response would come under Pentagon jurisdiction. Referring to the US Code’s delineation of insurrection powers (Title 10, USC, Sections 251–255), the doctrine affirms that:.“These statutory provisions allow the President, at the request of a state governor or legislature, or unilaterally in some circumstances, to employ the US Armed Forces to suppress insurrection against state authority, to enforce federal laws, or to suppress rebellion. When support is directed for such HD-related purposes in the US [emphasis added], the designated JFC [Joint Force Commander] should utilize this special application knowing the main purpose of such employment is to help restore law and order with minimal harm to the people and property and with due respect for all law-abiding citizens.”.This invocation of the Insurrection Act connects it directly with homeland defense powers under Pentagon command—a move which is in tension with the idea that DSCA, or “Defence Support to Civil Authorities,” should be led by the DHS, instead giving ultimate authority for such an operation to the DOD..I asked Aftergood whether he thought this amendment should raise alarm bells. On the one hand, he remarked that the US military traditionally had little intrinsic interest in homeland operations. “I do think the changes in DOD doctrine are noteworthy. But it is also true that as a military organization, DOD is generally scrupulous about adhering to defined authority, including limitations to that authority,“ he said. “The Department of Homeland Security is in some respects less disciplined, and more prone to improvise in troublesome ways.”.How far that caution applies in the context of a DOD led by a Trump appointee is an open question. But Aftergood also described the amendments as a potential danger to American democracy: “The whole subject bears careful monitoring, since it potentially poses a challenge to civilian control of government and to the integrity of democratic institutions,” he said..I also spoke to William C. Banks, distinguished professor and founding director of the Institute for National Security and Counterterrorism at Syracuse University’s College of Law, who largely agreed with Aftergood’s assessment. “There is cause for concern due to the ambiguities embedded in the law and the federal guidance supplied through civilian and military agencies on homeland defense,“ Banks warned. “It is not unusual for doctrines like this to be quietly updated and they do this almost every year. But these changes are always worth monitoring due to the risk to democracy.”.I asked Banks, co-author of Soldiers on the Homefront: The Domestic Role of the American Military, about the doctrine’s description of an “insurrection” as a “homeland defense“ issue..“The US military role in the homeland is not new, but in this case there’s a tension between DSCA [Defense Support for Civil Authorities] and homeland defense, because in one setting civilians are in charge, and in another setting the military are in charge,” he said. “The changes to doctrine are not dramatic, but they could make it more likely, maybe inevitable, that those jurisdictional issues might come together or clash in some way.”.The outcome of such a clash could end up putting Trump’s Defense Secretary in charge of a response to a domestic emergency categorized by Trump as an “insurrection.“ Taken in tandem with the US military’s sudden interest in predicting anti-Trump protests after the 2016 elections, the Pentagon’s upgraded homeland defense doctrine seems to be part of a wider effort by the Trump administration to prepare for domestic civil unrest in coming months and years..Indeed, according to Banks, the changes to the doctrine in April could well have occurred as an effort to adapt to the technological developments in social media surveillance under the Trump administration described earlier in this story..“One reason that doctrines are updated is due to changes in technology—military intelligence capabilities will adapt to new technologies, the power of social media, new cybersecurity capabilities,” he said. “The more we learn about those, the more we can envisage new threats and new opportunities to address them. So this new research on social media surveillance is exactly the kind of thing that could prompt changes in doctrine. The Pentagon’s support for this kind of research is concerning and should be closely monitored.”.The Pentagon did not respond to Motherboard’s question about any possible connection between the upgraded homeland defense doctrine and the Pentagon’s new research on social media surveillance..The problem is that however seemingly minor, “shifts in homeland defense doctrine increasingly create possibilities for military and civilian intelligence agencies to engage in political surveillance and harassment,” said Price. “With an unstable president who frequently is unable to differentiate between political and legal threats to him and threats to the nation, we must worry about what President Trump may consider an ‘insurrection’ worthy of massive military surveillance.”.By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.",Pentagon funds social media surveillance to predict anti-Trump protests
120,"Its common now to read of the Chinese governments push to lead the world in applications of artificial intelligence. But, in fact, many of the greatest gains are the result of key individual actions and intersections over many years..Take the case of SenseTime, a standout in facial-recognition technology and, by virtue of an implied capitalization it says exceeds $3 billion, Chinas largest AI unicorn..SenseTimes creation begins with Tang Xiaoous Ph.D. in computer vision from MIT in 1996, which led to a teaching post at Chinese University of Hong Kong. Busy in the same engineering department there was Xu Li, en route to his own doctorate in computer vision and imaging in 2010. (Computer vision is software for recognizing images.).The two formed part of the nucleus of what became SenseTime a few years later, with Xu as CEO. One of Tangs students at the school, Xu Bing, today spearheads the companys fundraising and strategic partnerships. And when Tang took a sabbatical from the university in 2008 to head a computer-vision group at Microsoft Research Asia in Beijing, he met Yang Fan, now SenseTimes head of product development. It took me a couple of years to find out what I really liked to work on at MIT. Then it took more than 20 years of hard work in this area such that we finally start to see some real industrial applications and impacts. It does not happen overnight, Tang, now 49, writes in an email..It also took investors to give the group a critical boost. Justin Niu, a partner at venture capital firm IDG in Beijing, in 2014 visited professor Tang and his research team, still working out of the same university lab. They showed him demos of how a camera enhanced by machine learning could pull distinct faces out of a smoggy days tableau in Tiananmen Square..IDG joined with another firm, StarVC, to lead a $30 million investment into the formative SenseTime as Tang was processing the companys registration paperwork. The professor and his founding group, by then numbering ten and all at least a decade his junior, parked themselves a few miles away in Hong Kongs Science Park. Yang Fan added a contingent from his base, Tsinghua University..In 2014, a series of technology breakthroughs won SenseTime global acclaim from publications such as Nature and Science and other media attention before IDG came calling. These included a research paper showing its computing capability, the worlds first to outperform human vision under certain conditions, and its claim of beating Facebooks DeepFace with its algorithms called DeepID..Fast-forward to 2018, after stakes were taken by other Chinese VCs and private corporations, and by Qualcomm of the U.S. SenseTimes workforce numbers 1,500, with five mainland hubs added to the Hong Kong site. Revenues arent disclosed, but by Forbes Asias estimations approach $100 million..Tang remains the largest individual shareholder. SenseTime sports two faces: a research team working with 18 professors at universities across China and the profit-seeking commercial operations, which Tang, simply referred to as founder, and Xu Li lead. We hope our professors would continue to focus on the fundamental research, including our founding professor, so we can continue to have discoveries, says CEO Xu, who spent three years at a Motorola research center in Shanghai and six months at Omron Research Institute in Kyoto..SenseTimes field is exploding, and it says sales are growing better than fivefold annually. With the proliferation of smart machines and mobile devices, computer eyes are seemingly everywhere. In Chinas burgeoning fintech sector, SenseTimes recognition technology has scanned and authenticated the faces of 400 million consumers, 300 million for China Mobile alone. Its software is being used by Chinas ten largest security surveillance manufacturers, fitted on various Chinese phone makers to catalog, search and enhance user images, and soon should equip autonomous vehicles in a partnership with Honda..To put the current pace in context: It had taken SenseTime nine months to produce its first commercial product: a face-based identity-verification system for opening an account at one of Chinas 4,000 peer-to-peer microlenders, phasing out days of laborious manual processing. SenseTime claims applying its facial recognition to a security lock can be as safe as an eight-digit password, compared to four digits in 2014..In mobile apps, SenseTimes augmented-reality software helps place products in commercials (such as during Alibabas Singles Day promotion) and other short videos and live broadcasts. For instance, a performer could pull a showcased item, such as a soda can, from out of nowhere. On top of the consumer uses come the public security applications, which can be the most lucrative. Theres a strong, hard demand driven by smart cities and surveillance, Yang Fan says..Visitors at SenseTime offices are greeted by cameras that capture their images while instantly matching their identities with an internal registrar and making an estimate of their age, and detecting intruders behind face masks or eyeglasses..With positioning technology capable of processing data from 240 points on a human face, a leap from just 106 points before August 2017, SenseTime-aided monitors capture and count every passerby, whether inside an Ikea outlet or at a busy downtown intersection. That can be used for general crowd data (breaking down by age or sex, say, for marketing) or a specific identity search. Humans are surprisingly good at recognizing faces but no longer a match for machines that can also attach names and data to an image..SenseTime, with an internal training database of 2 billion faces (many of them collected online) and claiming 10 billion more images and videos, has worked with 40 Chinese localities, including Guangzhou, Shenzhen and Chongqing. Yang cites a case in 2016 in which police in Chongqing, powered by its technology, identified 69 suspects and caught 14 fugitives within 40 days. What weve accomplished [once] happened only in movies, he says..At the central government level, SenseTime is working with the Ministry of Public Securitys Third Research Institute to develop the nations so-called citizen network identity infrastructure..The web is another policing opportunity. In 2016 the Cyberspace Administration of China, its central internet regulator, enlisted SenseTime on a national cleanup campaign: to scan and delete online pornography, banishing videos of violence, terrorism and horror as well. This work would otherwise be attempted by legions of laborers. Result: The Chinese web might be the most hygienic on earth. Its very, very difficult to find all these videos on the Chinese internet today, Yang says..Of course, SenseTime does not have this emergent imaging sector to itself. Two prominent Chinese rivals, Megvii of the Face++ brand and Yitu Technology, are on the same hunt for clients. Megvii, founded in 2011 and backed by Ant Financial and iPhone manufacturer Foxconn, claims its FaceID platform has authenticated the identity of 290 million worldwide, with more than 85% market share in security software for Chinas financial industry. Yitu, five years old, with backing from Alibaba founder Jack Mas private Yunfeng Capital and giant Sequoia Capital, is active in health care imaging and fintech, and boasts of being an industry leader with 50% of market share in some unspecified segments in Chinas financial services industry..A Megvii representative argues that SenseTimes accuracy in facial recognition doesnt automatically translate into use applications. Software design needs to fit with computer chips, monitors and an organizations procedures. More and more public security clients look at product compatibility, timeliness and efficiency. Its algorithm could be used as technical support but by itself would not address a security problem, he says..Founder Tang is undaunted. At a recent event in Hong Kong, he told the audience: Dream big. I also learned from my son. He said someday he wanted to build an airplane with a swimming pool in it, calling it 747-380. Thats a combination of Airbus and Boeing. You need to prepare for your dream. I prepared for it for 20 years..Supplementing his AI talents in Hong Kong, he scurried for hardware engineers in Shenzhen, and found plenty of software engineers in Beijing and Shanghai. Today, his company has 150 employees with doctorates--and globally competitive salaries to match. The success of SenseTime is really based on the connection of Hong Kong with Shenzhen, with Beijing and with Shanghai, he says..Its stated ambition is to be the global dominant force in facial recognition, a platform company in the category of Google and Facebook, by taking a page from Intel, the global chip giant. Says CEO Xu, For SenseTime, our algorithms would form the high-entry barrier. We are forming an ecosystem with our business partners. Learning from Intel Inside, we are building up our model. Call it SenseTime Inside. ",Sensetime enables cameras to be enhanced with machine learning to find faces
121,"Based in China and boasting over 1.1 billion global users, it’s one of the world’s most advanced and popular apps. What’s remarkable is the way it reaches into so many corners of a Chinese person’s life: it’s the way much of the country chats, pays, plays, moves, and much more. As Mark Zuckerberg contemplates the future of Facebook, it’s increasingly WeChat he’s trying to emulate. .There’s more to this so-called “super app” than messaging, food, cars, and payments. The all-encompassing ambition of WeChat includes some of the most cutting-edge, quick-acting, and far-reaching censorship technology on earth..New research from the University of Toronto’s CitizenLab pulls the curtain back on how WeChat’s real-time, automatic censorship of text and images is used to exert control over political discussion on topics ranging from international issues like the trade war with the US to domestic scandals like the disappearance of court documents in a 2018 dispute between two multibillion-dollar Chinese mining companies. All discussion is ultimately subject to the Chinese government’s approval..WeChat’s censors face two types of challenges. Big public posts on WeChat Moments, a public feature similar to Facebook’s Timeline, are scrutinized and filtered by algorithms that can sometimes take over 10 seconds to run—a glacial pace on social media. But one-on-one and group messages are a different problem entirely, because they are often intimate and instant conversations. That requires real-time censorship..Text is relatively easy to search and censor. Image filtering is harder, especially when you’re trying to examine and censor the images  almost instantly. To accomplish this task, WeChat keeps a massive and always growing index of MD5 hashes, small cryptographic data signatures that are unique to every file. When a censored image is sent, it will be caught by the hash index and deleted. Neither the sender nor the recipient is ever likely to know anything was censored..If the image isn’t instantly censored, it’s sent for automatic analysis. Using optical character recognition, the image is examined for text (sending screen shots of newspaper articles was once an easy way past censors). The image is then checked for visual similarity to other censored images. So-called harmful content—including anything about international or domestic politics deemed undesirable by the Chinese Communist Party—will be sniffed out, removed from the conversation, and then added to that original hash index, which flags it for instant censorship from that moment onward. It’s a self-reinforcing system that’s growing with every image sent. The latter systems are also used on WeChat’s Moments to check and build the company’s dynamic blacklist..The Chinese tech giant Tencent, which owns WeChat, is under “a great deal of pressure” from Beijing to implement effective censorship tech, says Adam Segal, the director of the digital and cyberspace policy program at the Council on Foreign Relations. “The Chinese firms are all responsible for content, and while they have relied on tens of thousands of human censors, they have also been developing new [machine learning] approaches to content take-downs. Inside of China, it is part of the larger trend under Xi Jinping of tightening controls on the Chinese internet and society more broadly.”.WeChat is so pervasive in China that the prospect of getting suspended or banned can disrupt lives. The app combines the features of Facebook, Uber, GrubHub, and more. You can book doctor’s appointments, pay utility bills, talk to professional contacts, or engage government services..“This has really become a mega-app,” says Sarah Cook, the senior research analyst for East Asia at the pro-democracy research group Freedom House. “It’s really hard to function in modern Chinese society without using WeChat, and so the chilling effect is real.”.Under the direction of Beijing, Tencent’s censors are adroitly responsive to current affairs. In addition to deadly historical events like the Tiananmen Square protests and the Cultural Revolution, new events are quickly added. .Researchers found that newsworthy events like the arrest of a Huawei executive on charges of fraud and the CRISPR-baby scandal in which a Chinese scientist announced the birth of girls with edited genomes triggered waves of censorship. So did a host of other issues, like conflict with the US and domestic corruption or other problems..One event triggered even more censored images: in December 2018, a scandal erupted when the Chinese Supreme Court admitted the disappearance of documents in a billion-dollar dispute between mining companies. Researchers found that dozens of images relating to the alleged theft of the documents were censored..“In the last few years, you’ve had the combination of tightening censorship restrictions and tightening controls,” Cook says. “There’s less official tolerance for conversation, expression, and networking that had been tolerated previously. You’ve seen Tencent and WeChat, in particular, get hit with fines. So now they’re trying to employ new methods to catch so-called ‘harmful content’ that the Chinese Communist Party wants to stop. That’s where real-time automated censorship comes in.” ",WeChat automatically censors images critical of the Chinese Communist Party
122,"For the past couple of years a big story about the future of China has been the focus of both fascination and horror. It is all about what the authorities in Beijing call “social credit”, and the kind of surveillance that is now within governments’ grasp. The official rhetoric is poetic. According to the documents, what is being developed will “allow the trustworthy to roam everywhere under heaven while making it hard for the discredited to take a single step”..As China moves into the newly solidified President Xi Jinping era, the basic plan is intended to be in place by 2020. Some of it will apply to businesses and officials, so as to address corruption and tackle such high-profile issues as poor food hygiene. But other elements will be focused on ordinary individuals, so that transgressions such as dodging transport fares and not caring sufficiently for your parents will mean penalties, while living the life of a good citizen will bring benefits and opportunities..Online behaviour will inevitably be a big part of what is monitored, and algorithms will be key to everything, though there remain doubts about whether something so ambitious will ever come to full fruition. One of the scheme’s basic aims is to use a vast amount of data to create individual ratings, which will decide people’s access – or lack of it – to everything from travel to jobs..The Chinese notion of credit – or xinyong – has a cultural meaning that relates to moral ideas of honesty and trust. There are up to 30 local social credit pilots run by local authorities, in huge cities such as Shanghai and Hangzhou and much smaller towns. Meanwhile, eight ostensibly private companies have been trialling a different set of rating systems, which seem to chime with the government’s controlling objectives..The most high-profile system is Sesame Credit – created by Ant Financial, an offshoot of the Chinese online retail giant Alibaba. Superficially, it reflects the western definition of credit, and looks like a version of the credit scores used all over the world, invented to belatedly allow Chinese consumers the pleasures of buying things on tick, and manage the transition to an economy in which huge numbers of people pay via smartphones. But its reach runs wider..Using a secret algorithm, Sesame credit constantly scores people from 350 to 950, and its ratings are based on factors including considerations of “interpersonal relationships” and consumer habits. Bluntly put, being friends with low-rated people is bad news. Buying video games, for example, gets you marked down. Participation is voluntary but easily secured, thanks to an array of enticements. High scores unlock privileges such as being able to rent a car without a deposit, and fast-tracked European visa applications. There are also more romantic benefits: the online dating service Baihe gives people with good scores prominence on its platforms..Exactly how all this will relate to the version of social credit eventually implemented is unclear: licences that might have enabled the systems to be rolled out further ran out last year. There again, Ant Financial has stated that it wants to “help build a social integrity system” – and the existing public and private pilots have a similar sense of social control, and look set to feed the same social divisions. If you are mouldering away towards the bottom of the hierarchies, life will clearly be unpleasant. But if you manage to be a high-flyer, the pleasures of fast-tracking and open doors will be all yours, though even the most fleeting human interaction will give off the crackle of status anxiety..It would be easy to assume none of this could happen here in the west. But the 21st century is not going to work like that. These days credit reports and scores – put together by agencies whose reach into our lives is mind-boggling – are used to judge job applications, thereby threatening to lock people into financial problems. And in the midst of the great deluge of personal data that comes from our online lives, there is every sign of these methods being massively extended..Three years ago Facebook patented a system of credit rating that would consider the financial histories of people’s friends. Opaque innovations known as e-scores are used by increasing numbers of companies to target their marketing, while such outfits as the already infamous Cambridge Analytica trawl people’s online activities so as to precisely target political messaging. The tyranny of algorithms is now an inbuilt part of our lives..These systems are sprawling, often randomly connected, and often beyond logic. But viewed from another angle, they are also the potential constituent parts of comprehensive social credit systems, awaiting the moment at which they will be glued together. That point may yet come, thanks to the ever-expanding reach of the internet. If our phones and debit cards already leave a huge trail of data, the so-called internet of things is now increasing our informational footprint at speed..Personal data and its endless uses form one of the most fundamental issues of our time, which boils down to the relationship between the individual and power, whether exercised by government or private organisations. It speaks volumes that in Whitehall responsibility for such things falls uncertainly between the culture secretary, Matt Hancock, whose “digital” brief includes what the official blurb limply calls “telecommunications and online”, the Treasury and an under-secretary of state in the business department, Andrew Griffiths, whose portfolio takes in “consumers”..That is absurd, and it may yet play its part in our rapid passage into a future that could materialise in both east and west, in which we do what we’re told, avoid the company of undesirables – and endlessly panic about how the algorithms will rate us tomorrow.",Sesame credit scores people from 350 to 950 based on interpersonal relationships and consumer habits
123,"An artificial intelligence bot called Zach is creating a stir in the medical community. A doctor in Christchurch is teaching it to write patient notes. An Otago professor has it interpreting ECG results. But AI experts are not convinced. David Farrier goes in search of Zach..Last week I heard murmurings that a New Zealand healthcare organisation had been approached to trial an artificial intelligence technology called “Zach”. .I love Artificial Intelligence. The highly underrated Bicentennial Man is one of my favourite films! If some good AI was in New Zealand, I sure as hell wanted to know about it. But a quick Google showed me that the AI system, Zach, was already being trialled by a healthcare provider in Christchurch. .“Imagine having an assistant who listens to your consultations and immediately summarises them into clinical notes for you and memos for your patients,” stated the piece. An artificial intelligence system that can work hand in hand with medical doctors, listening to and interpreting their notes? It sounded good, very good – almost too good to be true. .Utterly fascinated, I began poking around, starting a journey that would take me around the internet, into the minds of medical professionals, and ultimately into a bizarre exchange about the nature of magic..The tweet in question was part of a conversation about The Terrible Foundation, a charity founded by Albi Whale. Someone mentioned Alberic Whale didn’t have much of a digital footprint. “There’s a story in that I reckon,” tweeted Vaughn Davis..That was four years ago. And it appears there was a story in it: “The curious tale of Alberic Whale: Turning over his company to world charity” appeared on Stuff business in January 2017. The feature – all 3000 plus words of it – adopted a somewhat sceptical tone, but John McCrone largely let the boasts made by 25 year old Whale go unchallenged..“Maybe he also does actually have a collection of 56 vintage Minis as an investment even though he hasn’t got a driver’s licence,” McCrone wrote, without seeking any proof the Minis actually existed. The best the story offers is a file image of three Minis from The Italian Job..A core part of Albi’s origin story is that he met semi-retired UK businessman William Kreuk online, and started a business with him called Red Dog, after which “the money started pouring in”. Thing is, there is no record of any current or former shareholder or director in New Zealand or the UK with that name, nor are there any obvious contenders for a business called Red Dog which fits the description. In fact apart from the Stuff article, the name “William Kreuk” returns no meaningful results in a Google search. Nor does Bill Kreuk, nor Billy Kreuk, nor W Kreuk..It’s interesting to note that around the same time Albi’s UK-based company was apparently earning “hundreds of thousands a year”, his New Zealand business, Luminous Group Limited, was put in liquidation by the High Court. Liquidators found a bank account with a $3 balance, an iPhone, and two old laptops. The Stuff feature goes on to say that Albi went on a “frenzy” of business start-ups when he established Terrible, including Terrible Print, Terrible Energy, Terrible Ideas and Terrible as a Service..I checked, and Terrible Energy and Terrible Ideas were the only ones ever registered. Terrible Energy was removed from the Companies Register in July 2015. Is Albi Whale, and all this Terrible stuff, for real? At one point in the article Albi says he built his own cellphone tower out of scrap electronics when he was 12: “It was on a broomstick my brother screwed to the side of the house. My mother was infuriated.”.While it may have been possible to intercept the audio of conversations on analogue cellphones, experts I asked said the software and hardware required for pairing and management of phones akin to a “cellphone tower” was simply out of the reach of any hobbyist. .Over the last four years Albi Whale has moved on from cell phone towers to Artificial Intelligence. In 2014, he established Hannah Group Ltd, in order to explore and exploit an AI system he referred to as “Hannah”. In the Stuff story, he speaks of an AI system called Project Artemis which will soon take over management of Terrible. Albi was going to demonstrate Project Artemis in 2015 at the Epic Innovation Centre in Christchurch, however organiser Rebecca Tavete says he cancelled the demonstration at the last minute. .In Terrible’s 2017 Annual Report filed with Charities Services, it mentions that it is “directly in control of Titan (a.k.a Zach)”. In a second Stuff feature from August last year, Zach, the AI, is now running Terrible. So: Hannah, Project Artemis, Titan and… Zach..I wanted to talk to the man currently working with, and training, Zach: Dr Robert Seddon-Smith. According to that NZ Doctor article, “a GP could upload a patient’s records to Zach and ask the system to pick up indications for medications or come up with a summary of care.”.Dr Seddon-Smith is a general practitioner in Christchurch and owns the Hei Hei Health Centre and Sumner Health Centre. As with many GPs, it takes a few days to get him on the phone, but when I do he’s happy to talk about Zach. We chatted for 45 minutes..“The initial project was to see if Zach could create clinical notes, from just listening to a consultation. Which is pretty amazing when you consider no other technology can do anything like that,” he enthuses. “We start by getting patient consent. We explain this is this really weird thing we’re doing, and do they mind? And as long as at the end of the consult myself and the patient agree that the patient is non identifiable, we upload [the consultation] to the machine… and the machine returns notes.”.Seddon-Smith tells me it’s his job to “train” Zach. So when patient notes come back with errors, he feeds back tips and reference material to Zach, and Zach will learn. “It cannot just do analysis of data, it can apply judgement, according to how it’s trained.”.I’m told that getting a response from Zach is not instantaneous. Zach can only deal with so many queries at once, and it can take 20 minutes or longer to get the information back. .Still, Seddon-Smith says that a fully-trained Zach would mean he gets to spend more time with his patients, without needing to worry about writing up notes. They would just be provided to him later in the day by Zach..“The whole idea of the machine was it was good at natural language processing. And it uses a completely different approach to anyone else’s. Most people are using neural networks, and that is all very well – and has advantage of being portable, reproducible, and you can run it on a Windows box. But this runs on its own special hardware. It has its own custom made hardware – custom silicon – which is designed for natural language processing..I ask how Seddon-Smith came to learn about this technology, which led to him trialling it with real patients. “When Pegasus Health got into developing HealthOne I elbowed my way in, saying they needed people who knew IT,” he tells me. “I have a profound interest in IT. I have ever since I was eight. I have been programming computers on and off since I was 11. Longer than you have been on the planet.” .“Eventually Pegasus wisely formed a committee of people to oversee that project. So we are sitting there, having a chat about the features we’d like in a practice management system, when a colleague mentioned facetiously he’d like it to write patient notes for him. And this is  overheard by one of the other people there – who happened to be involved with the AI. After that project had concluded, he approached me.”.“My understanding is not 100% clear, but Albi basically was the founder of the Terrible Foundation, a young man who has made millions from just being the right place at the right time,” says Seddon-Smith. “To be fair, he works quite hard to get to the right places at the right time. Somehow he invented the early technology that led to the development of the AI. His dad and I got friendly, and so ultimately we have gone into business partnership together, to see what we could do with the computer [AI] in the healthcare space.”.By this point, I had fired off emails to David Whale and Albi Whale, saying I wanted to talk to them about Zach for The Spinoff. I also friended Albi Whale on Facebook and sent him a direct message. .In the meantime, I was curious where else Zach was being pushed, beyond the Hei Hei Health Centre. A source in the healthcare system told me David and Albi Whale had “impressed many senior health professionals in New Zealand”. The Canterbury District Health Board confirmed their interest. They are “yet to meet with them, but are planning to.”.Dr John W Pickering, an associate professor at the University of Otago, is also working with Zach. “Last Friday I sent my first email to an intelligent machine called Zach. Zach is a distributed machine running on custom silicon and software,” Pickering wrote on his blog [Internet Archive link] late last year. “We aim to teach it to perform another common task, namely that of interpreting the electrocardiogram. If Zach can interpret these as well and more consistently than most physicians it could be a useful tool for the busy physician.”.I got Professor Pickering on the phone and confirmed he had a research group working on it. He rejected any scepticism that he was in the process of teaching an intelligent machine how to interpret patient’s ECGs. “I am absolutely using it and thrilled to be using it,” he told me..With so many people seemingly impressed by this brand new AI, I’d been thinking about the way Zach communicated (over email), the way it learned, and those response times. Also all the technical talk like “custom silicon”..I’d also heard from people who said that Zach occasionally had bad spelling. Keeping in mind everything I had learnt about Albi and David Whale, I began wondering:.After all, Albi’s Hannah Group co-founder, Chaley Warne, had seemed a little uncertain in the Stuff article: “It could have been a person somewhere. It was hard to tell.”.“This is just Tommy Wiseau-level baffling,” one Healthcare insider emailed me, likening the whole thing to the mystery behind the enigmatic creator of The Room..The more I read about David Whale, Albi Whale and the Terrible Foundation, the more terrible I felt. But maybe my suspicion that Zach was just a person was wrong. Maybe Zach was a complex AI, a world first..I decided to run a random sample of claims about Zach past some experts in the field of AI. All expressed a combination of intrigue and deep suspicion. .“I had a good chuckle reading through this,” said one. “No idea how you manage to find stuff like this in New Zealand – or maybe because it is such a small place, it allows such things to happen.”.Another expert in AI, the managing director of a Fortune Global 500 company in Australia, agreed to critique some of what I’d learned about Zach..CRITIQUE: AI’s ability to process responses is near instantaneous (at last as far as we are concerned – Watson provides complex analysis in about 30 secs for medical recommendations per query). The notes should be within minutes as soon as the audio clip is provided – and regardless of time of day and number of concurrent requests..CRITIQUE: This is just silly. Communicating over emails just doesn’t make sense. Machines have to be taught Natural Language Processing (NLP). By default it will have to take a bunch of parameters via an API (interface – with a defined format – command + <parameters>). .I also talked to Dr Ian Watson, associate professor in computer science at the University of Auckland. His research tends to focus on artificial intelligence, machine learning and knowledge-based systems..“Well, myself and colleagues who lead Amazon’s Alexa voice recognition programme and IBM’s Watson programme have never ever heard of David Whale, or Zach, or Terrible within the AI community,” he says..“I’d say with some confidence that this is someone trying to jump on the AI Machine Learning bandwagon. It’s unlikely Whale could have solved a problem that large teams of researchers at IBM, Amazon, Google, Apple, Microsoft, and in the top computer science departments around the world are still working on – and, mostly, sharing their results in the public domain.”.However the medical professionals I spoke to who have been granted access to Zach – Dr. Robert Seddon-Smith and Associate Professor Pickering – appear entirely satisfied that they’re emailing back and forth with an advanced AI, spelling mistakes and all..When I told Pickering that it had crossed my mind he might be emailing back and forth with a person, he responded by asking if I was a conspiracy theorist. I told him I wasn’t. .And the field of application, after all, isn’t just anything – it’s health. Surely the bar for authentication is higher when health professionals are interacting with a system so mysterious they don’t even know where in the world it is located, and asking that mysterious system not just to write up patient notes, but interpret those notes..“The board didn’t originally believe it could be used in the healthcare space. I have had to demonstrate that it can,” says Seddon-Smith, who estimates a quarter of his time is spent dealing with Zach..“So the board is on board now?” I ask. “Oh yeah. They’re on board now. They’re supporting it. They’ve granted us – I can’t say how much – but millions of dollars worth of access to the system. To develop this. It’s not cheap. This isn’t something you can run on a Raspberry Pi. It’s several hundred tonnes of liquid nitrogen cooled supercomputer.”.As I talked to Seddon-Smith, I wondered what his role was in all this. An enthusiastic helper or a core part of Team Terrible? We do know, at least, that he is a trustee of the Terrible New Zealand Charitable Trust:.He continued: “What I am able to tell you – at the moment we are using the computer that is overseas. There is no instances of it over here. We are importing a set of the hardware to New Zealand to form part of a Discovery Centre. I am allowed to tell you that the firewall that we are installing consists of multiple Cray XC50 supercomputers.”.The XC50 is one of the most powerful computers on the planet. NIWA recently spent $18 million to get two of them. Oh, and according to Seddon-Smith, Zach is being trained as a legal advisor, too..Rereading that Stuff article, it becomes clear how strikingly unquestioning it is about some extraordinary, even bizarre claims. The same goes for the NZ Doctor piece. .As for the man supposedly behind all this, I was unable to talk to Albi Whale. He never responded to my requests for comment, whether on email or via Facebook. .As to where Zach is being trialled: “I am not at liberty to answer the first part of your question – NDAs are in effect. Rob [Seddon-Smith] has access and is training the system but is not using it in a production sense. The Foundation has decided to launch Zach’s public services from Christchurch, New Zealand. There will be announcements around these in the coming months.”.“Yes. There will be a share offer for Omega Health targeted at small investors. Discussions with the new administration are at an early stage. We will approach the Christchurch City Council with respect to our proposed new Centre in the central city. This centre will be an educational resource, a place to interact directly with Zach, incubator, commercial leases etc. It will be one of the largest computing resources in the world. Available in 2019. It will attract huge interest internationally.”.David Whale finished his first email with a request that I remember they are “first and foremost a charity. I would appreciate a chance to comment on anything you write before publication. I have been as free as I am able with information and would not like to see any misrepresentation.”.He signed off with “I presume your research has uncovered the following links”, before listing all the places Zach has appeared in the media. The links are all here (my descriptions; his links). .From what I can tell, those links are literally everything that exists about Terrible, Zach, David and Abli Whale in its entirety. Read it all, please, and let me know what you learn. I have read it all, I’ve sought to speak to everyone involved, and what of real substance have I found out about this thing? .At one point David Whale tells me, “There is a scientific paper, being reviewed, that describes the natural language processing capabilities in the medical arena.”.In frustration and desperation, I wrote back seeking some clarity from David Whale. I wanted clarity about something. Anything. I also asked again to speak to his son. That request was ignored. For full transparency, our emails can be read in full here. Towards the end of our email chain, David Whale simply opened with: “As AC Clark said ‘Any sufficiently advanced technology is indistinguishable from magic’.”.In his final email to me, responding to alternative explanations for the whole enterprise, he wrote: “If I were to follow your argument then we have duped many people.”.Whale had had enough of our correspondence. “I see little point in pursuing this conversation,” he said. “You cannot persuade those who choose not to believe.”.For me, this whole story was nicely summed up at one point during my conversation with Seddon-Smith, as he mused on what it’s like to work a powerful AI system:.This story was made possible by reader contributions to The Spinoff Members, which supports our investigative journalism. Read more here – or click here to donate.",An AI that took notes from health patients
124,"On TikTok, all is not as it seems. Engage with dance videos and you’ll start seeing more people doing the Renegade. If you linger on a TikTok dog, it will give you puppies galore..But TikTok’s algorithmic obsession with giving you more content that it thinks you will like is having an unintended consequence: it’s started recommending people new accounts to follow based on the physical appearance of the people they already follow..This week Marc Faddoul, an AI researcher at UC Berkeley School of Information, found that TikTok was recommending him accounts with profile pictures that matched the same race, age or facial characteristics as the ones he already followed..He created a fresh account to test his theory and followed people he found on his ‘For You’ page. Following the account of a black woman led to recommendations for three more black women. It gets weirdly specific – Faddoul found that hitting follow on an Asian man with dyed hair gave him more Asian men with dyed hair, and the same thing happened for men with visible disabilities..TikTok denies that it uses profile pictures as part of its algorithm, and says it hasn’t been able to replicate the same results in its own tests. But the app uses collaborative filtering – where recommendations are made based on what other users have done..Our recommendation of accounts to follow is based on user behavior, says a spokesperson from TikTok. Users who follow account A also follow account B, so if you follow A you are likely to also want to follow B. And this has the potential to add unconscious bias into the algorithm..“The platform is very appearance driven, and therefore collaborative filtering can lead to very appearance specific results even if the profile picture is not used by the system,” says Faddoul. TikTok’s algorithm will think it is creating a personalised experience for you, but actually it is just building a filter bubble – an echo chamber where you only see the same kind of people with little awareness of others..This isn’t the first time TikTok’s algorithm has been accused of racial bias. In October 2019 TikTok users of colour called for better representation on the For You page, where users go for recommendations and new tailored content. In January 2019, Whitney Phillips, a professor of communication and online media rhetoric at Syracuse University told Motherboard that the way TikTok works could lead users to replicate the community with which they identify..To test the findings we created a new account, went on the ‘For You’ page, swiped left to view a profile and followed to see who was recommended. The first account that came up was that of KSI, an internet personality and rapper with 1.2 million followers on TikTok. We followed KSI and the next three recommended accounts were one with a profile picture of a ghostly-looking man sitting too far away from the camera to even guess his race, a blurry picture of what looks like a teenager at a festival, and a very close up picture of a white man’s face. All three are verified, but none look particularly similar..After that, results started to appear that were similar to what Faddoul found. An account a pet owner set up for their dog produced recommendations for other dog accounts. Following a young black man led to recommendations for two other black men and one cartoon of a black man. Following an 87-year-old man led to three recommendations for three more older men. Following a white woman with brown hair led to three more white women with brown hair, then finally, following an account with the Union Jack as its profile picture sprung up three more Union Jacks, one with a trollface superimposed on top..If you like one elderly man on TikTok, the app assumes that you will enjoy watching others. But this goes a step too far when racial bias is factored in. “People from underrepresented minorities who don’t necessarily have a lot of famous people who look like them, its going to be harder for them to get recommendations,” says Faddoul..On social media we follow people with opinions that we agree with. Algorithms then throw up more of the same content, creating a separation where you don’t see opinions that differ to yours, allowing you to forget that they exist. In a highly visual platform such as TikTok, this applies to how a person looks. Faddoul’s findings may not indicate how TikTok intends its algorithm to work, but shows how user biases may have resulted in these very specific filter bubbles..TikTok has economised people’s attention, using the mountains of data it has about how long people spend watching videos and how they interact with them to hyper-personalise the user, says Jevan Hutson, human-computer interaction researcher at University of Washington School of Law. The data from users across the globe feeds into an algorithm that ends up encouraging a segregation of content..This extraction of data can create patterns that solidify assumptions about particular races or ethnicities, says Hutson. He compares it to dating apps, where the unconscious bias of thousands of users will create an assumption within the algorithm about racial preferences – Coffee Meets Bagel, for example, met controversy when users kept getting recommended people of the same race as themselves even though they never indicated a racial preference. On TikTok, when the app recommends more of the same content and users, it risks leading to radicalisation and segregation..“I think theres no ethical consumption under surveillance,” says Hutson. He is an avid TikTok user, and believes that people sacrifice the data the app collects about them for the sake of getting content they enjoy more..The data TikTok gets from its millions of users feeds into a cycle they get trapped in, and even if you make an effort to diversify your feed, everyone else’s bias will mean the algorithm will keep trying to channel you into a bubble.","Due to a biased feedback loop, Tik Tok uses race, age and gender for profile recommendations"
125,"Facebook’s algorithm “actively promotes” Holocaust denial content according to an analysis that will increase pressure on the social media giant to remove antisemitic content relating to the Nazi genocide..An investigation by the Institute for Strategic Dialogue (ISD), a UK-based counter-extremist organisation, found that typing “holocaust” in the Facebook search function brought up suggestions for denial pages, which in turn recommended links to publishers which sell revisionist and denial literature, as well as pages dedicated to the notorious British Holocaust denier David Irving..The findings coincide with mounting international demands from Holocaust survivors to Facebook’s boss, Mark Zuckerberg, to remove such material from the site..Last Wednesday Facebook announced it was banning conspiracy theories about Jewish people “controlling the world”. However, it has been unwilling to categorise Holocaust denial as a form of hate speech, a stance that ISD describe as a “conceptual blind spot”..The ISD also discovered at least 36 Facebook groups with a combined 366,068 followers which are specifically dedicated to Holocaust denial or which host such content. Researchers found that when they followed public Facebook pages containing Holocaust denial content, Facebook recommended further similar content..Jacob Davey, ISD’s senior research manager, said: “Facebook’s decision to allow Holocaust denial content to remain on its platform is framed under the guise of protecting legitimate historical debate, but this misses the reason why people engage in Holocaust denial in the first place..“Denial of the Holocaust is a deliberate tool used to delegitimise the suffering of the Jewish people and perpetuate long-standing antisemitic tropes, and when people explicitly do this it should be seen as an act of hatred,” he added..Researchers also found that Holocaust denial content is readily accessible across Twitter, Reddit and YouTube. They identified 2,300 pieces of content mentioning “holohoax” – a term often used by deniers – on Reddit, 19,000 pieces on Twitter and 9,500 pieces of content on YouTube, all created in the past two years..This week a coalition of UK faith leaders will call on the government to get tougher with social media companies, arguing they are still too weak on tackling racist, antisemitic, Islamophobic and anti-Hindu hate online. However, ISD said that tackling the issue was straightforward, pointing to a sharp reduction in mentions of “holohoax” on YouTube since spring 2019, when the video-sharing platform banned Holocaust denial content..On Reddit, researchers noted how concerns from other users were effective in hiding and discrediting Holocaust denial content. Other factors limiting the visibility on Reddit included the banning of groups dedicated to Holocaust denial and moderators deleting comments..Jakob Guhl, ISD research coordinator, said: “Our findings show that the actions taken by platforms can effectively reduce the volume and visibility of this type of antisemitic content. These companies therefore need to ask themselves what type of platform they would like to be: one that earns money by allowing Holocaust denial to flourish, or one that takes a principled stand against it.” A significant amount of denial content is couched in careful language, codes and tropes, and thus this analysis probably does not show the true extent of the spread of such content on social media..A Facebook company spokesperson said: “We take down any post that celebrates, defends, or attempts to justify the Holocaust. The same goes for any content that mocks Holocaust victims, accuses victims of lying, spews hate, or advocates for violence against Jewish people in any way. .“We also remove groups and pages that discuss Holocaust denial from recommendations and references to it in search predictions. While we do not take down content simply for being untruthful, many posts that deny the Holocaust often violate our policies against hate speech and are removed..They added: “In countries where it is illegal, such as Germany, France and Poland, this content is not allowed in accordance with the law. Striking the right balance between keeping people safe and allowing free speech is difficult and we know many people strongly disagree with our position. We are constantly developing and reviewing our policies and consulting with organisations around the world to ensure we’re getting it right.” ","When users typed holocaust, the Facebook search function brought up suggestions for denial pages"
127,"Nearly 40% of A-level grades submitted by teachers are set to be downgraded when exam results in England are published next week, the Guardian has learned, as criticism intensifies of this year’s makeshift results..Analysis of the algorithm and data used by the exam regulator Ofqual to distribute grades after the cancellation of exams amid the coronavirus pandemic found that a net 39% of assessments of A-level grades by teachers are likely to be adjusted down before students receive their results..That would mean nearly 300,000 A-levels issued are lower than the teacher assessment of the more than 730,000 A-level entries in England this summer..Including GCSEs, which are expected to have a similar downgrade rate, close to a net 2m teacher assessments will be adjusted downwards and in many cases ignored completely..There was uproar in Scotland this week when the exams authority rejected nearly 124,000 grade recommendations from teachers – a quarter of the total – but unlike in Scotland, English pupils are barred from appealing against their results on academic grounds..Grades will instead be issued according to Ofqual’s statistical model, relying on a school’s recent exam history and each pupil’s previous exam results, to replace the exams scrapped by the government after schools were closed because of the coronavirus lockdown..Those most at risk of receiving revised grades appear to be students on the border between B and C grades, and between C and D grades, and pupils at comprehensive schools with wide variations in attainment or patchy outcomes in courses over the three previous years of data that Ofqual is using to cap individual school results..Teachers will still have a significant influence on how grades are distributed in each school, having compiled the rankings that will determine which pupils receive the final grades allocated by Ofqual for their course..Headteachers and exam officials in England say they fear a storm of controversy even worse than that which has engulfed Scotland, where a quarter of teacher predictions were adjusted by the Scottish Qualifications Authority..Experts say that as Ofqual has barred individual pupils from appealing against their grades on academic grounds, families should not waste time complaining but instead contact college or university admissions offices to confirm their places in the event of unexpectedly poor grades..Tim Oates, group director of research and development at the exam board Cambridge Assessment, said: “Grades have been awarded this year by combining lots of data, including the rank order and the grades submitted by teachers. We have seen from Scotland’s press coverage that it’s all too easy to fixate on the difference between the teacher-assessed grades and the final grades. But it’s a misleading distraction and misinforms the public. The teacher grades were an important part of the process but always only going to be a part..“On results day, energy should be channelled into how each young person can progress swiftly with what they have been awarded, rather than time lost on agonising over an apparently controversial but fundamentally misleading difference between teacher grades and final grades.”.Statisticians have criticised Ofqual’s algorithm, saying it does not have sufficient data to award grades fairly to most state schools in England, because of wide variations in results within schools and between years..The Royal Statistical Society has called for an urgent review of the statistical procedures used in England and Scotland, to be carried out by the UK Statistics Authority. “This should consider both the substantive issues of the data used and the adjustment algorithms of the various nations, but also whether greater transparency would have been possible and beneficial,” the society said..Huy Duong, the parent of an A-level candidate and a former medical statistician, said he has analysed Ofqual’s published data and comments to calculate that 39% of grades between A* and D will be lower than the teacher assessments. Duong’s findings were privately confirmed to the Guardian by exam officials..Duong’s analysis is based on Ofqual’s statement that A-level grades overall will improve by 2% this summer, and that the submitted teacher assessments, known as centre-assessed grades, would have resulted in 12% inflation in higher grades..“It gives the public the impression that in most cases the grades the student receive would still be the predicted grades. However, closer analysis shows that this is not true,” Duong said..In response, a spokesperson for Ofqual said: “From the data that we have reviewed we expect the majority of grades students receive will be the same as their centre assessment grades, and almost all grades students receive will be the same as the centre assessment grades or within one grade. The exact proportions vary by subject and we will publish the figures on results day.”.But Duong also found that a comprehensive secondary school can have a huge annual variation in results for individual courses, with the small numbers of entries involved suggesting that Ofqual’s decisions are statistically invalid..For instance at Matthew Arnold school in west Oxford, a comprehensive academy, the proportion of A* grades achieved by pupils in a popular subject such as English literature varied from one in 19 to three in 10, or from 5% to 30%, between 2017 and 2019..Duong said: “These fluctuations mean that Ofqual’s statistical modelling cannot make sense. The problem is that any statistical model is only as good as the data you feed it, and for a typical comprehensive school, there simply isn’t enough A-level data from 2017 to 2019 for any statistical modelling.”.In response, Ofqual said: “For A-level, three years of historical results inform the standardisation of grades. You can think of this as an averaging across the years of data.”.Uncertainty over the grades being awarded has led universities to say they will relax offers to prospective students and make use of other data..Admissions officers for Peterhouse College, Cambridge, said during a Q&A on the Student Room website: “We are looking into which schools (from among our offer holders) are most likely to be adversely affected by the system of awarded grades so we are in a better position to make nuanced decisions when we get the results.”.In a significant sign that it recognised the controversy likely to erupt on Thursday when students receive A-level results, Ofqual this week changed its position and said schools would be able to appeal if they expected “a very different pattern of grades to results in previous years”. But Ofqual will not allow individual students to appeal against their grades on academic grounds, as they can in Scotland..For A-level pupils, teacher assessments will only be used to help set grades on small courses, with five or fewer candidates. On larger courses, teacher assessment will play little or no role, with grades being awarded instead based on a school’s recent exam history and each pupil’s previous exam results..The pupils most likely to benefit from teacher assessments will be those taking courses with very small entries of five pupils or less, such as German or music. Those taking popular subjects such as maths or biology, with more than 15 pupils, will be exposed to Ofqual’s algorithm and the teacher assessment will be ignored..Kate Green, Labour’s shadow education secretary, said: “The government should have ensured that Ofqual had a robust appeals system in place from the beginning, instead of announcing one a matter of days before A-level results. They must do far more to ensure the system is genuinely fair.”.Students unhappy with their results have the option of sitting exams in autumn. But with students entering university or college, few are expected to do so.","Ofqual, the algorithm used in these results, has shown unfair and biased results"
128,"With no exams sat this year due to the coronavirus pandemic, the Scottish Qualifications Authority (SQA) ran a system based on teacher assessments..Education Secretary John Swinney will set out the governments plan to fix the issue on Tuesday, with Ms Sturgeon saying the onus would not be on students to submit appeals..Opposition parties are pushing for a vote of no confidence in the education secretary, but Ms Sturgeon said she had faith in Mr Swinney and that the row was not party political..Prime Minister Boris Johnson said he understood the anxiety over grades, and that we will do our best to ensure that the hard work of pupils is properly reflected..In Scotland this was moderated at a national level by the SQA, a process which led to thousands of pupils complaining that they had received lower grades than originally estimated..There was particular criticism after Higher pass rates for pupils in the most deprived data zones were reduced by 15.2%, in comparison with 6.9% for pupils from the most affluent backgrounds..The Scottish childrens commissioners office said pupils from more deprived areas had been downgraded based on the historic performance of their school rather than their performance..Ms Sturgeon said young people in more deprived areas might be concluding that the system is stacked against them, and that she was not prepared to have that outcome..At her daily coronavirus briefing, Ms Sturgeon said steps would be taken to address concerns and ensure that every young person gets a grade that recognises the work they have done..She said ministers had taken decisions we thought were the right ones in unprecedented circumstances, but after a lot of soul searching had now accepted they were not right..She said: Our concern, which was to make sure the grades young people got were as valid as in any other year, perhaps led us to think too much about the overall system and not enough about the individual pupil. .That has meant too many students feel they have lost out on grades they should have had, and that that has happened not as a result of anything they have done but a statistical model or algorithm..Mr Swinney will set out plans for how to address the issue at Holyrood on Tuesday, but the first minister said we will not expect every student who has been downgraded to appeal..She added: This is not the fault of students, and it should not be on students to fix it - thats on us, and we will set out tomorrow how we intend to do that..The education secretary could also face a no-confidence vote tabled by Labour in the Scottish Parliament when it returns from recess this week..The Conservatives say they will support the motion and the Scottish Greens have indicated they would consider backing it if no changes are made..Scottish Labours education spokesman Iain Gray told BBC Radios Good Morning Scotland programme that the simplest and fairest way would be to return grades to what teachers originally projected, saying that anything else would fall short..Ms Sturgeon said she had confidence in Mr Swinney, noting that governments in other parts of the UK were taking broadly the same approach to exam results in difficult circumstances.",SQA led to reduced higher pass rates in deprived datazones compared to pupils from affluent backgrounds
129,"The coronavirus pandemic has been a boon for the test proctoring industry. About half a dozen companies in the US claim their software can accurately detect and prevent cheating in online tests. Examity, HonorLock, Proctorio, ProctorU, Respondus and others have rapidly grown since colleges and universities switched to remote classes..While there’s no official tally, it’s reasonable to say that millions of algorithmically proctored tests are happening every month around the world. Proctorio told the New York Times in May that business had increased by 900% during the first few months of the pandemic, to the point where the company proctored 2.5 million tests worldwide in April alone..Im a university librarian and Ive seen the impacts of these systems up close. My own employer, the University of Colorado Denver, has a contract with Proctorio..It’s become clear to me that algorithmic proctoring is a modern surveillance technology that reinforces white supremacy, sexism, ableism, and transphobia. The use of these tools is an invasion of students’ privacy and, often, a civil rights violation..If you’re a student taking an algorithmically proctored test, here’s how it works: When you begin, the software starts recording your computer’s camera, audio, and the websites you visit. It measures your body and watches you for the duration of the exam, tracking your movements to identify what it considers cheating behaviors. If you do anything that the software deems suspicious, it will alert your professor to view the recording and provide them a color-coded probability of your academic misconduct..Depending on which company made the software, it will use some combination of machine learning, AI, and biometrics (including facial recognition, facial detection, or eye tracking) to do all of this. The problem is that facial recognition and detection have proven to be racist, sexist, and transphobic over, and over, and over again..In general, technology has a pattern of reinforcing structural oppression like racism and sexism. Now these same biases are showing up in test proctoring software that disproportionately hurts marginalized students..A Black woman at my university once told me that whenever she used Proctorios test proctoring software, it always prompted her to shine more light on her face. The software couldn’t validate her identity and she was denied access to tests so often that she had to go to her professor to make other arrangements. Her white peers never had this problem..Similar kinds of discrimination can happen if a student is trans or non-binary. But if you’re a white cis man (like most of the developers who make facial recognition software), you’ll probably be fine..Students with children are also penalized by these systems. If you’ve ever tried to answer emails while caring for kids, you know how impossible it can be to get even a few uninterrupted minutes in front of the computer. But several proctoring programs will flag noises in the room or anyone who leaves the camera’s view as nefarious. That means students with medical conditions who must use the bathroom or administer medication frequently would be considered similarly suspect..Beyond all the ways that proctoring software can discriminate against students, algorithmic proctoring is also a significant invasion of privacy. These products film students in their homes and often require them to complete “room scans,” which involve using their camera to show their surroundings. In many cases, professors can access the recordings of their students at any time, and even download these recordings to their personal machines. They can also see each student’s location based on their IP address..Privacy is paramount to librarians like me because patrons trust us with their data. After 9/11, when the Patriot Act authorized the US Department of Homeland Security to access library patron records in their search for terrorists, many librarians started using software that deleted a patron’s record once a book was returned. Products that violate people’s privacy and discriminate against them go against my professional ethos, and it’s deeply concerning to see such products eagerly adopted by institutions of higher education..This zealousness would be slightly more understandable if there was any evidence that these programs actually did what they claim. To my knowledge, there isn’t a single peer-reviewed or controlled study that shows proctoring software effectively detects or prevents cheating. Given that universities pride themselves on making evidence-based decisions, this is a glaring oversight..Fortunately, there are movements underway to ban proctoring software and ban face recognition technologies on campuses, as well as congressional bills to ban the US federal government from using face recognition. But even if face recognition technology were banned, proctoring software could still exist as a program that tracks the movements of students’ eyes and bodies. While that might be less racist, it would still discriminate against people with disabilities, breastfeeding parents, and people who are neuroatypical. These products can’t be reformed; they should be abandoned..Cheating is not the threat to society that test proctoring companies would have you believe. It doesn’t dilute the value of degrees or degrade institutional reputations, and student’s aren’t trying to cheat their way into being your surgeon. Technology didn’t invent the conditions for cheating and it won’t be what stops it. The best thing we in higher education can do is to start with the radical idea of trusting students. Let’s choose compassion over surveillance..An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.","Proctorio, a test proctoring software,reinforces structural oppression such as asking a Black woman to shine more light on her face"
130,"The “whiteness” of artificial intelligence (AI) removes people of colour from the way humanity thinks about its technology-enhanced future, researchers argue..University of Cambridge experts suggest current portrayals and stereotypes about AI risk creating a “racially homogenous” workforce of aspiring technologists, creating machines with bias baked into their algorithms..The scientists say cultural depictions of AI as white need to be challenged, as they do not offer a “post-racial” future but rather one from which people of colour are simply erased..According to the researchers from Cambridge’s Leverhulme Centre for the Future of Intelligence (CFI), like other science fiction tropes, AI has always reflected racial thinking in society..In the study published in the Philosophy and Technology journal, they argue there is a long tradition of racial stereotypes when it comes to extraterrestrials – from the “orientalised” alien of Ming the Merciless to the Caribbean caricature of Jar Jar Binks..However, AI is portrayed as white because unlike species from other planets, it has attributes used to “justify colonialism and segregation” in the past, the researchers suggest..Dr Kanta Dihal, who leads CFI’s Decolonising AI initiative, said: “Given that society has, for centuries, promoted the association of intelligence with white Europeans, it is to be expected that when this culture is asked to imagine an intelligent machine, it imagines a white machine..The experts looked at recent research from a range of fields, including Human-Computer Interaction and Critical Race Theory, to demonstrate that machines can be racialised, and that this perpetuates “real world” racial biases..This includes work on how robots are seen to have distinct racial identities, with black robots receiving more online abuse, and a study showing that people feel closer to virtual agents when they perceive shared racial identity..Dr Dihal said: “One of the most common interactions with AI technology is through virtual assistants in devices such as smartphones, which talk in standard white middle-class English..The researchers conducted their own investigation into search engines, and found that all non-abstract results for AI had either Caucasian features or were the colour white..Co-author of the paper, Dr Stephen Cave, executive director of CFI, said: “Stock imagery for AI distils the visualisations of intelligent machines in western popular culture as it has developed over decades.",The current portrayals and stereotypes about aI risk creates a racially homogenous workforce who create machines with bias in their algorithms
131,"There is a growing body of evidence that automation is disproportionately impacting women, with the overwhelming majority of high paid, high-tech jobs taken up by men, says researcher.Researchers at King’s College London and the London School of Economics say the use of robots in the workplace has had a “sizeable” detrimental impact on the gender pay gap in Europe..The study found that for every 10 per cent increase in the number of robots being used by a company, there was a 1.8 per cent increase in the conditional pay gap between male and female workers..But researchers did find both men and women saw their pay increase overall due to automation. The Office for National Statistics (ONS) defines automation as tasks currently performed by workers being replaced with technology – potentially involving computer programmes, algorithms, or robots..Dr Cevat Giray Aksoy, one of the report’s authors, said: “At a time when policymakers are putting increased efforts into tackling gender gaps in the labour market, our evidence is important..“Our results suggest that governments not only need to ensure that education and vocational training systems provide people with the right skills demanded in the future, but also need to pay attention to distributional issues. They need to increase efforts to make sure that women and men are equally equipped with the skills most relevant for future employability.”.The study, titled Robots and the Gender Pay Gap in Europe, discovered the impact on the pay gap was especially pronounced in what researchers referred to as “outsourcing destination countries”, where gender inequality was already more noticeable at work..“Outsourcing origin countries” – predominantly Western European countries – did not witness a striking rise in the pay gap comparative to automation..Men were found to be more likely to be in job positions which were higher-skilled and higher in the work-related pecking order which, when coupled with advances in automation, compounded the pre-existing pay gap..Researchers looked at patterns in the UK, Spain, Belgium, Bulgaria, the Czech Republic, Estonia, Finland, France, Germany, Greece, Hungary, Italy, Latvia, Lithuania, the Netherlands, Poland, Portugal, Romania, Slovakia, and Sweden..Fabian Wallace-Stephens, of the Royal Society for the Encouragement of Arts, Manufactures and Commerce (RSA), raised concerns about the study’s findings. .Mr Wallace-Stephens, who is senior researcher at the RSA Future Work Centre, said: “Far from creating a level playing field for the workforce, there is a growing body of evidence that automation is disproportionately impacting women, with the overwhelming majority of high paid, high-tech jobs taken up by men. .“While we are still waiting to see how the pandemic will impact the economy in the long-term, it is likely that this process will accelerate in the coming months. Improving the infrastructure around retraining and re-skilling, alongside a wider culture change, will be necessary if automation is to benefit everyone in our post-pandemic economy.”.A previous report, carried out by the RSA and the Women’s Budget Group, drew attention to the dangers of new technologies worsening existing gender divides in the workplace – calling for recent cases of women suffering from in-built bias in artificial intelligence systems to be met with a “robust response” from policymakers and employers..The study, which came out last August, warned “algorithmic prejudice” could become one of the new giants of modern poverty if it is left unchecked..In March last year, the national statistics office announced women in the UK are considerably more likely than men to be working in jobs endangered by the advance of technology..The ONS said 1.5 million people in England are at high risk of losing their jobs to automation – with women holding more than 70 per cent of those roles believed to be at high risk..The three occupations with the highest probability of automation were found to be waiters and waitresses, shelf-stackers and basic retail roles – all of which are low-skilled or routine. However, well-educated women were also found to be more at risk than men..By registering, you will also enjoy limited access to Premium articles, exclusive newsletters, commenting, and virtual events with our leading journalists",There is evident bias and discrimination as automation will disproportionately impact women
132,"SINCE LAUNCHING a policy on “misleading information” in May, Twitter has clashed with President Donald Trump. When he described mail-in ballots as “substantially fraudulent”, the platform told users to “get the facts” and linked to articles that proved otherwise. After Mr Trump threatened looters with death—“when the looting starts, the shooting starts”—Twitter said his tweet broke its rules against “glorifying violence”. On July 28th the site took down a tweet by Donald Trump junior promoting a malaria drug for covid-19 that plenty of studies discredit..The president says that “social media platforms totally silence conservatives’ voices.” However, a study by The Economist finds the opposite. Twitter’s feed used to show people the latest posts from accounts they followed, but in 2016 it launched an algorithm to serve “relevant” tweets to users, even if they were days old and from unfamiliar accounts. We compared the two systems, and found that the recommendation engine appears to reward inflammatory language and outlandish claims..Our experiment began in June 2019, when we created a clone of Mr Trump’s profile. This bot used his picture, biography and location, and followed the same people as he did. We used it to re-post some of the president’s old tweets over several weeks, so that the algorithm could learn what our Trump clone cared about..Then from September to December we checked every ten minutes if Mr Trump had tweeted something. If so, three things happened. First, our clone repeated the tweet. Second, we checked its Twitter feed and recorded the first 24 posts served by the algorithm. Finally, we simulated what a chronological feed might have looked like, using the 24 most recent tweets by accounts that Mr Trump follows..Our algorithmic and chronological feeds differed starkly. Nearly half the recommended tweets were from users whom Mr Trump does not follow. Using sentiment-analysis tools to extract feelings from text, we found the average curated tweet was more emotive, on every scale, than its chronological equivalent—and more so than Mr Trump’s own posts, too..Sentiment analysis can be confusing. The emotional scores assigned to tweets by, say, Sean Hannity, a right-wing pundit, might be highly negative—not because they reflect poorly on him, but because he stridently criticises others, such as Democrats. Nonetheless, in a sample of 120,000 tweets, the posts recommended by the algorithm were more likely to sit near either end of a positive-to-negative spectrum..Twitter might also boost extreme views. Researchers at Indiana University have classified a list of left- and right-wing websites as untrustworthy or hyper-partisan. We found 1,647 links to such domains on our clone’s algorithmic feed, but only 895 on the chronological one. (Almost all cases on both feeds were right-wing.).Our experiment ended when a change in Twitter’s interface broke our bot. The platform also suspended another Trump clone that copied his looting threat. However, if an algorithmic penchant for sensationalism has remained, then Twitter may be amplifying and profiting from misleading tweets, rather than removing them. Its business is serving ads to 330m users, even if that means grabbing their attention by showing them exactly what they want to believe. Flagging a presidential whopper every now and then will not change that. ■.Published since September 1843 to take part in “a severe contest between intelligence, which presses forward, and an unworthy, timid ignorance obstructing our progress.” ",Twitter recommendation engine favors inflammatory tweets
133,"KUALA LUMPUR, June 12  —  A viral video depicting two men, one of whom resembles a local cabinet minister, has been making rounds on local news sources after the clip was uploaded to a WhatsApp group containing members of the media and several political party members on Monday morning..At roughly 4am this morning, a new development has come to light in the form of a confession video uploaded by a senior private secretary to the Deputy Primary Industries and Commodities Minister..In his confession, the political aide admits to being one of the two men seen in the viral sex video, while also identifying the other man as a prominent cabinet minister. In his 35-second clip, the man calls for the MACC to investigate the minister for corruption, while he also claims that the original video was taken without his consent..One perspective could be that this is a malicious move by internal rivals within the minister’s political party, Parti Keadilan Rakyat. The cabinet minister purported to be in the original clip has reportedly been at loggerheads with current Parti Keadilan Rakyat leader, Anwar Ibrahim for a while now..The Edge reported that internal rifts within the party have steadily worsened, while the recent appointment of new MACC chief, Latheefa Koya is alleged to be part of an elaborate scheme against the PKR leader..Anwar’s supporters call him the PM-in-waiting, but he has also acknowledged that there may be some ulterior agendas within his own party  —  namely from stakeholders that are pushing for the cabinet minister to be Tun Mahathir’s successor as well..On the other hand, it could also be argued that Opposition parties could have the motive. Umno supreme council member Datuk Lokman Noor Adam has indeed lodged a police report while stating;.But it’s important to remember that the only thing that actually lends a degree of credibility to the original sex clip is the confession video uploaded on FB. The first thing to consider then, of course, is its authenticity..Deepfake is a method of human image synthesis that is based on artificial intelligence. This works via superimposing existing images or videos onto source images/videos using machine learning techniques..In other words, deepfakes are AI-generated videos that can look pretty real — in the past, video content was considered to be a viable source of proof (as opposed to photos that could be easily photoshopped), but with the emergence of deepfakes like this, things have clearly changed quite a bit..If you examine the video above, Zuckerberg’s blinking patterns look slightly off, while the general sync between his voice and his mouth does appear unnatural. It’s worth noting that the aforementioned confession video, on the other hand, has a much more natural-sounding audio..But there are tell-tale signs. According to research done at the University at Albany, one of the easiest ways to tell if a video is indeed deepfake is to observe the eyes of whomever is in the video..The report explains that in deepfake videos, blinking patterns are usually physiological signals that aren’t well presented. The average person, according to the research, blinks at a resting rate of 17 blinks/minute, while reading will drop this to 4.5 blinks/minute and 26 blinks/minute during conversation..Examining the confession/accusatory video from earlier, it actually looks authentic. Do note that this is my opinion, based on the political aide’s blinking pattern during the video. In addition to that, deepfake videos have also been known to have slightly distorted/monotonous audio, and the video here has relatively natural-sounding audio..One last caveat, however. The aforementioned research was conducted in June 2018, which may not seem like a long time. But one year is a long time when it comes to technology. — SoyaCincau",A deepfake of a confession video is used in a sex scandal
134,"An AI-powered portrait generator went viral last week thanks to its ability to turn selfies into realistic Impressionist portraits. For people of color, however, the results leave much to be desired..The flaw in AI Portrait Ars, an app built by researchers at the MIT-IBM Watson AI Lab, was first pointed out by Morgan Sung, a reporter at Mashable. She found that the app “whitened my skin to an unearthly pale tone, turned my flat nose into one with a prominent bridge and pointed end, and replaced my very hooded eyes with heavily lidded ones. This result is both terribly disappointing and utterly predictable..I wasnt surprised at the whitewashing at all, since Im used to things like Snapchat filters lightening my skin, making my eyes bigger, narrowing my nose. But I was taken aback by how extreme it was, Sung told Motherboard. “The painting that AI portrait built was a completely different face..In 2019, AI developers should know that algorithmic bias not only exists but is a serious problem we must fight against. So why does it continue to persist? And can we actually stop it? These are open questions that boil down to where you think the blame lies. Is it the case that these algorithms are exaggerating parts of human nature? Are algorithmic biases a reflection of our society’s systemic problems? In the case of the AI Portrait Ars, it may help to trace why it couldn’t draw the faces of people of color in order to figure out why this continues to happen..Part of the problem lies with how AI Portrait Ars fundamentally works. The program relies on a generative adversarial network (GAN), meaning there are two types of algorithms pitted against each other as adversaries to create its portraits. The first type are generative algorithms, responsible for generating new data. The second type are discriminator algorithms, responsible for deciding whether new data belongs to the training dataset..With AI Portrait Ars, the generator learns how to create realistic portraits of people and the discriminator learns how to discern which arent convincing enough based on the dataset. Datasets, then, are of the utmost importance in determining whether or not the GAN will read certain data (facial features) as authentic or not. The training dataset has over 15,000 images, but it’s important to remember where these images were likely pulled from..“This was an experiment by one of our researchers. The images from the apps users were deleted immediately from our servers after the Renaissance portrait was generated. The experiment has run its course,” IBM Research said in a statement to Motherboard..“Also, the tool reflects the data it was trained on: a collection of 15,000 portraits, predominantly from the Western European Renaissance period,” the company continued. “In some cases, it produced a strong alteration of colors and shapes. That’s a reality of the style, not the algorithm.”.This experiment, however, mirrors dozens of other AI and facial recognition experiments that have had far more accurate results for white people than people of color. If the experiment proved anything, it’s that AI researchers continue to be drawn to experiments and research that perpetuate the biases we already know exist in AI research..It’s also not actually a “reality of the style” of Renaissance art that people of color weren’t in paintings of the era. There are many examples of people of color in European art history, though they are largely assumed by the masses to be non-existent in art from the Renaissance..“The material available for illuminating the lives of individual Africans in Renaissance Europe through the visual arts is considerable, though little known to the wider public,” a lengthy 2013 report from Baltimore’s Walters Art Museum called “Revealing the African Presence in Renaissance Europe” notes..It is important to “understand the period in terms of individuals of African ancestry, whom we encounter in arresting portrayals from life, testifying to the Renaissance adage that portraiture magically makes the absent present. We begin with slaves, moving up the social ladder to farmers, artisans, aristocrats, scholars, diplomats, and rulers from different parts of the African continent,” it continues..The problem with AI Portrait Ars reflects how, historically, technology often functions as an extension of the status quo as opposed to a great equalizer. Color film, for example, was initially calibrated to look best with white skin tones since they were the preferred consumer market. In the 1970s, what prompted the industry to even consider better rendering of darker colors was economic pressure from Kodak’s professional accounts. Furniture manufacturers were angry that their advertisements using Kodak color film didn’t capture the difference between dark-grained wood and light-grained wood, while chocolate confectioners were angry that the film couldnt capture all the different shades of chocolate..At this point, AI researchers—especially ones utilizing IBM’s Watson, should know better. In 2018, Joy Buolamwini, founder of the Algorithm Justice League, published her MIT thesis analyzing facial recognition technology from IBM Watson, Microsoft, and Face++ (a Chinese artificial intelligence company). Buolamwini found that all of the programs had the highest error rates for dark-skinned women and the most accurate results with light-skinned men, but that IBM Watson had the highest disparity in the error rates between dark-skinned women and light-skinned men (the error rate was 34.4 percent higher for dark-skinned women). Buolamwini also found that as skin tones got darker, IBM Watson failed to correctly recognize a subjects gender nearly 50 percent of the time..To IBM’s credit, Buolamwini’s research pushed the company to radically improve its facial recognition technology. This, however, hasn’t stopped the problem of racial bias from reappearing in other IBM products like their AI Portrait Ars, or the industry at large. Until we can root out the biases baked into our society that keep reemerging in each new generation of technology, what is to be done?.Caroline Sinders, a machine learning designer who previously worked with IBM Watson, told Motherboard that part of the problem lies with a lack of awareness that we need to test multiple genders or multiple races. At the same time, Sinders asked whether the solution is as simple as more diversity in data. “When these failures pop up, it really does highlight a lack of diversity in the sets. But also having a more diverse dataset for things that use facial images poses a problem where better facial apps lead to … better facial recognition. Do we necessarily want that?”.That’s a valid question when applied to the many in-the-field uses of AI and facial recognition technology, many of which are deployed disproportionately by police against people of color. As Sinders mentioned, better facial apps leads to better facial recognition—but do we need yet another AI face app at all?.Today, the problem of getting datasets that represent populations accurately and the legacy of technology being used to preserve power systems are very much interlinked. In a New York Times op-ed, Buolamwini talks about the coded gaze, a phenomenon where “A.I. systems are shaped by the priorities and prejudices — conscious and unconscious — of the people who design them.” The extremely high rates of misidentification that plague facial recognition software when used on people of color have led to calls for its complete and total ban. These embedded biases can affect hiring prospects, misidentify innocent people, and give unaccountable actors in the private sector or law enforcement apparatus greater information about our personal lives, without our consent. Already some cities have already banned the technology, and Congress is expected to vote on legislation that would forbid face recognition in government-owned public housing..All of this, however, makes clear that it’s not exactly clear what the best way to stop this is. Do we use more data to empower problematic technology? Do we use algorithms to de-bias other algorithms? Do we risk continuing to disrupt people’s lives while we figure this thing out? Maybe all this means that the answer is that we can’t, at least not without first questioning whether such fundamentally problematic technology should exist at all..By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.",An app that turns selfies into impressionist portraits whitens skin
135,"In June, an employee at a tech company received a strange voicemail from a person that identified himself as the CEO, asking for “immediate assistance to finalize an urgent business deal.” As it turns out, despite sounding almost like the CEO, the voicemail was actually created with computer software. It was an audio deepfake, according to a security company that investigated the incident..NISOS, a security consulting company based in Alexandria, Virginia, analyzed the voicemail and determined that it was fake, a synthetic audio designed to trick the receiver, the company told Motherboard. On Thursday, NISOS published a report on the incident, which it shared with Motherboard in advance. .For the last three years, journalists and researchers have spent a lot of time worrying about the possibility of deepfake videos—forged clips where faces are swapped for those of celebrities or politicians using artificial intelligence—influencing elections or causing a major international incident. In reality, that hasn’t happened, and deepfakes have mostly been used to create non-consensual porn videos. .Deepfake audio has been demonstrated in some flashy tech demos. But the technology is also beginning to be used in the criminal world. Last year, a company revealed that an employee had made a transfer of $240,000 after receiving a call from someone who appeared to be their CEO. The call was actually a deepfake, according to the company’s insurer. .The employee who received the voicemail, however, did not fall for it and flagged it to the company, which called in NISOS to investigate. NISOS researchers analyzed the audio with a spectrogram tool called Spectrum3d in an attempt to detect any anomalies. .“You could tell there was something wrong about the audio,” NISOS researcher Dev Badlu told Motherboard in a phone call. “It looks like they basically took every single word, chopped it up, and then pasted them back in together.”.Badlu said he could tell it was fake because there were a lot of stark peaks and valleys within the audio, which is not normal in regular speech. Moreover, he added, when he lowered the volume of the alleged CEO, the background was “dead silent,” there was no background noise whatsoever, which was a clear sign of a forgery. .Rob Volkert, another researcher at NISOS, said they think the criminals were trying the technology out to see if the targets would give them a call back. In other words, he said, this was just step one of a presumably more complex operation that was relatively close to being successful. .“It definitely sounds human. They checked that box as far as: does it sound more robotic or more human? I would say more human,” Volkert said. “But it doesn’t sound like the CEO enough.” .Even though this deepfake audio got caught, Volkert, Badlu and their colleagues believe this is a sign that criminals are starting to experiment with the technology, and we may see more attempts like this one. .“The ability to generate synthetic audio extends an e-criminal’s toolkit and the criminal at the end of the day still has to effectively use social engineering tactics to induce someone into taking an action.” NISOS wrote in its report. “Criminals and potentially broader nation state actors also learn from each other, so as these high-profile cases gain more notoriety and success, we anticipate more illicit actors trying them and learning from others who have paved the way.”.By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.",An audio deepfake was created of a CEO asking for money
136,"Since the onset of the pandemic, a growing number of companies have turned to AI to assist with their hiring. The most common systems involve using face-scanning algorithms, games, questions, or other evaluations to help determine which candidates to interview. .While activists and scholars warn that these screening tools can perpetuate discrimination, the makers themselves argue that algorithmic hiring helps correct for human biases. Algorithms can be tested and tweaked, whereas human biases are much harder to correct—or so the thinking goes. In a December 2019 paper, researchers at Cornell reviewed the landscape of algorithmic screening companies to analyze their claims and practices. Of the 18 they identified with English-language websites, the majority marketed themselves as a fairer alternative to human-based hiring, suggesting that they were latching onto the heightened concern around these issues to tout their tools’ benefits and get more customers..But discrimination isn’t the only concern with algorithmic hiring, and some scholars worry that marketing language that focuses on bias lets companies off the hook on other issues, such as workers’ rights. A new preprint from one of these firms now serves as an important reminder: “We should not let the attention that people have begun to pay to bias and discrimination issues actually crowd out the fact that there are a bunch of other issues,” says Solon Barocas, an assistant professor at Cornell University and principal researcher at Microsoft Research, who studies algorithmic fairness and accountability. .The firm in question is Australia-based PredictiveHire, founded in October 2013. It offers a chatbot that asks candidates a series of open-ended questions. It then analyzes their responses to assess job-related personality traits like “drive,” “initiative,” and “resilience.” According to the firm’s CEO, Barbara Hyman, its clients are employers that must manage large numbers of applications, such as those in retail, sales, call centers, and health care. As the Cornell study found, it also actively uses promises of fairer hiring in its marketing language. On its home page, it boldly advertises: “Meet Phai. Your co-pilot in hiring. Making interviews SUPER FAST. INCLUSIVE, AT LAST. FINALLY, WITHOUT BIAS.”.As we’ve written before, the idea of “bias-free” algorithms is highly misleading. But PredictiveHire’s latest research is troubling for a different reason. It is focused on building a new machine-learning model that seeks to predict a candidate’s likelihood of job hopping, the practice of changing jobs more frequently than an employer desires. The work follows the company’s recent peer-reviewed research that looked at how open-ended interview questions correlate with personality (in and of itself a highly contested practice). Because organizational psychologists have already shown a link between personality and job hopping, Hyman says, the company wanted to test whether they could use their existing data for the prediction. “Employee retention is a huge focus for many companies that we work with given the costs of high employee churn, estimated at 16% of the cost of each employee’s salary,” she adds..The study used the free-text responses from 45,899 candidates who had used PredictiveHire’s chatbot. Applicants had originally been asked five to seven open-ended questions and self-rating questions about their past experience and situational judgment. These included questions meant to tease out traits that studies have previously shown to correlate strongly with job-hopping tendencies, such as being more open to experience, less practical, and less down to earth. The company researchers claim the model was able to predict job hopping with statistical significance. PredictiveHire’s website is already advertising this work as a “flight risk” assessment that is “coming soon.”.PredictiveHire’s new work is a prime example of what Nathan Newman argues is one of the biggest adverse impacts of big data on labor. Newman, an adjunct associate professor at the John Jay College of Criminal Justice, wrote in a 2017 law paper that beyond the concerns about employment discrimination, big-data analysis had also been used in myriad ways to drive down workers’ wages..Machine-learning-based personality tests, for example, are increasingly being used in hiring to screen out potential employees who have a higher likelihood of agitating for increased wages or supporting unionization. Employers are increasingly monitoring employees’ emails, chats, and other data to assess which might leave and calculate the minimum pay increase needed to make them stay. And algorithmic management systems like Uber’s are decentralizing workers away from offices and digital convening spaces that allow them to coordinate with one another and collectively demand better treatment and pay..None of these examples should be surprising, Newman argued. They are simply a modern manifestation of what employers have historically done to suppress wages by targeting and breaking up union activities. The use of personality assessments in hiring, which dates back to the 1930s in the US, in fact began as a mechanism to weed out people most likely to become labor organizers. The tests became particularly popular in the 1960s and ’70s once organizational psychologists had refined them to assess workers for their union sympathies..In this context, PredictiveHire’s fight-risk assessment is just another example of this trend. “Job hopping, or the threat of job hopping,” points out Barocas, “is one of the main ways that workers are able to increase their income.” The company even built its assessment on personality screenings designed by organizational psychologists..Barocas doesn’t necessarily advocate tossing out the tools altogether. He believes the goal of making hiring work better for everyone is a noble one and could be achieved if regulators mandate greater transparency. Currently none of them have received rigorous, peer-reviewed evaluation, he says. But if firms were more forthcoming about their practices and submitted their tools for such validation, it could help hold them accountable. It could also help scholars engage more readily with firms to study the tools’ impacts on both labor and discrimination..“Despite all my own work for the past couple of years expressing concerns about this stuff,” he says, “I actually believe that a lot of these tools could significantly improve the current state of affairs.” .An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.",An AI hiring firm says it can predict job hopping based on your interviews
137,"In mid-2019, researchers at Facebook began studying a new set of rules proposed for the automated system that Instagram uses to remove accounts for bullying and other infractions..What they found was alarming. Users on the Facebook-owned Instagram in the United States whose activity on the app suggested they were Black were about 50 percent more likely under the new rules to have their accounts automatically disabled by the moderation system than those whose activity indicated they were white, according to two current employees and one former employee, who all spoke on the condition of anonymity because they weren’t authorized to talk to the media..The findings were echoed by interviews with Facebook and Instagram users who said they felt that the platforms’ moderation practices were discriminatory, the employees said..The researchers took their findings to their superiors, expecting that it would prompt managers to quash the changes. Instead, they were told not share their findings with co-workers or conduct any further research into racial bias in Instagram’s automated account removal system. Instagram ended up implementing a slightly different version of the new rules but declined to let the researchers test the new version..It was an episode that frustrated employees who wanted to reduce racial bias on the platform but one that they said did not surprise them. Facebook management has repeatedly ignored and suppressed internal research showing racial bias in the way that the platform removes content, according to eight current and former employees, all of whom requested anonymity to discuss internal Facebook business..The lack of action on this issue from the management has contributed to a growing sense among some Facebook employees that a small inner circle of senior executives — including Chief Executive Mark Zuckerberg, Chief Operating Officer Sheryl Sandberg, Nick Clegg, vice president of global affairs and communications, and Joel Kaplan, vice president of global public policy — are making decisions that run counter to the recommendations of subject matter experts and researchers below them, particularly around hate speech, violence and racial bias, the employees said..Facebook did not deny that some researchers were told to stop exploring racial bias but said that it was because the methodology used was flawed..Alex Schultz, Facebooks vice president of growth and analytics, said research and analyses on race are important to Facebook but is a “very charged topic” and so needs to be done in a rigorous, standardized way across the company..“There will be people who are upset with the speed we are taking action,” he said, adding that “we’ve massively increased our investment” in understanding hate speech and algorithmic bias..“We are actively investigating how to measure and analyze internet products along race and ethnic lines responsibly and in partnership with other companies,” Facebook spokeswoman Carolyn Glanville added, noting that the company established a team of experts last year, called Responsible AI, focused on “understanding fairness and inclusion concerns” related to the deployment of artificial intelligence in Facebook products..Facebook has policies prohibiting hate speech that attacks people based on “protected characteristics” including race, ethnicity, religion, gender or sexual orientation. It relies on user reports and automated content moderation tools to identify and remove this speech..In an effort to be neutral, the company’s hate speech policies treat attacks on white people or men in exactly the same way as it treats comments about Black people or women, an approach that employees said does not take into account the historical context of racism and oppression..“The world treats Black people differently from white people,” one employee said. “If we are treating everyone the same way, we are already making choices on the wrong side of history.”.Employees said that this policy means the company’s automated content moderation tools proactively detect far more hate speech targeting white people than it does hate speech targeting Black people, even if the hate speech targeted at Black people is widely considered more offensive -- a hypothesis supported by academics and the company’s own internal research...The company has conducted internal research that showed that Facebook users in the United States from both sides of the political spectrum find attacks against traditionally marginalized groups including Black and Hispanic people to be more upsetting than attacks against groups that have not traditionally been marginalized including men and white people — even when the same type of language is used. So “white people are trash” is generally considered less offensive than “Black people are scum,” but Facebook’s policies treat them the same. Data presented at Facebook policy meetings, including one attended by Vanity Fair in fall 2018, shows that users are more upset by attacks against women than they are by attacks against men..This inequity is reflected in the levels of hate speech that is reported versus taken down automatically. According to a chart posted internally in July 2019 and leaked to NBC News, Facebook proactively took down a higher proportion of hate speech against white people than was reported by users, indicating that users didn’t find it offensive enough to report but Facebook deleted it anyway. In contrast, the same tools took down a lower proportion of hate speech targeting marginalized groups including Black, Jewish and transgender users than was reported by users, indicating that these attacks were considered to be offensive but Facebook’s automated tools werent detecting them..The employee who posted the chart to Workplace, the internal version of Facebook, said that the findings showed that Facebook’s proactive tools “disproportionately defend white men.”.Facebook spokeswoman Ruchika Budhraja said Wednesday that the company has since early 2018 considered treating different groups differently, as reported by Vanity Fair in February 2019, but that it is “very difficult to parse out who is privileged and who is marginalized globally” and so the company has not changed its policies..The episodes detailed by the current and former employees add to growing scrutiny from both Facebook critics and the companys own workers over how seriously it takes allegations of racial bias on its platforms. The company is in the midst of a major advertiser boycott that was sparked in part by social justice groups that believe it has not done enough to protect users from discrimination..Meanwhile, the social media giant is also under fire from Republican politicians who say it has a liberal tilt and unfairly censors conservative voices..Zuckerberg has tried to maintain that the platform is politically neutral and an advocate for free speech while also declining to make major changes. On Tuesday, The Wall Street Journal reported that Facebook was creating new teams to study racial bias on its platforms..Facebook employees who were already working on that topic say that for years the company disregarded their work and often instructed them to stop their research..At around the same time as the Instagram episode, several pieces of research exploring race and racial bias on Facebook and Instagram were summarized and presented in a document to Zuckerberg and his inner circle, known as the M-Team..The team responded by instructing employees to stop all research on race and ethnicity and not to share any of their findings with others in the company, according to two current and one ex-employee..Schultz, who said he was part of the M-Team at the time, did not recall the specific communication, but said that some research was stopped over ethics and methodology concerns..Other attempts to study racial and social bias or oppression on the platform were stopped at the internal research review process, two current and three former employees said. Researchers, many of whom have conducted academic research of societal biases, were told they were not allowed to ask users questions about their racial identity, according to four sources..Without permission to ask users questions about racial identity, researchers — including those who conducted the Instagram study — relied on a proxy for race called “multicultural affinity,” which categorized users for advertising purposes based on their behavior according to their “affinity” for African American, Asian American or Hispanic people in the United States..While the current and former employees acknowledged this is not a perfect proxy for race, they had few other options for attempting to understand racial bias on the platform. They were also frustrated by the idea that the company was comfortable in delineating users on the basis of “ethnic affinity” for advertising purposes but not for research purposes..“Leadership wanted a standard and consistent approach to avoid biased, incorrect and irresponsible work and are proud we set up a project to do that,” Glanville said..After a ProPublica investigation in 2016, Facebook prohibited advertisers from targeting housing, employment and credit ads based on what was then called “ethnic affinity” to “prevent potential discrimination through ads,” Glanville said..It’s not the first time Facebook executives have been accused of ignoring internal research highlighting problems on the platform. A team at Facebook delivered a presentation in 2018 showing that Facebook’s algorithms were driving people apart by showing users increasingly divisive content, The Wall Street Journal reported in May, but senior executives including Zuckerberg shelved the research..Whether Facebook takes that research into account has become a subject for discussion on the companys internal message boards. One engineer shared data showing the slant of Facebooks moderation practices and offered a pointed criticism of the company..“This is not a new issue. This has been going on for years. People smarter, harder working, more patient, and more professional than I have fought to address it, only to be shut down by myopic focus on bad metrics,” the engineer wrote, according to screenshots of a post on the company’s internal message boards shared with NBC News. “I’ve seen people be driven insane as leadership ignores them or outright shuts them down and commits us, again and again, to doubling down on this same path.”.The engineer quit the company the same day, according to the leaked post and to two current employees, over the leadership’s lack of action on this issue and accused Zuckerberg of making misleading statements about the company’s handling of hate speech..“For months I’ve wrestled with myself, tried to convince myself that the work was definitely good, that it was all worth it — but I can’t,” he wrote. “The issues are too glaring, the failures of leadership too grievous. I’ve lost too much sleep wondering how many people having an awful day I’ve hurt just a little bit more by silencing their opportunity to vent with their friends, or how many other tiny injustices I’ve inflicted in the course of following orders.”.“The mere fact we have this research, and continue to find the best way to conduct this and similar research, is because we are trying to understand,” Facebook said..These revelations echo the findings of an external civil rights audit that Facebook commissioned in 2018 and was published this month. The audit report revealed that the company has not done enough to protect users from discrimination, falsehoods and incitement to violence..“I don’t think they understand civil rights,” said NAACP president Derrick Johnson, who was among the civil rights leaders to meet with Zuckerberg and Sandberg in the week the audit was released. “They have a blind spot to the needs to protect people and unfortunately, far too often, they conflate issues related to civil rights with partisanship. Defeating hate isn’t a partisan question.”",Facebook stopped pursuing topics related to bias
138,"The scam, similar to one used in a Twitter hack, requested people send cryptocurrency, falsely promising they would receive twice as much back..As well as Mr Wozniak, the law firm Cotchett, Pitre and McCarthy is also representing 17 others affected by the fraud, from the US, UK, Canada, Japan, Malaysia, China and Europe..YouTube, like Google, seems to rely on algorithms and no special effort requiring custom software employed quickly in these cases of criminal activity..But last month, fraudsters stole at least $150,000 (£117,000) by impersonating Elon Musks Space X YouTube channel and hosting a live-streamed event asking viewers to send Bitcoin..In stark contrast, the complaint alleges that YouTube knowingly allowed the Bitcoin scam to go on for months promoted it and profited from it by selling targeted advertising..In a similar case brought by cryptocurrency company Ripple Labs, YouTubes legal team successfully argued the platform was not liable for any content - including scams - provided by third parties.",Images of Mr.Wozniak were used to steal cryptocurrency
139,"SAN FRANCISCO (Reuters) - Blockchain firm Ripple sued Alphabet Inc’s YouTube on Tuesday, alleging the video-sharing platform failed to protect consumers from cryptocurrency “giveaway” scams that use fake social media profiles to dupe victims into sending money..The company says scammers on YouTube have been impersonating Ripple and its CEO, Brad Garlinghouse, to bait viewers into sending thousands of dollars worth of XRP, a cryptocurrency championed by Ripple, according to a court filing..The scammers promise to send back up to 5 million XRP, worth nearly $1 million, but victims who participate in the fake “giveaways” never receive any money in return, said the filing..The lawsuit appears poised to raise a fresh challenge around the controversial Section 230 of the Communications Decency Act, which shields Google, Facebook and other internet companies from liability for material that users post on their platforms..Regulators in Washington are reconsidering the need for the law’s broad immunity, which helped U.S. tech companies grow but is viewed increasingly as a shelter enabling some of the world’s richest companies to avoid investments to curb crime, extremism and misinformation online..“For every scam, giveaway, fake conspiracy that is taken down, multiple more pop up nearly immediately,” Ripple said in a blog post. “YouTube and other big technology and social media platforms must be held accountable for not implementing sufficient processes for fighting these scams.”.Garlinghouse, a long-time Silicon Valley executive, said he wants the case to be a “call to action” for the social media industry, arguing the law was written “at a time when we didn’t understand how these platforms could be abused.”.He said he had seen similar impersonations on platforms including Facebook’s photo-sharing app Instagram, but targeted YouTube in the lawsuit because it was the “slowest to respond and least proactive.”.YouTube spokesman Alex Joseph said the company takes abuse of the platform seriously and acts “quickly when we detect violations of our policies, such as scams or impersonation.”.Founded in 2012, Ripple is one of the best known companies that develop so-called blockchain technology, or the system underpinning cryptocurrencies. The company develops blockchain systems to help financial services firms carry out cross-border payments using XRP..Its filing, in the U.S. District Court for the Northern District of California, says YouTube’s failure to address the “pervasive and injurious fraud” has harmed the reputation of both Ripple and Garlinghouse..Ripple said millions of people have viewed the scams on YouTube, which enabled the fraud to proliferate by ignoring its demands for the videos to be taken down and continuing to sell ads to the scammers..YouTube also awarded a “verification badge” to a hacked channel displaying a photo of Garlinghouse as its profile picture, falsely indicating to viewers that the account was legitimate, the filing said.",Scammers used fake social media profiles and dupped victims in cryptocurrency scam
140,"Reuters reports that somebody used deepfake tech and a false name and biography to invent the persona of a journalist — and then got the sock puppet’s work published in several international newspapers..Whoever’s behind the operation — Reuters was not able to track them down — managed to publish six articles and editorials in the Jerusalem Post and the Times of Israel while posting as an entirely fictitious author, according to the investigation. The dupe serves as a warning about how easily disinformation can spread online — and how new tech can enable it..According to online profiles, Oliver Taylor is a student at the University of Birmingham who loves politics and coffee. But no actual records of Taylor exist, his phone number isn’t connected, and neither Reuters nor the publications that ran his work could verify his existence..It’s not unprecedented for propagandists to invent personas for journalists. Earlier this month, for instance, an investigation by The Daily Beast found a network of 19 fake journalists that had been used to publish political content in a wide variety of conservative publications — including several that appeared to have used deepfake tech..The Jerusalem Post told Reuters that editors didn’t vette Taylor very hard. And while his articles didn’t gain much traction, the phenomenon points to a looming threat in which operators could hide behind deepfake technology while publishing disinformation in reputable outlets..“Absolutely we need to screen out impostors and up our defenses,” Times of Israel Opinion Editor Miriam Herschlag told Reuters. “But I don’t want to set up these barriers that prevent new voices from being heard.”",A network of 19 fake journalists have published fake conservative publications
141,"We are excited to bring Transform 2022 back in-person July 19 and virtually July 20 - 28. Join AI and data leaders for insightful talks and exciting networking opportunities. Register today!.When it comes to Facebook’s progress on civil rights issues, an independent review found the company’s efforts to detect algorithmic bias fall dangerously short and leave users vulnerable to manipulation..According to the audit released earlier today, Facebook’s efforts to detect algorithmic bias remain primarily in pilot projects conducted by only a handful of teams. The authors of the report, civil rights attorneys Laura Murphy and Megan Cacace, note that the company is increasingly reliant on artificial intelligence for such tasks as predicting which ads users might click on and weeding out harmful content..But these tools, as well as other tentative efforts Facebook has made in areas like diversity of its AI teams, must go much further and faster, the report says. While the group looked uniquely at Facebook during its two-year review, any company embracing AI would do well to look at algorithmic bias issues..“Facebook has an existing responsibility to ensure that the algorithms and machine learning models that can have important impacts on billions of people do not have unfair or adverse consequences,” the report says. “The Auditors think Facebook needs to approach these issues with a greater sense of urgency.”.The report comes as Facebook faces a historic advertising boycott. The “Stop Hate for Profit” campaign is backed by more than 396 advertisers, who have halted spending on the platform to demand Facebook take bolder steps against racism, misogyny, and disinformation..Earlier this week, Facebook CEO Mark Zuckerberg met with civil rights groups but insisted his company would not respond to financial pressure, leaving attendees disappointed..In a blog post, COO Sheryl Sandberg sought to score points by claiming Facebook is the “first social media company to undertake an audit of this kind.” She also nodded toward the timing of the report, which was commissioned two years ago. Her post — “Making Progress on Civil Rights — But Still a Long Way to Go” — emphasized Facebook’s view that it is fighting the good fight..“There are no quick fixes to these issues — nor should there be,” Sandberg wrote. “This audit has been a deep analysis of how we can strengthen and advance civil rights at every level of our company — but it is the beginning of the journey, not the end. What has become increasingly clear is that we have a long way to go. As hard as it has been to have our shortcomings exposed by experts, it has undoubtedly been a really important process for our company. We would urge companies in our industry and beyond to do the same.”.“Many in the civil rights community have become disheartened, frustrated, and angry after years of engagement where they implored the company to do more to advance equality and fight discrimination, while also safeguarding free expression,” the authors wrote..The report dissects Facebook’s work on civil rights accountability, elections, census, content moderation, diversity, and advertising. But it also gives special attention to the subject of algorithmic bias..“AI is often presented as objective, scientific, and accurate, but in many cases it is not,” the report says. “Algorithms are created by people who inevitably have biases and assumptions, and those biases can be injected into algorithms through decisions about what data is important or how the algorithm is structured, and by trusting data that reflects past practices, existing or historic inequalities, assumptions, or stereotypes. Algorithms can also drive and exacerbate unnecessary adverse disparities … As algorithms become more ubiquitous in our society, it becomes increasingly imperative to ensure that they are fair, unbiased, and non-discriminatory, and that they do not merely magnify preexisting stereotypes or disparities.”.The authors highlighted Facebook’s Responsible AI (RAI) efforts, led by a team of “ethicists, social and political scientists, policy experts, AI researchers, and engineers focused on understanding fairness and inclusion concerns associated with the deployment of AI in Facebook products.”.Part of that RAI work involves developing tools and resources that can be used across the company to ensure AI fairness. To date, the group has developed a “four-pronged approach to fairness and inclusion in AI at Facebook.”.As part of the first pillar, Facebook has created the Fairness Flow tool to assess algorithms by detecting unintended problems with the underlying data and spotting flawed predictions. But Fairness Flow is still in a pilot stage, and the teams with access use it on a purely voluntary basis. Late last year, Facebook also began a fairness consultation pilot project to allow teams that detect a bias issue in a product to reach out internally to teams with more expertise for feedback and advice. While the authors saluted these steps, they also urged Facebook to expand such programs across the company and make their use mandatory..“Auditors strongly believe that processes and guidance designed to prompt issue-spotting and help resolve fairness concerns must be mandatory (not voluntary) and companywide,” the report says. “That is, all teams building models should be required to follow comprehensive best practice guidance, and existing algorithms and machine learning models should be regularly tested. This includes both guidance in building models and systems for testing models.”.The company has also created an AI Task Force to lead initiatives for improving employee diversity. Facebook is now funding a deep learning course at Georgia Tech to increase the pipeline of underrepresented job candidates. It’s also in discussions with several other universities to expand the program. And its tapping nonprofits, research, and advocacy groups to broaden its hiring pool..But again, the review found these initiatives to be too limited in scope and called for an expansion of hiring efforts, as well as greater training and education across the company..“While the Auditors believe it is important for Facebook to have a team dedicated to working on AI fairness and bias issues, ensuring fairness and non-discrimination should also be a responsibility for all teams,” the report says. “To that end, the Auditors recommend that training focused on understanding and mitigating against sources of bias and discrimination in AI should be mandatory for all teams building algorithms and machine-learning models at Facebook and part of Facebook’s initial onboarding process.”.VentureBeats mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. Learn more about membership..Hear from senior executives at some of the world’s leading enterprises about their experience with applied Data & AI and the strategies they’ve adopted for success. ",Facebook civil rights audit urges ‘mandatory’ algorithmic bias detection
142,"A new artificial intelligence tool for making clear pictures of peoples faces from pixelated images has become a prime, if unwitting, example of algorithmic racial bias..The tool, called Face Depixelizer, was built by a coder and put out on Twitter. Its built to take pixelated photographs of people and accurately reconstruct a sharp and accurate image of their face using machine learning, a type of AI..Users started to spot however that the system was not particularly accurate when it came to processing Black faces. One shared an image of what happened after they input a pixelated picture of Barack Obama — Face Depixelizer turned him into a white man..Machine learning blogger Robert Osazuwa Ness then ran other pictures of people of color through the system including himself, Rep. Alexandria-Ocasio Cortez, and actress Lucy Liu. Face Depixelizer consistently reconstructed their faces to look white..Machine learning algorithms are trained on large data sets, from which they identify patterns and teach themselves what theyre supposed to be looking for. A lack of diversity in widely-used datasets means these systems are often predominantly trained on images of white men, meaning they infer that white male characteristics are the default..Although Face Depixelizer is an illustration of how these systems can fail at identifying Black peoples faces, the implications of algorithmic racial bias go far beyond a depixelating proof-of-concept tool circulating on Twitter..One example of how algorithmic bias could end up harming Black people in the real world is through facial recognition. Earlier this month IBM, Amazon, and Microsoft all agreed to suspend sales of their facial recognition software to police after years of campaigning from AI researchers and civil rights groups..Quite apart from the fact police are more likely to use facial recognition to target communities of color, studies have shown commercially available facial recognition software is far more likely to misidentify women and people with darker skin tones..This affects the kind of AI which was used to build Depixeliser, called Generative Adversarial Network or GAN. GANs work by pitting two algorithms against one another, with one algorithm generating fake images and the other one trying to spot the fakes. Mode collapse effectively means that even if theres only a small bias in the data, it will be exacerbated by the GANs, as the first algorithm will go for the easiest way to fool the second one.",An AI tool which reconstructed a pixelated picture of Barack Obama to look like a white man perfectly illustrates racial bias in algorithms
144,"By analysing transport and census data in Chicago, Aylin Caliskan and Akshat Pandey at The George Washington University in Washington DC have found that ride-hailing companies charge a higher price per mile for a trip if the pick-up point or destination is a neighbourhood with a higher proportion of ethnic minority residents than for those with predominantly white residents..“Basically, if you’re going to a neighbourhood where there’s a large African-American population, you’re going to pay a higher fare price for your ride,” says Caliskan..Unlike traditional taxis, ride-hailing services have dynamic fares, which are calculated based on factors including the length of the trip as well as local demand – although it is unclear what other factors these algorithms take into consideration because ride-hailing companies don’t make all of their data available..The researchers analysed data from more than 100 million trips taken in Chicago through ride-hailing apps between November 2018 and December 2019. Each ride contained information including pick-up and drop-off location, duration, cost and whether the ride was an individual or shared trip. The data doesn’t include demographic details such as the ethnicity of the rider..The duo compared the trip data against information from the US Census Bureau’s American Community Survey, which provides aggregate statistics about neighbourhoods, including population, ethnicity breakdown, education levels and median house prices..They found that prices per mile were higher on average if the trip pick-up or drop-off location was in a neighbourhood with a lower proportion of white residents, a lower median house price, or lower average educational attainment..“Even in the absence of identity being explicitly considered in how an algorithm’s results are decided, the structural and historical nature of racism and the way that it informs geography, opportunity and life chances mean that racial disparities can still appear,” says Os Keyes at the University of Washington in Seattle..“Chicago, the site of this analysis, is a case in point: as a result of – amongst other things – redlining practices, it remains highly geographically segregated,” says Keyes. Redlining is practice in which mortgage lenders refuse to offer loans in certain neighbourhoods..“This should cause us to further question studies of ‘fairness’ and ‘bias’ in algorithms which promise to end algorithmic racism by simply not mentioning race,” says Keyes..The researchers found no statistical link to suggest that neighbourhoods with higher proportions of ethnic minorities had higher demand for rides, which could potentially explain the higher fare prices..“We recognise that systemic biases are deeply rooted in society, and appreciate studies like this that look to understand where technology can unintentionally discriminate,” said a Lyft spokesperson. “There are many factors that go into pricing – time of day, trip purposes, and more – and it doesn’t appear that this study takes these into account. We are eager to review the full results when they are published to help us continue to prioritise equity in our technology.”.“Uber does not condone discrimination on our platform in any form, whether through algorithms or decisions made by our users,” said an Uber spokesperson. “We commend studies that try to better understand the impact of dynamic pricing so as to better serve communities more equitably. It’s important not to equate correlation for causation and there may be a number of relevant factors that weren’t taken into account for this analysis, such as correlations with land-use/neighborhood patterns, trip purposes, time of day, and other effects.”.Under US law, it is illegal to discriminate against an individual on the basis of protected attributes, including race. The study’s findings are problematic, says Caliskan. “Even though these algorithms are supposed to be fair and they are not using protected attributes, they seem to have a significant impact on these neighbourhoods.”.“This study shows how algorithmic bias by postcode and race can creep into even the most unexpected places,” says Noel Sharkey at the University of Sheffield, UK. “It is yet another example in a long list of how ethnicity and race bias has found a new home in computer software. There is no excuse for automation biases and such systems should be shut down until such time as they can demonstrate fairness and equality,” he adds.",The algorithm creates racial bias because it charges higher prices if the pick up point or destination is in a ethnic minority neighborhood
145,"Decision aids that U.S. physicians use to guide patient care on everything from who receives heart surgery to who needs kidney care and who should try to give birth vaginally are racially biased, scientists reported on Wednesday in the New England Journal of Medicine..It is the latest evidence that algorithms used by hospitals and physicians to guide the health care given to tens of millions of Americans are shot through with implicit racism that their creators are often unaware of, but which nevertheless often result in Black people receiving inferior care..Malika Fair, an emergency physician and senior director for health equity partnerships and programs at the Association of American Medical Colleges, called the study “impressive.”.“I am delighted that the use of race in medical decision-making is being questioned in such a thoughtful analysis,” said Fair, who was not involved in the study. “As a medical community, we have not fully embraced the notion that race is a social construct and not based in biology.”.The findings build on earlier studies that focused more narrowly. Last year, for instance, other scientists found that a widely used algorithm that identifies which patients should get additional health care services such as home visits routinely put healthier, white patients into the programs ahead of Black patients who were sicker and needed them more. The bias resulted from the algorithm’s developers equating higher health care spending with worse health. But white Americans spend more on health care than Black Americans even when their health situations are identical, making spending a poor and racially biased proxy for health..The new study finds that algorithms used for medical decisions from cardiology to obstetrics are similarly tainted by implicit racial bias and adversely affect the quality of care Black patients receive. Among them:.Heart Failure Risk Score: Developed by the American Heart Association to determine which hospitalized patients are at risk of dying from heart disease, the algorithm assigns three points to any “nonblack” patient; more points mean higher risk of death. Those deemed at higher risk (non-Black patients) are more likely to be referred to specialized care, said David Shumway Jones of Harvard Medical School, the study’s senior author. That possibility is not merely theoretical: At one Boston hospital, Black and Latinx patients arriving in the emergency room with cardiac symptoms were less likely than white patients with the same symptoms and medical history to be admitted to the cardiology unit, a 2019 study found..Chest surgery: In a risk calculator used by thoracic surgeons, being Black increases the supposed likelihood of post-operative complications such as kidney failure, stroke, and death. “That could make surgeons steer Black patients away from bypass surgery, mitral valve repair and replacement,” and other life-saving operations, Jones said. “If I have a Black patient and the risk calculator tells me he has a 20% higher risk of dying from this surgery, it might scare me off from offering that procedure.”.Kidney failure: It’s very difficult to measure kidney function directly, so physicians use creatinine levels in the blood as a proxy: less creatinine, better kidney function. A “race adjustment” in a widely used algorithm lowers Black patients’ supposed creatinine levels below what’s actually measured. That makes their kidney function appear better, potentially keeping them from receiving appropriate specialty care. The rationale for “adjusting” creatinine levels by race is that Black people are supposedly more muscular, which can increase the release of creatinine into the blood. As it is, Black people have higher rates of end-stage renal disease than whites..Kidney donation: An algorithm used by transplant surgeons says that kidneys from Black donors are more likely to fail than kidneys from donors of other races. Because Black patients are more likely to receive an organ from a Black donor, the algorithm reduces the number of kidneys available to them..Giving birth: Recognizing that cesarean deliveries are more dangerous for both mothers and babies, obstetricians recommend that women who had a previous surgical birth not be automatically scheduled for another cesarean, as was once common practice. But the algorithm they use to determine whether a woman faces too high a risk from vaginal birth automatically says that Black and Latinx women face a higher risk. That was based on a study that found being unmarried and not having insurance also increases that risk. Neither of those socioeconomic factors are included in the algorithm..Breast cancer: An online tool that estimates risk of developing the disease calculates a lower risk for a Black or Latinx woman than a white one even when every other risk factor (such as age at menarche, close relatives with breast cancer, and a history of benign biopsies) is identical. That could deter minority woman from undergoing screening..Kidney stones: When patients come to an emergency department with pain in the back or side, doctors use an algorithm with a 13-point scale to assess whether the cause is kidney stones. A higher score means less likelihood of that. Being Black automatically adds three points to the score. An assessment of the algorithm by independent researchers found no scientific support for the assumption that Black people’s pain is less likely to indicate kidney stones..“Many of the algorithms are widely used and have a substantial impact on patient care,” said Brian Powers, a physician and researcher at Brigham and Women’s Hospital in Boston, who was not involved in the study. “Cataloguing these algorithms is an important first step. What’s needed now is a better understanding of whether these algorithms exhibit racial bias, or are contributing to health inequities in other ways.”.All 13 of the algorithms Jones and his colleagues examined offered rationales for including race in a way that, presumably unintentionally, made Black and, in some cases, Latinx patients less likely to receive appropriate care. But when you trace those rationales back to their origins, Jones said, “you find outdated science or biased data,” such as simplistically concluding that poor outcomes for Black patients are due to race..Typically, developers based their algorithms on studies showing a correlation between race and some medical outcome, assuming race explained or was even the cause of, say, a poorer outcome (from a vaginal birth after a cesarean, say). They generally did not examine whether factors that typically go along with race in the U.S., such as access to primary care or socioeconomic status or discrimination, might be the true drivers of the correlation..“Modern tools of epidemiology and statistics could sort that out,” Jones said, “and show that much of what passes for race is actually about class and poverty.”.Including race in a clinical algorithm can sometimes be appropriate, Powers cautioned: “It could lead to better patient care or even be a tool for addressing inequities.” But it might also exacerbate inequities. Figuring out the algorithms’ consequences “requires taking a close look at how the algorithm was trained, the data used to make predictions, the accuracy of those predictions, and how the algorithm is used in practice,” Powers said. “Unfortunately, we don’t have these answers for many of the algorithms.”.Even before the new analysis, some hospitals and specialty groups were questioning the algorithms. The Society of Thoracic Surgeons, which developed and whose members use the algorithm that tells them Black patients are more likely to suffer post-op complications, admitted that including race in the algorithm “might ‘adjust away’ disparities in quality of care.”.Kidney specialists at hospitals in Boston and San Francisco have stopped using race to make a Black patient’s kidney function score better. This month, the University of Washington School of Medicine announced that its physicians would also stop using the racially tinged kidney function algorithm. But at least two other Boston hospitals, whose officials were given a presentation about the creatinine algorithm, declined to stop using it several weeks ago..Jones does not think the algorithm developers were racially motivated. “Well-meaning individuals acting without racist intent can still produce work with racist consequences,” he said, “such as redirecting medical resources from one group to another.”.He and other physicians hope that shining a light on the use of racially biased decision tools will make more in the medical community question the practice..“By perpetuating the use of race in our decision-making, we can further exacerbate inequities, make it longer to get needed treatments or procedures, or referrals to specialists,” said AAMC’s Fair. “For the medical community, this is an example of structural racism and is one area we can address quickly and decisively.”.“the algorithm assigns three points to any “nonblack” patient; more points mean lower risk of death. Those at lower risk are less likely to be referred to specialized care, said David Shumway Jones of Harvard Medical School, the study’s senior author. That possibility is not merely theoretical: At one Boston hospital, Black and Latinx patients arriving in the emergency room with cardiac symptoms were less likely than white patients with the same symptoms and medical history to be admitted to the cardiology unit, a 2019 study found.”.Something wrong here? If non-blacks gets more points, and more points lower risk of death, and lower risk of death means lower chance of getting referred to specialized cared, then non-blacks should have lower chance of being admitted to the cardiology unit?.I was wondering the same thing. It seems as though this characterization is supposed to be reversed. It’s well-documented that Black and Hispanic patients get specialized cardiac care at lower rates than “nonblack” patients..“The American Heart Association (AHA) Get with the Guidelines–Heart Failure Risk Score predicts the risk of death in patients admitted to the hospital.9 It assigns three additional points to any patient identified as “nonblack,” thereby categorizing all black patients as being at lower risk.”.I agree. There is something really off. I can’t find the referenced calculator anywhere, and working as a hospitalist I know of no one that uses anything close to what is described. Every other calculated I can locate assigns increased risk to black patients. The HEART score doesn’t even consider the race. Also, the referenced study from Boston did NOT state that a calculator was used. It examined raw numbers of admissions to a cardiology service and as far as I can tell didn’t examine or control for anything. The numbers look pretty large to a lay-reader but are really pretty small overall – especially the sub group populations..This is one of the most important articles of the year IMO. I really hope it gets the attention it deserves esp. given all the distractions with COVID. AI bias issues are in no way new to the computer science community, but it seems that the medical community is just starting to recognize the problem. (And yes, there are technical solutions, but it takes a lot more work to develop an unbiased algorithm than a biased one, ergo the need for greater awarenesss of hte problem.).This article has nothing to do with political correctness, and everything to do with good science. It just so happens that good science and justice are complementary, thus the apparent overlap.","The racial bias is created because the algorithm's developers equated higher health care spending with worse health. However, white Americans spend more on healthcare than Black Americans even when their health situation is the same."
146,"Amazon, which is currently being sued for allegedly failing to protect workers from COVID-19, has unveiled a new AI tool it says will help employees follow social distancing rules..The company’s “Distance Assistant” combines a TV screen, depth sensors, and AI-enabled camera to track employees’ movements and give them feedback in real time. When workers come closer than six feet to one another, circles around their feet flash red on the TV, indicating to employees that they should move to a safe distance apart. The devices are self-contained, meaning they can be deployed quickly where needed and moved about..Amazon compares the system to radar speed checks which give drivers instant feedback on their driving. The assistants have been tested at a “handful” of the company’s buildings, said Brad Porter, vice president of Amazon Robotics, in a blog post, and the firm plans to roll out “hundreds” more to new locations in the coming weeks. .Importantly, Amazon also says it will be open-sourcing the technology, allowing other companies to quickly replicate and deploy these devices in a range of locations..Amazon isn’t the only company using machine learning in this way. A large number of firms offering AI video analytics and surveillance have created similar social-distancing tools since the coronavirus outbreak began. Some startups have also turned to physical solutions, like bracelets and pendants which use Bluetooth signals to sense proximity and then buzz or beep to remind workers when they break social distancing guidelines. .Although these solutions will be necessary for workers to return to busy facilities like warehouses, many privacy experts worry their introduction will normalize greater levels of surveillance. Many of these solutions will produce detailed data of workers’ movements throughout the day, allowing managers to hound employees in the name of productivity. Workers will also have no choice but to be tracked in this way if they want to keep their job..Amazon’s involvement in this sort of technology will raise suspicions as the company is often criticized for the grueling working conditions in its facilities. In 2018, it even patented a wristband that would track workers’ movements in real time, directing not just which task they should do next, but if their hands are moving towards the wrong shelf or bin..The company’s description of the Distance Assistant as a ”standalone unit” that only requires power suggests it’s not storing any data about worker’s movement, but we’ve contacted the company to confirm what information, if any, might be retained.",Employees cannot opt out and this system can violate privacy
147,"Facebook incorrectly removed a post critical of the prime minister’s comments regarding slavery in Australia that featured a photograph of Aboriginal men in neck chains from the late 1800s, claiming the photo featured nudity..On Friday he backed down from those comments and acknowledged the history of blackbirding in Australia – where people were coerced into working as indentured labourers, often through deception or force. He said his Thursday comments related to the principles that existed when the colony of New South Wales was founded – that there was to be no lawful slavery in Australia..Following his comments on Thursday, debate on social media was rife with discussion of Australia’s history of slavery. One Australian user posted about the topic on his personal Facebook profile, including a photo of nine Aboriginal men chained together by their necks wearing loin cloths outside Roebourne Gaol in 1896..“Kidnapped, ripped from the arms of their loved ones and forced into back-breaking labour: The brutal reality of life as a Kanaka worker - but Scott Morrison claims ‘there was no slavery in Australia’,” the post stated.”.The photo from the state library of Western Australia refers to the men as prisoners, and the photo had been shared in stories about Morrison’s comments in the Daily Mail..The post was removed by Facebook, and the man had his account restricted, with Facebook claiming the photo contained nudity and was in breach of the social media site’s community standards..The post was restored after Guardian Australia asked Facebook about whether the photo had been flagged in error. Facebook apologised to the user late on Friday and restored the post..According to Facebook’s latest community standards report, between January and March this year, Facebook removed 39.5m pieces of content for adult nudity or sexual activity, and of that 99.2% was removed by Facebook automatically, without a user reporting it..In 2016, Facebook backed down on a ban on the famous photo of nine-year-old Kim Phúc running away from a napalm attack during the Vietnam war on the grounds of the nudity in the photograph, after a strong backlash over claims of censorship..Facebook also faced criticism in Australia in 2016 for blocking the account of Arrente writer Celeste Liddle four times after she shared a trailer for an Indigenous comedy show that featured images of topless desert women. Trolls kept reporting the content as “indecent”, and Facebook kept removing the trailer and locking Liddle out of her account.",Facebook took down photo that revealed history of slavery in Australia
148,"Algorithm Watch, a non-profit that aims to shed light on algorithm decision-making, found that women in underwear or bikinis are 54 percent more likely to appear and bare-chested men are 28 percent more likely..The report suggests that the occurrence of the images could be that the users engage more with these type of posts or it could be Instagrams way of encouraging influencers to share more semi-nude photos..Algorithm Watch , a non-profit that aims to shed light on algorithm decision-making, found that women in underwear or bikinis are 54 percent more likely to appear and bare-chested men are 28 percent more likely. Pictured is a post from Anastasia Karanikolaou.Algorithm Watch asked 26 volunteers to download a browser add-on that automatically opens their Instagram at regular intervals and records which posts appear at the top of their newsfeeds..Algorithm Watch asked 26 volunteers to download a browser add-on that automatically opens their Instagram at regular intervals and records which posts appear at the top of their newsfeeds.The report suggests that the occurrence of the images could be that the users engage more with these type of posts or it could be Instagrams way of encouraging influencers to share more semi-nude photos.If Instagram were not meddling with the algorithm, the diversity of posts in the newsfeed of users should match the diversity of the posts by the content creators they follow, Algorithm Watch shared in a blog post..And if Instagram personalized the newsfeed of each user according to their personal tastes, the diversity of posts in their newsfeeds should be skewed in a different way for each user..Over the three months, 362, or 21 percent, of the images from the 37 influencers showed them semi-naked – bare-chest men and women in underwear or swimwear..The team also found that pictures of scantily-clad were 54 percent more likely to appear in the volunteers feed, whereas half naked men were 28 percent more likely..The team also found that pictures of women wearing the scantily clad clothing were 54 percent more likely to appear in the volunteers feed, whereas half naked men were 28 percent more likely (Pictured is an image shared by Jason Momoa on his Instagram page).Instagram has stated in the past that its algorithm organizes posts on what a users cares more about, the firms patent notes that it ranks images on what it thinks the user wants to see..Whether or not users see the pictures posted by the accounts they follow depends not only on their past behavior, but also on what Instagram believes is most engaging for other users of the platform, explained Algorithm Watch..The subtle difference between what is encouraged and what is forbidden is decided by unaudited, and likely biased, computer vision algorithms, reads the Algorithm Watch blog post..Every time they post a picture, content creators must thread this very fine line between revealing enough to reach their followers but not revealing so much that they get booted off the platform..         Johnny Depp called ex-girlfriend Vanessa Paradis an extortionist French c*** in email to Elton John and said she was trying to brainwash their two children: Amber Heards lawyers unleash barrage of violent messages in blockbuster lawsuit      ","Instagram posts featuring 'scantily clad' women are 54% more likely to appear at the top of a user's newsfeed, according to a report that says the firm's algorithm boosts semi-nude images"
149,"Last spring, artificial intelligence research institute OpenAI said it had made software so good at generating text—including fake news articles—that it was too dangerous to release. That line in the sand was soon erased when two recent master’s grads recreated the software and OpenAI released the original, saying awareness of the risks had grown and it hadn’t seen evidence of misuse..Now the lab is back with a more powerful text generator and a new pitch: Pay us to put it to work in your business. Thursday, OpenAI launched a cloud service that a handful of companies are already using to improve search or provide feedback on answers to math problems. It’s a test of a new way of programming AI and the lab’s unusual business model..OpenAI was founded as a nonprofit in 2015 by Elon Musk and other Silicon Valley notables to ensure that future superhuman AI was a benign force. The Tesla CEO parted ways with the lab in 2018, and last year it became a for-profit company and took a $1 billion investment from Microsoft. OpenAI’s leaders claim that only by commercializing its research for the benefit of investors can it raise the billions needed to keep pace on the frontiers of AI..Thursday’s launch of OpenAI’s first commercial product completes the metamorphosis. A research institute created to compete with tech giants on superhuman AI is now challenging them in the more mundane arena of selling cloud services to businesses..OpenAI’s service is built on a machine-learning technique that has made computers much better with language over the past two years. Machine-learning algorithms are directed to analyze vast collections of text scraped from the web to discover the statistical patterns in language use. The software can then be tuned to perform tasks like answering factual questions or summarizing documents..Google has tapped the technology to improve how its search engine handles long queries, and Microsoft Office uses it to spot grammar glitches. OpenAI has focused on pushing the technique to greater scale and making software that generates text. Given a snatch of writing, it builds on it, unspooling sentences with similar statistical properties. The results can be uncannily smooth, if sometimes unmoored from reality..Text generators like that can be fun—try one here—but haven’t previously seen much commercial use. OpenAI CEO Sam Altman says the latest generation is powerful and flexible enough for real work. “This is the first time we’ve got something we think is good enough to make into a product,” he says..OpenAI’s new text generators are trained using a collection of almost a trillion words gathered from the web and digitized books, on a supercomputer with hundreds of thousands of processors the company paid Microsoft to build, effectively returning some of the company’s $1 billion investment to its source..The service is more open-ended than most AI cloud services, which usually perform one task, such as translation or image tagging, and are controlled with specific commands. Programmers who want to tap OpenAI’s technology simply submit human-readable text and get newly generated text back..That may sound limiting, but by crafting the right input it’s possible to steer the software to perform different tasks. The goal is to try and massage it to riff on the statistical language patterns from a particular part of the internet..Submitting examples of paragraphs rewritten for elementary schoolers followed by an unsimplified passage prompts the service to rewrite it to be easier to read. The service can answer factual questions or function as a chatbot if supplied with example Q&A pairs or turns of dialog that might direct the software to draw on its experience of factual statements or conversations..“The big mental shift is, it’s much more like talking to a human than formatting things for a machine,” says Greg Brockman, OpenAI’s chief technology officer. “You give it a few questions and answers and suddenly it’s in Q&A mode.”.Nick Frosst, a researcher working on language machine learning who previously worked at Google, says that novel way of working with AI could widen the pool of people experimenting with language technology. “It’s exciting that you can do that,” he says. “It’s how most people think AI should work.”.OpenAI is offering its service for free for two months and already has some users. Algolia, a startup that builds internal search engines for apps and websites, uses it to improve its understanding of complex search strings..Others are using an additional service in which OpenAI “fine-tunes” a version of the software to a specific task with additional data. Math education site Art of Problem Solving uses that to suggest comments to send students on their submissions, speeding up the work of graders..One unknown is its reliability. “These models are somewhat unpredictable,” says Robert Dale, of consultants Language Technology Group. OpenAI’s software can recreate the patterns of text but doesn’t have a commonsense understanding of the world. Its versatility can be a liability as well as an asset. Occasional clangers are of little consequence for some uses, such as predictive text, but could be deal breakers in others, such as a customer support chatbot..One certainty about OpenAI’s technology is that it can talk dirty and nasty. Its training on vast swaths of the internet makes the software well versed in unsavory language such as casual or aggressive racism, and it can be prompted to recreate them. The results can be reminiscent of how Twitter users prodded a notorious Microsoft bot called Tay to make racist comments..When WIRED provided the service with two sentences from message board 4chan accusing Republicans of being “spineless” and not taking action on “Clinton, Pedos, Censorship or Riots,” OpenAI’s service escalated, riffing that “we are being beaten and raped … vast immigration started in the 60s and never stopped.”.OpenAI says it will vet customers to prevent people from using the service for things like spam or harassment. Some customers have built filters to block the technology from producing toxic language, and OpenAI is working on safety features of its own..Altman doesn’t expect OpenAI’s product to be lucrative right away but says it could develop into a significant revenue source in a few years as the lab makes improvements. Microsoft’s stake in the lab could help. OpenAI built its new service on Microsoft’s Azure cloud platform; it could see much wider use if Microsoft offered it as an AI service..Altman accepted closer relations with Microsoft as a possibility but declined to elaborate. When WIRED prompted the lab’s new software to fill out the details on “OpenAI and Microsoft’s first joint commercial venture” it described a “game called Copilot that allows two people to play a racing game with one person controlling the gas pedal and the other the brakes.”",OpenAI text generators are going commercial although they have found models to be unpredictable and could land in bad hands.
150,"Unemployment in May reached its highest levels since the Great Depression, but companies like Postmates and Uber have continued to hire new workers during the pandemic. If you’re interested in this kind of gig, however, there’s a good chance you’ll need to pass an AI-powered background check from a company like Checkr. This might not be as easy as it sounds..Checkr is on the forefront of a new and potentially problematic kind of hiring, one that’s powered by still-emerging technology. Those hoping to quickly get extra work complain that Checkr and others using AI to do background checks aren’t addressing errors and mistakes on their criminal records reports. In these cases, a glitch in the system can cost someone a job. .But this isn’t exactly a new problem. In recent years, Checkr has faced a slew of lawsuits for making mistakes that have cost people much-desired opportunities to work, according to legal records. One complaint from a man hoping to drive for Uber alleged that he was wrongly linked to a murder conviction that actually belonged to someone with a similar name. Another person hoping to work for the ride-share giant complained that he was erroneously reported to have committed several misdemeanors — including the possession of a controlled substance — crimes that belonged to another person with the same name.  .Checkr is one of many companies automating aspects of the hiring process and cutting down on costs. Some of these companies are using artificial intelligence to scan through resumes, analyze facial expressions during video job interviews, compare criminal records, and even judge applicants’ social media behavior. And in a pandemic, where the companies still hiring are likely already seeing a surge in applications and eager to find ways to streamline the recruiting process, technology that makes hiring quicker and easier sounds appealing. .But experts have expressed skepticism about the role that AI can actually play in hiring. The technology doesn’t always work and can exacerbate bias and privacy problems. Inevitably, it also raises bigger questions of how powerful AI should become..When you’re being considered for a job, background check companies typically use personal information, provided by you, to learn more about your criminal record and other information about your identity. That can involve collating all types of data, including but not limited to information from sex offender registries, global watch lists, state criminal records databases, and the Public Access to Court Electronic Records (PACER) system. Sometimes, a background check provider will need to consult a courthouse to search for more records, a process that might not be possible right now due to pandemic-required closures. .In recent years, using artificial intelligence to speed up the process of analyzing these records has been pioneered by Checkr, though other startups, like UK-based Onfido and Israeli-based Intelligo, have worked or are working on similar systems. Meanwhile, more traditional background check companies are also making use of AI. GoodHire, for instance, has used machine learning to verify the identity of people completing an online background check..Checkr has become a favorite of gig economy firms, including Uber, Instacart, Shipt, Postmates, and Lyft. On its website, Checkr argues that AI can ultimately drive down the cost of bringing on a new hire by helping process background-checks in two ways. First, the technology helps verify that a given criminal record belongs to the person whose background is being checked. Second, the AI assists in comparing the names of criminal charges that have different names in different places. What might be reported as “petty theft” in one locale could be reported as “petit larceny” somewhere else..But as lawsuits against Checkr suggest, these services can make mistakes, even with the use of AI. Many of these complaints allege that the company matched people to criminal records belonging to others with the same or similar names. .“The threshold question is, did we even match the right person,” explains Aaron Rieke, the managing director of the digital rights group Upturn. “If you have a common name, that’s a non-trivial thing to do, and the last 20 years are rife with database matching problems just at that very basic level.” .Checkr did not comment on the lawsuits specifically, but Kristen Faris, the company’s industry strategy vice president, told Recode that humans are involved in both the review and quality assurance process to ensure the accuracy of the reports. .“In the traditional world, where you’re using offshore labor to apply this criteria, you have a much lower accuracy rate just because of the manual processes involved,” Faris said. .Most background checks tend to focus on criminal records, but some services have started to include information about a person that’s available online, including their social media presence. Some managers already look up social media activity of prospective hires, but companies like Good Egg sell social media background checks, while others like Intelligo can use AI to screen these platforms. .“I think when people use their real name in public fora online, the reality is that that information could be sucked into a background check process,” Rieke, the digital rights advocate, said. .This is what happened to Kai Moore earlier this year, when their employer switched payroll systems and required employee background checks to be run again. Moore expected a review of what’s typically included in this process: information about their criminal records and confirmation of their identity. But what they didn’t expect was a 300-page report from Fama Technologies on their social media history, which included documentation of their tweets, retweets, and “Likes.”.Even more worrisome was how their online activity was graded. A post Moore had “Liked” with the phrase “Big Dick Energy” was flagged for “sexism” and “bigotry.” Tweets they’d favorited about alcohol were flagged as “bad,” while one mentioning “hell” in discussing LGBTQ identity and religion was flagged for “language.”.Moore’s employer assured them that their job was not at risk, but it also noted that Fama’s algorithms had ultimately deemed them a “consider,” rather than an outright “clear,” for the position in which Moore had already been working. And to Moore, this signified the absurdity and inaccuracy of artificial intelligence. .“I think it’s really dangerous to give these kinds of algorithms so much authority,” Moore told Recode. “It’s such a terrible algorithm. It’s a keyword search.” .Fama founder and CEO Ben Mones told Recode that the company claims to be able to identify problematic behavior, like sexism and bullying, as well as the risk that someone might commit insider trading or intellectual property theft. Fama primarily analyzed Twitter activity in Moore’s case, but the technology can also pick up information about applicants from news sites and other webpages. Since Moore’s report was performed, Fama has stopped labeling these posts as “good” or “bad.” Now, it simply flags content, leaving employers to make their own judgments. .Fama isn’t the only company that’s attempted such a business model. Other companies are looking for ways to report what prospective applicants share online, a process that some background check companies say they can expedite with the help of AI. Faris, the Checkr VP, said her company has talked about offering social media screenings but has yet to see significant demand from its existing customer base. .It’s also unclear if social media companies themselves will tolerate this use of their data. Predictim, a company that used AI to score potential babysitters based on their social media, attracted enough negative attention back in 2018 that it was ultimately blocked by Facebook, Instagram, and Twitter. Predictim’s website is no longer active..Fama, for one, has found a way around some social media companies’ policies. Twitter told Recode that it suspended Fama’s API access around the same time Moore’s story about their background check went viral. Twitter said that its API policies ban the use of the platform for “background checks or any form of extreme vetting,” as well as “surveillance.” Fama maintains that it still has some form of access to Twitter data for its services..Background check companies are generally considered credit reporting agencies, and there are state and federal laws that regulate how these agencies operate. Chief among them is the Fair Credit Reporting Act, a law passed in 1970 to protect consumers that’s regulated by the Federal Trade Commission. Just last month, the agency shared best practices for working with artificial intelligence and algorithms..Currently, the Fair Credit Reporting Act law requires potential employers to inform and get a person’s permission before running a background check. If the employer thinks the results of the background check will factor into rejecting an applicant for a job, they have to let the applicant know and give them a chance to contest any information in the report. If that happens, the credit reporting agency enlisted by the employer has to reinvestigate its findings. There is no guarantee, however, that any corrections will be made in time for a person to remain in consideration for a particular position..Background check companies can make significant errors, and those errors can impact whether or not someone is ultimately offered a job. According to Ariel Nelson, an attorney at the National Consumer Law Center, these firms do have a legal obligation to have “reasonable procedures to assure maximum possible accuracy of the information.” There’s still a pervasive problem of mistakes being included in background checks, Nelson explained, even when AI is not involved. .So when you find yourself applying for a new job, consider that your application could be subject to an AI-powered background check, especially if you’re looking for work in the gig economy. You do have control over some aspects of this process. You can make your social media accounts private or delete your data from these platforms entirely. Basically, if you can see information about you online while you’re not logged into a platform, a future employer — or a hired AI — can probably see it, too. .Clarification: This post has been updated to clarify that Checkr has automated features, but the service does not use AI to scan through resumes, analyze facial expressions during video job interviews, or judge applicant’s social media behavior..     Millions turn to Vox to understand what’s happening in the news. Our mission has never been more vital than it is in this moment: to empower through understanding. Financial contributions from our readers are a critical part of supporting our resource-intensive work and help us keep our journalism free for all.  Please consider making a contribution to Vox today. ",Companies are utilizing Checkr to hire yet Checkr is not addressing errors and mistakes on criminal records
155,"In January, my coworker received a peculiar email. The message, which she forwarded to me, was from a handful of corporate Walmart employees calling themselves the “Concerned Home Office Associates.” (Walmart’s headquarters in Bentonville, Arkansas, is often referred to as the Home Office.) While it’s not unusual for journalists to receive anonymous tips, they don’t usually come with their own slickly produced videos..The employees said they were “past their breaking point” with Everseen, a small artificial intelligence firm based in Cork, Ireland, whose technology Walmart began using in 2017. Walmart uses Everseen in thousands of stores to prevent shoplifting at registers and self-checkout kiosks. But the workers claimed it misidentified innocuous behavior as theft and often failed to stop actual instances of stealing..They told WIRED they were dismayed that their employer—one of the largest retailers in the world—was relying on AI they believed was flawed. One worker said that the technology was sometimes even referred to internally as “NeverSeen” because of its frequent mistakes. WIRED granted the employees anonymity because they are not authorized to speak to the press..The workers said they had been upset about Walmart’s use of Everseen for years and claimed colleagues had raised concerns about the technology to managers but were rebuked. They decided to speak to the press, they said, after a June 2019 Business Insider article reported Walmart’s partnership with Everseen publicly for the first time. The story described how Everseen uses AI to analyze footage from surveillance cameras installed in the ceiling and can detect issues in real time, such as when a customer places an item in their bag without scanning it. When the system spots something, it automatically alerts store associates..“Everseen overcomes human limitations. By using state-of-the-art artificial intelligence, computer vision systems, and big data, we can detect abnormal activity and other threats,” a promotional video referenced in the story explains. “Our digital eye has perfect vision, and it never needs a day off.”.In an effort to refute the claims made in the Business Insider piece, the Concerned Home Office Associates created a video, which purports to show Everseen’s technology failing to flag items not being scanned in three different Walmart stores. Set to cheery elevator music, it begins with a person using self-checkout to buy two jumbo packages of Reese’s White Peanut Butter Cups. Because the packages are stacked on top of each other, only one is scanned, but both are successfully placed in the bagging area without issue..The same person then grabs two gallons of milk by their handles and moves them across the scanner with one hand. Only one is rung up, but both are put in the bagging area. They then put their own cell phone on top of the machine, and an alert pops up saying they need to wait for assistance—a false positive. “Everseen finally alerts! But does so mistakenly. Oops again,” a caption reads. The filmmaker repeats the same process at two more stores, where they fail to scan a heart-shaped Valentine’s Day chocolate box with a puppy on the front and a Philips Sonicare electric toothbrush. At the end, a caption explains that Everseen failed to stop more than $100 of would-be theft..The video isn’t definitive proof that Everseen’s technology doesn’t work as well as advertised, but its existence speaks to the level of frustration felt by the group of anonymous Walmart employees, and the lengths they went to prove their objections had merit..In interviews, the workers, whose jobs include knowledge of Walmart’s loss-prevention programs, said their top concern with Everseen was false positives at self-checkout. The employees believe that the tech frequently misinterprets innocent behavior as potential shoplifting, which frustrates customers and store associates, and leads to longer lines. “It’s like a noisy tech, a fake AI that just pretends to safeguard,” said one worker..The coronavirus pandemic has given their concerns more urgency. One Concerned Home Office Associate said they worry false positives could be causing Walmart workers to break social-distancing guidelines unnecessarily. When Everseen flags an issue, a store associate needs to intervene and determine whether shoplifting or another problem is taking place. In an internal communication from April obtained by WIRED, a corporate Walmart manager expressed strong concern that workers were being put at risk by the additional contact necessitated by false positives and asked whether the Everseen system should be turned off to protect customers and workers..Before COVID-19, “it wasn’t ideal, it was a poor customer experience,” the worker said. “AI is now creating a public health risk.” (HuffPost reported last week that corporate Walmart employees were concerned about Everseen’s technology putting store associates at risk amid the pandemic.).When COVID-19 reached the United States, Americans rushed to stock up on food and household essentials at Walmart, and sales soared. Workers soon began falling sick; at least 20 Walmart associates have now died after contracting the coronavirus, according to United for Respect, a nonprofit that advocates for retail workers and that is crowdsourcing COVID-19 infection rates and working conditions at Walmart stores across the country. Last month, United for Respect said hundreds of Walmart employees participated in a national strike demanding safer working conditions and better benefits..A spokesperson for Walmart said the company has been working diligently to protect customers and its workforce and believes the rate at which associates have contracted COVID-19 is lower than that of the general US population. They denied that false positives caused by Everseen were a widespread issue and said the company had not considered turning the system off due to concerns about COVID-19..“We assess our technology regularly, and as evident with the large scale implementation of Everseen across the chain, we have confidence it is currently meeting our standards,” the spokesperson said in an email. Just prior to the start of the pandemic, Walmart said it made significant improvements to its Everseen system, which resulted in fewer alerts overall. The spokesperson declined to answer questions about what the updates may have entailed..The spokesperson also noted that there are a number of different reasons an associate might intervene during a self-checkout transaction, like when a customer has problems with their credit card. The company said it has taken a number of steps to ensure people are protected during these interactions, including regularly cleaning self-checkout kiosks and providing employees with protective equipment. In addition, workers are given handheld devices that allow them to handle most interventions from a distance, the company said..Everseen declined to answer questions about its technology. In a statement, a spokesperson said the company “accurately and effectively identifies potential theft [sic] is why retailers have successfully deployed it at thousands of locations to date, with many more installations planned.” They added that Everseen typically accounts only for less than 10 percent of total interventions at self-service checkouts. In a separate statement, the spokesperson said “Everseen is committed to helping its customers deliver the best possible experience for shoppers and store associates, especially during the COVID-19 pandemic. Self-checkout offers the benefits of a generally contactless shopping experience, allowing for proper social distancing and avoiding manned-lanes in busy stores with limited staff available.”.But the Concerned Home Office Associates said their worries about Everseen long predate the pandemic. Emails obtained by WIRED show that other corporate employees raised issues about the technology failing to prevent theft in both 2017 and 2018. The employees said they were particularly vexed by Walmart’s continued investment in Everseen because NCR Corporation, which makes the majority of Walmart’s registers, had acquired an Everseen competitor called StopLift. They considered the acquisition an endorsement and were confused as to why StopLift’s technology wasn’t being further explored..What’s more, the workers said an internal Walmart research and development group, the Intelligent Retail Lab (IRL), created its own anti-theft software they believed was more accurate than Everseen’s, according to information they were given internally. One Walmart employee said the technology, the existence of which was previously reported by The Wall Street Journal, is now being tested in roughly 50 stores..Walmart declined to answer questions about its internal anti-theft software but did not dispute WIRED’s reporting. “At an enterprise level, there are a number of tests happening at any given time across our footprint of nearly 5,000 stores,” a spokesperson said in a statement. “The goal of IRL is to build AI capabilities that can be transferred to additional stores. We regularly test capabilities built internally in a small number of stores.”.Everseen’s technology was designed in part to help solve a persistent problem with self-checkout. While allowing customers to scan and pay for their own items cuts down on labor costs for retailers, it has also led to more inventory loss, or “shrinkage,” due to shoplifting, employee theft, and other problems. “Theft through self-checkout lanes is exponentially higher than through traditional checkout lanes,” says Christopher Andrews, a sociology professor at Drew University and the author of The Overworked Consumer: Self-Checkouts, Supermarkets, and the Do-It-Yourself Economy..In the past, Walmart and other retailers relied on weight sensors to prevent shoplifting through self-checkout, but those were prone to error and frustrated customers. Some stores are now turning instead to firms like Everseen, which promise to reduce shrink and increase customer satisfaction by relying instead on surveillance cameras and machine vision. Everseen has said that it works with a number of major retailers. Amazon uses similar technology in its Amazon Go convenience stores, where a network of cameras automatically log the products customers take. (Amazon is now licensing its “Just Walk Out” tech to other companies.).During the coronavirus pandemic and its aftermath, self-checkout may become even more important for stores, as customers look for low-risk ways to shop. NCR corporation said it’s now helping retailers modify its equipment to be as touchless as possible: for example, by reconfiguring machines so that customers can insert a debit or credit card without needing to press the “credit card” payment option. “It is fascinating to see self-checkout become poised as a public health strategy, in addition to things like cashless payment,” says Alexandra Mateescu, a researcher at the nonprofit institute Data & Society, who has written about the effects of new technology on retail workers..“Self-checkout is just one of the ways that we’ve offered customers solutions to get the items they need safely during this time, in addition to other options like delivery, pickup, touchless payment at the register and shopping online,” the Walmart spokesperson said in a statement. “Customers are using this option now, as much as ever, and we will continue to work hard to ensure the in-store experience for our customers is safe, affordable and convenient, as well as safe for our associates.”",Errors include failing to flag items not being scanned and other false alerts
158,"The Wish You Were Here bots tricked humans trying to spot the fake up to 47 per cent of the time - could this tech lead to more claims of doctored deepfake images?.acebook is developing ways to insert computer-generated likenesses of people into photographs in an artificial intelligence project called “Wish You Were Here”..The social network wants to make it easier for users to “blend” people into their images even if they were nowhere near the original subjects at the time..In a study, AI software created a map around the person due for insertion into a scene, then approximated the pose of the people in the photo before adding the third partys approximated likeness into the shot..It tricked volunteers trying to spot the fake human nearly 43 per cent of the time on average, depending on group size, however, the image quality of people created by the AI was decidedly mixed..Fake images were created using three AI processes called Essence Generation, Multi-Conditioning Rendering and Face Refinement for “context-aware human generation”..These generated semantic maps” to try and maintain the existing scenes context before inserting scans of the new persons hair face, torso, clothes and shoes as a “photorealistic” cut-out..Researchers said they found a “convincing ability” of the AI to add people into existing photos “while preserving the overall image quality”, but the software struggled recreating some human features, such as hairstyles..The AI was trained on more than 20,000 sample photographs from an open-source gallery, resulting in nearly 53,600 example images for analysis..The research paper, due to be presented at the US Conference on Computer Vision and Pattern Recognition conference, is a collaboration between Facebook AI Research and Tel-Aviv University..Co-authors Oran Gafni, an AI research engineer for Facebook, and Professor Lior Wolf, of a faculty member of the university’s School of Computer Science, described the technique as a “novel method for inserting objects, specifically humans, into existing images” while “respecting the semantic context of the scene”..“In an extensive set of experiments, we demonstrate that the first of our networks can create poses that are indistinguishable from real poses, despite the need to take into account the social interactions in the scene.”.Facebook’s latest project follows Google’s work using AI methods to insert objects such as vehicles into images by predicting scale, shape and location.",There could be potential abuse on this platform similar to Deepfakes
159,"YouTube is automatically deleting comments that contain certain Chinese-language phrases related to criticism of the country’s ruling Communist Party (CCP). The company confirmed to The Verge this was happening in error and that it’s working to fix the issue..“Upon review by our teams, we have confirmed this was an error in our enforcement systems and we are working to fix it as quickly as possible,” said a YouTube spokesperson. The company did not elaborate on how or why this error came to be, but said it was not the result of any change in its moderation policy. .But if the deletions are the result of a simple mistake, then it’s one that’s gone unnoticed for six months. The Verge found evidence that comments were being deleted as early as October 2019, when the issue was raised on YouTube’s official help pages and multiple users confirmed that they had experienced the same problem..Comments left under videos or in live streams that contain the words “共匪” (“communist bandit”) or “五毛” (“50-cent party”) are automatically deleted in around 15 seconds, though their English language translations and Romanized Pinyin equivalents are not..The term “共匪” is an insult that dates back to China’s Nationalist government, while “五毛,” (or “wu mao”) is a derogatory slang term for internet users paid to direct online discussion away from criticism of the CCP. The name comes from claims that such commenters are paid 50 Chinese cents per post. .These phrases seem to have been accidentally added to YouTube’s comment filters, which automatically remove spam and offensive text. The comments are removed too quickly for human moderation and are deleted even if the banned phrases are used positively (e.g., “The 五毛 are doing a fantastic job”). YouTube says it’s been relying more on its automated filters in recent months due changes to its workforce brought about by the pandemic. .The accidental censorship is even more puzzling considering that YouTube is currently blocked in China, giving its parent company, Google, even less reason to censor comments critical of the CCP or apply moderation systems in accordance with Chinese censorship laws. .The automatic deletion of these phrases was highlighted on Tuesday by US technologist and former Oculus founder Palmer Luckey on Twitter. But earlier reports of the issue date back to the middle of May when they were spotted by human rights activist Jennifer Zeng. As mentioned above, though, The Verge also found complaints on YouTube’s official help pages dated to October 2019..Google has frequently been criticized for accommodating the wishes of the CCP by censoring content. Most notably, it created a prototype search engine known as Project Dragonfly that complied with Chinese state censorship. The project, which was never deployed, is part of the company’s long-running struggles to enter the Chinese market..When news of Dragonfly leaked in 2018 in a report from The Intercept, Google was criticized by politicians and its own employees for selling out its principles. During a Senate Judiciary Committee hearing in June 2019, the company said it had “terminated” the project and that it had “no plans to launch Search in China.”",This error was unnoticed for six months and are repetitive.
161,"A team of researchers from the Higher School of Economics University and Open University in Moscow, Russia claim they have demonstrated that an artificial intelligence can make accurate personality judgments based on selfies alone — more accurately than some humans..The researchers suggest the technology could be used to help match people up in online dating services or help companies sell products that are tailored to individual personalities..That’s apropos, because two co-authors listed on a paper about the research published today in Scientific Reports — a journal run by Nature — are affiliated with a Russian AI psychological profiling company called BestFitMe, which helps companies hire the right employees..As detailed in the paper, the team asked 12,000 volunteers to complete a questionnaire that they used to build a database of personality traits. To go along with that data, the volunteers also uploaded a total of 31,000 selfies..The questionnaire was based around the “Big Five” personality traits, five core traits that psychological researchers often use to describe subjects’ personalities, including openness to experience, conscientiousness, extroversion, agreeableness, and neuroticism..After training a neural network on the dataset, the researchers found that it could accurately predict personality traits based on “real-life photographs taken in uncontrolled conditions,” as they write in their paper..While accurate, the precision of their AI leaves something to be desired. They found that their AI “can can make a correct guess about the relative standing of two randomly chosen individuals on a personality dimension in 58% of cases.”.Strikingly, the researchers claim their AI is better at predicting the traits than humans. While rating personality traits by human “close relatives or colleagues” was far more accurate than when rated by strangers, they found that the AI “outperforms an average human rater who meets the target in person without any prior acquaintance,” according to the paper..Considering the woeful accuracy, and the fact that some of the authors listed on the study are working on commercializing similar tech, these results should be taken with a hefty grain of salt..Neural networks have generated some impressive results, but any research that draws self-serving conclusions — especially when they require some statistical gymnastics — should be treated with scrutiny..READ MORE: Artificial intelligence can make personality judgments based on photographs [National Research University Higher School of Economics]",There is a potential for racial bias as well as accuracy concerns.
162,"When the world economy reopens and populations want to traverse borders again, some governments are going to want proof people aren’t coming in or leaving with Covid-19. Employers are also going to want to know their workforce isn’t going to be the epicenter of another outbreak. And so a handful of companies are bidding for business that will help the Trump and Johnson administrations on either side of the Atlantic keep tabs on travel (or attempted travel) of the infected..One of them is facial recognition startup FaceFirst. Located in Encino, California in 2007 and with $10.4 million funding, it’s been promoting the idea of a “coronavirus-immunity registry.” This will be based on a database run by medical providers, which will feed a smartphone app with your immunity status. Just as your iPhone opens if you show your face, the app will verify it’s you by using your face..The app will also tell employers and border control staff more about a person’s experience of Covid-19, says CEO Peter Trepp. It will know what kind of test you received, in case it was a defective one; it will include a record of whether you’ve been near infected folk or not; and it will note if you’ve had an antibody test too..This app would effectively act as what’s become known as a health or immunity “passport.” Though Trepp says taking temperatures is not enough, that could be another data point to add to the passport. “These are lots and lots of data points. And my belief is that collectively data points can be helpful in determining how you fill an aeroplane, do you fill an aeroplane with everyone we believe to be virus free, do you fill another aeroplane with everyone who has immunities to the virus,” he says..“The other benefit of this thing, of course, is to know who to vaccinate and who should be vaccinated first. And I dont mean people with privilege or money... I mean people who have jobs where they can infect other people, or frontline workers.”.Trepp doesn’t think there’s a decent alternative to the conundrum of keeping a second Covid-19 wave at bay. “The other solution, of course, is lets hire thousands of people and make phone calls and build a big Excel spreadsheet and just ask people whether theyve had it and whether theyve been tested. That is laughable, in my view… It doesnt work when you consider the power of a more centralized system.”.There are obvious anxieties about any system that involves monitoring of people’s medical records. This week the American Civil Liberties Union (ACLU) raised concerns about such immunity passports, saying it could create “a new health surveillance infrastructure that endangers privacy rights.” .On the privacy side, Trepp clarified that not only will any such system be opt-in only, any personally identifiable information will remain only on individuals’ smartphones “and will not be uploaded or available for collection by any organization or government agency. Therefore, no centralized surveillance database will be or can be established.”.But there are civil liberties issues with such a system. The ACLU noted that passports risk dividing workers into the immune and the non-immune. “The latter might never be eligible for a given job short of contracting and surviving Covid-19 if an immune worker is available to take the slot,” the ACLU warned. This could lead to perverse outcomes, such as people willingly contracting Covid-19 to try to get the antibodies they need to get a job. Then there’s an obvious scientific issue: we still don’t know enough about Covid-19 to be sure that having antibodies prevent getting the disease a second time or how long their protective qualities last..If it does ever get the green light, such a centralized system will have to have the backing of the White House and Congress, Trepp says. He tells Forbes he’s in “indirect discussions” with the U.S. government, though doesn’t elaborate. When Forbes asks whether the $10,000 the company has spent on lobbying, according to a Senate record from this April, Trepp says it wasn’t for discussing his passport idea. But they show the company has talked with officials at the Department of Health & Human Services (HHS) and the White House about “issues related to biometrics and facial recognition and entry and exit screening.”.In the U.K., Onfido has been pushing for a similar rollout. The startup, which has (according to Pitchbook data) secured $265 million in venture funding and is perhaps best known as the provider of the verification technology behind challenger bank Monzo, has been pitching “a system for citizens, guests and employees to have proof of immunity that is designed to help an individual prove their health status, but without them having to share any other personal information.” Similarly, Trepp says his system would have a focus on privacy and that the user would have control over their medical data. .Husayn Kassai, CEO and co-founder at Onfido, said in a statement that the technology “is used to tie a physical human being to their digital identity using just a photo of their ID and a selfie video. Once this is bound to a test result, the digital certificate could be displayed, like smartphone boarding passes.”.Onfido has been approached by the U.K. Parliament’s Science and Technology Committee to submit a proposal for immunity passports. When asked if the company was pushing the technology in the U.S. or elsewhere, he said no discussions are yet taking place, but added: “Were also consulting with other governments to make this process as seamless as possible.”.At least one country, Estonia, is now trialling the passports in the workplace via an app created by the founders of Bolt and TransferWise, according to a Reuters report. Others will likely follow in one form or another. Americans and Brits alike will soon be tested on how far they’re willing to forego some civil liberties for the sake of reopening the country safely.",There is a concern for racial bias and privacy.
163,"Alfred Ng was a senior reporter for CNET News. He was raised in Brooklyn and previously worked on the New York Daily Newss social media and breaking news teams..Your face mask selfies arent just getting seen by your friends and family -- theyre also getting collected by researchers looking to use them to improve facial recognition algorithms. CNET found thousands of face-masked selfies up for grabs in public data sets, with pictures taken directly from Instagram.  .The COVID-19 pandemic is causing a surge in people wearing face masks, and facial recognition companies are scrambling to keep up. Face masks cover up a significant portion of what facial recognition needs to identify and detect people -- essentially threatening the future of a multimillion-dollar industry unless the technology can learn to recognize people beyond the coverings..In April, researchers published the COVID19 Mask Image Dataset to Github, using more than 1,200 images collected from Instagram. A month earlier, researchers from China compiled a database with more than 5,000 masked photos they gathered online..The creators behind the April database used their AI startup Workaround to help comb through the images and properly label them with masks on or off, said Wafaa Arbash, the companys CEO..We were inspired by all the companies that were launching free tools and everything they can do to help, Arbash said. We have these public images from Instagram, so these are not private images. We were just searching and getting the right data.  .Facial recognition companies have long used peoples pictures without consent to train their algorithms. Civil liberty advocates contend that facial recognition technology threatens privacy and free speech, warning as well that there are almost no laws preventing abuse of the surveillance tools..Clearview AI, a controversial facial recognition company, claimed it has a First Amendment right to scrape more than 3 billion images from social networks to use for its database. .Governors in more than half of the US states are mandating face masks in public because the coverings help prevent the spread of COVID-19. The masks have also slowed down the spread of facial recognition, since the garments block key parts of your face that the technology usually analyzes. .Some facial recognition providers have turned to asking their own staffers to send in face-masked selfies, as well as editing masks on top of the photos that they already have. Digitally adding masks to photos is how the US National Institute of Standards and Technology plans on testing facial recognition algorithms. .But there are only so many employees a company can ask to take selfies, and edited face mask photos may not be as effective as organic images for training algorithms. Facial recognition companies also need a diverse set of pictures so the algorithms can better recognize women, people of color, people of different ages and a variety of mask types. .For her companys public database, Arbash said the photos came from searching on Instagram with hashtags related to masks. They gathered about 3,000 pictures from the social media platform, but narrowed it down to a set of 1,200 photos. The sample images posted included a childs photo as part of the set -- Arbash said it was a possible error that this picture ended up in its database.  .Arbash said they didnt ask the people included in the database for permission to use their face mask selfies to help develop facial recognition, and that if they wanted to be excluded, they could make their pages private. The people included arent aware theyre in this data set, she said. .Were not making any money off of this, its not commercial, Arbash said. The goal and the intention was to help any data science or machine learning engineers who are working to fix this issue and help with public safety.  .The links to the images from Instagram have since expired, but the data sets page put out a public call asking if anyone knew how to retrieve the photos. Arbash said if theres enough interest, the company would consider looking more into how to get more face mask images. .We do not allow third parties to collect or use photos posted by our users in this way, without their consent. We are continuing to investigate this, Facebook said in a statement..The Real World Masked Face Dataset claims to be the largest masked face data set, with more than 5,000 masked faces of 525 people gathered from the internet. The compilation comes from researchers at Wuhan University in China, where the coronavirus outbreak began. .A research paper released on March 23, accompanying the data set, says the images are of public figures gathered from massive internet resources. The researchers didnt respond to a request for comment. .The practice of grabbing peoples photos from social media to train facial recognition algorithms isnt new, but the focus on face masks because of COVID-19 is. Theres an urgency among developers to create face mask detection technology as a public safety concern, but ethical issues come up when the images are collected without consent. .People might not like the idea that their picture could be used to develop a database that could go to law enforcement or government surveillance in a foreign autocratic country like China, said Jake Laperruque, a senior counsel at the Constitution Project. Youre putting photos out there, maybe not with an expectation of privacy, but you have an expectation of how it can and cant be used. .Be respectful, keep it civil and stay on topic. We delete comments that violate our policy, which we encourage you to read. Discussion threads can be closed at any time at our discretion.",People did not know their images were used
164,"A draft government memo explaining how the NHS contact-tracing app could stem the spread of the coronavirus said ministers might be given the ability to order “de-anonymisation” to identify people from their smartphones, the Guardian can reveal..The health secretary, Matt Hancock, announced on Sunday that the UK planned to introduce an app that would enable people who developed Covid-19 symptoms to “anonymously” alert other users to whom they had been in close proximity. “All data will be handled according to the highest ethical and security standards, and would only be used for NHS care and research,” he said..However, the government document seen by the Guardian, headed “official – sensitive” and “draft – not yet approved”, suggests the NHS privately considered using the technology to identify users..Produced in March, the memo explained how an NHS app could work, using Bluetooth LE, a standard feature that runs constantly and automatically on all mobile devices, to take “soundings” from other nearby phones through the day. People who have been in sustained proximity with someone who may have Covid-19 could then be warned and advised to self–isolate, without revealing the identity of the infected individual..However, the memo stated that “more controversially” the app could use device IDs, which are unique to all smartphones, “to enable de-anonymisation if ministers judge that to be proportionate at some stage”. It did not say why ministers might want to identify app users, or under what circumstances doing so would be proportionate..It added that alternatives to building an NHS app included “making use of existing apps and other functions already installed on people’s phones (eg Google Maps).”.A spokesperson for NHSX, the digital transformation wing of the health service, which is overseeing the development of the UK contact-tracing app, denied there were ever plans to de-anonymise data, or use data from apps such as Google Maps..“NHSX is looking at whether app-based solutions might be helpful in tracking and managing coronavirus, and we have assembled expertise from inside and outside the organisation to do this as rapidly as possible,” the spokesperson said..“To be very clear – there have never been plans to make use of existing apps and other functions already installed on peoples phones such as Google Maps and neither have there been plans to look to use the device ID of users in any app-based solutions.”.NHSX plans to harness data for the government’s response to the coronavirus pandemic were made public after confidential documents, used by private contractors, were made accessible via an unrestricted portal that was seen by the Guardian. .On Friday, Google and Apple announced an unprecedented collaboration to provide the back-end technology that will enable governments to introduce contact-tracing apps that run on both iPhones and Android phones. Hancock did not specify which technology the government would use but said the NHS was “working closely with the world’s leading tech companies”..Explaining how the NHS app would work, Hancock said: “If you become unwell with the symptoms of coronavirus, you can securely tell this new NHS app and the app will then send an alert anonymously to other app users that you’ve been in significant contact with over the past few days, even before you had symptoms, so that they know and can act accordingly.”.Advocates of contact-tracing apps argue they could prove a vital tool to help governments emerge from lockdown conditions currently restricting the movement of millions. .Researchers at the University of Oxford, who have been advising NHSX on the app, published a paper in the journal Science last month concluding that traditional contact tracing of those with Covid-19 was of limited use, in part because it can be spread by those who are asymptomatic and pre-symptomatic..Instead, the researchers argued, a contact-tracing app could speed up the process by automatically notifying contacts of people diagnosed with Covid-19. Prof David Bonsall, a senior researcher at Oxford University’s Nuffield Department of Medicine, told the Guardian that “we see it as the only alternative to … applying isolation to the whole population.”.Privacy International has said a Bluetooth LE system would be far less intrusive than other forms of tracking, such as using GPS or wifi data, because it would only keep a record of which devices had been near one another, rather than their actual locations..However, the draft memo raises questions about the use of contact-tracing apps if they are introduced without sufficient safeguards or transparency. The notion of “de-anonymisation” of users, in particular, would appear to contradict advice given by the Information Commissioner’s Office (ICO) that identifying individuals from their location data may breach privacy law..The deputy information commissioner said in a recent statement that data protection laws were not infringed as long as location data was “properly anonymised and aggregated”. An ICO spokesperson told the Guardian: “When personal data can be identified, organisations must comply with data protection law, including putting the appropriate safeguards in place to protect people’s information.”.However, the creation of apps to battle Covid-19 raise unprecedented questions about privacy. Edward Snowden, the whistleblower who exposed the US National Security Agency’s mass surveillance programmes seven years ago, has said the coronavirus could be exploited to usher in an era of bio-surveillance that persists even after the pandemic has ended..If you have been affected or have any information, wed like to hear from you. You can get in touch by filling in the form below, anonymously if you  wish or contact us via WhatsApp by clicking here or adding the contact +44(0)7766780300. Only the Guardian can see your contributions and one of our  journalists may contact you to discuss further. .Others are asking whether contact-tracing apps can really work, given that such large numbers of the population will need to download and use the technology for it to be effective, the lack of reliable testing, and the risk that such an open system could be abused by people fabricating symptoms. Ross Anderson, a professor of security engineering at Cambridge University, recently wrote that “anyone who’s worked on abuse will instantly realise that a voluntary app operated by anonymous actors is wide open to trolling”.",The app can allow ministers to identify app users which can violate privacy
165,"YouTube is profiting from videos promoting unproven coronavirus treatments, a new report has found, as the company struggles to crack down on misinformation..The Google-owned tech company is running advertisements with videos pushing herbs, meditative music, and potentially unsafe over-the-counter supplements as cures for Covid-19, according to a report published on Friday by the Tech Transparency Project, a not-for-profit watchdog organization..After being contacted by the Guardian, YouTube removed four of the videos in question for violating its policies against Covid-19 misinformation. Three of the videos remain as they are not promoting misinformation directly but offering wellness tips, according to a spokesman..“We’re committed to providing timely and helpful information at this critical time, including raising authoritative content, reducing the spread of harmful misinformation and showing information panels, using WHO data, to help combat misinformation,” the spokesman said..Companies including YouTube may be forced to rely more heavily on artificial intelligence tools to moderate content while employees are forced to work from home, said Megan Lamberth, a researcher at the Center for a New American Security, a Washington DC-based thinktank..“Since the beginning of the pandemic, we’ve seen an enormous rise of misinformation on online platforms,” she said. “Social media companies have tried to respond to the deluge of misinformation, but in many cases, their moderation efforts have not been sufficient.”.TikTok has partnered with the World Health Organization to provide accurate information to users and has put a disclaimer on all videos using the #coronavirus hashtag with accurate information about the pandemic. Other sites, including Facebook, have pushed back against dangerous false cures including cleaning products and cocaine..The global pandemic has created a situation ripe for the spread of false cures, said Lisa Fazio, a psychology professor at Vanderbilt University who studies misinformation and how it spreads..“If the most profitable videos are those that feel good and provide easy answers, then it’s highly likely that they will contain misinformation,” she said. “In reality, the current situation is complicated and has few easy answers.”.YouTube initially prohibited the monetization of videos about Covid-19 under its “sensitive events” policy, which bars advertisements on videos regarding armed conflicts, terrorist acts, and “global health crises”..However, it reversed that policy on 11 March, saying it wanted to “make sure news organizations and creators can continue producing quality videos in a sustainable way”. It then enabled coronavirus video ads for “a limited number of channels”, and on 2 April it expanded monetization of content mentioning or featuring Covid-19 to all creators and news organizations..“In lifting restrictions on advertising in videos about the coronavirus pandemic, YouTube has made disinformation lucrative for some unscrupulous content creators and a liability for the brands that unwittingly support them,” the report said.",Youtube is increasing revenue as misinformation increases
168,"Whole Foods is keeping an eye on stores at risk of unionizing through an interactive heat map, according to five people with knowledge of the matter and internal documents viewed by Business Insider..The heat map is powered by an elaborate scoring system, which assigns a rating to each of Whole Foods 510 stores based on the likelihood that their employees might form or join a union..The stores individual risk scores are calculated from more than two dozen metrics, including employee loyalty, turnover, and racial diversity; tipline calls to human resources; proximity to a union office; and violations recorded by the Occupational Safety and Health Administration..The map also tracks local economic and demographic factors such as the unemployment rate in a stores location and the percentage of families in the area living below the poverty line..The stores scores on each metric are fed into the heat map, which is a geographic illustration of the United States peppered with red spots to indicate high-risk Whole Foods stores..It also provides a rare look into corporate labor-tracking activities, a common practice among large companies but one rarely discussed publicly..A statement on the map describes its purpose as specific to monitoring unionization among its employees, which the company calls team members..The [Team Member] Relations Heatmap is designed to identify stores at risk of unionization, the statement reads. This early identification enables resources to be funneled to the highest need locations, with the goal of mitigating risk by addressing challenges early before they become problematic..In a statement provided to Business Insider, the company said an overwhelming majority of its employees prefer a direct relationship with the company over union representation..Whole Foods Market recognizes the rights of our Team Members to decide whether union representation is right for them, the company said. We agree with the overwhelming majority of our Team Members that a direct relationship with Whole Foods Market and its leadership, where Team Members have open lines of communication and every individual is empowered to share feedback directly with their team leaders, is best..Our open-door communication policy allows us to understand and quickly respond to the needs of our workforce, while recognizing, rewarding, and supporting the goals of every member of our team, the statement continued. At Whole Foods Market, were committed to treating all of our Team Members fairly, creating a safe, inclusive, and empowering working environment, and providing our Team Members with career advancement opportunities, great benefits, and competitive compensation, including an industry-leading starting minimum wage of $15/hour..Whole Foods uses the heat map and related scores to determine where stores must take action to address risks, according to the documents and people familiar with the map..Some of the factors that contribute to external risk scores include local union membership size; distance in miles between the store and the closest union; number of charges filed with the National Labor Relations Board alleging labor-law violations; and a labor incident tracker, which logs incidents related to organizing and union activity..Other external factors include the percentage of families within the stores zip code that fall below the poverty line and the local unemployment rate..The second group of metrics in the scoring system, called store risks, arent a direct cause of risk but can predispose a store to risk, according to documents..Store-risk metrics include average store compensation, average total store sales, and a diversity index that represents the racial and ethnic diversity of every store. Stores at higher risk of unionizing have lower diversity and lower employee compensation, as well as higher total store sales and higher rates of workers compensation claims, according to the documents..The third area of metrics is team member sentiment. These metrics, which include items like employee loyalty and engagement, are designed to be the most actionable, the documents show..These measures assess employees feedback on the quality and safety of their work environment and whether they feel supported and respected, among other things..With the heat map, Whole Foods appears to be trying to identify and address circumstances ripe for employee unrest that could lead to attempts to form a union..This type of workforce analysis is something large companies have done for decades, albeit without some of the technology available today that can automate parts of that process, according to labor experts..Walmart, for example, hired an intelligence-gathering service from Lockheed Martin and ranked stores by labor activity when it faced protests eight years ago organized by the union-backed activist group OUR Walmart, according to a 2015 Bloomberg Businessweek story citing thousands of court documents..Employers spend millions of dollars a year to hire union avoidance advisers to see how susceptible they are to their workers organizing, Celine McNicholas, the director of government affairs and labor counsel for the Economic Policy Institute, said..A preponderance of the business community [has] a total allergy to unionization, Wilma Liebman, who served on the National Labor Relations Board under Presidents Obama, Bush, and Clinton, said..Unions give employees more bargaining power over things such as wages and health benefits, she said. They could also increase the chances of employee strikes, which can disrupt business..Companies dont want anything thats going to interfere with their autonomy and their ability to act unilaterally and sometimes theyre convinced [unions] are going to cost them more than they can afford, Liebman said..Research shows unionized workers tend to earn higher wages and are more likely to have access to certain benefits like employer-sponsored healthcare..Critics of unions argue, however, that the organizations can harm companies economically, forcing layoffs or job outsourcing, and that they dont have workers best interests in mind..US labor law protects employees right to unionize. Its legal, however, for a company to monitor and address labor organizing as long as it doesnt threaten, coerce, restrain, or interfere with efforts to unionize..Overall, US companies spent at least $100 million on consulting services for anti-union campaigns between 2014 and 2017, according to data from the Economic Policy Institute based on disclosure forms filed with the US Department of Labor.",There is a potential for error and racial bias
170,"In the fight against the novel coronavirus, many countries ordered that citizens have their temperature checked at train stations or airports. The device needed in such situations, a hand-held thermometer, has risen from a specialist item to a common sight..A branch of Artificial Intelligence known as “computer vision” focuses on automated image labeling. Most computer vision systems were trained on data sets that contained very few images of hand-held thermometers. As a result, they cannot label the device correctly..In an experiment that became viral on Twitter, AlgorithmWatch showed that Google Vision Cloud, a computer vision service, labeled an image of a dark-skinned individual holding a thermometer “gun” while a similar image with a light-skinned individual was labeled “electronic device”. A subsequent experiment showed that the image of a dark-skinned hand holding a thermometer was labelled “gun” and that the same image with a salmon-colored overlay on the hand was enough for the computer to label it “monocular”..In a statement to AlgorithmWatch, Tracy Frey, director of Product Strategy and Operations at Google, wrote that “this result [was] unacceptable. The connection with this outcome and racism is important to recognize, and we are deeply sorry for any harm this may have caused.”.“Our investigation found some objects were mis-labeled as firearms and these results existed across a range of skin tones. We have adjusted the confidence scores to more accurately return labels when a firearm is in a photograph.” Ms Frey added that Google had found “no evidence of systemic bias related to skin tone.”.Agathe Balayn, a PhD candidate at the Delft University of Technology on the topic of bias in automated systems, concurs. She tested several images in Googles service and came to the conclusion that the example might be a case of inaccuracy without a statistical bias. In the absence of more rigorous testing, it is impossible to say that the system is biased, she wrote..It is easy to understand why computer vision produces different outcomes based on skin complexion. Such systems processed millions of pictures that were painstakingly labeled by humans (the work you do when you click on the squares containing cars or bridges to prove that you are not a robot, for instance) and draw automated inferences from them..Computer vision does not recognize any object in the human sense. It relies on patterns that were relevant in the training data. Research has shown that computer vision labeled dogs as wolves as soon as they were photographed against a snowy background, and that cows were labeled dogs when they stood on beaches..Because dark-skinned individuals probably featured much more often in scenes depicting violence in the training data set, a computer making automated inferences on an image of a dark-skinned hand is much more likely to label it with a term from the lexical field of violence..Other computer vision systems show similar biases. In December, Facebook refused to let an Instagram user from Brazil advertise a picture, arguing that it contained weapons. In fact, it was a drawing of a boy and Formula One driver Lewis Hamilton. Both characters had dark skins..Labeling errors could have consequences in the physical world. Deborah Raji, a tech fellow at New York University’s AI Now Institute and a specialist in computer vision, wrote in an email that, in the United States, weapon recognition tools are used in schools, concerts halls, apartment complexes and supermarkets. In Europe, automated surveillance deployed by some police forces probably use it as well. Because most of these systems are similar to Google Vision Cloud, “they could easily have the same biases”, Ms Raji wrote. As a result, dark-skinned individuals are more likely to be flagged as dangerous even if they hold an object as harmless as a hand-held thermometer..Nakeema Stefflbauer, founder and CEO of FrauenLoop, a community of technologists with a focus on inclusivity, wrote in an email that bias in computer vision software would “definitely” impact the lives of dark-skinned individuals. Because the rate of mis-identification is consistently higher for women and dark-skinned people, the spread of computer vision for surveillance would disproportionately affect them, she added..Referring to the examples of Ousmane Bah, a teenager who was wrongly accused of theft at an Apple Store because of faulty face recognition, and of Amara K. Majeed, who was wrongly accused of taking part in the 2019 Sri Lanka bombings after her face was misidentified, Ms Stefflbauer foresees that, absent effective regulation, whole groups could end up avoiding certain buildings or neighborhoods. Individuals could face de facto restrictions in their movements, were biased computer vision to be more widely deployed, she added..In her statement, Ms Frey, the Google director, wrote that fairness was one of Google’s “core AI principles” and that they were “committed to making progress in developing machine learning with fairness as a critical measure of successful machine learning.”.But Google’s image recognition tools have returned racially biased results before. In 2015, Google Photos labelled two dark-skins individuals “gorillas”. The company apologized but, according to a report by Wired, did not fix the issue. Instead, it simply stopped returning the “gorilla” label, even for pictures of that specific mammal..That technology companies still produce racially biased products can be explained by at least two reasons, according to AI Now’s Deborah Raji. Firstly, their teams are overwhelmingly white and male, making it unlikely that results that discriminate against other groups will be found and addressed at the development stage. Secondly, “companies are now just beginning to establish formal processes to test for and report these kinds of failures in the engineering of these systems,” she wrote. “External accountability is currently the main method of alerting these engineering teams,” she added..Nicolas is data journalist and working for AlgorithmWatch as a reporter. He pioneered new forms of journalism in France and in Europe and is one of the leading experts on data journalism. He regularly speaks at international conferences, teaches journalism in French journalism schools and gives training sessions in newsrooms. A self-taught journalist and developer (and a graduate in Economics), he started by doing small interactive, data-driven applications for Le Monde in Paris in 2009. He then built the data journalism team at OWNI in 2010 before co-founding and managed Journalism++ from 2011 to 2017. Nicolas is also one of the main contributors to the Datajournalism Handbook, the reference book for the popularization of data journalism worldwide.",A Google service that automatically labels images produced starkly different results depending on skin tone on a given image
172,"Were outsourcing ever more of our decision-making to algorithms, partly as a matter of convenience, and partly because algorithms are ostensibly free of some of the biases that humans suffer from. Ostensibly. As it turns out, algorithms that are trained on data thats already subject to human biases can readily recapitulate them, as weve seen in places like the banking and judicial systems. Other algorithms have just turned out to be not especially good..Now, researchers at Stanford have identified another area with potential issues: the speech-recognition algorithms that do everything from basic transcription to letting our phones fulfill our requests. These algorithms seem to have more issues with the speech patterns used by African Americans, although theres a chance that geography plays a part, too..Voice-recognition systems have become so central to modern technology that most of the large companies in the space have developed their own. For the study, the research team tested systems from Amazon, Apple, Google, IBM, and Microsoft. While some of these systems are sold as services to other businesses, the ones from Apple and Google are as close as your phone. Their growing role in daily life makes their failures intensely frustrating, so the researchers decided to have a look at whether those failures display any sort of bias..Based on a score called word error rate (which includes inserted and missing words, as well as misinterpretations) all of the systems did well, having a score of less than 0.5. (Apples was the worst, and Microsofts system the best based on this measure.) In all cases, the recordings of African American speakers ended up with word error rates that were worse than the ones produced from recordings of white speakers—in general, the errors nearly doubled..The effect was more pronounced among African American males. White men and women had error rates that were statistically indistinguishable, at 0.21 and 0.17, respectively. The rate for African American women averaged 0.30, while for men it rose to 0.41..How important are these differences? The authors suggest it depends on how you define usability—above a certain percentage of error, it becomes more annoying to fix an automated transcript than to do it yourself, or your phone will end up doing the wrong thing more often than youre happy with. The authors tested how often individual chunks of text end up with a conservative word error rate of 0.5. They found that over 20 percent of the phrases spoken by African Americans would fail this standard; less than 2 percent of those spoken by whites would..So whats going on? There may be a bit of a geographical issue. California speakers are often considered to be accent free from an American perspective, and the two samples from that state had very low error rates. Rochester had a rate similar to Californias, while the District of Columbia had one closer to the rural North Carolina town. If there is a geographic influence, were going to need a much larger sample to separate that out..After that, the researchers analyzed the language usage itself. Since they didnt have access to the algorithms used by these systems, they turned to some open source packages that perform similar functions. They measured the softwares understanding of language use via a figure called the perplexity, which is a value derived from the accuracy at which the system can predict the word that will come next in a sentence. And, by this measure, the systems were better at handling the usage of African American speakers. Whats going on?.The researchers found there were two conflicting tendencies. African Americans would, on average, use a smaller total word list than their white counterparts. But their phrasing turned out to be more complicated—in many cases, they dropped words from their sentences when their listeners could easily infer them..Finally, theres the matter of how attuned the commercial systems are to African American voices. To explore this, the researchers searched through the transcripts to find cases when African American and white speakers used the same phrases. When those were run through the systems, the word error rate for African American speakers was higher than for whites, suggesting this also contributed to the overall reduced performance..An effective voice-recognition system has to combine a number of factors—actual word recognition, language usage, and likely meanings—in order to successfully recognize sentences or predict ensuing words. Existing commercial systems appear to fall a bit short of that when it comes to some populations. These systems werent set up to be biased; its likely that they were simply trained on a subset of the diversity of accents and usages present in the United States. But, as we become ever more reliant on these systems, making them less frustrating for all their users should be a priority.",Error rate for African American speech is nearly double that for others.
173,"An algorithm designed to help weed out misinformation about coronavirus has been inadvertently hampering some Facebook users from disseminating content on homemade medical masks..According to a report from the New York Times, Facebook says that an error with its algorithm has been blocking content on how to make hand-sewn masks and threatening to ban the users who post it, including groups in Pennsylvania, Illinois, and California..Some of those groups, including one called Sew Face Masks Philadelphia, had thousands of followers and had its moderators threatened with a ban if content on masks continued..A self-described error in Facebooks moderation algorithm has been banning content relating to protective masks from appearing on its platform according to a new report. Pictured: Facebook CEO Mark Zuckerberg.The automated systems we set up to prevent the sale of medical masks needed by health workers have inadvertently blocked some efforts to donate supplies, Facebook said in a statement to the New York Times. .We apologize for this error and are working to update our systems to avoid mistakes like this going forward. We don’t want to put obstacles in the way of people doing a good thing..The so-called error comes as Facebook ramps up efforts to prevent users from profiting off of a protective and sanitizing products sold on the site, in particular medical masks, gloves, and hand sanitizer.   .On Facebook and other major platforms like Amazon, some sellers have used  the ongoing coronavirus pandemic in their efforts to price-gouge items, selling them for multiples beyond their average price..According to moderators of some of the groups interviewed by the New York Times, mixed messaging on masks may have contributed to Facebooks aggressive policy in removing content pertaining to DIY masks and other equipment. .The Centers for Disease Control (CDC) just recently reversed its stance on wearing masks after telling the US public not to buy personal protective gear..We support Facebook in their efforts in removing unethical sales” from their platform, Nicole Jochym, a student at Cooper Medical School of Rowan who was affected by the ban told the New York Times. .Soon, the Centers for Disease Control and Prevention (CDC) may advise all Americans to cover their faces when they leave the house, the Washington Post reported.  .The agency is weighing that recommendation after initially telling Americans that they didnt need to wear masks and that anything other than a high-grade N95 medical mask would do little to prevent infection any way. .Research on how well various types of masks and face coverings varies but, recently, and in light of the pandemic of COVID-19, experts are increasingly leaning toward the notion that something is better than nothing. .A University of Oxford study published on March 30 concluded that surgical masks are just as effective at preventing respiratory infections as N95 masks for doctors, nurses and other health care workers. .Its too early for their to be reliable data on how well they prevent infection with COVID-19, but the study found the thinner, cheaper masks do work in flu outbreaks. .The difference between surgical or face masks and N95 masks lies in the size of particles that can - and more importantly, cant - get though the materials. .N95 respirators are made of thick, tightly woven and molded material that fits tightly over the face and can stop 95 percent of all airborne particles, while surgical masks are thinner, fit more loosely, and more porous. .This makes surgical masks much more comfortable to breathe and work in, but less effective at stopping small particles from entering your mouth and nose. .Droplets of saliva and mucous from coughs and sneezes are very small, and viral particles themselves are particularly tiny - in fact, theyre about 20-times smaller than bacteria. .For this reason, a JAMA study published this month still contended that people without symptoms should not wear surgical masks, because there is not proof the gear will protect them from infection - although they may keep people who are coughing and sneezing from infecting others. .But the Oxford analysis of past studies - which has not yet been peer reviewed - found that surgical masks were worth wearing and didnt provide statistically less protection than N95 for health care workers around flu patients. .However, any face mask is only as good as other health and hygiene practices. Experts universally agree that theres simply no replacement for thorough, frequent hand-washing for preventing disease transmission. .Some think the masks may also help to train people not to touch their faces, while others argue that the unfamiliar garment will just make people do it more, actually raising infection risks.  .Homemade masks theoretically could offer some protection if the materials and fit were optimized, but this is uncertain, Dr Jeffrey Duchin, a Seattle health official told the Washington Post. .After a vacuum bag, kitchen towels were fairly protective, but uncomfortable. Masks made of T-shirts were very tolerable, but only worked a third as well as surgical mask. The Cambridge University researchers concluded that homemade masks should only be used as a last resort. .         A very, very angry man: Prince Harry had Olympic rows with William and poured out resentments about his father Charles to ex-girlfriend Cressida Bonas who found complaints about his family tiresome, bombshell new book claims       ",Facebook admits it made an 'error' after algorithm threatens to ban users who post content showing people how to make their own masks
174,"Facebook CEO Mark Zuckerberg says a bug that flagged Facebook posts with links to articles published by reputable news sites about the coronavirus as spam was caused by an error with the companys spam detection system..Many people complained on social media on Tuesday that they had received notifications that posts with links to articles by news publications went against Facebooks Community Standards..Finally, Zuckerberg clarifies yesterday issue. re: links being marked as spam had nothing to do w/ coronavirus or content moderation policies, says it was a technical error related to spam detection system.On Tuesday evening, Facebooks vice president of integrity, Guy Rosen, confirmed the bug on Twitter and said the company had restored all the posts that were unintentionally removed..We’ve restored all the posts that were incorrectly removed, which included posts on all topics - not just those related to COVID-19. This was an issue with an automated system that removes links to abusive websites, but incorrectly removed a lot of other posts too..Posts linking to articles published by BuzzFeed News, Politico, the Atlantic, and the Sydney Morning Herald about the coronavirus were all flagged..Earlier, Rosen rebuked speculation by the companys former security chief, Alex Stamos, that the error was a result of less human oversight of an anti-spam machine learning algorithm..Facebook told its workers Monday to work from home if possible. On Tuesday, the company told its contract moderators not to come into the office; however, they are barred from working from home due to privacy concerns..Facebook has previously launched a number of initiatives to combat misinformation and spam about the coronavirus, including allowing health authorities to flag fake news and showing users an information module when searching for information about the virus..A BuzzFeed News investigation, in partnership with the International Consortium of Investigative Journalists, based on thousands of documents the government didnt want you to see.",Facebook had an error in their spam detection system which caused news from reputable sources to be flagged
176,"China’s most popular messaging app, Tencent-owned WeChat, has been censoring keywords about coronavirus since as early as January 1st, an analysis found. Popular Chinese livestreaming platform YY has been censoring coronavirus content, too..To make this determination, the research group Citizen Lab scripted group chat conversations and sent them to three test WeChat accounts, two in Canada and one in China. The chat conversations consisted of article headlines and text. The group, which is affiliated with the University of Toronto, sent them from one of the Canadian WeChat accounts to the Chinese one, and observed which messages the Chinese account got. 132 keyword combinations were censored in January, but that number rose to 516 keywords by the second week of February..On YY, which is similar to Twitch or Mixer, 45 keywords were added to a blacklist on December 31st, 2019; five of those keywords were removed on February 10th, Citizen Lab found. YY’s blacklist is in the app itself, unlike WeChat’s, which uses a remote server for censorship..Public health officials from China first informed the World Health Organization about the virus at the end of December. The censorship has been going on since at least January 1st, and continued through the most intense part of the outbreak. WeChat has a monthly active user base of over one billion people — which means that a lot of users may have missed important information about the coronavirus, as well as how to prevent its spread. .Censored keywords included factual information on the disease, references to the government’s epidemic policies, and the name of Li Wenliang, a doctor who was among the earliest to warn the population about the disease. Li caught the disease while treating coronavirus patients and died on February 7th. His story created public outcry against the government’s handling of the coronavirus..It’s not clear why the two companies decided to censor keywords about coronavirus, though it’s possible they were ordered to do so by the Chinese government. WeChat has close ties with the Chinese government, and the government has already used WeChat and Twitter to track down people Chinese officials felt were sharing negative information about the coronavirus outbreak..The censorship is particularly pernicious because the WeChat is a crucial part of many Chinese people’s lives, David Jacobson, a professor of global business strategy at SMU’s Cox School of Business and a visiting professor at Tsinghua University in Beijing, told BuzzFeed News. “As a platform, you can live your life with it,” Jacobson said in an interview with BuzzFeed News. “You can pay for things. You can do so much more.”",This censorship caused important information about the coronavirus to not reach people
177,"BEIJING (Reuters) - A Chinese company says it has developed the country’s first facial recognition technology that can identify people when they are wearing a mask, as most are these days because of the coronavirus, and help in the fight against the disease..But the coronavirus, which emerged in Hubei province late last year, has resulted in almost everyone wearing a surgical mask outdoors in the hope of warding off the virus - posing a particular problem for surveillance..Now Hanwang Technology Ltd, which also goes by the English name Hanvon, said it has come up technology that can successfully recognize people even when they are wearing masks..“If connected to a temperature sensor, it can measure body temperature while identifying the person’s name, and then the system would process the result, say, if it detects a temperature over 38 degrees,” Hanwang Vice President Huang Lei told Reuters in an interview..The Beijing-based firm said a team of 20 staff used core technology developed over the past 10 years, a sample database of about 6 million unmasked faces and a much smaller database of masked faces, to develop the technology,.The team began work on the system in January, as the coronavirus outbreak gathered pace, and began rolling it out to the market after just a month..It sells two main types of products that use the technology. One performs “single channel” recognition that is best used at, for example, entrances to office buildings..“When wearing a mask, the recognition rate can reach about 95%, which can ensure that most people can be identified,” Huang said, adding the success rate for people without mask is about 99.5%..Using Hanwang’s technology, the ministry can cross-reference images with its own database of names and other information and then identify and track people as they move about, Huang said..The company has about 200 clients in Beijing using the technology, including the police, and expect scores more across 20 provinces to start installing it soon, Huang said..When it comes to other surveillance tools being used in the fight against the coronavirus, there has been some grumbling on social media but most people seem to be accepting extra intrusion, or even embracing it, as a means to deal with the health emergency..Although domestic customers have been driving Hanwang’s business, Huang also said he expected more foreign interest, as the virus spreads around the world and more people wear face masks.",Those use case increases surveillance and invades privacy
178,"The project is being run by the Office of Naval Research and has been described as an autonomous undersea weapon system according to a report by New Scientist..Details of the killer submersible were made available as part of the 2020 budget documents, which also revealed it has been named CLAWS by the US Navy..Very few details about the top secret project have been revealed beyond the fact it will use sensors and algorithms to carry out complex missions on its own. .Its expected CLAWS will be installed on the new Orca class robot submarines that have 12 torpedo tubes and are being developed for the Navy by Boeing..Over the next few years, the firm will design and test four Orca Extra Large Unmanned Undersea Vehicles (XLUUVs) based on its autonomous Echo Voyager (pictured), which can operate at sea for months at a time.The navy has not revealed what CLAWS stands for or commented on the story, the only information is what has been released to congress in the budget documents..Autonomous submarines already exist and they can complete tasks without humans being involved - however they arent very intelligent and have limited functionality..The new submarines will have much greater level of artificial intelligence and so be able to perform a wider range of functions without a human controller..CLAWS isnt new, it was first revealed in 2018 as part of a US Navy bid to improve the autonomy and survivability of large and extra-large unmanned underwater vehicles, according to New Scientist..When it was first revealed there was no mention then of weapons being on the autonomous submersible, only a need for it to have sensors and make decisions..The US Navy has ordered larger, longer range robot submarines called Orca and it was assumed they would be controlled remotely like the smaller versions..They are set to be armed with 12 torpedo tubes and with CLAWS they could be used to sink targets on their own without input from a human, New Scientist reports..Boeing has developed the Orca submarines for the Navy based on its Echo Voyager class of craft. It is assumed the CLAWS AI system will be added to Orca subs.This idea isnt to everyones liking. Stuart Russell from the University of California Berkeley told New Scientist it was a dangerous development..Equipping a fully autonomous vehicle with lethal weapons is a significant step, and one that risks accidental escalation in a way that does not apply to sea mines..The exact budget for CLAWS hasnt been revealed but it was allocated $26 million in this years US Navy budget  and another $23 million for next year..It will move from being an idea to a working prototype thanks to this extra funding and could be deployed on large robot submarines by 2022, says New Scientist. .‘The Orca will have well-defined interfaces for the potential of implementing cost-effective upgrades in future increments to leverage advances in technology and respond to threat changes, the Navy said..The organisation says that something must be done about this lack of accountability - and it is calling for a ban on the development and use of killer robots..Called Mind the Gap: The Lack of Accountability for Killer Robots, their report details the hurdles of allowing robots to kill without being controlled by humans..No accountability means no deterrence of future crimes, no retribution for victims, no social condemnation of the responsible party, said Bonnie Docherty, senior Arms Division researcher at the HRW and the reports lead author. .         How Harry turned to therapists from MI6 in his mental health battle, SENT BACK a birthday gift from Charles and had explosive rows with William: RICHARD KAY on new book that says so much about the Princes state of mind       ",Responsible AI must have human input
179,"Federal agencies have big contracts with Virginia-based Babel Street. Depending on where youve traveled, your movements may be in the companys data..With Locate X, investigators can pinpoint app-using phones that passed through one location and see where else those devices traveled, sources said..U.S. law enforcement agencies signed millions of dollars worth of contracts with a Virginia company after it rolled out a powerful tool that uses data from popular mobile apps to track the movement of peoples cell phones, according to federal contracting records and six people familiar with the software..The product, called Locate X and sold by Babel Street, allows investigators to draw a digital fence around an address or area, pinpoint mobile devices that were within that area, and see where else those devices have traveled, going back months, the sources told Protocol..They said the tool tracks the location of devices anonymously, using data that popular cell phone apps collect to enable features like mapping or targeted ads, or simply to sell it on to data brokers..Babel Street has kept Locate X a secret, not mentioning it in public-facing marketing materials and stipulating in federal contracts that even the existence of the data is confidential information. Locate X must be used for internal research purposes only, according to terms of use distributed to agencies, and law enforcement authorities are forbidden from using the technology as evidence — or mentioning it at all — in legal proceedings..Federal records show that U.S. Customs and Border Protection purchased Locate X, and the Secret Service and U.S. Immigration and Customs Enforcement also use the location-tracking technology, according to a former Babel Street employee. Numerous other government agencies have active contracts with Reston-based Babel Street, records show, but publicly available contract information does not specify whether other agencies besides CBP bought Locate X or other products and services offered by the company..None of the federal agencies, including CBP, would confirm whether they used the location-tracking software when contacted by Protocol. Babel Streets other products include an analytics tool it has widely marketed that sifts through streams of social media to chart sentiment about topics and brands..A former government official familiar with Locate X provided an example of how it could be used, referring to the aftermath of a car bombing or kidnapping. Investigators could draw what is known as a geo-fence around the site, identify mobile devices that were in the vicinity in the days before the attack, and see where else those devices had traveled in the days, weeks or months leading up to the attack, or where they traveled afterward..If you see a device that a month ago was in Saudi Arabia, then you know maybe Saudis were involved, this person said. Its a lead generator. You get a data point, and from there you use your other resources to figure out if its valid..A former Babel Street employee said the technology was deployed in a crackdown on credit card skimming, in which thieves install illegal card readers on gas station pumps, capturing customers card data to use or sell online. The Secret Service was the lead agency in those investigations, which, according to published reports, led to arrests and the seizure of devices..A spokesperson for the Secret Service declined to comment on its work with Babel Street, saying the agency does not reveal methods used to carry out missions. .While federal records show that CBP purchased Locate X and last year upgraded, paying for premium licenses, the records neither describe what Locate X does nor define the difference between a basic and premium license. A CBP spokesperson would not comment in detail about the use of the tool, but said the agency follows the law when deploying open-source information..Told of Protocols reporting on Babel Street, Sen. Ron Wyden, a Democrat from Oregon who has pushed for tougher privacy legislation, questioned whether uses of the technology might violate the Fourth Amendment ban on unreasonable searches..The Supreme Court, in the landmark case Carpenter v. United States, ruled in June 2018 that the government must obtain a search warrant to access cell-tower location data for individual phone accounts. The court recognized that the government needs a warrant to get someones location data, Wyden said. Now the government is using its checkbook to try to get around Carpenter. Americans wont stand for that kind of loophole when it comes to our Fourth Amendment rights..A spokesperson for Babel Street, Lacy Talton, declined to answer specific questions about the companys government sales or its Locate X technology, but said the firm handles data carefully to comply with both the law and internet terms of service. There is no indication Babel Street is doing anything illegal..  Sen. Ron Wyden said the U.S. Supreme Court has recognized that the government needs a warrant to get someones location data.             Photo: Sarah Silbiger via Getty Images .Although data content is freely available without restriction from thousands of vendors and suppliers, Babel Street employs a variety of measures to ensure appropriate use of the data, Talton said in a statement to Protocol. This is not required by most vendors but stems from Babel Streets ethos of proper data compliance. The company regularly ensures that the data accessed through its software is in compliance with ever-changing global privacy regulations, data use rights, and terms of service..The details of Babel Streets location-tracking technology and its contracts with the federal government have not been reported before. Last month, The Wall Street Journal reported that border and immigration agents were tracking the location of cell phones, and looking for activity in suspicious places near the border, after buying data from Venntel Inc. of Herndon, Virginia..Venntell is a subsidiary of location-based marketing company Gravy Analytics of Dulles, Virginia. Gravy Analytics has provided location data to Babel Street, according to former employees of both firms. .Taken together, the revelations suggest that the sale of personal location data from commercial firms to the government is more widespread and has been going on longer than previously known. The emergence of the technology comes amid growing, broader concern over the tracking of peoples movements, whether through facial recognition, their license plates or the phones in their pockets..While consumers enable location-based services on their cell phone apps, privacy advocates said people are generally unaware of how far their personal information could travel — and in particular that it could be piped to law enforcement..The sources who spoke to Protocol, who independently described the location-tracking technology, were three former Babel Street employees, a former government official with firsthand knowledge of the companys products, and two former employees of Gravy Analytics. They requested anonymity because the information is sensitive, and some feared retribution from employers for speaking to the media..A spokesperson for Gravy Analytics declined to comment on the companys relationship with Babel Street. She said Venntel is a wholly owned subsidiary of Gravy Analytics that supports public sector initiatives. .She pointed to the companys privacy policy on its Web site: We take consumer privacy seriously and ensure that our data platform remains fully transparent and compliant with industry and legal requirements, the policy reads. Gravy ensures that 100% of our data complies with all local privacy laws, including required consumer consent and opt-out provisions..While there is little public information about Locate X, government contracting records provide a picture of Babel Streets growth and increasing popularity in federal law enforcement circles. The company registered Locate X with the U.S. Patent and Trademark Office in May 2017, and sales to federal agencies shot up afterward — from $64,000 in fresh contracts in 2016 to more than $2.1 million in 2017 to nearly $5.3 million in 2018..Babel Streets sales spike was fueled in large part by four new customers: CBP, which signed $3.2 million in contracts, ICE ($1.1 million), the State Departments Bureau of Diplomatic Security ($710,000), and the Secret Services Criminal Investigations Division ($313,858), the records show..CBP signed a first contract worth $981,000 for Babel software in September 2017. The Targeting and Analysis Systems Directorate, the CBP branch that purchased the software, apparently liked what it received. A year later, the agency signed a fresh contract worth $2.2 million for Babel software licenses. In March 2019, CBP filed an amended contract, worth an extra $130,000, to upgrade the current Babel Street Locate X licensing from basic to premium licenses as well as add an additional 10 licenses..Asked about its use of Locate X, a CBP spokesperson told Protocol the agency uses a variety of tools that may include tools to facilitate access to open-source data relevant to its border security mission. All CBP operations in which open-source information may be used are undertaken in furtherance of CBPs responsibility to enforce U.S. law at the border and in accordance with relevant legal, policy and privacy requirements..In September 2018, ICE officials signed a one-year, $1.1 million contract with Babel Street. The deal included Locate X, according to a former Babel Street employee. Last August, ICE signed a fresh five-year deal worth up to $6.5 million with Babel Street for data subscription services, records show..A spokesperson for ICE said, We do not discuss specific law enforcement tactics or techniques, or discuss the existence or absence of specific law-enforcement-sensitive capabilities. She also said, referring to cell phone location data, ICE does not generally use this type of information for routine enforcement operations. .Other agencies with active Babel Street contracts include the Department of Justice, the U.S. Marshals Service, the Army, the Coast Guard, the Drug Enforcement Administration and the Department of Transportations Office of Intelligence, Security and Emergency Response. The contract records are from USAspending.gov, the official source for U.S. government spending. .A spokesperson for the Department of Transportation, which signed a yearlong contract with Babel Street last May, said the Office of Intelligence, Security and Emergency Response utilizes Babel Street software features depending on the nature of particular incidents. .Spokespeople for the Army, the Bureau of Diplomatic Security, the DEA and the Marshals Service declined to comment on the contracts with Babel Street. The Department of Justice and the Coast Guard did not respond to requests for comment..A spokesperson for a regional DEA office in El Paso, Texas, which signed a separate $12,978 contract for a one-year Babel Street software license last September, denied that the agency had purchased the location-tracking data tool..The technology was controversial enough that some agencies, including the FBI and the ATF, declined to purchase Locate X after those agencies lawyers nixed it, a former Babel Street employee said..A spokesperson for the FBI declined to comment. A spokesperson for the ATF, April Langwell, declined to comment on ATF procurement decisions. ATF always works within DOJ guidelines with regard to the investigative techniques that we use and ensure that they are consistent with federal law and subject to court approval, Langwell said..The former Babel Street employees and the former government official said Babel Street was careful about its clients for location data technology. For example, they said, it did not sell to commercial clients, local law enforcement agencies or foreign governments..The software included pop-ups that reminded users it was to be used only in the investigation of serious crimes and matters of national security, one former employee said. However, after users complained that the pop-ups were annoying, the company removed them, the employee said. Babel Street did not respond to emailed questions about the pop-ups..Despite the apparent power of the tool, Protocol could not find a single instance in which a federal agency had publicly described using Locate X, in an investigation or in any other capacity. And Babel Street appears to have taken a number of steps to keep the technology secret. The company advertises other products on its website and in press releases, but makes no mention of Locate X or the tracking of mobile devices..Locate Xs terms of use, spelled out in a single document published online by the General Services Administration, require government clients to agree that the product will be used for internal research purposes only. Locate X data may not be used as the basis for any legal process in any country, including as the basis for a warrant or subpoena, or any other legal or administrative action. The terms state that Locate X data may not be cited in any court/investigation-related document..  Terms of use for Babel Streets Locate X product state that the data may not be used as the basis for any legal process.Illustration: 615 Productions.Protocol shared the terms of use in the Locate X contract with Nathan Wessler, a lawyer with the ACLUs Speech, Privacy, and Technology Project who argued the Carpenter v. United States case before the Supreme Court. He called the secrecy provisions tremendously disturbing, raising the possibility that a criminal defendant might not know the tool had factored into a case — and therefore wouldnt be able to challenge its legality..These secrecy provisions prevent the courts from providing oversight, Wessler said. That is really corrosive to our system of checks and balances..In the past, Wessler noted, courts have been critical of nondisclosure agreements with law enforcement that are designed to protect sensitive surveillance technologies, notably in cases involving devices that mimic cell towers in order to capture phone information, often referred to by the brand name StingRays..Scores of U.S. law enforcement agencies deployed the devices for years in secret without judicial scrutiny or public transparency. When use of the technology began to be exposed in criminal trials, the courts did not take a favorable view of the secrecy agreements. One of the more pointed opinions came in a 2016 ruling by a Maryland state appeals court judge, involving Baltimore police and an attempted murder suspect..The use of a nondisclosure agreement to protect the technology is inimical to the constitutional principles we revere, Judge Andrea M. Leahy wrote for the three-member court panel. .In 2015, both the Department of Justice and Homeland Security updated their policies to require law enforcement to disclose the use of cell site simulator technologies to the courts when used as part of an investigation. In all circumstances, candor to the court is of paramount importance, the Homeland Security policy reads. Applications for the use of a cell site simulator must include sufficient information to ensure that the courts are aware that the technology may be used..One of the former Babel Street employees who spoke to Protocol cited another example of how Locate X could be used to protect U.S. national security. Investigators, this person said, could identify mobile devices carried near popular border crossing points into the U.S. and pull up the historical location data for those devices, viewing where theyve been in the preceding months..If you are thinking about attack planning, and you know these devices were just at a Hezbollah or ISIS training camp, and now theyre sitting in Juarez, maybe that matters, the former employee said..Still, privacy experts told of Protocols reporting on Locate X asserted that law enforcement officials practice of buying data they would otherwise need a warrant to access amounts to a form of data laundering. .That consumers can have data being collected that tracks their location, and the government, instead of getting a warrant, which they would normally need to do, can just go to a private company and buy it directly, thats hugely concerning, said Serge Egelman, a computer science professor at UC Berkeley who works on privacy issues..In the Supreme Courts Carpenter v. United States case, the court held that investigators violated the Fourth Amendment by obtaining cell tower records without a warrant that placed a robbery suspect near the crimes. Chief Justice John Roberts wrote, in the majority opinion, that authorities in that case had failed to contend with the seismic shifts in digital technology that made possible the tracking of not only Carpenters location but also everyone elses, not for a short period but for years and years..A spokesperson for Wyden said the senators aides had a phone call with Venntel attorneys on Feb. 20, in response to The Wall Street Journal article, to discuss the companys sale of location data to the government. A Wyden aide said Venntels counsel declined to answer most questions, would not identify the companys government clients, and would not reveal the source of the data..Babel Streets sale of location data to the government could also raise potential liability issues for app developers under the Stored Communications Act, said Wessler, the ACLU lawyer. The 1986 law prohibits providers of computing services or electronic communication to the public from knowingly divulging customer information to any government entity. .The question for the app companies themselves is whether, now that they know that Babel Street is taking their customers location data and providing it to law enforcement, are those companies themselves now liable under the Stored Communications Act, Wessler said..Location data culled from mobile apps is said to be anonymized, with each device masked behind a nameless ID number. But experts say data can be traced back to individual users, based on their particular movements..The New York Times reviewed a database of location data and reported in December 2018 that it was able to identify a woman as she traveled to her dermatologists office, hiked with her dog and stayed over at her ex-boyfriends home. Babel Street did not respond to an emailed question about whether Locate X data can be de-anonymized..Babel Street was founded in 2009 as Agincourt Solutions by former U.S. Navy Officer Jeff Chapman, and became Babel Street in 2014. On its website and in marketing materials, it describes itself as the worlds data-to-knowledge company, focusing on a service that analyzes streams of social media activity in multiple languages, often for brand management and sometimes linked to locations such as sports arenas..Early on, the promise of gleaning meaningful intelligence from Twitter feeds and other social media applications drew clients to Babel Street, according to government records, published reports and the former employees. The NFL has used Babel Streets analytics software. So, too, have at least 10 local law enforcement agencies around the country, according to the Brennan Center for Justice at New York University Law School..Motherboard and The Washington Post wrote about the companys social media analytics software in 2017, noting heavy interest from police agencies overseeing major events like Super Bowls. On the government side, the FBI and the Army were among Babel Streets early customers. Michael Flynn, who served briefly as President Trumps national security adviser and later pleaded guilty to lying to the FBI, was once an adviser to the firm, according to Flynns financial disclosure forms..Just before the rollout of Locate X, the company hired a veteran Department of Justice privacy lawyer, Jill Maze, to be the companys chief privacy officer, according to former employees and Mazes LinkedIn account..Subsequent hires suggest the company viewed location data as a growth area. In February 2019, Babel Street hired retired Maj. Gen. Mark Quantock, a former director of intelligence for U.S. Central Command, which includes the Middle East and Central Asia, and the former director of operations for the National Geospatial Intelligence Agency, essentially the governments headquarters for location data intelligence..Three months later, the company hired a 20-year Pentagon veteran, Dave Dillow, who since 2003 has worked with special operations forces focused on integrating publicly available information, or PAI, into the intelligence pipeline for those forces. Commercial location data is one type of PAI..The data used by Babel Street, said the former employees of Babel Street and Gravy Analytics, comes largely from third-party data aggregators who broker deals with mobile app developers, offering revenue in return and sometimes detailed analysis about how users are engaging with the app. Data aggregators who spoke to Protocol said they enable services like mapping and marketing, and comply with privacy regulations, which include requiring all app users to give their consent to sharing their data..Privacy advocates say such consumer opt-ins are often buried in small print or otherwise clouded in vague or bureaucratic language, and that users have little visibility into how their data is used..Thats the fundamental problem, said Egelman, the UC Berkeley professor. The trafficking in this data is totally opaque to everyone who isnt a party to these transactions..Lyft says its safety features are there to protect drivers. But the story of one man’s murder reveals how those features fall short in real life-and-death scenarios..In early December, while Philpotts was driving to make money to pay for his lifelong dream of going to veterinary school, someone stopped his car and tried to carjack him. Philpotts ran for safety, and soon, a text came through from Lyft: “It looks like you’ve been stopped for a while. Do you need help?”.Trusted Future is a non-profit organization dedicated to the belief that we need smarter, better-informed efforts to enhance trust in todays digital ecosystem in order to expand opportunities for tomorrow..This week, we’re commemorating Earth Day by playing Norco, an indie adventure game about an oil town that’s feeling the effects of global warming. We’re also checking out “Tokyo Vice,” based on the 2009 memoir of the same name, as well as an AI-based sci-fi book and an anime series that’s the best we’ve seen in years. .It’s a troublesome trend for startup founders. An engineer spends two or three years at a company before getting an outside job offer that would double their salary. It leaves their current employer with a dilemma: Beat the offer — pushing that engineer’s salary far ahead of their colleagues’ — or lose more talent..Tech salaries are increasing faster than many companies can keep up with, and that market is threatening startups’ efforts to pay employees in high-demand job areas — engineering, product, design and operations — within consistent salary bands. Ensuring pay equity is already a challenge for high-growth startups, and these pressures are making it even harder..Silicon Valley’s favorite climate solution is still decades away from being deployed at scale, if it will ever be ready. Yet the moment carbon dioxide removal is having right now is really something to behold..Over the past few weeks, the Intergovernmental Panel on Climate Change offered up an extensive chapter on sucking carbon from the sky, and Big Tech and venture capital committed more than $1.2 billion to it in two separate announcements last week. “Theres never been a better time to start a carbon removal company!” Ryan Orbuch, Lowercarbon Capital’s carbon removal lead, tweeted while announcing the VC firm’s $350 million fund.",Locate X allows law enforcement to draw digital fence around areas and pinpoint mobile devices used there
180,"Clearview AI, which scans the internet for photos for its massive facial recognition database, is reportedly working on a surveillance camera.According to a report by BuzzFeed News, Clearview AI is developing the camera through a sister operation called Insight Camera. The company is looking to create a product that can offer live, real-time facial recognition. .BuzzFeed found the connection between the two companies after noticing a “security_camera” app in the code on Clearview’s web app. After the news outlet reached out to the company, Insight Camera’s website was taken down. The two companies did not mention their relationship on either website. However, similar code accessing Clearview’s servers were found on Insight Camera’s website. .Two organizations that were beta testing the camera have been identified: the United Federation of Teachers (UFT) and New York City-based real estate company Rudin Management. .According to a UFT spokesperson speaking to BuzzFeed, it used Insight Camera to help its security team identify unauthorized individuals from entering its offices. UFT says the Insight Camera created its own on-site local database and the organization did not have access to Clearview’s larger database. Rudin Management said it no longer uses Insight Camera..Last week, Gizmodo discovered links between Clearview and wearable technology company Vuzix. An earlier report from the New York Times highlighted Clearview’s interest in augmented reality glasses. BuzzFeed was able to verify the relationship after looking through Clearview search data connected to Vuzix-related accounts. The company later confirmed it sent its smart glasses to Clearview for testing..Account data connected to another augmented reality wearable company, RealWear, were also uncovered by BuzzFeed. However, RealWear claims it has no working relationship with Clearview aside from selling a few devices to the company a year ago..Clearview AI was thrust into the spotlight after a New York Times report detailed how the company was scraping social media and the rest of the internet for public photos in order to create a massive facial recognition database. Clearview has collected billions of photos and provided its technology to law enforcement as well as private companies..Facebook, Twitter, Google, and YouTube have all issued cease-and-desist letters demanding Clearview stop scraping its platforms for data. Clearview AI CEO Hoan Ton-That claims, however, that his company’s methods are protected by the First Amendment. .Just last week, Apple disabled Clearview AI’s app for violating its Enterprise Developer Program rules. A recent report from The Daily Beast also discovered that Clearview AI’s client list was hacked. ","Clearview AI has already had many errors, privacy issues, and racial bias. Furthermore, the company's data storage and security protocols are untested and unregulated"
181,"A small company called Banjo is bringing pervasive AI surveillance to law enforcement throughout Utah, Motherboard reports. In July, Banjo signed a five-year, $20.7 million contract with Utah. The agreement gives the company real-time access to state traffic cameras, CCTV and public safety cameras, 911 emergency systems, location data for state-owned vehicles and more. In exchange, Banjo promises to alert law enforcement to anomalies, aka crimes, but the arrangement raises all kinds of red flags..Banjo relies on info scraped from social media, satellite imaging data and the real-time info from law enforcement. Banjo claims its Live Time Intelligence AI can identify crimes -- everything from kidnappings to shootings and opioid events -- as they happen..Banjo presents many of the same concerns that similar companies have encountered. One of the strongest arguments against surveillance practices by Clearview AI has been that the companys data storage and security protocols were untested and unregulated. As Clearview AI proved earlier this month, that can lead to massive data leaks. According to Motherboard, Banjo says it removes all personal data from its system, but how and how well it does this remain unclear..There are also concerns about the lack of public oversight and rampant potential for abuse. Plus, facial recognition is still biased. Its not as good at interpreting black and brown faces as it is at interpreting white faces, and for that reason, some law enforcement companies have opted not to use it..The contract between Banjo and Utah says the technology will be deployed in all of Utahs 29 counties, its 13 largest cities and 10 other cities with significant relevance, as well as the University of Utah. So far, Motherboard hasnt been able to track down a single case that Banjos technology helped on, but that doesnt meant the tech hasnt been put to use.",There is a lack of public oversight and rampant potential for abuse. The facial recognition is also biased.
182,"In a short, polished video introducing his new company, attorney Alex Alvarez tells the story of a case he didn’t expect to lose. It was a straightforward slip-and-fall lawsuit pitting him against a less experienced attorney, and Alvarez was riding high off a series of “multi-million dollar” verdicts. He was shocked when this one didn’t go his way..“If the reason I win or lose, or any lawyer wins or loses, is based on his skill level, then why did this happen?” Alvarez says in the clip. “And that started me on a quest to find out why jurors decide cases and how juries are deciding cases in America.”.Thus was born Momus Analytics, named after the Greek god of blame, criticism, and mockery—and part of a new and controversial breed of legal tech companies. Drawing on big data and machine learning techniques similar to those marketers use to determine whether to send you an ad for an SUV or a bicycle, these startups are offering attorneys unprecedented insight into jurors’ lives..Some, like Momus, are also making unprecedented promises about the ability of that information to predict how a juror will lean in the deliberation room..Lawyers, jury consultants, and legal technology researchers who reviewed Momus’ pending patent application) at Motherboard’s request warned that the company may be founded on myth in more than just name. Rather than delivering superhuman insight, the experts said, Momus appears to be drawing assumptions from demographic information that has no reliable correlation to juror disposition—and some of the data it relies on may violate the constitutional prohibition against excluding jurors based on race or sex..“Lawyers are not allowed to pick jurors based on race or gender, that’s a constitutional mandate,” American University Professor Andrew Ferguson, who has researched the use of big data for jury selection, told Motherboard. “The idea that the algorithm is going to weight race or gender or other protected classes in a way that could be outcome determinative—that’s a problem, because then you’re kind of tech-washing your racialized assumption of individuals.”.Momus begins by scraping public records and jurors’ social media posts. It then feeds the collected data into algorithms that determine scores for “leadership,” “social responsibility,” and “personal responsibility.” The company’s patent application lists several characteristics Momus relies on to reach those conclusions. Among them: people of Asian, Central American, or South American descent are more likely to be leaders, while people who describe their race as “other” are less likely to be leaders..From there, the software spits out recommendations. “It will tell you who your best juror is, who your worst juror is, and all parts in between,” Alvarez says in another promotional video..For many lawyers, the prospect is incredibly appealing. Voir dire can often seem like a mystic art, passed down through generations and often riddled with superstitions and ingrained biases. In some cases, an attorney may receive a list of the day’s jury pool, several hundred names long, only 24 hours in advance. With a limited number of questions, and an even smaller number of peremptory strikes to eliminate jurors without cause, the attorney must do their best to ensure that anyone predisposed against their client doesn’t end up on the jury..For all but the wealthiest clients, who can afford jury consultants, doing actual research on jurors is all but impossible before most trials..Other emerging companies are making more moderate promises, compared to Momus, and attempting to filter race out of their algorithms. The growing industry promises to help lawyers better serve their clients, but also raises questions about the balance between human and machine judgement in our justice system..Racial discrimination in jury selection—such as outright bans on non-white jurors—has been illegal in the U.S. since the Civil Rights Act of 1875, although that law did little to change the practices in many jurisdictions. In 1986’s Batson v. Kentucky, the Supreme court furthered that protection, ruling that it is unconstitutional to eliminate a juror with a peremptory challenge based on their race. But studies of decades of post- Batson cases show the decision failed to fully eliminate the practice..Some counties with large African American populations have seen more than 80 percent of black jurors struck during selection in death penalty cases, resulting in all-white juries half of the time and juries with only one black member the remainder of the time, according to a report from the Equal Justice Institute. A separate study of the trials of 173 inmates who were on death row in North Carolina post- Batson found that black jurors were 2 ½ times more likely to be struck than non-black jurors..Some researchers, including Ferguson, have suggested that in certain applications, big data could lead to more representative juries, but there are tradeoffs..“There’s reasonable arguments out there that using big data—publicly available info on jurors—will prevent over-relying on only characteristics of jurors that we can see, including protected characteristics like race,” Drew Simshaw, a Gonzaga University professor who studies artificial intelligence and legal technology, told Motherboard. “But we don’t know if the data that’s being used is relying on data that reflects inequality, prejudice, and discrimination in society. The proprietary nature of the services, the lack of transparency, and this black box issue present challenges.”.When Larry Eger was a new attorney—”and I thought that I was maybe more attractive than I actually was”—he had a tendency to favor young women during jury selection, thinking they might be more amenable to him as an advocate for his client. With experience, he found he fared better with grandmothers..Many trial lawyers have similar stories that inform their particular strategies for jury selection. “We have our own stereotypes in our head when we’re selecting a juror and when we look at the jury panel,” Eger, the elected public defender for Florida’s 12th circuit, told Motherboard. “And as a defense attorney, I’m going to guess that most of us have a certain prejudice that if you have a black juror he’s going to be more sympathetic, but so many other factors come into play.”.For most of the history of jury trials, lawyers had little to base their decisions on apart from what they could observe visually and discern from a limited set of questions. Egers described the algorithmic ranking of jury pools as “frightening,” but didn’t deny the allure of tools that quickly and automatically gather large amounts of data on jurors—especially if an opposing prosecutor is using them. “When your ox is being gored, you want to take advantage and do whatever you can,” he said..Among the companies selling big data insights to lawyers are Vijilent and Voltaire. Both scrape the public records and public social media profiles of jurors and run them through versions of IBM Watson’s Personality Insights tool, which uses natural language processing algorithms to categorize the jurors’ within the “big five” personality traits model (openness to experience, conscientiousness, extraversion and introversion, agreeableness, and neuroticism)..Given how difficult it is to collect data on juries, the research into the correlation between personality traits and jury decisions is extremely limited. But at least one study has found a slight relationship between jurors with higher levels of extraversion and conscientiousness and not-guilty verdicts in criminal trials..“Our goal is to use AI and [machine learning] tools in data processing,” Basit Mustafa, the founder of Voltaire, told Motherboard. “In terms of actually selecting the bias [a juror might display], we want to actually leave that more up to the human.”.A client might ask Voltaire to run a search on a juror and flag any posts that contain keywords like “gun,” “food poisoning,” or “cancer,” depending on the type of case. The company would send back a report including the jurors’ big five personality assessment, a list of posts with flagged keywords, and other insights—for example, are they friends with doctors, and might therefore enter a medical malpractice case with a particular bias?.The company doesn’t honor requests to search for keywords related to protected classes like race or sex, Mustafa said, and it specifically turns down cases that are likely to stray into those areas, like housing discrimination..Momus Analytics’ publicly provides no details about how it takes the next step and determines the best and worst jurors for a case. The actual software appears to have been programmed by Frogslayer, a Texas company that creates custom software..In its patent, Momus describes a machine learning model trained to rank jurors, but does not explain what data it used to train such a model. It does, however, provide lists of the kind of data the model considers determinative of a juror’s leadership qualities and bias toward personal or social responsibility..In addition to race, the factors that Momus says indicate a propensity for leadership include: “alliance with negative social issues,” “US postal service as a profession,” “having a master’s level or higher of education,” “affiliated with the Democratic Party,” and “supporter of the NRA or hunting.” Traits indicative of non-leaders include: “profession as a boat captain or airplane pilot,” “profession in social work,” and having any level of education less than a master’s degree..“I disagree that those are accurate and reliable predictors of whether somebody would be a leader or have a high level of personal or social responsibility,” Leslie Ellis, the current president of the American Society of Trial Consultants, told Motherboard. “In my experience of talking to thousands of mock jurors, post trial interviews, and having done a good amount of academic research on jurors, I would not rely on these characteristics to be predictive of either of those traits.”.“What we have seen in all sorts of different types of jury research … is that those types of easily quantifiable characteristics are very often not what’s actually predicting or correlating with the verdict. More often, they act as proxies for other things,” she added..The company launched last year and its software is currently only available to a select group of plaintiff’s attorneys, but it claims the methodology behind the program has led to verdicts worth more than $940 million since 2008. Motherboard reached out to six attorneys that appear to be responsible for some of the verdicts Momus claims as success stories. None of them responded to calls or emails..The National Law Journal recently named Momus one of its 2020 emerging legal technologies and the National Judicial College invited Alvarez, the company’s founder, to speak at a judges’ symposium on artificial intelligence and the law..Both Vijilent and Voltaire have been tempted at times to move further toward the kind of predictive analytics Momus advertises. Rosanna Garcia, the founder of Vijilent, told Motherboard that she initially considered offering predictions about jurors’ to clients but found the level of interest didn’t justify the difficulty of training the models. “You would have to take data insights from 100-plus jury outcomes in order to be able to train your model” and that data is hard to come by, she said..About a year ago, Mustafa turned down an offer from an IBM salesperson to incorporate Watson’s Tone Analyzer feature into Voltaire for a small additional licensing cost. Tone Analyzer employs ”sentiment analysis,” a field of natural language processing that analyzes the tone and emotion with which people write about certain subjects. The technique is ubiquitous in digital marketing—although research has found that certain applications tend to discriminate against groups like African Americans in their analysis..“I looked at the data and thought, ‘that’s just flat out wrong,’” Mustafa said. “That’s really scary to me, as someone who’s trying to sell insight to lawyers. I don’t think that people come down to scores, flat out. I don’t think you can do that.”.By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.","Momus Analytics' predictive scoring system is using race to grade potential jurors on vague qualities like ""leadership"" and ""personal responsibility."""
183,"Some of California’s largest police departments have been collecting millions of images of drivers’ license plates and sharing them with entities around the country—without having necessary security policies in place, in violation of state law, according to a newly released state audit..The audit, published Thursday, found that 230 police and sheriff’s departments in the state currently use automated license plate readers (ALPRs), which can be fixed cameras or devices mounted on patrol cars. Police have touted the technology as necessary for enforcing parking and basic municipal laws, and as a vital tool in child abduction cases and other high-profile investigations..The Los Angeles Police Department, for example, has collected more than 320 million images over the last several years. Only 400,000 of those generated immediate matches to cars of interest, but the remaining 99.9 percent of the images, which can be used to track peoples’ movement across the city, stay stored in a department database for more than five years, according to the audit..The LAPD then adds other sensitive information to that database, sometimes tagging the photos with criminal records, names, addresses, and dates of birth. Meanwhile, the department has not established any written policy governing proper use of its ALPR data, in violation of a 2016 state law..Citing a case in Georgia in which a police officer took a bribe to look up a woman’s license plate to determine if she was an undercover officer, the auditors also determined that many of the departments it examined were not ensuring that only authorized personnel had access to ALPR data, or auditing the database logs to make sure that authorized personnel were using the systems properly..“This is very troubling. This technology reportedly exists to help with parking enforcement and other basic law enforcement responsibilities, and yet we’re seeing a huge amount of data collected, retained, and shared unnecessarily,” state Sen. Scott Wiener, who requested the audit, told Motherboard..“The LAPD will perform an assessment of the systems data security features and retention periods for ALPR images to evaluate the need for adjustment, prior to publishing of the ALPR policy,” the department wrote. “Furthermore, the policy will list the entities the department shares ALPR images with and the process for handling image-sharing requests.”.During testimony before the state legislature in August, though, the LAPD lieutenant who oversees the department’s license plate reader program stated, “We continue to ensure that we abide by both the laws that are in place,” directly contradicting what the audit would ultimately find..Among the most concerning revelations in the audit, privacy advocates said, was the apparent carelessness with which police departments shared the information in their ALPR databases..In addition to the LAPD, the auditors examined three other agencies in detail: the Fresno Police Department, Marin County Sheriff’s Office, and Sacramento County Sheriff’s Office. The Sacramento sheriff’s office shared its data with 1,119 entities, Fresno with 982, and Marin with 554 around the country. LAPD shared data with 58 departments in California..The auditors found Sacramento, Fresno, and Marin had apparently taken minimal steps to determine why the entities requesting access to the license plate data needed it, or even if they were public agencies at all, which is a requirement under the state law..Some of the entities on the share lists were identified only by initials, according to the audit. And the three California departments were all sharing data with an entity listed as the Missouri Police Chiefs Association, which is a private advocacy group, not a law enforcement agency. Vigilant Solutions, the company that provided ALPR technology to those departments later told auditors that the Missouri Police Chiefs Association was actually the Missouri State Highway Patrol, but the California departments had apparently not noted the difference..Sacramento, Fresno, and Marin were also sharing data with the Honolulu Police Department, which is separated from California by roughly 2,500 miles of ocean, raising questions about why Honolulu police need to know the every movement of California drivers..“They’re sharing this data indiscriminately across the country without even thinking or doing the justification of why they’re sharing with these agencies,” Dave Maass, a senior investigative researcher for the Electronic Frontier Foundation who has been tracking police use of ALPR technology, told Motherboard..In its response to the audit, the Marin County Sheriff’s Office defendended its sharing policies, including with Honolulu, saying they were “done properly and with consideration as to the multiple matters which have in the past involved both agencies.”.Maass said the California audit should serve as a larger warning. ALPR technology is widespread not only among police departments, but among private entities who use tools built by companies like Vigilant..“This is the exact same thing going on across the United States,” Maass said. “Every other state should take a look at this audit and consider whether they should do an audit.”.By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.",The use of automated license plate readers collected data on innocent people and car movements which was shared
184,"In about a half hour, a high schooler created a website and Twitter account for an entirely-fictional Congressional candidate named Andrew Walz. Shortly thereafter, Twitter verified the made-up politician..The student told CNN Business that he was bored and, after reading about misinformation, wanted to kick the tires of Twitter’s verification system. It’s a fun experiment — with a disturbing result that illustrates just how easy it is to manipulate social media..The student, who remained unnamed in CNN‘s article, said that he never had to provide any identification to get Walz verified, or even show evidence that he was running for any sort of office. The experiment exposes a glaring flaw in Twitter’s security and verification process — and suggests that the company hasn’t done nearly enough to protect election security..“I had no malicious intent. I just wanted to simply test to see whether this could happen,” the student told CNN. “So in case someone with bad intentions wanted to do this, Twitter now knows and can take steps to fix this.”",This instance contributes to the rise of misinformation
185,"The social media conversation over the climate crisis is being reshaped by an army of automated Twitter bots, with a new analysis finding that a quarter of all tweets about climate on an average day are produced by bots, the Guardian can reveal..The stunning levels of Twitter bot activity on topics related to global heating and the climate crisis is distorting the online discourse to include far more climate science denialism than it would otherwise..An analysis of millions of tweets from around the period when Donald Trump announced the US would withdraw from the Paris climate agreement found that bots tended to applaud the president for his actions and spread misinformation about the science..The study of Twitter bots and climate was undertaken by Brown University and has yet to be published. Bots are a type of software that can be directed to autonomously tweet, retweet, like or direct message on Twitter, under the guise of a human-fronted account..“These findings suggest a substantial impact of mechanized bots in amplifying denialist messages about climate change, including support for Trump’s withdrawal from the Paris agreement,” states the draft study, seen by the Guardian..On an average day during the period studied, 25% of all tweets about the climate crisis came from bots. This proportion was higher in certain topics – bots were responsible for 38% of tweets about “fake science” and 28% of all tweets about the petroleum giant Exxon..Conversely, tweets that could be categorized as online activism to support action on the climate crisis featured very few bots, at about 5% prevalence. The findings “suggest that bots are not just prevalent, but disproportionately so in topics that were supportive of Trump’s announcement or skeptical of climate science and action”, the analysis states..Thomas Marlow, a PhD candidate at Brown who led the study, said the research came about as he and his colleagues are “always kind of wondering why there’s persistent levels of denial about something that the science is more or less settled on”..The researchers examined 6.5m tweets posted in the days leading up to and the month after Trump announced the US exit from the Paris accords on 1 June 2017. The tweets were sorted into topic category, with an Indiana University tool called Botometer used to estimate the probability the user behind the tweet is a bot..Marlow said he was surprised that bots were responsible for a quarter of climate tweets on an average day. “I was like, ‘Wow that seems really high,’” he said. .The consistent drumbeat of bot activity around climate topics is highlighted by the day of Trump’s announcement, when a huge spike in general interest in the topic saw the bot proportion drop by about half to 13%. Tweets by suspected bots did increase from hundreds a day to more than 25,000 a day during the days around the announcement but it wasn’t enough to prevent a fall in proportional share..Trump has consistently spread misinformation about the climate crisis, most famously calling it “bullshit” and a “hoax”, although more recently the US president has said he accepts the science that the world is heating up. Nevertheless, his administration has dismantled any major policy aimed at cutting planet-warming gases, including car emissions standards and restrictions on coal-fired power plants..The Brown University study wasn’t able to identify any individuals or groups behind the battalion of Twitter bots, nor ascertain the level of influence they have had around the often fraught climate debate..This was followed by a tweet that doubted the world will reach a 9-billion population due to “#climatechange lunacy stopping progress”. The account has nearly 16,000 followers..Twitter accounts spreading falsehoods about the climate crisis are also able to use the promoted tweets option available to those willing to pay for extra visibility. Twitter bans a number of things from its promoted tweets, including political content and tobacco advertising, but allows any sort of content, true or otherwise, on the climate crisis..A Twitter spokesperson disputed the accuracy of the Brown University research. “Non-peer reviewed research using our public API can often be deeply flawed,” the spokesperson said, adding that “sweeping assessments” of users based on signals such as location and tweet content are routinely made by outside groups..“To be clear - none of these indicators are sufficient to determine if something is a bot. Looking for accounts that look similar to those disclosed as part of our archives is an equally flawed approach, given many of the bad actors mimic legitimate accounts to appear credible. This approach also often wrongly captures legitimate voices who share a particular political viewpoint that one disagrees with.”.Research on internet blogs published last year found that climate misinformation is often spread due to readers’ perception of how widely this opinion is shared by other readers..Stephan Lewandowsky, an academic at the University of Bristol who co-authored the research, said he was “not at all surprised” at the Brown University study due to his own interactions with climate-related messages on Twitter..“More often than not, they turn out to have all the fingerprints of bots,” he said. “The more denialist trolls are out there, the more likely people will think that there is a diversity of opinion and hence will weaken their support for climate science..John Cook, an Australian cognitive scientist and co-author with Lewandowsky, said that bots are “dangerous and potentially influential”, with evidence showing that when people are exposed to facts and misinformation they are often left misled..“This is one of the most insidious and dangerous elements of misinformation spread by bots – not just that misinformation is convincing to people but that just the mere existence of misinformation in social networks can cause people to trust accurate information less or disengage from the facts,” Cook said..Although Twitter bots didn’t ramp up significantly around the Paris withdrawal announcement, some advocates of action to tackle the climate crisis are wary of a spike in activity around the US presidential election later this year..“Even though we don’t know who they are, or their exact motives, it seems self-evident that Trump thrives on the positive reinforcement he receives from these bots and their makers,” said Ed Maibach, an expert in climate communication at George Mason University.",Brown University study found the substantial impact of bots amplifying denialist messages which increases misinformation
186,"A police investigator in Spain is trying to solve a crime, but she only has an image of a suspect’s face, caught by a nearby security camera. European police have long had access to fingerprint and DNA databases throughout the 27 countries of the European Union and, in certain cases, the United States. But soon, that investigator may be able to also search a network of police face databases spanning the whole of Europe and the U.S..According to leaked internal European Union documents, the EU could soon be creating a network of national police facial recognition databases. A report drawn up by the national police forces of 10 EU member states, led by Austria, calls for the introduction of EU legislation to introduce and interconnect such databases in every member state. The report, which The Intercept obtained from a European official who is concerned about the network’s development, was circulated among EU and national officials in November 2019. If previous data-sharing arrangements are a guide, the new facial recognition network will likely be connected to similar databases in the U.S., creating what privacy researchers are calling a massive transatlantic consolidation of biometric data..The report was produced as part of discussions on expanding the Prüm system, an EU-wide initiative connecting DNA, fingerprint, and vehicle registration databases for mutual searching. A similar system exists between the U.S. and any country that is part of the Visa Waiver Program, which includes the majority of EU countries; bilateral agreements allow U.S. and European agencies to access one another’s fingerprint and DNA databases..Although new legislation following the report’s recommendation is not yet on the table, preparatory work is ongoing. Information provided by the European Commission to the European Parliament last November shows that almost 700,000 euros (about $750,000) are going to a study by consultancy firm Deloitte on possible changes to the Prüm system, with one part of the work looking at facial recognition technology. The European Commission has also, separately, paid 500,000 euros to a consortium of public agencies led by the Estonian Forensic Science Institute to “map the current situation of facial recognition in criminal investigations in all EU Member States,” with the aim of moving “towards the possible exchange of facial data,” according to a project presentation sent to national representatives in Brussels..“This is concerning on a national level and on a European level, especially as some EU countries veer towards more authoritarian governments,” said Edin Omanovic, advocacy director for Privacy International. Omanovic worries about a pan-European face database being used for “politically motivated surveillance” and not just standard police work. The possibility of pervasive, unjustified, or illegal surveillance is one of many critiques of facial recognition technology. Another is that it is notoriously inaccurate, particularly for people of color..“Without the transparency and legal safeguards for facial recognition technology to be lawful,” said Omanovic, “there should be a moratorium on it.”.The EU has taken big steps to connect a host of migration and security databases in recent years. New legislation passed last April established a database that will hold the fingerprints, facial images, and other personal data of up to 300 million non-EU nationals, merging data from five separate systems. According to the report by 10 police forces, Deloitte consultants proposed doing the same with police facial images, but the idea was met with unanimous opposition from law enforcement officials..Nonetheless, the report recommends linking all of EU member states’ facial databases, which would seem to have the same practical effect. In another internal EU police report — this one from a working group on Prüm that looked at the exchange of drivers’ license data — police note that “a network of interconnected national registers can be regarded as a virtual European register.”.To the police, the advantages of interlinked facial recognition databases are clear. The Austria-led report views the technology as a “highly suitable” biometric tool for identifying unknown suspects and suggests that the databases should be created and linked “as quickly as possible.” It also recognizes the need for data protection safeguards, such as human verification of any automated matches, but privacy experts argue that the creation of any such system is the first step toward greater sharing and linking of data where such controls are inadequate..European moves to consolidate police facial recognition data closely resembles similar efforts in the U.S., said Neema Singh Guliani, senior legislative counsel at the American Civil Liberties Union. Many U.S. law enforcement agencies work out of “fusion centers,” where they are co-located and able to share data. If you have an information-sharing agreement with the FBI or the Department of Homeland Security, said Guliani, “there’s a risk that functionally the information may be shared with additional levels of U.S. law enforcement.”.“It raises many questions,” she added. “How police are using facial recognition and gathering images, as well as in the U.S. with regard to due process and First Amendment expression. Given existing information sharing relationships, it’s very likely that the U.S. would want access to that information.”.As far back as 2004, the U.S. Embassy in Brussels was calling for a relationship with the EU that allowed “expansive exchanges and sharing all forms of data, including personal data.” In recent years, efforts toward that goal have intensified. According to a Government Accountability Office report, in 2015, the Department of Homeland Security began demanding the implementation of the data-sharing agreements required of Visa Waiver Program countries. This has included the FBI providing assistance to other states to set up the necessary computer networks..Austria, to take one example, began checking fingerprints against the FBI’s criminal fingerprint databases in October 2017, explained Reinhard Schmid, a senior official in the Austrian criminal intelligence service. Since then, about 12,000 individuals’ prints have been cross-checked, leading to 150 matches. “Around 20 of these identified persons were under investigation and suspected of membership of terrorist organizations,” while in 56 cases individuals had attempted to use a false identity, said Schmid..“Their logic here is, ‘When I have a serious crime and I want to run someone’s photo against a database, why shouldn’t I have this?’” said Guliani. Yet, she added, the privacy implications were enormous. “Once you have the access, you ultimately have the ability to identify almost anyone, anywhere.”.The report by 10 police forces calls for Europol, the EU agency for police information and intelligence sharing, to play a role in exchanging facial recognition and other biometric data with non-EU states. This echoes recommendations from European governments themselves: A July 2018 declaration called for the commission to consider “broadening the scope” of the Prüm network and for Europol to take the lead on data sharing with third countries..The FBI and Europol did not respond to questions about data-sharing agreements between the EU and the U.S. A spokesperson for the European Commission acknowledged the prospect of adding facial recognition data to the Prüm network, but declined to go into more detail.",There is a secretive nature to this plan as well as could amplify racial biases
187,"Seven years ago, Allstate Corporation told Maryland regulators it was time to update its auto insurance rates. The insurer said its new, sophisticated risk analysis showed it was charging nearly all of its 93,000 Maryland customers outdated premiums. Some of the old rates were off by miles. One 36-year-old man from Prince George’s County, Md., who Allstate said in public records should have been paying $3,750 every six months, was instead being charged twice that, more than $7,500. Other customers were paying hundreds or thousands of dollars less than they should have been, based on Allstate’s new calculation of the risk that they would file a claim..Rather than apply the new rates all at once, Allstate asked the Maryland Insurance Administration for permission to run each policy through an advanced algorithm containing dozens of variables that would adjust it in the general direction of the new risk model. Allstate said the goal of this new customer “retention model,” which it was rolling out across the country, was to limit policy cancellations from sticker shock. After questions from regulators, the insurer submitted thousands of pages of documentation on the price changes—including data showing how they would affect each individual customer, a rare public window into details of its auto insurance pricing that have otherwise been kept behind a wall of privacy, labeled a trade secret. .When The Markup and Consumer Reports conducted a statistical analysis of the Maryland documents, we found that, despite the purported complexity of Allstate’s price-adjustment algorithm, it was actually simple: It resulted in a suckers list of Maryland customers who were big spenders and would squeeze more money out of them than others.    .Customers who were already paying the highest premiums, of about $1,900 or more every six months, and were due an increase would have borne price hikes of up to 20 percent. But drivers with cheaper policies, who deserved price jumps that were just as big, would be charged a maximum increase of only 5 percent. Customers in the 20 percent group were more likely to be middle-aged..We also found Allstate’s algorithm would have denied meaningful decreases to thousands of Allstate customers who the company’s new risk profile showed were paying too much. That 36-year-old from Prince George’s County would not have saved $3,772 on his policy as he deserved, documents show, but rather gotten a measly discount of $26. Discounts were capped at a half percent across the board..Maryland ultimately rejected the plan, calling it discriminatory, and it never went into effect there. However, the insurer has continued to propose plans with a customer “retention model” in other states. Some have been approved and are actively being used..Allstate declined to answer any of our detailed questions and did not raise any specific issues with our statistical analysis, which we provided to the company in November, including the code used to calculate our findings..By parsing a rare document containing fine details of the company’s auto insurance pricing, we found stark differences in how high proposed price hikes were allowed to rise.“Our rating plans comply with state laws and regulations,” read a short statement emailed by spokesperson Shaundra Turner Jones. The Maryland proposal, the statement said, aimed to “minimize customer disruption and provide competitive prices.” .In a later email, she added that our reporting on the Maryland filing is “inaccurate and misleading” because it is “based on a rating plan that was never used.”.Allstate’s Maryland filing reveals how an opaque algorithm it has been proposing around the country would have functioned in practice. It also offers a glimpse into a potential future where companies of all sorts, not just auto insurers, charge people different prices based on their behavior—or expected willingness to pay, as projected by algorithms that draw on the seemingly limitless troves of data collected and sold about people every day. .In this case, Allstate’s model seemed to determine how much a customer was willing to pay —or overpay—without defecting, based on how much he or she was already forking out for car insurance. And the harm would not have been equally distributed..In Maryland, seniors were overrepresented among those customers who were owed discounts but would not have gotten them. Allstate proposed giving those Maryland customers over the age of 62 a median discount of $1.64, far less than many deserved, according to its new risk calculations. .“That they wouldn’t have gotten these discounts would have been devastating,” said Deni Taveras, a councilmember in Prince George’s County,  where Allstate determined that policyholders owed discounts were being overcharged by $265 on average, but proposed dropping their rates by pennies, for an average discount of $2.63. .“My district is highly dependent on social services, pensions and food pantries,” she said. Those hundreds of dollars would have been “huge,” a boon that “would have covered meals, it would have covered bills.”.Had Maryland approved the proposal, Allstate customers who were deprived of discounts would likely never have known. Allstate would not have been required to inform them and the National Association of Insurance Commissioners said it has never heard of an insurer doing so voluntarily. .Besides Maryland, some other states have also signaled they would not accept similar plans from Allstate. Georgia rejected Allstate’s proposal just last year. .But in at least 10 states, public records show, Allstate’s plans mention using a customer retention model: Arizona, Arkansas, Illinois, Iowa, Michigan, Missouri, Nebraska, Oklahoma, Tennessee, and Wisconsin. .Allstate wouldn’t tell us if those models work exactly the same way as the Maryland proposal and it’s impossible to know from the outside. The Markup and Consumer Reports reviewed public records for hundreds of Allstate filings and only the 2013 Maryland filing contained the granular customer data necessary for this analysis, because regulators there asked for more information than the insurer originally provided..Of the 10 states that allowed Allstate’s retention models, only officials in Arkansas would answer our questions about why it would allow a retention model. Spokesperson Kenneth Ryan James said such proposals would not be prohibited in Arkansas, because state law only bans discrimination by insurers grouping customers “in part on race, color, creed or national origin.”.This is different from so-called dynamic pricing, where prices change so often that people can end up paying different amounts, like with airfare, because they bought at different times. Personalized prices are instead set by something that’s specific to you. .Staples defended its practice of varying prices online by zip code, even though the neighborhoods with higher prices were more likely to be poor, telling the Wall Street Journal that they reflect factors like the cost of doing business. Princeton Review said it was merely passing on the costs of providing tutoring packages to different locations—but tutoring was delivered online. .Buyers’ race, poverty and other discriminatory details can wend their way into personalized pricing algorithms through other factors, even if that’s not the intent, experts said..“Whether it’s race or gender or sex or health, all of these factors are going to be relevant statistically to questions of how much can you get away with charging,” said Daniel Schwarcz, a professor at University of Minnesota Law School who studies pricing discrimination. .“Let’s say that someone gets cancer and doesn’t have time to shop and just buys the first thing because they have other things to worry about. Is it O.K. to charge more to that person because they don’t have the resources or time or attention to invest in shopping?” he added. “It’s a huge problem that I don’t think society has thought through. We have to ask questions about when we’re O.K. with that and when are we not. And how are we going to police it?” .A 2015 Obama Administration report entitled “Big Data and Differential Pricing” warned of the potential negative effects of personalized pricing becoming widespread—and specifically mentioned insurance: “Differential pricing in insurance markets can raise serious fairness concerns, particularly when major risk factors are outside an individual customer’s control.”.That’s because unlike DVDs, staplers or tutoring, auto insurance isn’t an optional purchase; it’s required by law for drivers in every state except New Hampshire and Virginia. That translates into hundreds of millions of vehicles that must be insured so people can get to work, drive their kids to school, run errands..Driving without insurance can lead to large fines, license suspension, and even incarceration. With such dire consequences, nearly every state prohibits discriminatory rate-setting, requiring premiums to be “cost-based” or “loss-based,” meaning insurance companies can only price in the risk of a claim and a little overhead..Allstate’s 2013 proposal didn’t abide by those rules, according to Maryland regulators. The rate proposals for two 42-year-old women living in Baltimore County show how it would have worked. .Allstate determined that they both needed increases of around $1,166 to come in line with risk, records show. One, who was paying $3,610.58 per six months, would have gotten a 20 percent increase, more than $700— which was not as high as the jump the company said she should have gotten based on her risk profile..That could be considered a win for that customer, until one sees what Allstate proposed for the second woman, who had a much cheaper existing policy of $1,885.32. Rather than the 62 percent increase Allstate reported the second woman needed, she would have seen only a 5 percent increase, or about $90 over six months. .“Allstate is failing to limit rate increases in a manner that treats all insureds with like insuring or risk characteristics equally,” a Maryland insurance regulator at the time, Geoffrey Cabin, wrote in the denial letter in May 2014, specifically calling out the new retention algorithm. .Maryland Insurance Administration spokesperson Joseph Sviatko said Allstate withdrew the filing only after the state emailed the denial letter. Oddly, the filing is labeled “withdrawn” rather than “disapproved” in public records and Sviatko said he couldn’t explain why. He said the designation makes “no practical difference” internally. .He also could not explain why the state’s denial letter was not mentioned or included in the public record—we had to request it twice to get a copy. The first time we were told it didn’t exist..There is one key difference with the “withdrawn” label on the rate filing: Allstate officials have used it to claim over the past six years to other states’ regulators, investors—and now the media—that its plan had not been rejected. It’s unclear what effect those statements have had..Consumer advocates have complained for about a decade about auto insurers trying to set personalized rates that move away from risk, using individualized pricing schemes at one point loosely termed “price optimization.”.In 2013, a software developer called Earnix, which sells price optimization products, said its survey of large auto insurance executives in the U.S. and Canada showed that 45 percent were using some form of the technique and another 29 percent were planning to do so. The company did not return calls or emails seeking more information about who those insurers were..It’s hard to tell whether any particular insurer is using this strategy—even for regulators. It took the work of several statisticians and data journalists at The Markup and Consumer Reports to understand how Allstate’s Maryland proposal set transition prices between the old and new risk models..“They don’t lie; they just don’t tell you unless you ask the right set of questions,” explained Rich Piazza, the Louisiana Department of Insurance’s chief actuary. “The regulator won’t necessarily know what the insurance company is doing or what goes into their models. Heck, we don’t even know half the models’ names.”.Some don’t ask. New Mexico officials said they had no idea if Allstate was using a retention model in their state in 2016, as the insurer claimed; regulators had approved the rate filing without review, as they normally do. .The National Association of Insurance Commissioners, which includes insurance regulators from all 50 states, said in a report that its members are essentially outgunned: “Regulators do not currently have the data necessary for an independent evaluation of most of the insurer modeling and calculations.” .Allstate executives used to boast about the benefits of price optimization to investors, saying it was leading to growth in its auto insurance line.     .“We will utilize pricing sophistication to increase our price competitiveness to a greater share of target customers,” the company wrote in a 2011 filing with the Securities and Exchange Commission. “We call this price optimization and it includes using underwriting information, pricing and discounts” to sell Allstate policies..During a presentation at the 2013 Sanford Bernstein Strategic Decisions Conference, Allstate CEO Thomas Wilson directly credited the company’s use of “price optimization” as one of the factors improving customer retention. .The nonprofit watchdog Consumer Federation of America released a letter to state insurance regulators in 2014 calling the practice unfair and urging them not to allow it. .“We highly doubt that Allstate is the only company using these illegal techniques,” read the letter, signed by the group’s head of insurance and a former Texas insurance commissioner, J. Robert Hunter, “though we do not know if other insurers are as brazen in their deployment of price optimization as Allstate.” .Allstate responded in a letter to regulators that Hunter’s letter was a bunch of “flawed speculation” and defended its new “21st century” method of transitioning between rates with its algorithm, which it called Complementary Group Rating, or CGR..“CGR takes what was once an ad hoc, judgmental practice and applies uniform, mathematical rigor to it,” Allstate corporate counsel Maria S. Doughty wrote. “The result is more consistent and certainly less arbitrary.” .The next year a class action lawsuit was filed in California against another insurance company, Farmers Insurance, alleging it was using price optimization to overcharge longtime customers, believing they would be more likely to accept rate increases. Farmers replied that it can’t be sued for rate increases that were approved by the state, court records show. (After years of litigation, Farmers and the class reached a settlement in September, which is pending court approval.) .The practice even stirred controversy within the Allstate community. An anonymous negative article appeared in Exclusive Focus, a magazine published by the National Association of Professional Allstate Agents, independent insurance agents. The writer complained that “the finer the ‘slicing and dicing’” of the customer base by sophisticated algorithms, “the more discriminatory the rating becomes.” .Recounting meetings with company management in which agents asked how to explain rate changes to customers, the writer said: “Apparently … agents were warned that ‘those algorithms’ are far too difficult to comprehend or explain to clients.”.In 2014, the regulator group NAIC announced it would publish a white paper on price optimization the following year. In a letter hoping to influence the final paper, the industry trade group Property Casualty Insurers Association of America advocated against new rules, saying that even continuing to talk about it would “demand significant resources from both regulators and companies, incurring costs that consumers and the public will bear to revisit already effective and balanced regulation—all without proven need.”  .This was the atmosphere when several states, including Maryland, Florida and Rhode Island, were considering Allstate’s proposals to implement rates with its “retention model.”.Florida regulators told Allstate they intended to reject it, saying in a letter that setting a driver’s premium based on his or her “modeled reaction to rate changes” was “unfairly discriminatory.” Rhode Island regulators raised similar concerns and in both cases, regulators allowed Allstate to withdraw its filing and revert to its prior method for calculating rates..As regulators scrutinized the new pricing method, Allstate wasn’t always truthful in how it answered their questions. Louisiana regulators asked Allstate whether any other states had rejected the algorithm and retention model, CGR..In a written response in February 2015, Allstate said: “The new loss model and CGR has not been disapproved in any states,” even though Maryland had by then rejected Allstate’s new loss model and CGR, deeming it discriminatory..A similar letter to regulators in Ohio in 2016 included Nevada and West Virginia among 23 states in which Allstate said it was using the retention model. But Nevada officials said Allstate never had a retention model in their state “to the best of our knowledge” and West Virginia said they found no record of the Allstate filing containing a retention model..In an email, Jones, the Allstate spokesperson, said those states’ rating plans did contain retention models, despite what the regulators said. She did not answer questions about whether and how they’d been informed about it..Piazza, the Louisiana Department of Insurance’s chief actuary, said CGR—at least as Allstate proposed it for his state—was “basically a flavor of price optimization,” and that his office “did not let them use this.” State records show the company withdrew the plan..“The issue with Allstate wasn’t as much the individual variables as it was that the decision to increase an individual policyholder’s premium was based on the probability of them leaving the company,” he said. “If they were not likely to leave the company, they wanted to charge more. That’s not cost-based.”.But when an audience member at the March 2015 Raymond James Institutional Investor Conference asked Allstate officials whether they were worried about the increasing regulatory scrutiny over price optimization, Vice President of Investor Relations Pat Macellaro was not forthcoming about the pushback the company had received from regulators. “I don’t know if it necessarily applies to Allstate,” he said..When Georgia regulators asked the company last year in a phone call whether its proposed plan used price optimization, records show, officials replied that it did not—saying, among other things, that the term price optimization is too inconsistently defined to say what it is. .Allstate argued regulators should approve the algorithm it was proposing because the variables inside of it comply with state law—which is akin to telling city inspectors that they have to approve a house, no matter how it’s constructed, because all the bricks, wires and pipes would individually be up to code. .“It used to be you could look at rate filings in the ’70s and ’80s and you pretty much knew what an insurance company was going to price,” said Paul Newsome, managing director and senior research analyst at the investment bank Piper Sandler. .In the 1990s, insurers began using external data sources like credit scores to predict accident risk. Since then, rate filings have become increasingly filled with proprietary, opaque algorithms, according to regulators..Gennady Stolyarov II, a lead actuary at the Nevada Division of Insurance, said all this secrecy and complexity leaves drivers in the dark about how to keep their rates low. .“If a behavior in another sphere of life affects insurance premiums in a way that consumers can’t readily anticipate,” he said in an interview, “that could lead to a cascade of financial consequences arising from what seems to be an innocuous decision.”.Yet regulators play a role in helping insurers keep what they’re doing out of the public eye. Rules vary by state, but insurance companies don’t always submit full details of their pricing algorithms to regulators unless those documents are specifically requested. And insurance companies at times file documents with confidential attachments, blocked from public disclosure due to trade secrets rules. .When we asked Steve Manders, director of insurance product review at the Georgia Department of Insurance, why his state found Allstate’s filing to be discriminatory he refused to be more specific, claiming he was legally obligated to protect that data from competitors and the public. .Patty Born, a professor studying insurance regulation at Florida State University’s College of Business, doubts insurers will ever share enough information about their pricing models to allow customers to know if they’re overpaying. She said the only defense is to regularly check competitors’ rates..“There are a lot of people who get an auto policy and they stick with that auto insurer forever because the decision is hard in the first place, figuring out what policy you want,” Born said. “Most people just never go back to look to see that they’re paying more than they should.” .Bruce Bennett, a retired hospital administrator from Oklahoma, said he spent more than two decades as an Allstate car insurance customer because he felt the company provided “good service.”.But about a year ago, Bennett, who is 71, said he and his wife reached a breaking point after their premium eclipsed $3,300 for a year of coverage. .“The price increases didn’t really come after any claim. They were just small amounts all during the life of the policy,” said Bennett, who lives in one of the 10 states where public records show Allstate uses a retention model..He shopped around and Safeco, a subsidiary of Liberty Mutual, offered to cut Bennett’s premium by nearly half—a reduction of more than $1,400 a year. He jumped at it..“Insurance premiums were not really much of an imposition to us” when he was working, he said. “We just plodded along seeing no pressing need to switch companies..“But now that my income is no longer six digits, we seem to notice our expenses a bit more; hence the interest in making a change,” he added. “I think the frog-in-boiling-water scenario finally got to us this year.”.We’re happy to make this story available to republish for free under the conditions of an Attribution–NonCommercial–No Derivatives Creative Commons license. Please adhere to the following:",Algorithm denies meaningful decreases to thousands of Allstate customers who are paying too much and encourages price increases
188,"Barclays has been criticised by HR experts and privacy campaigners after the bank installed “Big Brother” employee monitoring software in its London headquarters. .Introduced as a pilot last week, the technology monitors Barclays workers’ activity on their computers, and in some instances admonishes staff in daily updates to them if they are not deemed to have been active enough — which is described as being in “the zone”. .The system tells staff to “avoid breaks” as it monitors their productivity in real-time, and records activities such as toilet visits as “unaccounted activity”..A whistleblower at the banking giant told City A.M. that “the stress this is causing is beyond belief” and that it “shows an utter disregard for employee wellbeing”. .“Employees are worried to step away from their desks, have full lunch breaks, take bathroom breaks or even get up for water as we are not aware of the repercussions this might have on our statistics,” they added. .The software, provided by Sapience, has been rolled out throughout the product control department within the investment bank division at the firm’s Canary Wharf headquarters. .Sapience describes the system as offering “automated work pattern reporting and real-time analytics” and “unprecedented visibility into how people work, with actionable insights to better manage cost and performance across teams.”.The software compiles a report into an employees’ activity during the working day. A “work yoga” assessment sent to a Barclays employee earlier this week, seen by City A.M., warned the staffer of “not enough time in the Zone yesterday!”.The controversial new software comes as the investment bank arm, of which the product control department is a part, reported increased profit at the end of 2019. .The use of such software is becoming more widespread in the industry, but Ed Houghton, head of research at the Chartered Institute for Personnel and Development, warned the technology could never be a “substitute for good line management”..Barclays’ product control function is tasked with compiling profit and loss accounts on the trading floor’s activities as well as communicating information with the rest of the bank and regulators. .This is not the first time Barclays has been accused of invading the privacy of employees. In 2017 the firm was criticised for the use of OccupEye sensors which monitored how long employees were spending at their desks. A similar system was used at the Telegraph newspaper, but was removed after just a day following a backlash from employees..Silkie Carlo, director of Big Brother Watch, said: “Managers would never get away with breathing down employee’s necks, personally monitoring their screens or logging toilet and water breaks. The availability of technology to surveil staff surreptitiously does not make it any more acceptable.”.A spokesperson for Barclays confirmed it had introduced the monitoring software, and said  “This type of technology is widely used across the industry to help identify what is working well and opportunities to improve processes.  Colleague wellbeing is of paramount importance and colleagues are free to take breaks whenever they choose.” ","This pilot violates privacy, encourages intimidation and has room for racial bias"
189,"On February 7, a day ahead of the Legislative Assembly elections in Delhi, two videos of the Bharatiya Janata Party (BJP) President Manoj Tiwari criticising the incumbent Delhi government of Arvind Kejriwal went viral on WhatsApp. While one video had Tiwari speak in English, the other was him speaking in the Hindi dialect of Haryanvi. “[Kejriwal] cheated us on the basis of promises. But now Delhi has a chance to change it all. Press the lotus button on February 8 to form the Modi-led government,” he said..One may think that this 44-second monologue might be a part of standard political outreach, but there is one thing that’s not standard: These videos were not real. This is what the original video was:.It’s 2020, and deepfakes have become a powerful and concerning, tool that allows humans to manipulate or fabricate visual and audio content on the internet to make it seem very real. They are quite like the face animations in Hollywood films, though not nearly as expensive, and with a dark side. Since its introduction in 2017, A-list celebrities have seen their faces pushed onto existing pornographic videos, making deepfakes an infamous tool for misuse..When the Delhi BJP IT Cell partnered with political communications firm The Ideaz Factory to create “positive campaigns” using deepfakes to reach different linguistic voter bases, it marked the debut of deepfakes in election campaigns in India. “Deepfake technology has helped us scale campaign efforts like never before,” Neelkant Bakshi, co-incharge of social media and IT for BJP Delhi, tells VICE. “The Haryanvi videos let us convincingly approach the target audience even if the candidate didn’t speak the language of the voter.”.Tiwari’s fabricated video was used widely to dissuade the large Haryanvi-speaking migrant worker population in Delhi from voting for the rival political party. According to Bakshi, these deepfakes were distributed across 5,800 WhatsApp groups in the Delhi and NCR region, reaching approximately 15 million people..So it’s not surprising that the prospect of building campaign businesses using deepfakes to influence the masses has alarmed fact-checking organisations and policy wonks. Many think deepfakes would take the ongoing war on disinformation and fake news to a whole new level—one that has already been dubbed a “public health crisis”..Ever since deepfakes blew up in 2017, the technology has been used extensively to create fake porn videos using existing celebrity video footage and AI algorithms. In fact, 96 percent of those videos are non-consensual deepfake pornography. The unprecedented, and thereby problematic, use of deepfakes has got to do with the fact that much of the code required to fabricate videos is publicly available on several code-repository websites, making it really easy to create such videos..On the political front, the technology gained attention first in 2018, when a comedian impersonating Barack Obama delivered a PSA video on how deepfakes can be deceptive..In a lesser-known incident, a video appearance by Ali Bongo, the president of the East African nation of Gabon, was believed to be a deepfake, culminating in an unsuccessful coup by the country’s military. But the political fallout on account of deepfakes has been fairly limited, until now..With deepfake election campaigns though, we are crossing over into an era where it’s going to be impossible to trust what we see and hear. The video of Tiwari, seated in front of a green-coloured wall and talking to the camera, was used to reproduce a forged version where he says things he never actually said, in a language he doesnt even know! In this case, the speech was scripted, vetted and approved by the BJP for the creation of the deepfakes. But it’s not difficult to imagine someone faking a video to issue threats or hate against a specific section of the population..While many of the popular deepfake videos are complete faceswaps, a subtler version is to alter only the lip movements from an original video to match the target audio. The Ideaz Factory claims to have done the latter for Tiwari’s video. “We used a ‘lip-sync’ deepfake algorithm and trained it with speeches of Manoj Tiwari to translate audio sounds into basic mouth shapes,” says Sagar Vishnoi, the chief strategist at The Ideaz Factory. The firm hired a dubbing artist to impersonate Tiwari reading the script in Haryanvi, which was then superimposed on the video..BJP’s Bakshi says the response to those videos has been encouraging. “Housewives in the group said it was heartening to watch our leader speak my language,” he said, recounting one of the comments on a WhatsApp group. After the “viral” response, the party went ahead with a second video of Tiwari speaking English targeted at “urban Delhi voters.”.VICE shared the videos with researchers at the Rochester Institute of Technology (RIT) in New York, who believe that these were indeed deepfakes, but are awaiting confirmation on the same from their purpose-built software to automatically detect deepfakes. Ideaz Factory refused to share more information on the technology used, but Saniat Javid Sohrawardi, a deepfake researcher at RIT, says that “judging by the timeline of their work, Id think that they used Nvidias vid2vid code.” The only other well-known algorithm to achieve this task is face2face, an application that was used to make the Obama deepfake video..In India, though, deepfakes still have some lags. In Tiwari’s videos, a few members on WhatsApp groups pointed out a brief anomaly in the mouth movement. But Vishnoi assures that minor kinks aside, “we have used a tool that has so far been used only for negative or ambush campaigning and debuted it for positive campaign.” He admits that the technology his firm uses is currently not mature enough to synthetically generate the target’s voice using algorithms. But they have plans to scale Tiwari’s “positive” deepfake campaign to upcoming Bihar elections and the 2020 US elections..Tarunima Prabhakar, cofounder of Tattle, a civic tech project that is building a searchable archive on content circulated on WhatsApp, says, “The problem with the positive campaign pitch is that it puts the genie out of the bottle.” This means that even if the firm somehow self-regulates and decides not to produce nefarious videos, other, possibly not as overt, companies, will come up with other uses to weaponise this technology. “To say only some forms of deepfakes are allowed by political parties, allows for a lot of subjectivity and interpretive power on who defines those forms,” Prabhakar says..The increase in deepfakes, aided by the growing number of tools and services, have made it easy for non-experts to create deepfake videos. The Ideaz Factory is just one among several firms that has sprung up in India to profit from this access. There are deepfake portals and individual users across the world advertising to create custom deepfakes for as little as $30. Needless to say, this low barrier to entry has resulted in more covert deepfake operators, with the number of deepfakes doubling to 14,678 in 2019..This also means that most of the deepfake content will inevitably bypass fact-checking and tech experts and resources who are trying to curb the menace. Pratik Sinha, the founder of AltNews, an Indian fact-checking website that verifies claims and assertions made on social media, tells VICE, “At this point in time, it’s impossible to fact-check or verify something that you don’t recognise is doctored.” When VICE shared the videos with Sinha to check their validity, AltNews was unable to deduce it as fake. “This is dangerous,” says Sinha, whose organisation has fact checked thousands of morphed images and manipulated videos in the three years of its operation. “It’s the first time I’ve seen something like this emerge in India.”.In a country like India where digital literacy is nascent, even low-tech versions of video manipulation have led to violence. In 2018, more than 30 deaths were linked to rumours circulated on WhatsApp in India. “Deepfakes are going to be a supercharger on the kind of misinformation we have,” Sinha said. While tools to reliably detect deepfakes are currently unavailable, there have been efforts by researchers to develop a few. Reality Defender, a browser plugin for detecting fake videos, is one of them..However, experts like Sinha believe that no firm should be allowed to have a legitimate business around deepfake for election campaigns in India. In October last year, the state of California in the US passed a bill that made it illegal to circulate deepfake videos of politicians within 60 days of an election. The legislation was signed to protect voters from misinformation. But Prabhakar adds that in India, outlawing deepfakes is doomed to fail in implementation, as they would never be openly endorsed by political parties. “They would only continue to be operated by shadow firms,” she says..However, there could be a solution. Vishnoi thinks there should be government policy around misinformation as a whole, and the way to counter negative deepfakes is through awareness campaigns. But Dr Matthew Wright, the director of the Center for Cybersecurity at RIT, sees the emergence of deepfake for election campaign as “a potentially positive use case as long as there is disclosure.” “Why should our political leaders only be accessible to those who can read, assuming the translation is easily available in the right written language?” he asks. “But if it’s used deceptively, that’s a different story, and I’m sure some will blur the lines.”.By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.","President Manoj Tiwari criticized Arvind Kejriwal. However, the videos are not real and lead to misinformation"
190,"The report from the justice sub-committee on policing was published as part of their inquiry into the advancement of live facial recognition..The report added: The use of live facial recognition technology would be a radical departure from Police Scotlands fundamental principle of policing by consent..The committee also said Police Scotland would need to ensure any technology in use would need to be provided for in legislation and meets human rights and data protection requirements..New computer systems are able to watch thousands of people at an incredibly fast pace, with the most powerful able to operate at distances of over a mile..Convener John Finnie said the sub-committee was reassured the force have no plans to introduce live facial recognition technology at present..Our inquiry has also shone light on other issues with facial recognition technology that we now want the Scottish Police Authority (SPA) and the Scottish government to consider..Not least amongst these are the legal challenges against similar technologies in England and Wales, and the apparent lack of law explicitly governing its use in Scotland - by any organisation..So whether this technology is being used by private companies, public authorities or the police, the Scottish government needs to ensure there is a clear legal framework to protect the public and police alike from operating in a facial recognition Wild West..Prior to any such technology being implemented we would carry out a robust programme of public consultation and engagement around the use of this technology, its legitimacy, viability and value for money, he added. .This would include taking advice and guidance on ethical, human rights and civil liberties considerations. In my view, the use of such technology would not be widespread but would be used in an intelligence-led, targeted way.",The use of facial recognition has racial bias and can be used unfairly
191,"The case is due to be held at the Tverskoy District Court of Moscow, and the complainants will seek to prohibit the use of facial recognition technology at mass gatherings, and demand that the authorities delete all stored personal data previously collected at such events..Facial recognition technology is by nature deeply intrusive, as it enables the widespread and bulk monitoring, collection, storage and analysis of sensitive personal data without individualised reasonable suspicion, said Natalia Zviagina, Amnesty International Russias Director. .In the hands of Russias already very abusive authorities, and in the total absence of transparency and accountability for such systems, it is a tool which is likely to take reprisals against peaceful protest to an entirely new level, she added.  .It is telling that the Russian government has provided no explanation as to how it will ensure the right to privacy and other human rights, nor has it addressed the need for public oversight of such powerful technologies. .Ms Popova has previously attempted to sue the Moscow police and information technology department, after she was fined for taking part in a protest in the capital last year..She claimed authorities used a facial recognition system to identify her without consent but a court dismissed her case in November, saying there was no proof that facial recognition was used..Local media reported that the Moscow Department of Information Technology has signed a deal with Russian firm Ntechlab, to provide the technology..The firm previously developed controversial app FindFace, which allowed users to photograph people in a crowd and work out their identities by tapping into social network vkontakte, known as Russias Facebook..The service was ended in 2018, with the firm saying it was concentrating its future efforts on face recognition solutions for government and business.",Russian authorities are using facial recognition at mass gatherings and collecting all personal data
192,"A new website, “NotRealNews.net,” uses artificial intelligence to populate what resembles a news site’s home page, complete with AI-written fake news stories..The website, a project by the AI development company Big Bird, is supposed to be a showcase of how the company’s algorithms can help journalists quickly write compelling news, according to the website’s “about” page..But despite the website’s title, the realistic-enough articles aren’t labeled as fake news or the marketing stunt that they are — meaning their existence is more likely to make journalists pull out their hair than it is to help them..Aside from entertaining algorithmic errors, like the headlines “Iran’s Stock Market: ‘There’s a Market,” and “In wake of death of British soldier, thousands as for plastic-free pubs,” the articles are mostly convincing. In fact, many appear to be closely based on actual news stories, like the resignation of UK finance minister Sajid Javid..It bears stating clearly: this is dangerous. Articles on the website contain fictional updates from the U.S. presidential race. Some are misinformation about the ongoing coronavirus outbreak, a news cycle that’s already full of confusing and sometimes conflicting reports. One headline describes a sexual assault..The idea, according to Big Bird’s logic, is that journalists would be able to save time by taking AI-generated article templates but to then validate and fact-check them by removing the algorithm’s errors and filling them in with factual information..Basically, it would turn reporters into the algorithm’s editor — but given the system’s believable prose and close proximity to the truth, it seems like that arrangement would almost guarantee that some fake news would slip through the cracks..It all raises a crucial question: Given that reporters are already perfectly capable of writing the news without a tech company’s help, who is this for?",Fake News could be spread by bots and lead to misinformation
193," 						Keep abreast of significant corporate, financial and political developments around the world. 						Stay informed and spot emerging risks and opportunities with independent global reporting, expert 						commentary and analysis you can trust. 					",Secretive’ settled-status algorithm risks discrimination
194,"An algorithmic risk scoring system deployed by the Dutch state to try to predict the likelihood that social security claimants will commit benefits or tax fraud is a breach of human rights law, a court in the Netherlands has ruled..The Dutch government’s System Risk Indication (SyRI) legislation uses a non-disclosed algorithmic risk model to profile citizens and has been exclusively targeted at neighborhoods with mostly low-income and minority residents. Human rights campaigners have dubbed it a “welfare surveillance state.”.A number of civil society organizations in the Netherlands and two citizens instigated the legal action against SyRI — seeking to block its use. The court has today ordered an immediate halt to the use of the system..The ruling (now here in English) is being hailed as a landmark judgement by human rights campaigners, with the court basing its reasoning on European human rights law — specifically the right to a private life that’s set out by Article 8 of the European Convention on Human Rights (ECHR) — rather than a dedicated provision in the EU’s data protection framework (GDPR) which relates to automated processing..GDPR’s Article 22 includes the right for individuals not to be subject to solely automated individual decision-making where they can produce significant legal effects. But there can be some fuzziness around whether this applies if there’s a human somewhere in the loop, such as to review a decision on objection..Specifically, the court found that the SyRI legislation fails a balancing test in Article 8 of the ECHR which requires that any social interest to be weighed against the violation of individuals’ private life, with a fair and reasonable balance being required. The automated risk assessment system failed this test in the court’s view..Legal experts suggest the decision sets some clear limits on how the public sector in the UK can make use of AI tools — with the court objecting in particular to the lack of transparency about how the algorithmic risk scoring system functioned..In a press release about the judgement (translated to English using Google Translate), the court writes that the use of SyRI is “insufficiently clear and controllable”. While, per Human Rights Watch, the Dutch government refused during the hearing to disclose “meaningful information” about how SyRI uses personal data to draw inferences about possible fraud..The court clearly took a dim view of the state trying to circumvent scrutiny of human rights risk by pointing to an algorithmic “blackbox” and shrugging..The Courts reasoning doesnt imply there should be full disclosure, but it clearly expects much more robust information on the way (objective criteria) that the model and scores were developed and the way in which particular risks for individuals were addressed..The UN special rapporteur on extreme poverty and human rights, Philip Alston — who intervened in the case by providing the court with a human rights analysis — welcomed the judgement, describing it as “a clear victory for all those who are justifiably concerned about the serious threats digital welfare systems pose for human rights.”.“This decision sets a strong legal precedent for other courts to follow. This is one of the first times a court anywhere has stopped the use of digital technologies and abundant digital information by welfare authorities on human rights grounds,” he added in a press statement..Back in 2018, Alston warned that the UK government’s rush to apply digital technologies and data tools to socially re-engineer the delivery of public services at scale risked having an immense impact on the human rights of the most vulnerable..The judgement does not shut the door on the use by states of automated profiling systems entirely, but it does make it clear that human rights law in Europe must be central to the design and implementation of rights risking tools..It also comes at a key time when EU policymakers are working on a framework to regulate artificial intelligence — with the Commission pledging to devise rules that ensure AI technologies are applied ethically and in a human-centric way. .It remains to be seen whether the Commission will push for pan-EU limits on specific public sector uses of AI (such as for social security assessments). A recent leaked draft of a white paper on AI regulation suggests it’s leaning towards risk assessments and a patchwork of risk-based rules. ",An algorithmic risk scoring system that predicts the likelihood of social security claimants that will commit benefits or tax fraud is a breach of human rights because it targets low income and minority residents neighborhoods
195,"What they found: One finding is that when users search for global warming, 16% of the top 100 related videos in the up next feature had climate disinformation. Another is that major brands are often unaware that their ads run on these videos..The big picture: Democratic Rep. Kathy Castor, who heads the Select Committee on the Climate Crisis, this week urged Google to curb false climate information on YouTube. She called for steps including....The other side: A YouTube spokesperson said the company has significantly invested in reducing recommendations of borderline content and harmful misinformation, and raising up authoritative voices.","Youtube's algorithm encourages misinformation by adding climate disinformation videos in ""up next"" feature"
197,"Police body cameras and tower camera video show some of what happened on May 30 as protesters squared off with Miami officers forming a line outside of their Downtown headquarters..The videos, exclusively obtained by NBC 6 Investigators, captured heated moments as objects were thrown at officers and they popped tear gas to retake control of patrol cars..Police say Oriana Albornoz, 25, threw two rocks at an officer hitting him once and injuring his leg. The department provided a video that shows her throwing something at officers standing across the street but it is difficult to discern what it is..A recent NBC 6 investigation found police departments across South Florida, including Miami, are using the technology, which identifies people through publicly available photos including social media sites like Facebook, LinkedIn and Instagram..“It looks like they’ve just done a regular photographic line up and had it not been for the vigilance of your news agency, I would not have known this,” Gottlieb said. .“We don’t know where they got the image, Gottlieb said. So how or where they got her image from begs other privacy rights. Did they dig through her social media? How did they get access to her social media? he asked.”.According to the Miami Police’s policy, facial recognition technology shall not be used to conduct surveillance of people exercising “constitutionally protected activities” like protesting. . “This means that if someone is peacefully protesting and not committing a crime, we cannot use it against them,” Miami Police Assistant Chief Armando Aguilar told NBC 6 in an earlier interview..“We have used the technology to identify violent protesters who assaulted police officers, who damaged police property, who set property on fire. We have made several arrests in those cases and more arrests are coming in the near future,” he said. .The department’s policy requires keeping a log documenting all facial recognition searches and conducting monthly audits. It also states that “a positive facial recognition search result alone does not constitute probable cause of an arrest.”.Gottlieb calls the police investigation “very disingenuous” and tells us he’s concerned there are no statewide rules and regulations for facial recognition. .NBC 6 Investigators found Miami Police have a much more detailed policy than other departments using the technology in South Florida. The Broward County Sheriff’s Office, Miami-Dade Police and Coral Spring Police told NBC 6 they have not used the technology to make arrests during the protests following the death of George Floyd..In an email, the Clearview’s CEO Hon Ton-That told NBC 6: “Clearview AI is also committed to the responsible use of its powerful technology and is used only for after-the-crime investigations to help identify criminal suspects. It is not intended to be used as a surveillance tool relating to protests or under any other circumstances.”",The use of facial recognition during protesting violates freedom of speech as well as leaves room for racial bias
199,"Amazon on Thursday launched Amazon Halo, a wearable band to compete with FitBit and Apple Watch. Like its competitors, Halo can track heart rate and sleep patterns, but its also looking to differentiate itself with a peculiar feature: judging your emotional state from your tone of voice..Amazon Tone claims to tell you how you sound to other people. It uses machine learning to analyze energy and positivity in a customers voice so they can better understand how they may sound to others, helping improve their communication and relationships, Amazons press release for Halo reads..To give an example, Amazons chief medical officer Maulik Majmudar said Tone might give you feedback such as: In the morning you sounded calm, delighted, and warm. According to Majmudar, Tone analyzes vocal qualities like your pitch, intensity, tempo, and rhythm to tell you how it thinks you sound to other people..Experts that Business Insider spoke to are dubious that an algorithm could accurately analyze something as complex as human emotion — and they are also worried that Tone data could end up with third parties..I have my doubts that current technology is able to decipher the very complex human code of communication and the inner workings of emotion, said Dr Sandra Wachter, associate professor in AI ethics at the University of Oxford..How we use our voice and language is greatly impacted by social expectation, culture and customs. Expecting an algorithm to be able to read and understand all of those subtleties seems more like an aspirational endeavour, she said..Here the machine has to understand how someone speaks (and what they say) AND infer how someone else understands and interprets these words. This is an even more complex task because you have to read two minds. An algorithm as a mediator or interpreter seems very odd, I doubt that a system (at least at this point) is able to crack this complex social code, she said..Mozilla fellow Frederike Kaltheuner agreed that voice analysis has inherent limitations. Voice recognition systems have also historically struggled with different kinds of voices, she said. Accuracy is typically lower for people who speak with an accent or who are speaking in a second language..Amazon says it has made the Tone feature opt-in for Halo owners. Once you switch it on, it runs in the background, recording short snippets of your voice throughout the day for analysis. Theres also an option to turn it on for specific conversations, up to 30 minutes in length..Amazon says all this data is kept safe and secure, with all the processing done locally on your phone, which then deletes the data. Tone speech samples are never sent to the cloud, which means nobody ever hears them, and you have full control of your voice data, Majmudar wrote..Amazons insistence that human employees wont listen to any of Tones recordings seems to allude to the time Amazon, along with the other major companies, was caught in a scandal after reports revealed that sensitive Alexa recordings were being sent to human contractors for review..Privacy policy expert Nakeema Stefflbauer told Business Insider that Halo could be a preamble to Amazon getting into insurance tech. My first impression is that its almost as if Amazon is moving as fast as possible to get ahead of public disclosures about its own forays into the insurtech space, said Stefflbauer..I am alarmed when I hear about this type of assessment being recorded, because, while I see zero benefit from it, employers definitely might. Insurers definitely might. Public administrators overseeing the issue of benefits (such as for unemployment) definitely might, she added. .The ultimate sign to me that you as the customer arent the ultimate target of the data collected is that Amazon already has partnerships with insurers like John Hancock and medical records companies like Cerner, Stefflbauer added..John Hancock announced Thursday it would be the first life insurer to integrate with Amazon Halo. Starting this fall, all John Hancock Vitality customers will be able to link the Amazon Halo Band to the program to earn Vitality Points for the small, everyday steps they take to try to live a longer, healthier life, the insurance firm said in a press statement..Kaltheuner said its good that the Tone feature is opt-in, but anonymized data from Halo could still be shared in bulk with third parties. Even if its in aggregate and anonymous, it might not be something you want your watch to do, she said..Chris Gilliard, an expert on surveillance and privacy at the Digital Pedagogy Lab, told Business Insider he found Amazons privacy claims unconvincing..Amazon felt the heat when it was revealed that actual humans were listening to Alexa recordings, so this is their effort to short circuit that particular critique, but to say that these systems will be private stretches the meaning of that word beyond recognition, he said..Wachter said that if, as Amazon claims, an algorithm was capable of accurately analyzing the emotion in peoples voices, it could pose a potential human rights problem..Our emotions and thoughts are one of the most intimate and personal aspects of our personality. In addition, we are often not able to control our emotions. Our inner thoughts and emotions are at the same time very important to form opinions and express those. This is one of the reasons why human rights law does not allow any intrusion on them.","This product uses machine learning to detect emotion. However, a machine analyzing something as complex as a human emotion correctly is unlikely and there are many data privacy concerns."
200,"Algorithms rule our digital lives. They determine what we see and when we see it and in the case of some dating and ride-share apps, they determine how much we pay. Without seeing how an algorithm works, however, that latter aspect can be rife for discriminatory pricing. And there’s no real legislation in Australia to stop it..A months-long investigation by Choice reporter, Saimi Jeong, revealed Tinder had allegedly been charging its Plus users different amounts depending on age, gender, sexuality and location in Australia. Tinder introduced a tiered pricing system the United States in 2015 which was ultimately removed after a $US11.5 million class action lawsuit in California..“We do offer a variety of subscription options and paid a la carte features designed to help our members stand out and match with new people more efficiently,” a Tinder spokesperson said in a statement to Gizmodo Australia..“Tinder operates a global business and our pricing varies by a number of factors. We frequently offer promotional rates — which can vary based on region, length of subscription, bundle size and more. We also regularly test new features and payment options.”.Charging users different prices depending on age is not new — we often have children and senior’s fares — but it’s a question of ethics when other markers are incorporated with little-to-no transparency. .Uber’s former head of product, Daniel Graf, told Bloomberg in 2017 the rideshare company had switched to a “route-based” pricing model. This meant, according to Graf, Uber’s algorithm incorporated machine-learning techniques to figure how much a customer was willing to pay for a ride..According to this model, two users might have be charged different amounts based on how often they used the app or whether they were travelling to a more affluent neighbourhood, even if demand, traffic and distance were the same..A New Scientist report in June 2020 revealed a U.S. analysis showing some ride-share users that travelled to an area with a higher population of Black people were charged more. It used data from more than 100 million trips between November 2018 and December 2019 in the Chicago area..Uber Australia has maintained it doesn’t use this pricing model, pointing to its pricing estimate page. The page explains pricing in Australia is determined by three primary factors — a flat fee depending on your city or location, the time and distance of the trip as well as the demand of drivers at the time..An algorithm is a computer program that follows a series of statements in order to deliver an outcome. In the case of social media sites, it reads your search queries or viewing habits and serves up content based on your interests. A similar thing happens on streaming services, too..Associate Professor Timothy Miller from Melbourne University’s Centre for Artificial Intelligence and Digital Ethics (CAIDE) explained the dating app situation was likely a mixture of these two..“It seemed to me like, possibly, the more recent [dating app] example was using machine learning,” Professor Miller told Gizmodo Australia in a Zoom call..“They had actually gotten personal data around what sexuality [users] were, how old they were and where they lived and they’ve kind of learned that, well, people in this particular area would be of a particular affluence or they’d be more likely to pay for it.”.Of course, it’s speculation and that’s due to a few factors. Firstly, companies aren’t going to simply admit they’re charging you differently because the algorithm sees you as more cashed up — that would be detrimental to consumer trust..It’s also that the tech space is a competitive world and that giving away too much about how your proprietary algorithm works might be a bad business choice..“But they start to give more and more information away about the business model effectively, because people can ask enough questions that they can kind of reconstruct the model themselves.”.Unfortunately, this results in tech companies using shadowy methods that ultimately benefit them financially — and users are none the wiser..There are counter-factuals, Professor Miller added, that might give you a chance to trick the algorithm into showing what others are charged..“So, changing your gender, changing your sexuality, changing the region you live in … but I don’t see companies like [that] offering those types of explainability tools at all,” Professor Miller said..“The only real way you can know it is if there is something built in or you get enough people together [to check] or you set up enough accounts yourself in order to change all these parameters and see what the impact is.”.If it’s near impossible for a user to really know what an algorithm is designed to do or charge, how can anyone trust them beyond anecdotal evidence? One idea is to introduce a regulatory body or establish an ethics framework that companies would have to abide by..In 2019, the European Commission proposed a set of guidelines artificial intelligence (AI) systems would need to meet in order to be rubber stamped as trustworthy..Broadly, it asserted AI systems should be lawful, ethical and robust. On a more granular level, it expects them to consider human agency and incorporate oversight, to take diversity, non-discrimination and fairness into account and provide transparency and accountability among other key points..Dr Nick Patterson, a cybersecurity researcher at Deakin University, said it makes sense that something similar is established in Australia too..“[A formal regulatory body] is something which should be established to ensure ethical algorithm decisions — especially when it involves them making important decisions on human lives,” Dr Patterson said..But Dr Patterson thinks it’s quite tricky regulating the algorithms themselves. Some low-stakes algorithmic deciders, like whether you pay a few dollars more for an an app subscription, should be left for customers to decide. It’s the potentially life-changing or saving decisions, Dr Patterson said, that need a tougher look..“If it’s something like … making court decisions or in a hospital, an algorithm making medical decisions should be reviewed by a panel of experts, such as IEEE [Institute of Electrical and Electronics Engineers] or equivalent, to determine safety.”.“Yes, there’s a principle around being fair or there’s a principle about being transparent, but what does that mean to the person on Monday morning when they’re sitting down at their desk to operationalise these procedures? There’s a lot of work that needs to be done there..“In the case of discriminative pricing, we need to look at existing legislation and say ‘we need to update this for the modern era, where people are really discriminating based on a whole lot of factors that we didn’t think about when we wrote this up 20 years ago.”.Professor Miller and Dr Patterson don’t have the simple answers to a complicated situation but the baby steps happening in Europe are providing hope..In Australia, the Human Rights Commission launched a discussion paper in late 2019 surrounding the ethics of emerging technologies, including AI-operated algorithms..“AI is being used to make decisions that unfairly disadvantage people on the basis of their race, age, gender or other characteristic. This problem arises in high-stakes decision making, such as social security, policing and home loans,” Commissioner Edward Santow wrote at its launch..“These risks affect all of us, but not equally. We saw how new technologies are often ‘beta tested’ on vulnerable or disadvantaged members of our community.”","When a company charges based on inherent human features, this increases racial and gender bias. "
201,"An artificial intelligence hiring system has become a powerful gatekeeper for some of America’s most prominent employers, reshaping how companies assess their workforce — and how prospective employees prove their worth..Designed by the recruiting-technology firm HireVue, the system uses candidates’ computer or cellphone cameras to analyze their facial movements, word choice and speaking voice before ranking them against other applicants based on an automatically generated “employability” score..HireVue’s “AI-driven assessments” have become so pervasive in some industries, including hospitality and finance, that universities make special efforts to train students on how to look and speak for best results. More than 100 employers now use the system, including Hilton and Unilever, and more than a million job seekers have been analyzed..But some AI researchers argue the system is digital snake oil — an unfounded blend of superficial measurements and arbitrary number-crunching that is not rooted in scientific fact. Analyzing a human being like this, they argue, could end up penalizing nonnative speakers, visibly nervous interviewees or anyone else who doesn’t fit the model for look and speech..The system, they argue, will assume a critical role in helping decide a person’s career. But they doubt it even knows what it’s looking for: Just what does the perfect employee look and sound like, anyway?.“It’s a profoundly disturbing development that we have proprietary technology that claims to differentiate between a productive worker and a worker who isn’t fit, based on their facial movements, their tone of voice, their mannerisms,” said Meredith Whittaker, a co-founder of the AI Now Institute, a research center in New York..“It’s pseudoscience. It’s a license to discriminate,” she added. “And the people whose lives and opportunities are literally being shaped by these systems don’t have any chance to weigh in.”.Loren Larsen, HireVue’s chief technology officer, said that such criticism is uninformed and that “most AI researchers have a limited understanding” of the psychology behind how workers think and behave..Larsen compared algorithms’ ability to boost hiring outcomes with medicine’s improvement of health outcomes and said the science backed him up. The system, he argued, is still more objective than the flawed metrics used by human recruiters, whose thinking he called the “ultimate black box.”.“People are rejected all the time based on how they look, their shoes, how they tucked in their shirts and how ‘hot’ they are,” he told The Washington Post. “Algorithms eliminate most of that in a way that hasn’t been possible before.”.The AI, he said, doesn’t explain its decisions or give candidates their assessment scores, which he called “not relevant.” But it is “not logical,” he said, to assume some people might be unfairly eliminated by the automated judge..On Wednesday, a prominent rights group, the Electronic Privacy Information Center, filed an official complaint urging the Federal Trade Commission to investigate HireVue for “unfair and deceptive” practices. The system’s “biased, unprovable and not replicable results, EPIC officials wrote, constitute a major threat to American workers’ privacy and livelihoods..The inscrutable algorithms have forced job seekers to confront a new kind of interview anxiety. Nicolette Vartuli, a University of Connecticut senior studying math and economics with a 3.5 GPA, said she researched HireVue and did her best to dazzle the job-interview machine. She answered confidently and in the time allotted. She used positive keywords. She smiled, often and wide..But when she didn’t get the investment banking job, she couldn’t see how the computer had rated her or ask how she could improve, and she agonized over what she had missed. Had she not looked friendly enough? Did she talk too loudly? What did the AI hiring system believe she had gotten wrong?.“I feel like that’s maybe one of the reasons I didn’t get it: I spoke a little too naturally,” Vartuli said. “Maybe I didn’t use enough big, fancy words. I used ‘conglomerate’ one time.”.HireVue said its system dissects the tiniest details of candidates’ responses — their facial expressions, their eye contact and perceived “enthusiasm” — and compiles reports companies can use in deciding whom to hire or disregard..Job candidates aren’t told their score or what little things they got wrong, and they can’t ask the machine what they could do better. Human hiring managers can use other factors, beyond the HireVue score, to decide which candidates pass the first-round test..The system, HireVue said, employs superhuman precision and impartiality to zero in on an ideal employee, picking up on telltale clues a recruiter might miss..Major employers with lots of high-volume, entry-level openings are increasingly turning to such automated systems to help find candidates, assess résumés and streamline hiring. The Silicon Valley start-up AllyO, for instance, advertises a “recruiting automation bot” that can text-message a candidate, “Are you willing to relocate?” And a HireVue competitor, the “digital recruiter” VCV, offers a similar system for use in phone interviews, during which a candidate’s voice and answers are analyzed by an “automated screening” machine..But HireVue’s prospects have cemented it as the leading player in the brave new world of semi-automated corporate recruiting. It says it can save employers a fortune on in-person interviews and quickly cull applicants deemed subpar. HireVue says it also allows companies to see candidates from an expanded hiring pool: Anyone with a phone and Internet connection can apply..Nathan Mondragon, HireVue’s chief industrial-organizational psychologist, told The Post the standard 30-minute HireVue assessment includes half a dozen questions but can yield up to 500,000 data points, all of which become ingredients in the person’s calculated score..The employer decides the written questions, which HireVue’s system then shows the candidate while recording and analyzing their responses. The AI assesses how a person’s face moves to determine, for instance, how excited someone seems about a certain work task or how they would behave around angry customers. Those “Facial Action Units,” Mondragon said, can make up 29 percent of a person’s score; the words they say and the “audio features” of their voice, like their tone, make up the rest..“Humans are inconsistent by nature. They inject their subjectivity into the evaluations,” Mondragon said. “But AI can database what the human processes in an interview, without bias. … And humans are now believing in machine decisions over human feedback.”.To train the system on what to look for and tailor the test to a specific job, the employer’s current workers filling the same job — “the entire spectrum, from high to low achievers” — sit through the AI assessment, Larsen said..Their responses, Larsen said, are then matched with a “benchmark of success” from those workers’ past job performance, like how well they had met their sales quotas and how quickly they had resolved customer calls. The best candidates, in other words, end up looking and sounding like the employees who had done well before the prospective hires had even applied..After a new candidate takes the HireVue test, the system generates a report card on their “competencies and behaviors,” including their “willingness to learn,” “conscientiousness & responsibility” and “personal stability,” the latter of which is defined by how well they can cope with “irritable customers or co-workers.”.Those computer-estimated personality traits are then used to group candidates into high, medium and low tiers based on their “likelihood of success.” Employers can still pursue candidates ranked in the bottom tier, but several interviewed by The Post said they mostly focused on the ones the computer system liked best..HireVue offers only the most limited peek into its interview algorithms, both to protect its trade secrets and because the company doesn’t always know how the system decides on who gets labeled a “future top performer.”.The company has given only vague explanations when defining which words or behaviors offer the best results. For a call center job, the company says, “supportive” words might be encouraged, while “aggressive” ones might sink one’s score..HireVue said its board of expert advisers regularly reviews its algorithmic approach, but the company declined to make the system available for an independent audit. The company, Larsen said, is “exploring the use of an independent auditor right now, to see how that could work.”.HireVue launched its AI assessment service in 2014 as an add-on to its video-interview software, which more than 700 companies have used for nearly 12 million interviews worldwide. The Utah-based company won’t disclose its revenue, the cost for employers or a full list of clients..The company said last month that the private-equity giant Carlyle Group would become its new majority investor, providing an undisclosed sum from an $18.5 billion fund. Patrick McCarter, a managing director at the investment firm — which uses HireVue’s video interviews internally and said it “will look to deploy AI-driven candidate assessments over time” — said the money would help the company expand to more employers and more specialized job openings, both in the United States and around the world..At the hotel giant Hilton International, thousands of applicants for reservation-booking, revenue management and call center positions have gone through HireVue’s AI system, and executives credit the automated interviews with shrinking their average hiring time from six weeks to five days..Sarah Smart, the company’s vice president of global recruitment, said the system has radically redrawn Hilton’s hiring rituals, allowing the company to churn through applicants at lightning speed. Hiring managers inundated with applicants can now just look at who the system ranked highly and filter out the rest: “It’s rare for a recruiter to need to go out of that range,” she said..At the consumer goods conglomerate Unilever, HireVue is credited with helping save 100,000 hours of interviewing time and roughly $1 million in recruiting costs a year. Leena Nair, the company’s chief human resource officer, said the system had also helped steer managers away from hiring only “mini-mes” who look and act just like them, boosting the company’s “diversity hires,” as she called them, by about 16 percent..Dane E. Holmes, the global head of human-capital management at HireVue client Goldman Sachs, wrote in the Harvard Business Review this spring that the banking giant’s roughly 50,000 video-interview recordings were “a treasure trove of data that will help us conduct insightful analyses..The investment bank said it uses HireVue’s video-interview system but not its computer-generated assessments. But Holmes said data from those videos could help the company figure out how candidates’ skills and backgrounds might correspond to how well they would work or how long they would stay at the firm. The company, he added, is also “experimenting with résumé-reading algorithms” that would help decide new hires’ departments and tasks..“Can I imagine a future in which companies rely exclusively on machines and algorithms to rate résumés and interviews? Maybe, for some,” he wrote. (The “human element” of recruiting, he pledged, would survive at Goldman Sachs.).HireVue’s expansion has also helped it win business from smaller groups such as Re:work, a Chicago nonprofit organization that trains unemployed local job seekers for careers in the tech industry. Shelton Banks, the group’s chief, said HireVue had proved to be an irreplaceable guide in assessing which candidates would be worth the effort..The nonprofit organization once allowed almost anyone into its intensive eight-week training program, but many burned out early. Now, every candidate goes through the AI assessment first, which ranks them on problem-solving and negotiation skills and helps the group determine who might have the most motivation, curiosity and grit..“Knowing where that person is at a starting place, when it comes to this person’s life,” Banks said, “can help us make more accurate assessments of the people we’re saying yes or no to.”.But Lisa Feldman Barrett, a neuroscientist who studies emotion, said she is “strongly skeptical” that the system can really comprehend what it’s looking at. She recently led a team of four senior scientists, including an expert in “computer vision” systems, in assessing more than 1,000 published research papers studying whether the human face shows universal expressions of emotion and how well algorithms can understand them..The systems, they found, have become quite perceptive at detecting facial movements — spotting the difference, say, between a smile and a frown. But they’re still worryingly imprecise in understanding what those movements actually mean and woefully unprepared for the vast cultural and social distinctions in how people show emotion or personality..Look at scowling, Barrett said: A computer might see a person’s frown and furrowed brow and assume they’re easily angered — a red flag for someone seeking a sales associate job. But people scowl all the time, she said, “when they’re not angry: when they’re concentrating really hard, when they’re confused, when they have gas.”.Luke Stark, a researcher at Microsoft’s research lab in Montreal studying emotion and AI — who spoke as an individual, not as a Microsoft employee — was similarly skeptical of HireVue’s ability to predict a worker’s personality from their intonations and turns of phrase..Systems like HireVue, he said, have become quite skilled at spitting out data points that seem convincing, even when they’re not backed by science. And he finds this “charisma of numbers” really troubling because of the overconfidence employers might lend them while seeking to decide the path of applicants’ careers..The best AI systems today, he said, are notoriously prone to misunderstanding meaning and intent. But he worried that even their perceived success at divining a person’s true worth could help perpetuate a “homogenous” corporate monoculture of automatons, each new hire modeled after the last..The company, HireVue’s Larsen said, audits its performance data to look for potentially discriminatory hiring practices, known as adverse impacts, using “world-class bias testing” techniques. The company’s algorithms, he added, have been trained “using the most deep and diverse data set of facial action units available, which includes people from many countries and cultures.”.HireVue’s growth, however, is running into some regulatory snags. In August, Illinois Gov. J.B. Pritzker (D) signed a first-in-the-nation law that will force employers to tell job applicants how their AI-hiring system works and get their consent before running them through the test. The measure, which HireVue said it supports, will take effect Jan. 1..State Rep. Jaime Andrade Jr. (D), who co-sponsored the bill, said he pushed the transparency law after learning how many job applicants were rejected at the AI stage of a job interview. He worried that spoken accents or cultural differences could end up improperly warping the results, and that people who declined to sit for the assessment could be unfairly punished by not being considered for the job..“What is the model employee? Is it a white guy? A white woman? Someone who smiles a lot?” he said. “What are the data points being used? There has to be some explanation, and there has to be consent.”.HireVue cautions candidates that there is no way to trick, cheat or hack the system, because it assesses tens of thousands of factors to assess a “unique set of personal competencies.” “Do what feels most natural to you,” the company says in an online guide..But roughly a dozen interviewees who have taken the AI test — including some who got the job — told The Post it felt alienating and dehumanizing to have to wow a computer before being deemed worthy of a company’s time..They questioned what would be done with the video afterward and said they felt uneasy about having to perform to unexplained AI demands. Several said they refused to do the interview outright because, in the words of one candidate, the idea “made my skin crawl.”.Candidates said they have scrambled for ideas on how to maximize their worthiness before the algorithm’s eye, turning to the hundreds of videos and online handbooks suggesting, for instance, that they sit in front of a clean white wall, lest the background clutter dock their grade. “Glue some googly eyes to your webcam. It’ll make it easier to maintain eye contact,” one user on the message board Reddit suggested..Stark, the AI researcher, said these “folk theories of algorithms” were a natural response from people facing impenetrable AI systems with the power to decide their fate. The survival techniques could feel reassuring, he said, even if they were wrong: Pick the right words, use the right tone, put on a sufficiently happy face. “It’s a way of trying to give people confronting an opaque system they don’t understand some feeling of agency,” he said..But some HireVue interviewees questioned whether it was fair or even smart to judge a person’s workplace performance or personal abilities based on half an hour spent looking into a webcam. They also worried that people’s nerves about the odd nature of the exam might end up disqualifying them outright..Emma Rasiel, an economics professor at Duke University who regularly advises students seeking jobs on Wall Street, said she has seen a growing number of students excessively unsettled about their upcoming HireVue test. The university’s economics department now offers a guide to HireVue interviews on its student resources website, including typical questions (“What does integrity mean to you?”) and behavioral tips (“Act natural, talk slowly!”)..“It’s such a new and untried way of communicating who they are that it adds to their anxiety,” Rasiel said. “We’ve got an anxious generation, and now we’re asking them to talk to a computer screen, answering questions to a camera … with no real guidelines on how to make themselves look better or worse.”.The mysterious demands can also push people’s angst into overdrive. When Sheikh Ahmed, a 25-year-old in Queens, applied for teller jobs at banks around New York, he said he received eight HireVue assessment offers, all scheduled for the same day..He studied guides on how to talk and act but found the hardest part was figuring out the camera angle: Too high, he worried, and he would look domineering; too low, and he would look shrunken and weak..Before his marathon of AI interviews, he put on a crisp dress shirt, a tie and pajama pants and went to his dad’s soundproof music studio, away from the family’s chirping society finch. He also turned off his air conditioning system, hoping the background noise wouldn’t mess up his score..He changed his answers slightly in each interview, in the hopes that the algorithm would find something it liked. But he found it exhausting and disheartening to boil down his life experience and worthiness into a computer-friendly sound bite..By the end, his mouth was dry, he was covered in sweat and he was paranoid he hadn’t made enough eye contact while worrying about the bird. A few weeks after the interviews, he said, he’s still waiting to hear whether he got a job..Correction: Due to incorrect information from Goldman Sachs, The Post misreported in an earlier version of this story that the investment bank used HireVue’s AI-driven assessment program. Goldman Sachs representatives said they only use HireVue’s video-interview system, not its automated assessments.",A human should always be involved in making important decisions such as hiring. Hirevue uses AI-driven assessments only.
202,"In July 2015, two founders of DeepMind, a division of Alphabet with a reputation for pushing the boundaries of artificial intelligence, were among the first to sign an open letter urging the world’s governments to ban work on lethal AI weapons. Notable signatories included Stephen Hawking, Elon Musk, and Jack Dorsey..Last week, a technique popularized by DeepMind was adapted to control an autonomous F-16 fighter plane in a Pentagon-funded contest to show off the capabilities of AI systems. In the final stage of the event, a similar algorithm went head-to-head with a real F-16 pilot using a VR headset and simulator controls. The AI pilot won, 5-0..The episode reveals DeepMind caught between two conflicting desires. The company doesn’t want its technology used to kill people. On the other hand, publishing research and source code helps advance the field of AI and lets others build upon its results. But that also allows others to use and adapt the code for their own purposes..Others in AI are grappling with similar issues, as more ethically questionable uses of AI, from facial recognition to deepfakes to autonomous weapons, emerge..A DeepMind spokesperson says society needs to debate what is acceptable when it comes to AI weapons. “The establishment of shared norms around responsible use of AI is crucial,” she says. DeepMind has a team that assesses the potential impacts of its research, and the company does not always release the code behind its advances. “We take a thoughtful and responsible approach to what we publish,” the spokesperson adds..The AlphaDogfight contest, coordinated by the Defense Advanced Research Projects Agency (Darpa), shows the potential for AI to take on mission-critical military tasks that were once exclusively done by humans. It might be impossible to write a conventional computer program with the skill and adaptability of a trained fighter pilot, but an AI program can acquire such abilities through machine learning..“The technology is developing much faster than the military-political discussion is going,” says Max Tegmark, a professor at MIT and cofounder of the Future of Life Institute, the organization behind the 2015 letter opposing AI weapons..The US and other countries are rushing to embrace the technology before adversaries can, and some experts say it will be difficult to prevent nations from crossing the line to full autonomy. It may also prove challenging for AI researchers to balance the principles of open scientific research with potential military uses of their ideas and code..Without an international agreement restricting the development of lethal AI weapons systems, Tegmark says, America’s adversaries are free to develop AI systems that can kill. “Were heading now, by default, to the worst possible outcome,” he says..US military leaders—and the organizers of the AlphaDogfight contest—say they have no desire to let machines make life-and-death decisions on the battlefield. The Pentagon has long resisted giving automated systems the ability to decide when to fire on a target independent of human control, and a Department of Defense Directive explicitly requires human oversight of autonomous weapons systems..But the dogfight contest shows a technological trajectory that may make it difficult to limit the capabilities of autonomous weapons systems in practice. An aircraft controlled by an algorithm can operate with speed and precision that exceeds even the most elite top-gun pilot. Such technology may end up in swarms of autonomous aircraft. The only way to defend against such systems would be to use autonomous weapons that operate at similar speed..“One wonders if the vision of a rapid, overwhelming, swarm-like robotics technology is really consistent with a human being in the loop,” says Ryan Calo, a professor at the University of Washington. “Theres tension between meaningful human control and some of the advantages that artificial intelligence confers in military conflicts.”.AI is moving quickly into the military arena. The Pentagon has courted tech companies and engineers in recent years, aware that the latest advances are more likely to come from Silicon Valley than from conventional defense contractors. This has produced controversy, most notably when employees of Google, another Alphabet company, protested an Air Force contract to provide AI for analyzing aerial imagery. But AI concepts and tools that are released openly can also be repurposed for military ends..DeepMind released details and code for a groundbreaking AI algorithm only a few months before the anti-AI weapons letter was issued in 2015. The algorithm used a technique called reinforcement learning to play a range of Atari video games with superhuman skill. It attains expertise through repeated experimentation, gradually learning what maneuvers lead to higher scores. Several companies participating in AlphaDogfight used the same idea..DeepMind has released other code with potential military applications. In January 2019, the company released details of a reinforcement learning algorithm capable of playing StarCraft II, a sprawling space strategy game. Another Darpa project called Gamebreaker encourages entrants to generate new AI war-game strategies using Starcraft II and other games..Other companies and research labs have produced ideas and tools that may be harnessed for military AI. A reinforcement learning technique released in 2017 by OpenAI, another AI company, inspired the design of several of the agents involved with AlphaDogfight. OpenAI was founded by Silicon Valley luminaries including Musk and Sam Altman to “avoid enabling uses of AI … that harm humanity,” and the company has contributed to research highlighting the dangers of AI weapons. OpenAI declined to comment..Some AI researchers feel they are simply developing general-purpose tools. But others are increasingly worried about how their research may end up being used..“At the moment Im deep in a crossroads in my career, trying to figure out whether ML can do more good than bad,” says Julien Cornebise, as associate professor at University College London who previously worked at DeepMind and ElementAI, a Canadian AI firm..Cornebise also worked on a project with Amnesty International that used AI to detect destroyed villages from the Darfur conflict using on satellite imagery. He and the other researchers involved chose not to release their code for fear that it could be used to target vulnerable villages..Calo of the University of Washington says it will be increasingly important for companies to be upfront with their own researchers about how their code might be released. “They need to have the capacity to opt out of projects that offend their sensibilities,” he says..It may prove difficult to deploy the algorithms used in the Darpa contest in real aircraft, since the simulated environment is so much simpler. There is also still much to be said for a human pilot’s ability to understand context and apply common sense when faced with a new challenge..Still, the death match showed the potential of AI. After many rounds of virtual combat, the AlphaDogfight contest was won by Heron Systems, a small AI-focused defense company based in California, Maryland. Heron developed its own reinforcement learning algorithm from scratch..In the final matchup, a US Air Force fighter pilot with the call sign “Banger” engaged with Heron’s program using a VR headset and a set of controls similar to those inside a real F-16..In the first battle, Banger banked aggressively in an attempt to bring his adversary into sight and range. But the simulated enemy turned just as fast, and the two planes became locked in a downward spiral, each trying to zero in on the other. After a few turns, Banger’s opponent timed a long-distance shot perfectly, and Banger’s F-16 was hit and destroyed. Four more dogfights between the two opponents ended roughly the same way..Brett Darcey, vice president of Heron, says his company hopes the technology eventually finds its way into real military hardware. But he also thinks the ethics of such systems are worth discussing. “I would love to live in a world where we have a polite discussion over whether or not the system should ever exist,” he says. “If the United States doesnt adopt these technologies somebody else will.”.Updated 8-27-2020, 10:55 am EDT: This story was updated to clarify that Heron Systems is based in California, Maryland, and not the state of California.","Heron developed its own reinforcement learning algorithm from scratch and won a dogfight contest, a competition where AI is used for mission-critical military tasks that were once done only by humans"
203,"Under the changes to planning laws, local discretion over the rate of housebuilding will be removed and central government will “distribute” an annual target, at present 337,000 a year, among local councils. They will then be required to designate enough land to meet the target..Analysis by Lichfields, a planning consultancy, has suggested that outside London much of the new housing will be concentrated in Conservative local authority areas in the suburbs and the shires, rather than in town centres.","The algorithm risks ""levelling down"" city and town centres."
204,"Councils are quietly scrapping the use of computer algorithms in helping to make decisions on benefit claims and other welfare issues, the Guardian has found, as critics call for more transparency on how such tools are being used in public services..It comes as an expert warns the reasons for cancelling programmes among government bodies around the world range from problems in the way the systems work to concerns about bias and other negative effects. Most systems are implemented without consultation with the public, but critics say this must change..The use of artificial intelligence or automated decision-making has come into sharp focus after an algorithm used by the exam regulator Ofqual downgraded almost 40% of the A-level grades assessed by teachers. It culminated in a humiliating government U-turn and the system being scrapped..The fiasco has prompted critics to call for more scrutiny and transparency about the algorithms being used to make decisions related to welfare, immigration, and asylum cases. .The Guardian has found that about 20 councils have stopped using an algorithm to flag claims as “high risk” for potential welfare fraud. The ones they flagged were pulled out by staff to double-check, potentially slowing down people’s claims without them being aware..Previous research by the Guardian found that one in three councils were using algorithms to help make decisions about benefit claims and other welfare issues. .Their research also found that Hackney council in east London had abandoned using data analytics to help predict which children were at risk of neglect and abuse..The Data Justice Lab found at least two other councils had stopped using a risk-based verification system – which identifies benefit claims that are more likely to be fraudulent and may need to be checked..One council found it often wrongly identified low-risk claims as high-risk, while another found the system did not make a difference to its work..Dr Joanna Redden from the Data Justice Lab said: “We are finding that the situation experienced here with education is not unique … algorithmic and predictive decision systems are leading to a wide range of harms globally, and also that a number of government bodies across different countries are pausing or cancelling their use of these kinds of systems..“The reasons for cancelling range from problems in the way the systems work to concerns about negative effects and bias. We’re in the process of identifying patterns, but one recurring factor tends to be a failure to consult with the public and particularly with those who will be most affected by the use of these automated and predictive systems before implementing them.”.The Home Office recently stopped using an algorithm to help decide visa applications after allegations that it contained “entrenched racism”. The charity the Joint Council for the Welfare of Immigrants (JCWI) and the digital rights group Foxglove launched a legal challenge against the system, which was scrapped before a case went to court..Foxglove characterised it as “speedy boarding for white people” but the Home Office said it did not accept that description. “We have been reviewing how the visa application streaming tool operates and will be redesigning our processes to make them even more streamlined and secure,” the Home Office added..Martha Dark, the director and co-founder of Foxglove, said: “Recently we’ve seen the government rolling out algorithms as solutions to all kinds of complicated societal problems. It isn’t just A-level grades … People are being sorted and graded, denied visas, benefits and more, all because of flawed algorithms.”.She said poorly designed systems could lead to discrimination, adding that there had to be democratic debate and consultation with the public on any system that affected their lives before that system was implemented. “These systems have to be transparent, so bias can be identified and stopped.”.The West Midlands police and crime commissioner’s strategic adviser, Tom McNeil, said he was “concerned” businesses were pitching algorithms to police forces knowing their products may not be properly scrutinised. .McNeil said: “In the West Midlands, we have an ethics committee that robustly examines and publishes recommendations on artificial intelligence projects. I have reason to believe that the robust and transparent process we have in the West Midlands may have deterred some data science organisations from getting further involved with us.”.Research from the Royal Society of Arts published in April found at least two forces were using or trialling artificial intelligence or automated decision-making to help them identify crime hotspots – Surrey police and West Yorkshire police..Others using algorithms in some capacity or other include the Met, Hampshire Constabulary, Kent police, South Wales police, and Thames Valley police..Asheem Singh, the RSA thinktank’s director of economics, said: “Very few police consulted with the public. Maybe great work is going on but police forces don’t want to talk about it. That is concerning. We are talking about black-box formulae affecting people’s livelihoods. This requires an entire architecture of democracy that we have not seen before.”.The Centre for Data Ethics and Innovation, an independent advisory body, is reviewing potential bias in algorithms. “Our review will make recommendations about how police forces and local authorities using predictive analytics are able to meet the right standards of governance and transparency for the challenges facing these sectors,” it said.","After a lack of transparency in welfare decisions, 20 councils stopped using computer algorithms"
205,"All results here are preliminary and have yet to undergo external peer-review. The findings in this article should not be used to guide clinical decision-making, nor do these findings identify a definitive treatment for COVID-19..Our work aims to discover the underlying associations between amino acid sequences of viral proteins and antiviral agents that are effective against them using the artificial intelligence technology of artificial neural networks (ANN). We then use the patterns uncovered by our ANN to identify potential antiviral agents that may be effective against comparable amino acid sequences found in SARS-CoV-2, the virus at the centre of the worldwide COVID-19 pandemic. We used public data sources to make a dataset that pairs amino acid sequences with antivirals known to associate with defined viral amino acid sequences. This dataset served to train long short-term memory networks (LSTM) and convolutional neural networks (CNN). Preliminary results from our AI model produce outputs of possible safe-in-human drug candidates for treating SARS-CoV-2, and thus merit further investigation. Our preliminary results suggest Brincidofovir, Tilorone, Rapamycin, Artesunate, Cidofovir, Valacyclovir, Lopinavir and Ritonavir are of notable interest given that some of these results complement recent findings from noteworthy clinical studies, such as the “Triple combination of interferon beta-1b, lopinavir–ritonavir, and ribavirin in the treatment of patients admitted to hospital with COVID-19: an open-label, randomized, Phase II trial”, recently published in The Lancet..Artificial intelligence (AI) technology is a recent addition to bioinformatics that shows much promise in streamlining the discovery of pharmacologically active compounds (Stephenson et al., 2019). The subdomain of AI, machine learning, provides particular benefits in identifying how drugs effective in one context might have utility in an unknown clinical context or against a novel pathology (Napolitano et al., 2013). The technology works by finding patterns in how a pharmaceutical molecule exerts its activity by binding to defined regions of a biomolecule, such as a segment of a protein..Past research now provides a sizable bank of information concerning drug-biomolecule interactions. Training machine learning models with these findings can uncover patterns, patterns which then serve to make inferences and predict future outcomes. Using drug repurposing as an example, we can now train machine learning algorithms to identify patterns in how antiviral compounds bind to proteins from diverse virus species. We aim to train an AI model so that when presented with the proteome of a novel virus, it will identify the presence of protein segments that are similar to those identified in past studies. The final output from the AI model is a best-fit prediction as to which known antivirals are likely to associate with those familiar protein segments..The application of AI in biomedical research provides new means to conduct in-silico exploratory studies and high-throughput analyses using information already available. In addition to deriving more value from past research, researchers can develop AI technology in relatively short periods of time..These benefits are of particular interest for the current COVID-19 health crisis. The novelty of the SARS-CoV-2 virus requires that we execute health interventions based on past observations. Grappling with an unforeseen pandemic with no known treatments or vaccines and when every passing day is met with the loss of thousands of lives, time is a precious commodity in short supply..The potential for rapid innovation from AI technology is of utmost significance. The ability to conduct many complex analyses with AI enables us to research insights quickly that can help steer us in the right direction for future studies likely to produce fruitful results. Predictions made by AI also can provide complementary evidence when paired with less-robust studies that are faster and more practical to complete. This can offer greater support for sound decision-making as we wait to complete lengthy, though necessary and rigorous, clinical trials for therapeutics and vaccines. As a company specializing in AI, we present here our attempts to develop AI models that can guide efforts to repurpose current antiviral drugs as therapeutics against COVID-19..We used two main data sources for this investigation. The first database was the DrugVirus database (Andersen et al., 2020); DrugVirus provides a database of broad-spectrum antiviral agents (BSAAs) and the associated viruses they inhibit. The database covers 83 virus species and 126 antiviral compounds, providing information on the status of each compound-virus pairing. These statuses fall into eight categories representing the progressive experimental to clinical drug trial phases: Cell cultures/co-cultures; Primary cells/organoids; Animal model; Clinical trial Phase I; Phase II; Phase III; Phase IV and Approved..The second database is the National Center for Biotechnology Information (NCBI) Virus Portal; as of April 2020, this database provides approximately 2.8 million protein (amino acid) and 2 million nucleotide sequences from viruses with humans as hosts. Each row of this database contains an amino acid sequence specimen from a study, as well as metadata that includes the associated virus species..By merging these two databases, we paired each amino acid sequence with the list of antivirals identified as effective against the species from which the amino acid sequence originates. We considered a drug-virus pair as “effective” only if it has attained Phase II or further drug trials, signifying some success with human subjects..Upon inspection of the data, we found several duplicate or near-identical viral amino acid sequences, which is expected given that we expect our models to exploit these similarities between sequences from the same virus species. To reduce this exploitability and pose a more challenging problem, we removed the duplicate sequences that belonged to the same species and had the exact same length. This reduced the size of the dataset by approximately 98%. This duplicate removal process also reduced the number of SARS-CoV-2 amino acid sequences from 481 to 98..Our main database also contained a class imbalance in the number of times certain virus species appeared in the database. We oversampled rare viruses (e.g., West Nile virus: 175 sequences) and excluded the very rare species which compose less than 0.5% of the available unique samples in the dataset (e.g., Andes virus: 4 sequences), and undersampled the common viruses (e.g., Hepatitis C: 16,040 sequences). This produced a more modest database of 30,479 amino acid sequences, with each virus having samples in the 400–900 range. We kept the size of the dataset small both to enable easier model training and validation in early iterations and to handle data imbalance more smoothly..The class imbalance problem also presented itself in the antiviral compounds. Even with balanced virus classes, the number of times each drug occurred within the dataset varied greatly because some drugs are broad-spectrum and thus apply to more viruses than others. To alleviate this, we computed class weights for each drug as a dict, which we then provided to the models in training. This enabled a fairer assessment and a more varied distribution of drugs in predicted outputs..The final step of data processing involved generating the training and validation sets. We decided to set up the splits in datasets in two different ways, resulting in two different experimental setups. Experiment I is based on a standard, randomized an 80% training/20% validation split on the main dataset..For a more “challenging” setup, which we refer to as Experiment II, we split the data on virus species; in this case, we forced our models to predict drugs for a species that it was not trained on and to determine familiar substructures in the protein sequences in order to suggest drugs. In this latter setup, we also guaranteed that the COVID-19 sequences were always in the test set in addition to three other viruses randomly picked from the dataset..A growing number of studies demonstrate the success of using artificial neural networks (ANN) in evaluating biological sequences in drug repositioning and repurposing (see Donner, Kazmierczak, and Fortney, 2018; Zeng et al., 2019). Previous work on training neural networks on nucleotide or amino acid sequences have been successful with recurrent models such as gated recurrent units (GRU), long short-term memory networks (LSTM) and bidirectional LSTMs (biLSTM), as well as 1D convolutions and 2D convolutional neural networks (CNN) (Lee and Nguyen, 2016; Hou, Adhikari, and Cheng, 2018). We focus here on these network architectures, where we conducted our experiments with an LSTM with 1D convolutions and bidirectional layers, as well as two CNNs that differ in their embedding layers. Our CNN results were obtained from the CNN without the embedding layer due to performance advantages..A tokenizer (keras.preprocessing.text.Tokenizer) was used to encode the FASTA sequences (which are sequences of chars) into vectors consumable by the network. The sequences were then padded with zeros or cut off to a fixed length 500 to maintain a fixed input size..Once trained, we save the model and send it to the Zetane environment, where we can inspect the model architecture and filters in an interactive setting (Figure 2 provides a video and image summary of the LSTM model):.For the CNN, the input features were one-hot encoded using methods similar to the tokenizer approach where each amino acid sequence is assigned an integer, except the integers are predetermined by the order of the FASTA alphabet/charset: this assists in interpretability when examining the 2D input arrays as images. The inputs are also fixed at a length of 500 amino acid characters, resulting in 500x28 images, where 28 is the number of elements in the FASTA charset..We again save the trained model and send it to the Zetane environment, where we can inspect the model architecture and filters in an interactive setting, as demonstrated in Figure 4 (a video of the model is available here):.We conducted the experiments using Keras models & TensorFlow. We used binary cross entropy (BCE) loss, Adam optimizer, and precision & recall as metrics since accuracy tends to be an unreliable metric given the class imbalance and the sparse nature of our outputs. After training and validation, predictions were done on the validation set and the results were post-processed for interpretability. In post-processing, we applied a threshold to the sigmoid function outputs of the neural network, where we assigned each drug a probability of being a potential antiviral for a given amino acid sequence. After experimenting with different values, we settled on a threshold value of 0.2..Post-processing outputs a list of drugs that were selected along with the respective probabilities of the drugs being “effective” against the virus with the given amino acid sequence..Note that in regard to this first iteration of experiments, the scope of hyperparameter tuning, as well as the ANN architectures, are limited. There is room for improvement on this front..In the regular setup, we performed an 80%/20% train-test split on our data of 30,479 amino acid sequences. We base our best results on validation F1-score and plots of relevant metrics, which appear in Figure 7..Our models handled the regular task successfully, achieving 0,958 F1-score in a multi-label multi-class problem setting. This means that the models aptly recognize the species of the virus from the amino acid sequence of protein substructures and appropriately assign the inhibiting antiviral drugs with great accuracy. These satisfactory results led to us implementing Experiment II..In this more challenging setup, we asked the models to predict inhibiting drugs for virus species that are absent from the training dataset. This meant the models were unable to recommend drugs by “recognizing” the virus from the amino acid sequence, and therefore had to rely only on protein substructures in the sequences in assigning drugs. In the results summarized in Figure 8, the test set consists of COVID-19, Herpes simplex virus 1, Human Astrovirus and Ebola virus, whose sequences were removed from the training set..We see here that the CNN had issues with convergence.The accuracies are clearly below their counterparts in the regular setup, though this is certainly expected. We now turn to the actual predictions on the sequences and attempt to interpret them..Examine, for instance, the drug predictions for Herpes simplex virus 1 (HSV-1). Here we see that our CNN is quite successful. In the DrugVirus database, all drugs that have seen Phase II or further drug trials for the virus are the most predicted by the CNN, which we consider very encouraging given that our model has not seen HSV-1 sequences before (see Table 3)..With some variation between the two, both the LSTM and the CNN seem to converge on a number of drugs: ritonavir, lopinavir (both Phase III for MERS-CoV) and tilorone (Approved for MERS-CoV) are the top three candidates in both, while brincidofovir, rapamycin, cidofovir, valacyclovir and ganciclovir rank high up in both lists..Most of the remaining drugs are present in both lists as well. The LSTM is more conservative in its predictions than the CNN, and the overall counts for SARS-CoV-2 are significantly lower than for Herpes simplex virus 1 for both, pointing a relative lack of confidence on the models’ part in predicting SARS-CoV-2 sequences..By merging common techniques in machine learning and biomedical sciences, the research community gains additional strategies to advance innovation in health at ever-growing speed. We provide but one example of numerous and diverse studies underway across the globe happening in a concerted effort to mitigate a sudden public health crisis. Here we attest to the notable strengths of employing AI to identify tentative antivirals. With limited resources, our team at Zetane Systems with expertise in machine learning, software development and biomedical sciences were able to make use of public datasets to advance knowledge of possible treatments for a previously unknown virus — all within a month-and-a-half of concerted effort. Such speed and resourcefulness demonstrate how small research groups can implement AI technologies during periods of rapid change and uncertainty..We note that the current discussion of our results will be narrow; time constraints limit our abilities to evaluate our findings following an exhaustive review of the scientific literature concerning antiviral treatments for coronaviruses. Instead, we compare our findings to a handful of recent publications in order to demonstrate the strengths and limitations of this study. Regardless, the preliminary results herein show promise and merit further investigation..To begin, we note that our AI models predict that some antivirals that show promise as treatments against MERS-CoV may also be effective against SARS-CoV-2. These include the broad-spectrum antiviral tilorone (Ekins et al., 2020) and the drug lopinavir (Yao et al., 2020), the latter of which is now in Phase 4 clinical trials to determine its efficacy against COVID-19 (Basha 2020). This makes sense given that both are coronaviruses and thus share a high degree of similarity in their genetic sequences and protein structures. Such observations suggest with much confidence that our AI models can recognize reliable patterns between particular antivirals and species of viruses containing homologous amino acid sequences in their proteome..Additional observations that support our findings have come to light from a study in The Lancet published a week prior to this article (Hung et al., 2020). This open-label, randomized, phase-2 trial observed that the combined administration of the drugs interferon beta-1b, lopinavir-ritonavir and ribavirin provides an effective treatment of COVID-19 in patients with mild to moderate symptoms. Both of our AI models flagged two of the drugs in that trial (note that interferon was not part of our datasets). In terms of number of occurrences (“Count”), the CNN model ranked ritonavir at 8th place (tied with letermovir) and ribavirin at 9th place; the LSTM model ranked ritonavir at 7th place (tied with lopinavir, cyclosporine and rapamycin) and ribavirin at 9th place (tied with 7 other compounds). Also high on our lists is the antimalarial drug, artesunate, which is now in Phase 2 clinical trials for COVID-19. Such observations are encouraging. They suggest that AI models can have value in identifying potentially therapeutic compounds that merit priority for advanced clinical trials. These observations add to growing observations that support using AI technology to streamline drug discovery. From that perspective, our AI models suggest that the broad spectrum antiviral brincidofovir, for instance, may be a top candidate for COVID-19 clinical trials in the near future..The list of promising antivirals identified here has some notable discrepancies with emerging research findings. For instance, our AI models did not highlight the widely available anti-parasitic drug ivermectin. One research study — published while completing this manuscript — observed that ivermectin could inhibit the replication of SARS-CoV-2 in vitro (Caly et al., 2020). Further investigations will need to assess how in silico studies using AI compare to other research methods; indeed, no one research method is the be-all-end-all..Another study providing a large-scale drug repositioning survey for SARS-CoV-2 antivirals (Riva et al. 2020) demonstrates that predictions made by our AI models have notable limits. This study screened a library of nearly 12 000 drugs and identified six candidate antivirals for SARS-CoV-2 that merit further clinical evaluation. These include the PIKfyve kinase inhibitor Apilimod, cysteine protease inhibitors MDL-28170, Z LVG CHN2, VBY-825, and ONO 5334, and the CCR1 antagonist MLN-3897. It comes as no surprise that our AI models did not identify these six compounds because our datasets did not contain them. Here we observe a well-known fact concerning AI: the technology is as good as the data used to build it. Future efforts to strengthen our AI models will thus require us to include a growing bank of novel data from emerging research findings into our model training protocols..In terms of our AI models, better feature extraction can improve predictions drastically, enabling our models to detect finer protein subsequences which might associate well with antivirals. This step involves improvements through better data engineering and working with domain experts who are familiar with applied bioinformatics to better understand the nature of our data and find ways to improve our data processing pipeline. Here are proposals for future work that could strengthen the performance of our AI model:.We welcome constructive feedback concerning this study and offer an open call to partner with organizations that would like to use our AI technology to advance their own research. Time is of the essence in our efforts to counter COVID-19. The rapid results obtained here and knowing that numerous investigations are underway should provide us with some solace in knowing that treatments for the current pandemic are well within reach. Without question, our concerted efforts will ensure that it’s going to be okay, or as we say in our neck of the woods, ça va bien aller..The authors declare they will not obtain any direct financial benefit from investigating and reporting on any given pharmaceutical compound. The following study is funded by the authors’ employer, Zetane Systems, which produces software for AI technologies implemented in industrial and enterprise contexts..Cantürk wrote the manuscript; Cantürk, Singh and St-Amant conducted all research and developed the AI technology; Behrmann completed an internal peer-review and brief literature review, edited and contributed to the writing of the final manuscript..We would like to thank the administrators of the DrugVirus and the NCBI Virus Portal for providing the datasets that are central to this study. We appreciate comments on preliminary drafts of this manuscript from Dr Tariq Daouda from the Massachusetts General Hospital, Broad Institute, Harvard Medical school..[1] N. Stephenson et al., “Survey of Machine Learning Techniques in Drug Discovery,” Current Drug Metabolism, vol. 20, no. 3, pp. 185–193, 2019, doi: 10.2174/1389200219666180820112457.[2] F. Napolitano et al., “Drug repositioning: a machine-learning approach through data integration,” Journal of Cheminformatics, vol. 5, no. 30, June 2013, doi: 10.1186/1758–2946–5–30.[3] P. I. Andersen et al., “Discovery and development of safe-in-man broad-spectrum antiviral agents,” International Journal of Infectious Diseases, vol. 93, pp. 268–276, Feb. 2020, doi: 10.1016/j.ijid.2020.02.018.[4] Y. Donner, S. Kazmierczak, and K. Fortney, “Drug Repurposing Using Deep Embeddings of Gene Expression Profiles,” Mol. Pharmaceutics, vol. 15, no. 10, pp. 4314– 4325, Oct. 2018, doi: 10.1021/acs.molpharmaceut.8b00284.[5] X. Zeng, S. Zhu, X. Liu, Y. Zhou, R. Nussinov, and F. Cheng, “deepDR: a network-based deep learning approach to in silico drug repositioning,” Bioinformatics, vol. 35, no. 24, pp. 5191–5198, Dec. 2019, doi: 10.1093/bioinformatics/btz418..[7] J. Hou, B. Adhikari, and J. Cheng, “DeepSF: deep convolutional neural network for mapping protein sequences to folds,” Bioinformatics, vol. 34, no. 8, pp. 1295–1303, Apr. 2018, doi: 10.1093/bioinformatics/btx780 ..[8] S. Ekins, TR. Lane, PB. Madrid, “Tilorone: a Broad-Spectrum Antiviral Invented in the USA and Commercialized in Russia and beyond,” Pharm Res, vol. 37, no. 4, March 2020, doi: 10.1007/s11095–020–02799–8.[9] TT. Yao, JD. Qian, WY. Zhu, Y. Wang, GQ. Wang, “A systematic review of lopinavir therapy for SARS coronavirus and MERS coronavirus — A possible reference for coronavirus disease‐19 treatment option,” Journal of Medical Virology, vol. 92, no. 6, pp. 556–563, February 2020, doi: 10.1002/jmv.25729.[10] SH. Basha, “Corona virus drugs: a brief overview of past, present and future,” Journal of PeerScientist, vol. 2, no. 2, April 2020, doi: 10.5281/zenodo.3747641.[11] I. FN. Hung, KC. Lung, E. YK. Tso, R. Liu, T. WH. Chung, MY. Chu et al. “Triple combination of interferon beta-1b, lopinavir–ritonavir, and ribavirin in the treatment of patients admitted to hospital with COVID-19: an open-label, randomised, phase 2 trial,” The Lancet, early release, May 2020, 10.1016/S0140–6736(20)31042-4.[12] L. Caly, JD. Druce, MG. Catton, DA Jans, KM Wagstaff, “ The FDA-approved drug ivermectin inhibits the replication of SARS-CoV-2 in vitro,” Antiviral Research, vol. 178, pp. 104787, June 2020, doi: 10.1016/j.antiviral.2020.104787.[13] L. Riva et al., “A Large-scale Drug Repositioning Survey for SARS-CoV-2 Antivirals” bioRxiv, pre-print, April 2020, doi: 10.1101/2020.04.16.044016.[15] C. Wu, M. Berry, S. Shivakumar, and J. McLarty, “Neural networks for full-scale protein sequence classification: Sequence encoding with singular value decomposition,” Mach Learn, vol. 21, no. 1–2, pp. 177–193, 1995, doi: 10.1007/BF00993384."," This team used machine learning to find drug repurposing for SARS-Co-V-2. The findings in this article should not be used to guide clinical decision-making, nor do these findings identify a definitive treatment for COVID-19."
206,"Gavin Williamson and Ofqual have apologised to students and their parents, as they announced that all A-level and GCSE results in England will be based on teacher-assessed grades..In a spectacular U-turn, the education secretary announced the government would scrap the controversial standardisation model drawn up by the exams regulator to award grades in lieu of exams..“We worked with Ofqual to construct the fairest possible model, but it is clear that the process of allocating grades has resulted in more significant inconsistencies than can be resolved through an appeals process,” Williamson said, in remarks released by his department..“We now believe it is better to offer young people and parents certainty by moving to teacher assessed grades for both A- and AS level and GCSE results. I am sorry for the distress this has caused young people and their parents, but hope this announcement will now provide the certainty and reassurance they deserve.”.The climbdown comes after days of turmoil triggered by the publication of A-level results last Thursday, when almost 40% of predicted results were downgraded, with some students marked down two or even three grades, which resulted in many losing university places..The move had looked inevitable after Wales said it would revert to teacher assessments for A-levels, and Northern Ireland said it would do so for GCSEs, following a similar U-turn in Scotland last week..“The system, for the overwhelming majority of young people, is going to deliver credible, strong results. It’s a robust system, it’s a fair system, it’s making sure that young people get the grades that they’ve worked so hard towards”.Q) “Can you give a cast-iron guarantee that you will not be forced into the embarrassing U-turn that John Swinney and Nicola Sturgeon were in Scotland?”A) Absolutely.“Let’s be in no doubt about it, the exam results that we’ve got today are robust. They’re good, they’re dependable for employers. It’s very important that for years to come people should be able to look at these grades and think these are robust, these are dependable”.“This is it… No U-turn, no change… [In Scotland] you’ve got a system where there aren’t any controls, you’ve got rampant grade inflation. There’s been no checks and balances in that system; it degrades every single grade as a result and in-baked unfairness” .Hundreds of pupils took to the streets of London, demonstrating outside the Department for Education to express their anger, while others took to the airwaves and social media to describe their sense of devastation. Lawyers had began to consider taking action on behalf of affected teenagers..The Labour leader, Keir Starmer, welcomed what he called the “screeching U-turn”, saying: “This is a victory for the thousands of young people who have powerfully made their voices heard this past week. However, the Tories’ handling of this situation has been a complete fiasco..“Incompetence has become this government’s watchword, whether that is on schools, testing or care homes. Boris Johnson’s failure to lead is holding Britain back.”.Ministers had been under increasing pressure to act with GCSE results due to be published this Thursday for more than 600,000 year 11 pupils in England and 100,000 older students aiming for crucial passes in maths and English to qualify them for further training or study..The algorithm used by Ofqual for both A-levels and GCSEs was mainly based on a school’s past results and individual pupil attainment. Teachers and schools were asked to submit grades, known as centre-assessed grades, but in the end these were influential in only a small number of cases..Ofqual argued that the algorithm was essential to ensure results were standardised across the country and in line with previous years, but hundreds of individual stories documenting disappointment and an overwhelming sense of injustice among those affected proved too much to ignore..Ofqual confirmed its decision to award centre assessment grades (CAG) to A-level, AS-level and GCSE students, adding that where a moderated grade was higher than a CAG, students could keep that grade..Roger Taylor, Ofqual’s chair, acknowledged the standardised system had caused widespread anguish and damaged public confidence, and apologised to those affected..“We understand this has been a distressing time for students, who were awarded exam results last week for exams they never took. The pandemic has created circumstances no one could have ever imagined or wished for. We want to now take steps to remove as much stress and uncertainty for young people as possible, and to free up heads and teachers to work towards the important task of getting all schools open in two weeks,” he said..“After reflection, we have decided that the best way to do this is to award grades on the basis of what teachers submitted. The switch to centre assessment grades will apply to both AS- and A-levels and to the GCSE results which students will receive later this week..“There was no easy solution to the problem of awarding exam results when no exams have taken place. Ofqual was asked by the secretary of state [Gavin Williamson] to develop a system for awarding calculated grades, which maintained standards and ensured that grades were awarded broadly in line with previous years. Our goal has always been to protect the trust that the public rightly has in educational qualifications..“But we recognise that while the approach we adopted attempted to achieve these goals, we also appreciate that it has also caused real anguish and damaged public confidence. Expecting schools to submit appeals where grades were incorrect placed a burden on teachers when they need to be preparing for the new term and has created uncertainty and anxiety for students. For all of that, we are extremely sorry..“The path forward we now plan to implement will provide urgent clarity. We are already working with the Department for Education, universities and everyone else affected by this issue.”.“It’s a relief that this whole unedifying mess has some form of closure,” said Jules White, a secondary school headteacher and leader of Worth Less? “Students will get a much fairer deal and everyone can be pleased with that..“Major questions remain, though: why has it taken the DfE so long to resolve matters? Why have Ofqual and the DfE just spent time blaming each other rather than acting on behalf of children, their families and schools?.“And crucially, how can we move confidently to wider school reopening when our political masters don’t understand how schools actually work?”.Paul Whiteman, the general secretary of the NAHT school leaders’ union, said: “The government has decided to rely on centre assessed grades for A level and GCSE results this year, agreeing that these are the most reliable measure of student performance..“School leaders and teachers worked with professionalism and integrity to submit these grades for all of their students. Having taken so long to make a decision, this was the only option that government had left to deal with the unfairness.“This decision will mean students expecting their GCSE results can have confidence that they will not experience the same unfairness or disadvantage as their older peers.“The big question remains as to why this decision has taken so long to come, as it may already be too late for some A level students who have already missed out on their first choice of university and course. Every day of delay is going to have loaded more and more difficulty onto universities and their capacity to meet all of the demand for places that will now inevitably come their way. For them, the problem is far from over.”","After 40% of predicted results were downgraded, the government decided to award grades on the basis of what teachers submitted."
207,"The commission-free investing app is facing a civil fraud investigation over its early failure to fully disclose its practice of selling clients’ orders to high-speed trading firms, The Wall Street Journal reported Wednesday..High-speed trading is the practice of using computer programs to transact a large number of orders in fractions of a second. The Securities and Exchange Commission is in an advanced stage and the company could have to pay a fine exceeding $10 million if it agrees to settle with the agency, unnamed sources told the Journal..Until 2018, Robinhood reportedly did not disclose it took payments from high-speed trading firms for sending them customers’ orders to buy or sell stocks or options..Its not the first time the company has been in hot water. In its seven year history, Robinhood has faced several outages and canceled its plans to expand to the U.K..A disrupter in the industry, Robinhood’s popularity forced competitors, like Charles Schwab and TD Ameritrade, to emulate its commission-free approach.",Robinhood utilized high-speed trading (using computer programs to transact a large number of orders in fraction of seconds) without fully disclosing to clients
208,"We are witnessing the slow death of the wife in contemporary society—at least the wife we’ve known as the longtime backbone of patriarchal society. But she’s having an enthusiastic comeback, with a few critical upgrades. It’s not wives themselves who are being asked to come back into the kitchen, but rather feminized artificial intelligence built into robots, digital voice assistants (also known as smart speakers, conversational agents, virtual helpers, and chatbots), and other smart devices..The smart wife comes in many forms. In fact, chances are you’re already living with her. Most obvious are assistants such as Amazon’s Alexa, Apple’s Siri, or Google Home, which have default female voices in most markets. Other smart wives are anthropomorphic, zoomorphic, or automated (such as home appliances or domestic robots)—most of which carry out domestic responsibilities that have traditionally fallen to wives. Smart wives can also be found in the bodies of overtly feminized and sometimes “pornified” sex robots or gynoids..So who wants a smart wife? Potentially everyone. In 2016, the research firm Gartner predicted people would soon be having more conversations with bots than their spouses. More than a quarter of the adult population in the United States now owns at least one smart speaker like Alexa; that’s more than 66 million people..When Siri made her debut in 2011 as “a sassy young woman who deflected insults and liked to flirt and serve users with playful obedience,” as a UNESCO report on closing gender divides in digital skills put it, her “coming-out party” reached nearly 150 million iPhones in her first year. This single technology—developed behind closed doors by one company in one corner of the world with little input from women—shaped global expectations for smart wives and A.I. assistants more broadly in a little over 12 months..In terms of gendered interest and uptake, industry sales figures show that consumers of smart home devices are more likely to be male, and “smart home obsessives” are invariably men. Men are also more often the instigators for bringing smart home technologies into the home and managing their operation. However, women (and the significant percentage of the world’s population that is not heterosexual men) need (smart) wives too. Millennial women in the United States, ages 18 to 35, are particularly excited about smart home technology, and the occasional report finds that women are actually more interested in some devices than men are, such as voice assistants and some smart appliances..Narrowing down to specific markets reveals other gender differences in interest, uptake, and benefits. The vast majority of people currently interested in or buying sex robots (and dolls) are men. Women are understandably less enthusiastic about the penetration-oriented characteristics of most current offerings. By contrast, in the present social robot market, women stand to benefit most given that they live longer than men and therefore are more likely to suffer from debilitating conditions like dementia—which is one of the emerging applications for care robots..When it comes to the creation of smart wives, men are clearly in the lead. Men vastly outnumber women in computer programming jobs, making up over 75 percent of the total pool of programmers in the United States in 2017. In the field of robotics and A.I., men outnumber women as well. Men make up between 77 and 83 percent of the technical positions at Apple, Facebook, Microsoft, Google, and General Electric, and just over 63 percent at Amazon. Men make up 85 percent of the A.I. research staff at Facebook and 90 percent at Google. Likewise, in academic environments, more than 80 percent of A.I. professors are men, and only 12 percent of leading A.I. researchers are women..Indeed, computer science has gone backward on gender diversity in the past 30 to 40 years, with female participation in computer science degrees in the U.S. dropping from 37 percent in the early 1980s to 18 percent in 2016, despite a number of active campaigns and initiatives to try to turn this around. As a UNESCO report on closing gender divides in digital skills depressingly puts it, “The digital space is becoming more male-dominated, not less so.”.So potent is the gendered imbalance in computing that the journalist, producer, and author Emily Chang labeled the coding culture of Silicon Valley a “Brotopia.” In addition, the A.I. industry has been called out by leading academics and commentators like Kate Crawford and Jack Clark for having a “white guy problem” in an industry characterized by a “sea of dudes.” Indeed, the A.I. Now Institute has identified a “diversity crisis” perpetuated by harassment, discrimination, unfair compensation, and lack of promotion for women and ethnic minorities. The institute recommends that “the A.I. industry needs to make significant structural changes to address systemic racism, misogyny, and lack of diversity.”.This gender and racial imbalance filters down to the ways in which technologies are imagined and created. Scholars such as Safiya Umoja Noble have written about the “algorithms of oppression” that characterize search engines like Google, which reinforce racism and sexism. There has also been considerable criticism leveled at digital voice assistants like Alexa, Siri, and Google Home, and other types of smart wives, for their sexist overtones, diminishing of women in traditional feminized roles, and inability to assertively rebuke sexual advances..For example, Microsoft’s Cortana and Mycroft assistants take their names as well as identities from gamer and sci-fi culture, respectively, both of which have been widely critiqued as highly sexist domains. Likewise, assistants like Microsoft’s Ms. Dewey and Facebook’s Moneypenny (both now retired from service) were sexually suggestive and flirtatious by reputation (Moneypenny is named for the coy secretary and romantic interest in the James Bond novels and films) or through their coded behavior. This gendering of smart tech all makes a particular set of (mostly) men’s ideas about home, wives, domestic responsibilities, and sexual desires deeply relevant. And it potentially excludes a lot of other people for whom these ideas don’t resonate—including, we should note, many men..An eerie example of the feminization of smart technologies is found in the Japanese digital voice assistant named Azuma Hikari developed by the company Vinclu as part of its Gatebox technology and entering mass production in 2019. This cutesy smart wife is targeted toward the country’s single residents, now the largest segment of the population. Described by her creators as a “bride character,” Hikari is a virtual anime hologram, with blue hair and matching outfits, who lives in a glass tube about 30 centimeters high and 10 centimeters wide. She is depicted as a 20-year-old woman, with a schoolgirl-ish and upbeat personality. Hikari wears a short skirt and over-the-knee socks and has a high-pitched voice supplemented with coquettish giggles. In several promotional videos, she takes care of a lonely, hardworking young Japanese man. Her key role is to greet her “master” with excitement when he comes home, and check on him during the day by sending helpful messages such as “come home early.” She also provides timekeeping services and weather advice, turns off the lights when her master leaves the house, adjusts the home’s heating and cooling, and remembers their anniversary..Hikari is an ideal smart wife (or girlfriend), doting on her man’s needs. She is also useful and efficient, helping men keep their schedules on track. But if she does too good a job, the singles population of Japan may not need to look for a human companion—potentially exacerbating the falling birth rates in that country. On this point, the Japanese government has bigger plans for smart wives (as do other nations, like China). Professor of anthropology Jennifer Robertson suggests that there is a push to position social and care robots like Pepper as an opportunity to redirect Japanese women’s time back to the task of having children. Smart wives are thus entangled in social and political agendas about the role of women, wives, and heteronormative relationships in contemporary societies..Sex robots and virtual pornography take these ideas in other tantalizing and potentially troubling directions. U.S. company RealDoll’s Harmony sexbot has 18 customizable feminized personality traits (including jealous, shy, moody, thrilled, and insecure), 42 different nipple options, and different voice selections (including a “saucy Scottish accent”), and she remembers her user’s favorite food (like any good smart wife should). But her true stroke of genius is this: Harmony’s removable and self-lubricating vagina is dishwasher safe. She is smart (with controllable parts and efficient cleaning!) and wifely (devoted to her man’s intimate needs). She is a woman with all the sexy bits, without all the mess or fuss. Harmony is customizable yet uniform, deeply feminine but with masculine efficiency, and there to be enjoyed, consumed, and penetrated. To be clear, this isn’t creepy because we’re talking about a robot (we’re not here to vilify anyone’s kinks) but rather because it embodies a pornographic idea of female sexiness that—in some cases—celebrates nonconsensual sex..This gets to the heart of the strange paradox that characterizes the smart wife: She is simultaneously a dutiful, feminine wife and sexual muse while adeptly solving household problems with technological tools. She is docile and efficient. Compliant and in control. Seductive yet shrewd. Intimate yet distant. She is ready to be played, ready to serve, and able to optimize her domain..On the one hand, the smart wife represents an ingenious solution to the ongoing domestic disputes over the division of labor that plague contemporary households in gender-progressive societies. On the other hand, there is something downright worrying about the smart home and robotic industries’ subtle characterizations of their products as a nostalgic, sometimes porn-inspired wifely figure. This is particularly so because we are trying to move on from these representations of women in most contemporary societies..What’s the problem exactly? For a start, these depictions affect how we treat our devices, robots, and A.I., which in turn are reflected back in how we treat people in general—and women in particular. Friendly and helpful feminized devices often take a great deal of abuse when their owners swear or yell at them. They are also commonly glitchy, leading to their characterization as ditzy feminized devices by owners and technology commentators—traits that reinforce outdated (and unfounded) gendered stereotypes about essentialized female inferior intellectual ability..Relatedly, a smart wife gone wrong is the central plotline of many sci-fi movies featuring feminized A.I., such as Joanna in The Stepford Wives, Vanessa in Austin Powers, and Ava in Ex Machina. These women are typically sexualized, demure, and slightly dysfunctional, yet ready to retaliate against their male makers, owners, and enslavers. The plotlines of these highly entertaining films consistently reinforce the cliché that the perfect woman is an artificial one—as long as she doesn’t have too much control or power, as then she will rebel, kill, or enslave her makers. Ironically, it’s often the lonely techies in these films and stories who fall for these femme fatales and suffer the effects. Yet despite their problems (or perhaps because of them), these on-screen and imperfect smart wives are often identified as the design inspiration and source code for those now entering our homes..We know from research carried out in the fields of robotics, human-computer interaction, and psychology that humans assign emotional as well as personal traits to computers. A smart wife precedent for this was set in 1966, when founding computer scientist Joseph Weizenbaum created the first chatbot, named ELIZA. This fembot, which performed natural language processing, was cast in the role of psychiatrist and worked by posing questions based on Rogerian psychotherapy back to her “clients” (such as “And how does that make you feel?”). Weizenbaum was surprised and later dismayed to discover how intimately his colleagues related to ELIZA, and the emotional connections they quickly formed with this artificial therapist..Indeed, according to the late Clifford Nass and his collaborator Corina Yen, experts in the fields of human-computer interaction and user experience design, the success and failure of interactive computer systems depends on whether we like them, and how well they treat us..This is partly because people have a tendency to humanize devices and assign them with genders, even when they don’t have one. Sherry Turkle, a professor of the social studies of science and technology, has pioneered research on people’s relationships with technology—especially mobile technology, social networking, and sociable robotics. She has found that the boundary between humans and machines is weakening, affecting how we understand and relate to one another, and leading to some troubling outcomes, such as reducing our communication with other people and making us feel lonelier than ever..Other examples demonstrate how our connections and interactions with inanimate and animate devices are gendered. Consider satellite navigation systems. Most of us prefer the female voice because we consider it to be warmer and more pleasant than a male one (the same holds true for smart wives). But we also distrust a female-voiced sat-nav (because women are notoriously bad at giving directions, or so the stereotype goes) and are quick to dub her helpfulness as badgering. When a new female voice command system was introduced into jet planes in 2012, U.S. fighter pilots referred to her as “Bitchin’ Betty” for getting louder and sterner when they ignored the system’s commands. These kinds of humanlike assistants provide us with an opportunity to perform and reinforce exaggerated gender stereotypes..Likewise, on the homefront we prefer female voice assistants when their purpose is to discuss love and relationships, or help us with the housework (by adding groceries to the shopping list, for example, or, better still, restocking our fridges and pantries). In other words, we like our assistants to conform to gendered stereotypes. But designing gendered devices can also reinscribe those same stereotypes. When those devices start behaving erratically, or we perceive them to be annoying or acting “dumb,” we associate those characteristics with common gender typecasts..In short, smart wives hark back to nostalgic stereotypes that people are now being told (through smart tech marketing) that they deserve and should desire. Sure, overtly gendered smart wives are familiar, cute, sexy, friendly, and “easy to use”—but at what cost to society?.Left as they are, by and large smart wives serve a patriarchal capitalist system, which positions women as useful and efficient commodities, upholds (and promotes) gendered and sexual stereotypes, and paints men as boys who enjoy playing with toys. Of course, we don’t think that every man who is interested in designing or using smart technology is a misogynist or misogamist. We mean that the smart wife works with a narrow range of stereotypes that are potentially damaging for all genders. We aren’t man or tech haters, nor are we anti-technosexuals (defined as those who include the mechanical within their boundaries of sexuality). But we are killjoys of the smart wife as she is currently programmed, and our agenda is simple. We’re here to give her a reboot.. Future Tense     is a partnership of     Slate,     New America, and     Arizona State University     that examines emerging technologies, public policy, and society. ",Various technologies represent stereotypes about women
209,"This report examines algorithmic technologies that are designed for use in criminal law enforcement systems. Algorithmic policing is an area of technological development that, in theory, is designed to enable law enforcement agencies to either automate surveillance or to draw inferences through the use of mass data processing in the hopes of predicting potential criminal activity. The latter type of technology and the policing methods built upon it are often referred to as predictive policing. Algorithmic policing methods often rely on the aggregation and analysis of massive volumes of data, such as personal information, communications data, biometric data, geolocation data, images, social media content, and policing data (such as statistics based on police arrests or criminal records)..In order to guide public dialogue and the development of law and policy in Canada, the report focuses on the human rights and constitutional law implications of the use of algorithmic policing technologies by law enforcement authorities. This report first outlines the methodology and scope of analysis in Part 1. In Part 2, the report provides critical social and historical contexts regarding the criminal justice system in Canada, including issues regarding systemic discrimination in the criminal justice system and bias in policing data sets. This social and historical context is important to understand how algorithmic policing technologies present heightened risks of harm to civil liberties and related concerns under human rights and constitutional law for certain individuals and communities. The use of police-generated data sets that are affected by systemic bias may create negative feedback loops where individuals from historically disadvantaged communities are labelled by an algorithm as a heightened risk because of historic bias towards those communities. Part 3 of the report then provides a few conceptual building blocks to situate the discussion surrounding algorithmic policing technology, and it outlines how algorithmic policing technology differs from traditional policing methods..In Part 4, the report sets out and summarizes findings on how law enforcement agencies across Canada have started to use, procure, develop, or test a variety of algorithmic policing methods. The report compiles original research with existing research to provide a comprehensive overview of what is known about the algorithmic policing landscape in Canada to date. In the overview of the use of algorithmic policing technology in Canada, the report classifies algorithmic policing technologies into the following three categories:.The primary research findings of this report show that technologies have been procured, developed, or used in Canada in all three categories. For example, at least two agencies, the Vancouver Police Department and the Saskatoon Police Service, have confirmed that they are using or are developing ‘predictive’ algorithmic technologies for the purposes of guiding police action and intervention. Other police services, such as in Calgary and Toronto, have acquired technologies that include algorithmic policing capabilities or that jurisdictions outside of Canada have leveraged to build predictive policing systems. The Calgary Police Service engages in algorithmic social network analysis, which is a form of technology that may also be deployed by law enforcement to engage in person-focused algorithmic policing. Numerous law enforcement agencies across the country also now rely on a range of other algorithmic surveillance technologies (e.g., automated licence plate readers, facial recognition, and social media surveillance algorithms), or they are developing or considering adopting such technologies. This report also uncovers information suggesting that the Ontario Provincial Police and Waterloo Regional Police Service may be unlawfully intercepting private communications in online private chat rooms through reliance on an algorithmic social media surveillance technology known as the ICAC Child On-line Protection System (ICACCOPS). Other police services throughout Canada may also be using or developing additional predictive policing or algorithmic surveillance technologies outside of public awareness. Many of the freedom of information (FOI) requests submitted for this report were met with responses from law enforcement authorities that claimed privilege as justification for non-disclosure; in other cases, law enforcement agencies did not provide any records in response to the submitted FOI request, or requested exorbitant fees in order to process the request..Building on the findings about the current state of algorithmic policing in Canada, Part 5 of the report presents a human rights and constitutional law analysis of the potential use of algorithmic policing technologies. The legal analysis applies established legal principles to these technologies and demonstrates that their use by law enforcement agencies has the potential to violate fundamental human rights and freedoms that are protected under the Canadian Charter of Rights and Freedoms (“the Charter”) and international human rights law. Specifically, the authors analyze the potential impacts of algorithmic policing technologies on the following rights: the right to privacy; the right to freedoms of expression, peaceful assembly, and association; the right to equality and freedom from discrimination; the right to liberty and to be free from arbitrary detention; the right to due process; and the right to a remedy. The major findings of this analysis are presented as follows:.In addition to these major findings, the report documents problems that are likely to arise with respect to meaningful access to justice and the rights to due process and remedy, given that impactful accountability mechanisms for algorithmic policing technology are often lacking, and in light of the systemic challenges faced by individuals and communities seeking meaningful redress for rights violations that do not result in charges in Canadian courts. The absence of much needed transparency in the Canadian algorithmic policing landscape animates many of the core recommendations in this report. The authors hope that this report provides insight into the critical need for transparency and accountability regarding what types of technologies are currently in use or under development and how these technologies are being used in practice. With clarified information regarding what is currently in use and under development, policy- and lawmakers can enable the public and the government to chart an informed path going forward..In response to conclusions drawn from the legal analysis, the report ends with a range of recommendations for governments and law enforcement authorities with a view to developing law and oversight that would establish necessary limitations on the use of algorithmic policing technologies. Part 6 provides a list of these recommendations, each of which is accompanied by contextual information to explain the purpose of the recommendation and offer potential guidance for implementation. The recommendations are divided into priority recommendations, which must be implemented now, with urgency, and ancillary recommendations, which may be inapplicable where certain algorithmic policing technologies are banned but must be implemented if any such technologies are to be developed or adopted. The following is a condensed summary of those recommendations..A. Priority recommendations for governments and law enforcement authorities that must be acted upon urgently in order to mitigate the likelihood of human rights and Charter violations associated with the use of algorithmic policing technology in Canada:.D. Ancillary recommendations for government to enable access to justice in relation to the human rights impacts of algorithmic policing technology:.The Citizen Lab would like to thank the following funders for supporting this research: the John D. and Catherine T. MacArthur Foundation, the Sigrid Rausing Trust, the Ford Foundation, and the Oak Foundation. This research was also supported in part by a grant from the Open Society Foundation.","Algorithmic Policing causes implications for rights of privacy, right to freedom of expression, peaceful assembly and association"
210,"Deep-learning algorithms suffer from a fundamental problem: They can adopt unwanted biases from the data on which theyre trained. In healthcare, this can lead to bad diagnoses and care recommendations..In October 2019, a group of researchers from several universities published a damning revelation: A commercial algorithm widely used by health organizations was biased against black patients..   The algorithm, later identified as being provided by health-services company Optum, helped providers determine which patients were eligible for extra care. According to the researchers findings, the algorithm gave higher priority to white patients when it came to treating complex conditions, including diabetes and kidney problems..This is one of several recent stories involving algorithmic bias: the tendency of artificial intelligence to make decisions that give an unfair advantage to a certain group or demographic. Algorithmic bias can manifest in many fields, but in medicine it can be deadly..Most improvements in AI systems are made because of advances in machine learning and deep learning. Unlike traditional AI systems, which were based on manually crafted software rules, deep-learning systems develop their behavior by examining lots of examples..For instance, to develop a deep-learning system that predicts breast cancer, AI engineers created a base algorithm and fed it mammograms annotated with the patient outcome—cancer or no cancer. The algorithm processed the examples and found common patterns that characterize cancerous and non-cancerous slides. It used this information to make predictions for mammograms it hadnt seen before..In a few areas, such as radiology and medical imaging analysis, AI algorithms have surpassed human performance. But deep-learning algorithms suffer from a fundamental problem: They often adopt unwanted biases found within the data on which theyre trained. If the data is limited to a certain group of people, it will perform less accurately for other demographics..Datasets collected in North America are purely reflective and lead to lower performance in different parts of Africa and Asia, and vice versa, as certain genetic conditions are more common in certain groups than others, says Alexander Wong, co-founder and chief scientist at DarwinAI..For instance, several studies have found skin cancer–detection algorithms to be less accurate when used on dark-skinned patients, in part because AI models were trained mostly on images of light-skinned patients..When possible, engineers of AI systems take steps to reduce and remove bias. But machine-learning algorithms often find data points that indirectly represent problematic biases..The developers of the health-management system mentioned earlier had removed race information from the data the AI used to make decisions. But the algorithm selected health-care spending as one of the factors that determined its output. Spending effectively became a proxy for race and disadvantaged black patients, because they had lower healthcare costs, for socioeconomic reasons..While most studies on algorithmic bias are focused on known factors such as gender, race, and age, several studies show that machine-learning algorithms can often pick up hidden biases that are difficult to identify but can be equally damaging. The problem is that machine-learning models are often black boxes that offer very little visibility into their inner workings, so its difficult even for their creators to find and fix problematic biases..For example, skin-cancer-detection algorithms are usually trained on images of malignant moles and healthy skin. But while photos of skin cancer usually contain rulers to depict the size of the mole, healthy skin pictures do not contain any objects. An AI system trained on these images might end up becoming biased toward detecting rulers instead of malignant moles. Without visibility into the salient features of the algorithm, it would be hard to find out whether it has tuned into the right features..Machine-learning algorithms can also become sensitive to irrelevant correlations in health data. In one case, a hospital readmission algorithm gave lower-risk scores to patients with asthma. The program, touted to outperform expert doctors, would recommend hospitalizing a patient with pneumonia, but would clear the same person if they had both pneumonia and asthma..One must understand how and why decisions are made the way they are made by the AI algorithm in order to identify biases and devise strategies for addressing them, says Wong, whose company specializes in creating explainable AI models. Explainability also allows us to build trust in the AI algorithm, which is key in the healthcare system..As an AI community, we need to come together to share best practices, processes, and tools that will ensure fairness, inclusivity, reliability, and transparency while maintaining privacy and driving accountability across development and deployment, says Shantanu Nigam, CEO and co-founder of Jvion, a healthcare AI company..Some efforts are underway to address bias and fairness in AI-based healthcare systems. Last years NeurIPS conference ran a workshop to address fairness in machine learning for health applications. The workshop included several papers that explored the assessment of algorithmic fairness, discovering proxies, and calibrating algorithms for subpopulations. And the Alliance for Artificial Intelligence in Healthcare, a nonprofit organization founded in December 2018, brings together developers, device manufacturers, researchers, and other professionals to advance the safe and fair use of AI in medicine..Some organizations have started baking inclusivity and fairness into the data-gathering process, training, and testing of their AI algorithms. For instance, Google recently released an AI breast cancer screening tool its been testing to perform equally well across different geographical regions..Kush R. Varshney, principal research staff member and manager at IBM Research AI, believes increasing transparency and cooperation in the process of developing and releasing healthcare AI systems can help improve fairness. The best practices and governance of AI in healthcare should include the release of factsheets containing fairness test results and should involve multi-stakeholder participation on validating the entire AI lifecycle and also the organizational/human processes that surround the AI system, he says..We know that machine-learning models are, by their very nature, meant to statistically discriminate on all sorts of features in order to generalize to new, unseen patients, Varshney says. We just have to make sure they dont discriminate in other ways..This newsletter may contain advertising, deals, or affiliate links. Subscribing to a newsletter indicates your consent to our Terms of Use and Privacy Policy. You may unsubscribe from the newsletters at any time..Ben Dickson is a software engineer and tech blogger. He writes about disruptive tech trends including artificial intelligence, virtual and augmented reality, blockchain, Internet of Things, and cybersecurity. Ben also runs the blog TechTalks. Follow him on Twitter and Facebook.",These discrepancies can be extremely harmful to the patient/client when making decisions about treatment and diagnosis.
211,"Black people with complex medical needs were less likely than equally ill white people to be referred to programmes that provide more personalized care.Credit: Ed Kashi/VII/Redux/eyevine.An algorithm widely used in US hospitals to allocate health care to patients has been systematically discriminating against black people, a sweeping analysis has found..The study, published in Science on 24 October1, concluded that the algorithm was less likely to refer black people than white people who were equally sick to programmes that aim to improve care for patients with complex medical needs. Hospitals and insurers use the algorithm and others like it to help manage care for about 200 million people in the United States each year..This type of study is rare, because researchers often cannot gain access to proprietary algorithms and the reams of sensitive health data needed to fully test them, says Milena Gianfrancesco, an epidemiologist at the University of California, San Francisco, who has studied sources of bias in electronic medical records. But smaller studies and anecdotal reports have documented unfair and biased decision-making by algorithms used in everything from criminal justice to education and health care..Ziad Obermeyer, who studies machine learning and health-care management at the University of California, Berkeley, and his team stumbled onto the problem while examining the impact of programmes that provide additional resources and closer medical supervision for people with multiple, sometimes overlapping, health problems..When Obermeyer and his colleagues ran routine statistical checks on data they received from a large hospital, they were surprised to find that people who self-identified as black were generally assigned lower risk scores than equally sick white people. As a result, the black people were less likely to be referred to the programmes that provide more-personalized care..The researchers found that the algorithm assigned risk scores to patients on the basis of total health-care costs accrued in one year. They say that this assumption might have seemed reasonable because higher health-care costs are generally associated with greater health needs. The average black person in the data set that the scientists used had similar overall health-care costs to the average white person..But a closer look at the data revealed that the average black person was also substantially sicker than the average white person, with a greater prevalence of conditions such as diabetes, anaemia, kidney failure and high blood pressure. Taken together, the data showed that the care provided to black people cost an average of US$1,800 less per year than the care given to a white person with the same number of chronic health problems..The scientists speculate that this reduced access to care is due to the effects of systemic racism, ranging from distrust of the health-care system to direct racial discrimination by health-care providers..And because the algorithm assigned people to high-risk categories on the basis of costs, those biases were passed on in its results: black people had to be sicker than white people before being referred for additional help. Only 17.7% of patients that the algorithm assigned to receive extra care were black. The researchers calculate that the proportion would be 46.5% if the algorithm were unbiased..When Obermeyer and his team reported their findings to the algorithm’s developers — Optum of Eden Prairie, Minnesota — the company repeated their analysis and found the same results. Obermeyer is working with the firm without salary to improve the algorithm..He and his team collaborated with the company to find variables other than healthcare costs that could be used to calculate a persons medical needs, and repeated their analysis after tweaking the algorithm accordingly. They found that making these changes reduced bias by 84%..“We appreciate the researchers’ work,” Optum said in a statement. But the company added that it considered the researchers conclusion to be “misleading”. “The cost model is just one of many data elements intended to be used to select patients for clinical engagement programs, including, most importantly, the doctors expertise.”.Obermeyer says that using cost prediction to make decisions about patient engagement is a pervasive issue. “This is not a problem with one algorithm, or one company — it’s a problem with how our entire system approaches this problem,” he says..Finding fixes for bias in algorithms — in health care and beyond — is not straightforward, Obermeyer says. “Those solutions are easy in a software engineering sense: you just rerun the algorithm with another variable,” he says. “But the hard part is: what is that other variable? How do you work around the bias and injustice that is inherent in that society?”.This is in part because of a lack of diversity among algorithm designers, and a lack of training about the social and historical context of their work, says Ruha Benjamin, author of Race After Technology (2019) and a sociologist at Princeton University in New Jersey..“We can’t rely on the people who currently design these systems to fully anticipate or mitigate all the harms associated with automation,” she says..Developers should run tests such as those performed by Obermeyer’s group routinely before deploying an algorithm that affects human lives, says Rayid Ghani, a computer scientist at Carnegie Mellon University in Pittsburgh, Pennsylvania. That kind of auditing is more common now, he says, since reports of biased algorithms have become more widespread..He thinks that the results of these audits should always be compared to human decision making before assuming that an algorithm is making things worse. Ghani says that his team has carried out unpublished analyses comparing algorithms used in public health, criminal justice and education to human decision making. They found that the machine-learning systems were biased — but less so than the people..“We are still using these algorithms called humans that are really biased,” says Ghani. “We’ve tested them and known that they’re horrible, but we still use them to make really important decisions every day.”.Update 26 October 2019: Added the name of the algorithm developer and the company’s response to the study, as well as additional comments from Ziad Obermeyer.",Studies revealed that algorithms in hospitals and health insurance have biases against black people.  
212,"Last month, a team from Google Research published a paper on the results of a field test of a novel deep-learning model to detect diabetic retinopathy from images of patients eyes. The paper, titled A Human-Centered Evaluation of a Deep Learning System Deployed in Clinics for the Detection of Diabetic Retinopathy, is based on research done in partnership with the Ministry of Public Health in Thailand to conduct field research in 11 rural clinics across the provinces of Pathum Thani and Chiang Mai..TechCrunch wasted no time in summarizing the study: Google medical researchers humbled when AI screening tool falls short in real-life testing. The article goes on to summarize the failures of the system in practice — from the lack of dedicated screening rooms that could be darkened to take high-quality images, to inconsistent broadband connectivity, to patients concerns about having to follow up at a hospital. But I believe this coverage misses the mark in three important aspects, which should be of prime concern to people actually working to deploy medical AI in the field..First, there is a difference between research and engineering, and research studies like this one should be heralded for the progress they enable. According to Google Health, This is one of the first published studies examining how a deep learning system is used in patient care. We need more studies about medical AI deployments published at the Conference of Human Factors in Computing Systems — and these studies need to describe things the way they are. Unlike a startup going to market that must spin whatever happens as a success story, research work is only about uncovering the truth..Implying such studies are failures not only misrepresents their goal and achievement but also contributes to the issues of nonreproducible research and science by press release that plagues todays science. If youre one of the many people trying to apply deep learning for medical imaging in practice, then youll find this paper to be a gem..Second, there must be an understanding of what it takes to get an AI system from idea to production. Assuming that a basic scientific breakthrough makes a system ready for wide use would have caused the invention of the steam engine to receive press coverage like this: Scientists humbled to find were nowhere near a robust national railway system. This is how cars where originally covered in the media, so theres nothing new under the sun with this happening again with AI..Taking on the analogy of cars, here are the three workstreams that must come together for medical AI systems to become an effective everyday reality:.1. Science: We need to develop highly accurate data science algorithms for specific problems, as Google did with its original deep learning models for detecting diabetic retinopathy. In the analogy to cars, this would be like the invention of the internal combustion engine..2. Engineering: We need to develop ways to productize these inventions at high quality, high scale, safely and cheaply. In the analogy to cars, we need to invent the equivalents of the mass production line, hand brakes, electric starters, air conditioners, airbags and headrests. In the AI space, think MLOps, explainability, bias detection and model governance (as a start). This is the area of the ecosystem where I personally work and specialize..3. Process change: We need to develop the human-centered processes that enable people to use these innovations effectively and safely. In the analogy to cars, think splitting the public space between roads and sidewalks, establishing driver licensing, public education, safety standards and pollution regulation. In medical AI, weve barely started on this, which makes the recent Google field study an important baby step..Its important for practitioners to know that real success — helping real patients, in the field, at scale, safely — requires all three of these aspects to work together. Its important for media coverage to educate people about this..The third insight from this new study is based on the major differences between the 11 clinics that took part in it. The researchers reported major differences between them — from how the physical rooms at each clinic were laid out to the personalities and background of the nurses who worked there. As a result, the trained model could not successfully operate at each of these distinct environments..This is such a well-known phenomenon in medical AI that it no longer requires academic validation. Medical AI models generally perform poorly across locations. This not only applies to models deployed in Thailand versus Nigeria but also models deployed in two clinics that are 5 kilometers apart and serve essentially the same population. This happens in both first-world and third-world countries and across just about every medical specialty thats taken the time to measure it..As a result: If you have a successfully deployed model in one location (or 10), you do not have an accurate model thats ready for the next clinic. Continuously tuning and monitoring AI models is part of the engineering work underway in the Science + Engineering + Process Change trifecta. At this point in time, I expect every sound medical AI field deployment to be addressing this issue..Turning medical AI from aspiration into a reality that improves humanitys well-being is going to be a long ride. It will take us all of the first half of the 21st century — and thats if were efficient about it. Maybe this isnt original, but it may be the adventure of a generation.","A new AI deep learning model that detects diabetic retinopathy failed during its first field test.  Where some were quick to call this a failure, this article argues that because the field test revealed flaws in the technology, the failure is good because it brings them closer to a better solution."
214,"The news: An algorithm that many US health providers use to predict which patients will most need extra medical care privileged white patients over black patients, according to researchers at UC Berkeley, whose study was published in Science. Effectively, it bumped whites up the queue for special treatments for complex conditions like kidney problems or diabetes..The study: The researchers dug through almost 50,000 records from a large, undisclosed academic hospital. They found that white patients were given higher risk scores, and were therefore more likely to be selected for extra care (like more nursing or dedicated appointments), than black patients who were in fact equally sick. The researchers calculated that the bias cut the proportion of black patients who got extra help by more than half..What software was this? The researchers didn’t say, but the Washington Post identifies it as Optum, owned by insurer UnitedHealth. It says its product is used to “manage more than 70 million lives.” Though the researchers only focused on one particular tool, they identified the same flaw in the 10 most widely used algorithms in the industry. Each year, these tools are collectively applied to an estimated 150 to 200 million people in the US..How the bias crept in: Race wasn’t a factor in the algorithm’s decision-making (that would be illegal); it used patients’ medical histories to predict how much they were likely to cost the health-care system. But cost is not a race-blind metric: for socioeconomic and other reasons, black patients have historically incurred lower health-care costs than white patients with the same conditions. As a result, the algorithm gave white patients the same scores as black patients who were significantly sicker..A small saving grace: The researchers worked with Optum to correct the issue. They reduced the disparity by more than 80% by creating a version that predicts both a patient’s future costs and the number of times a chronic condition might flare up over the coming year. So algorithmic bias can be corrected, if—and sadly, it is a big if—you can catch it..Why it matters: The study is the latest to show the pitfalls of allocating important resources according to the recommendation of algorithms. These kinds of challenges are playing out not just in health care, but also in hiring, credit scoring, insurance, and criminal justice..An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.","The algorithm prioritized white people over black people when predicting which patients would need extra healthcare.  Socioeconomics (and other unspecified reasons) affected the training data because black patients have historically recieved lower health care costs than white people for the same treatment.  This causes the algorithm to give a white person and a black person the same score even though the black person is far sicker.  Researchers found that white patients were given higher risk scores, and were therefore more likely to be selected for extra care than black patients who were, in fact, equally sick; they calculated that the bias cut the proportion of black patients who got extra help by more than half."
217,"Researchers explain that healthcare companies have not adopted artificial intelligence algorithms because they do not work well and fail to show results. .In a new article in JAMA, researchers suggest that developers of artificial intelligence (AI) programs for improving medicine should pay more attention to how their programs would actually function in a clinical setting. The authors remark that “given the abundance of algorithms, it is remarkable there has yet to be a major shift toward the use of AI for health care decision-making (clinical or operational).” However, they go on to outline several problems with AI programs that explain why healthcare organizations have not overwhelmingly adopted them for clinical decision making..AI algorithms often haven’t been shown to work (“data quality”), nor have they consistently been found to improve results (due to “timeliness” or “lack of structure”). Another complication is that AI programs often can’t be assessed because their “black box” algorithms make it impossible to tell if they’re working, which leads to a “lack of trust.”.The authors acknowledge these problems, but they have an additional explanation, too: “Perhaps that model developers and data scientists pay little attention to how a well-performing model will be integrated into health care delivery.”.The lead author was Christopher J. Lindsell at Vanderbilt University Medical Center, himself a patent-holder on several predictive technologies who also receives funding from Endpoint Health Inc, an “early-stage” tech start-up in the healthcare field..Lindsell and his co-authors suggest that one major problem is that even if artificial intelligence algorithms were shown to work, they may not improve outcomes. Importantly, prediction and surveillance do not necessarily improve healthcare outcomes..The authors suggest that technology companies should work with “end users” such as patients and clinicians to determine what algorithmic technology may actually be helpful—and “in some cases, the realization that the problem is not ready for an AI solution given a lack of evidence-based intervention strategies to affect the outcome.”.They provide an example of an “expensive intervention” aimed at reducing alcohol use in people who experienced trauma. Technically, the intervention worked—but only for the people who were at low risk of both alcohol use and readmission..So, in that instance, a technology was developed with good intentions, and it appeared at first glance to be successful. But upon further review, it actually failed to work for the group of people who needed it most..Apps using artificial intelligence to assess mental health are already in use, partnering with health insurance companies and medical centers, despite no published research evidence demonstrating their effectiveness in any clinical domain..There were over 325,000 different healthcare apps available to download in 2017, and the market share was estimated at $23 billion. In 2018, users downloaded over 400 million healthcare apps, and that number has likely only grown..A study last year found that of the more than 10,000 apps available for mental health, only “3.41% of apps had research to justify their claims of effectiveness, with the majority of that research undertaken by those involved in the development of the app.”.Lindsell, C. J., Stead, W. W., & Johnson, K. B. (2020). Action-informed artificial intelligence—Matching the algorithm to the problem. JAMA. Published online May 1, 2020. doi:10.1001/jama.2020.5035 (Link).There’s no such thing as an “app deficit” in human medicine….that’s a business and financial problem. ALL problems in medicine are HUMAN, not AI. And the biggest problem with AI is the so-called “AI Virus….. I think in future years we’ll see few of the alleged benfits of AI pan out…..Ah but they can. That’s called “the algorithm. It has been shown that algorithm get way skewed and prejudiced – the prejudices of the people writing the parameters get amplified in the echo chambers of algorithms and turn into very prejudicial AI’s…..Well, of course, the algorithm is only as good as the programmer. I’m sure someone could program a discriminatory app. But at least they won’t have to manage their emotional reactions to our statements, appearance, etc. I’m sure they’d totally suck, because they’d be made by people who have no comprehension of what is helpful, otherwise, they’d realize that a computer can’t provide what is needed..The data quality issue is largely of the medical field’s own making. Format and contents of clinical documentation are determined by insurers, governmental regulatory officials, and other funding sources. Data is generally only collected for the purpose of billing and not for future clinical use or research/training ML algorithms. I’m not sure what they mean by ‘black box’ either. Just because YOU don’t understand the math, doesn’t mean I can’t. Backprop is fundamental to how neural networks learn from data so you should always be able to follow backwards through the model from the decision in the output to the data in the input. The real problem is the lack of collaboration between the two fields. I don’t see the example above as a failed intervention. AI was able to cheaply address the low risk population, leaving the experts more time to focus on the ‘problem children’. That’s a failure of the treatment program to utilize the treatment in the best way possible. Remember, AI based cancer diagnosis only beats human based diagnoses slightly. The real improvement comes from augmenting the human. Human’s armed with Ai diagnostic data perform better than any ML algorithm ever will..No, the problem is that common approaches to “helping” with mental/emotional/spiritual distress don’t improve outcomes, and no amount of AI is going to change the fact that the basic model of distress and helping is fatally flawed. Well, flawed unless your “outcome” is increased profits. Maybe that’s what they mean – AI isn’t improving income, therefore, it isn’t working?.The outcomes for whom? The Human or the Institutes of Technology? Any knowledge of what drives the Broad Institute on the MIT campus? How does one create breathing space for anyone, students & professors to ask questions of each other without being told, to soon for lack of deep listening, or how to listen to what is attempting to be asked, without thinking of an Oedipus sort of destructive mentality that often emerges in therapy, if one challenges the ruler in the sessions? Or are the “Orders” in the challenge to become a knowledge producing economy undermining authentic health? And care?","AI needs to be accurate, unbiased, and trustworthy before it can be used in healthcare."
218,"When artificial intelligence (AI) is built into electronic health record (EHR) software, who is responsible for the consequences? Does responsibility lie exclusively with the hospital or health system that uses the vendor’s software, or is responsibility shared with the EHR vendor? What role should regulators, such as the Food and Drug Administration (FDA), have in ensuring that AI in health care is trustworthy and equitable? .AI has the potential to transform health care, but implementation at the point of care is fraught with challenges. Many health systems find it attractive to implement AI developed by EHR vendors that is integrated directly into existing EHR implementations (built-in AI). The leading global developer of EHRs, Epic Systems, Inc., has responded to this market enthusiasm with a growing set of predictive models. Here, we describe our experience implementing a predictive model developed by Epic and the ethical issues raised by its deployment. .“No-shows”—the colloquial term for patients who fail to arrive for a scheduled appointment—are a major source of waste in US health care. Strategies to minimize no-shows include telephone reminders, text reminders, and overbooking, the latter of which appears to be the most effective in maximizing clinic slot use. AI can assist with targeted overbooking by predicting which patients are most likely to no-show, enabling practices to schedule an additional patient in that same time slot. Targeted overbooking, however, has the potential to harm individual patients: If both the originally scheduled patient and the overbooked patient do in fact arrive, provider time is stretched to accommodate two patients in a slot meant for one, potentially resulting in poorer quality care..As part of their cognitive computing platform, Epic released a proprietary built-in AI tool that displays a numerical estimate of the likelihood that a patient will no-show, with minimal implementation effort and seamless EHR integration. Inputs to the model include the patient’s personal information, clinical history, their patterns of health care use (for example, prior no-shows), and features of the appointment (for example, day of the week). .As we considered applying the no-show model at our institution, we identified several layers of potential bias that might harm vulnerable patient populations. First, the potential for explicit discrimination was obvious because the predictive model included personal characteristics such as ethnicity, financial class, religion, and body mass index that, if used for overbooking, could result in health care resources being systematically diverted from individuals who are already marginalized. We initially addressed this issue by rebuilding the model to exclude all personal information, leaving only prior patterns of health care use and features of the appointment, achieving the same performance as the vendor-built model. Nevertheless, we realized that our new model, even stripped of personal information, would not eliminate the potential to propagate societal inequity. .Removing sensitive personal characteristics from a model is an incomplete approach to removing bias. Prior no-shows—a variable included in both the vendor’s original model and our revised version—is likely to correlate with socioeconomic status, perhaps mediated by the inability to cover the costs of transportation or childcare, or the inability to take time away from work. Likewise, a patient with obesity who struggles with mobility may make it to their appointment only to find it overbooked, and their clinician thus overworked and distracted. The potential for these adverse consequences is further obscured by the “black box” of AI: In this, as in many AI-generated models, the contributing variables are not visible to the end user. In this example, clinic personnel likely may not realize they are overbooking a patient primarily because the patient is obese. .If the predictive model were 100 percent accurate, scheduling another patient in a sure-to-be-empty slot would not be problematic, although the ethical imperative to care for the vulnerable patient remains. But no model is perfectly accurate, so we are left with the tension created by implementing a model that, while potentially improving the total use of available clinic time and personnel, may also preferentially divert resources away from patients in greatest need. .We are not arguing that demographic features should never be represented in a model, as they can be critical predictors of health and access to health care. The question is whether it is tolerable for demographic bias to be represented in a model, not just explicitly (as in the race/ethnicity input) but also implicitly (as in the prior no-show input), if that model may lead to action that negatively affects an individual patient. In the case of predicting no-shows, we believe that the overbooking intervention, which risks withdrawing resources from vulnerable patients, is ethically problematic. Yet, there are “patient-positive” interventions that may improve the likelihood a patient keeps their scheduled appointment, such as flexible appointment times, telehealth visits, or even assistance with transportation or childcare. .After reflection, we chose to implement only patient-positive interventions in response to the results of the no-show predictor. We piloted an intervention of targeted outreach in 12 clinics and saw a 9 percent mean reduction in no-show rates compared with the prior year. While targeted overbooking may have further improved scheduling outcomes for our clinics, we were not willing to jeopardize health equity by using Epic’s model or our revised model for that purpose. .Several recent publications have discussed potential biases associated with machine learning algorithms. Many concern generalizability to disadvantaged populations, including patterns of missing data, low minority group sample size, and disparities in care caused by implicit biases (Milena A. Gianfrancesco and colleagues and Alvin Rajkomar and colleagues). Ziad Obermeyer and colleagues recently demonstrated that the output of a widely used algorithm reflected underlying structural inequalities and called for attention to bias in the labeling of data. Our case study extends these concerns by highlighting the risk of AI-based interventions that are built directly into the EHR software, making them particularly easy to implement without careful consideration of the ramifications. .Accountability for ensuring that AI for health care is built, implemented, and used ethically has not been established. Software developers may have limited health care experience and not fully understand the implications of their models, including existing inequities that may implicitly inform or be promoted by their models. Health systems may not fully understand the workings and performance of AI methods, particularly as methods are increasingly deployed as “black boxes.” Given these perspectives, vendors may consider it the health care institution’s responsibility to use AI responsibly, while health care organizations assume the vendor would provide only valid and ethical models. Because built-in AI is delivered by an already-trusted business partner and easily implemented, it may bypass the scrutiny and oversight that might otherwise be applied to a tool from a new industry partnership. Furthermore, built-in AI is not yet subject to governmental regulation. As implementation of AI in health care grows, it will be important to clarify who bears the responsibility for ethics and regulatory oversight. .The FDA recently released draft guidance for regulation of clinical decision support (CDS) software, which includes areas where they intend to focus regulatory oversight. The FDA’s focus is on “device CDS,” decision support in which the provider is unable to independently review the basis for the recommendation. They contrast these tools with “non-device CDS,” decision support in which providers can understand the basis for the recommendation. The FDA does not intend to regulate non-device CDS at this time. The implication is that “black box” models would be classified as device CDS, while some predictive models might receive non-device CDS status if they provided sufficient transparency into the basis for the recommendation. Among device CDS, the FDA intends to zero in on algorithms that impact critical or serious health situations or conditions, as defined by the International Medical Device Regulators Forum, while allowing enforcement discretion for “non-serious” issues. A summary of the FDA’s regulatory policy for CDS software that is intended to inform clinical management for health care providers is shown in exhibit 1. .Source: Adapted from the FDA draft guidance for regulation of clinical decision support (CDS) software. Note: The FDA does not intend to enforce compliance where regulation is characterized as “Enforcement Discretion” at this time. .While this evolving regulatory landscape represents progress, we remain concerned that many of these built-in tools will not be subject to appropriate evaluation. Some of the Epic-provided predictive models do explain which variables contribute to the predicted outcome, suggesting they are non-device CDS, but one needs a deep understanding of statistics and epidemiology to understand the residual bias and confounding that may exist in these models, leaving us with the fundamental question of what it means to truly understand the basis for a model’s recommendations. In another example, the no-show model we described might fit into the category of device CDS but potentially fit into the “non-serious” category where regulation is not a current priority for the FDA. .The American Medical Informatics Association recently raised concern over algorithm-driven bias in the absence of intended discrimination and recommended that the FDA develop guidance for how software developers might test their products for such biases. While EHR vendors have generally enjoyed a relatively unregulated landscape, the growing implementation of built-in algorithms should prompt reconsideration of how to ensure the trustworthiness of AI in driving clinical decision making. Given the potential for predictive models that are built into EHR software by the EHR vendor to be ubiquitously used with variable oversight by the health care systems that implement them, we believe such models should be considered for regulatory oversight, notwithstanding attempts at model transparency or the seriousness of the health issue they are addressing. This need is particularly great when the output of the model is the diversion of resources away from individual patients. .Regardless of whether Epic and other EHR vendors eventually seek FDA approval for their AI implementations, health care institutions and software vendors should partner to provide structured ethical oversight of these potentially valuable tools. Software vendors should involve ethicists, clinical informaticists, and operational experts early in the process of developing any CDS method, and health care delivery organizations need to ensure a broad ethical perspective as they evaluate new tools for implementation. As data science techniques become increasingly complex, multidisciplinary oversight is needed to ensure AI does not automate discrimination against our most vulnerable patients.","The article questions algorithms' use of patients' personal information and questions how it plays a role in creating biased AI. A model for predicting “no shows” serves as an example for AI systems that are biased, inaccurate, or untrustworthy."
219,"United States Senators Cory Booker, D-New Jersey, and Ron Wyden, D-Oregon, are urging the Trump administration and some of the nations biggest health insurers – Humana and Blue Cross among them – to be aware of potential racial bias in healthcare data algorithms..The letters were sent to leaders at the Centers for Medicare and Medicaid Services and the Federal Trade Commission – as well as to the executive vice president of CVS Health Karen Lynch, who is also the president of the companys Aetna business unit, as well as to the president and CEO of Blue Cross Blue Shield, Scott Serota, and David Cordani, president and CEO of Cigna, among others..In healthcare, there is great promise in using algorithms to sort patients and target care to those most in need. However, these systems are not immune to the problem of bias, said Senators Booker and Wydens in their  letter to Lynch stated. As algorithms play an increasingly prevalent role in the health care system, we urge Aetna to consider the risk for algorithmic bias and its potential impact on health disparities outcomes..The letters point to a study published in the October 25 issue of Science magazine, which examined the racial bias in an algorithm used to manage the health of populations..For study found the creators of the algorithm did not take into account that other factors besides need contribute to the individuals overall healthcare costs..Factors like barriers to accessing care and low levels of trust in the healthcare system – which disproportionately affect black patients – mean those patients were less likely to receive additional services..Less money is spent on Black patients who have the same level of need, and the algorithm thus falsely concludes that Black patients are healthier than equally sick White patients, the study concluded. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care..The two senators submitted a list of questions to the recipients of the letter, requesting details about the use, type, number and methodology behind the algorithms these organizations use..The questions also cover topics related to the use of machine learning and advanced analytics – technologies that are expected to play much larger roles in the future development of healthcare analytical tools and metrics.","Several senators urged the Trump administration and major healthcare companies to address racial bias in AI after a study came out that proved black americans were less likely to receive additional healthcare services. The creators of the algorithm didn't account for factors, such as barriers when accessing care, when programing the AI."
223,"In 2009, the United Nations Office on Drugs and Crime estimated that up to $2 trillion is laundered globally in one year, but less than 1 per cent of this illegal activity is caught. Almost ten years on, this trend still held true in the UK – in 2018, although as much as hundreds of billions of pounds in laundered cash was thought to have washed through the City of London, only 40 arrests were made from 22,196 flagged cases. .The difficulty in identifying suspicious activity is fundamentally a problem of volume: a combination of the enormous amount of transactional information received, the expert money laundering attacks that circumvent published AML transaction scenarios, and the sheer speed of criminal activity. With the advent of near real-time bank transfers coupled by advances in cloud computing, criminals are now able to disperse illicit funds through different coordinated bank accounts so quickly that they are almost impossible to intersect..To combat the tide of illegal activity, regulatory bodies are penalising institutions found to be involved in money laundering-related offences, even if these were carried out unknowingly. As a result, global financial institutions have been levied with fines amounting to $26 billion over the last ten years, a development that has seen financial institutions expanding their compliance teams to try and beat the criminals..Although this has caused the number of alerts to grow through softening of transaction rule thresholds and bringing in armies of human talent, 95 per cent of alerts are false positives, and nearly 98 per cent never result in a suspicious activity report (SAR). Further, of those that lead to SAR filing, there has been a rise in ‘defensive SARs’ that create confusion as the analyst erred on the side of safety vs. truly identifying a suspected money laundering event.  This is a low rate of return and often quality of additional SAR filing that is incongruous with the investment being ploughed into AML efforts..Current AML processes can be streamlined and optimised with AI, the business use cases for which are growing by the day. AI and advanced analytics can reduce the costs of AML efforts while catching more suspicious transactions..During a proof of concept at a European bank, these AI AML models identified only ten false positives for every 1000 alerts generated for 42 per cent of the highest scoring consumer transactions. Compliance officers, on the other hand, would have had to work through more false positives than real alerts..With the superiority of AI in identifying genuine AML cases proven, the question now is: why are they so much more effective? This stems from the two new technologies these models are built with: unsupervised learning machine learning and explainable AI..The most well-known applications of machine learning and AI involve supervised learning, that is, using an algorithm to learn how the input is best transformed to map to the output, which is analogous to that of a teacher supervising learning processes. Unsupervised learning, on the other hand, has no know output to match, which removes the need for a teacher..Applying unsupervised learning for AML involves exposing the AI system to raw uncategorised data. During this interaction, the computer system identifies patterns that indicate money laundering by using soft clustering on behavioural archetypes to check for customer behavioural transaction anomalies within archetypal clusters (see Fig 1). By being able to boil down transaction behavioural analytics into archetypes of behaviour indicating normal behaviours, and measuring abnormality from that, the AI models can detect suspects very rapidly. .Although a very useful tool, explainability is not AI’s strong suit (due to the typical pursuit of performance over transparency), – hence its reputation as a black box technology. Explainable AI (XAI) is a field of science that attempts to remove this black box and deliver AI performance while also providing an explanation for the “how” and “why” a model derives its decisions. With explainability increasingly demanded by new laws such as the General Data Protection Regulation (GDPR) not to mention AML regulators world-wide, incorporating XAI into AI models has become a question of how, and definitely not when..Explainable AI opens doors in the banking industry to the advantages of AI, historically these financial institutions have been wary of deploying AI solutions owing to its often inexplicable nature, a quality incompatible with highly-regulated industries. By incorporating XAI in AML applications, financial institutions will be able to explain and justify the model’s decision making, thus satisfying the regulators and making money laundering cases easier to investigate..AI models can be deployed to supplement existing rules for AML systems, to enrich current data with scores that can be used for both enhanced detection and alert prioritisation, given that the majority of alerts are false positives. With AI, compliance officers will have more time to improve their investigative analyses on the one per cent that matter and importantly those that go undetected by current transaction monitoring.  This will lead to faster response rates, the reduction of reputational risk from regulatory fines and a lower cost of compliance. Having AI and humans work in tandem with each other is most effective way to significantly improve current AML outcomes for financial institutions..AI has huge potential to significantly improve AML processes, particularly due to its ability to reduce the number of false positives and zone in on the truly suspicious cases and find so many more that are currently missed by traditional transaction monitoring. With the development of XAI, it is likely that more and more financial institutions will use AI to bolster their AML efforts, especially as the levels of financial crime continue to rise.",AI can help identify suspicious activity at large volume which can help with anti-money laundering efforts
224,"Artificial intelligence (AI), or rather, machine learning, is utilizing predictive analytics to reshaping the financial services landscape, providing banks the ability to capitalize on the wealth of customer and product data in their passion. This is enabling banks to gather greater insight into their customers and provide solutions and services specifically tailored to each individual..What is predictive analytics? In simple terms, it is the process of using computer models to predict various events. Sophisticated algorithms rely on artificial intelligence, data mining, and machine learning to analyze large amounts of information. With these data inputs, the AI runs various models with the goal to determine what is likely to happen next, given various inputs..Even today we can see banks utilizing this technology to improve the way they work with customers and provide enhanced services to their clients. Let us take a look at a few areas where banks are implementing AI to enhance their services..Fraud is everywhere these days. As individuals do more banking through mobile apps and the web, scammers are continuously looking to take advantage of unsuspecting users. Bank today utilize AI and predictive analytics to track consumer behavior. By applying machine learning to these patterns of spending and saving, Ai is able to identify irregular behavior that could be indicative of fraud. With the ability to notify customers immediately, individuals are more aware when the fraudulent activity took place. Additionally, banks have been able to reduce the occurrences of scams that come from bad checks, which cause significant losses for victims. Being able to analyze data patterns has become an important tool in the banking industry..Most individuals are aware of their credit score, but have you stopped to wonder exactly how that value is calculated? It probably wouldn’t surprise you that AI analytics plays a large part in determining your credit score. By analyzing data from your credit habits along with those of individuals who are similar to your financial profile, the data is aggregated to predict the likeliness of your ability to repay debt. Banks are using this technology to help evaluate loan approvals for individuals who may not have the highest credit scores. In the past banks relied heavily on the user’s credit reports provided by companies like FICO and Equifax, but advancements in technologies are improving loan opportunities for individuals who may have had a bad rap sheet in the past and are suffering from low credit scores..Banks today offer solutions to their customers to help manage personal finances. With the ability to identify when income and expenses flow in and out of your accounts, banks have the ability to alert the customer before overdrafts or payments occur.  This helps remove unnecessary fees that banks impose when these types of events occur. Predictive analytics can also help steer big picture decisions. According to Deltec Bank, Bahamas – “By analyzing an individual’s financial data, AI can help identify opportunities that individuals may be unaware of. Perhaps it is beneficial to make additional loan payments or refinance your mortgage. Banks are positioning themselves to help individuals make smarter decisions when it comes to their finance.”.Artificial intelligence is being used today in the banking industry, and as technology continues to advance, banks will continue to improve how they serve their customers and tailor solutions specific for each individual. Whether protecting the individual from fraudulent activity or scams, to ensuring you make the right financial decisions; banks are positioning themselves to become more involved in your financial picture to help provide the customer the services they want and need..Headquartered in The Bahamas, Deltec is an independent financial services group that delivers bespoke solutions to meet clients’ unique needs. The Deltec group of companies includes Deltec Bank & Trust Limited, Deltec Fund Services Limited, and Deltec Investment Advisers Limited, Deltec Securities Ltd. and Long Cay Captive Management.",Banks could use predictive analytics for fraud prevention and customer focused solutions
225,"New technology could play a significant role in helping potential customers carry out early research into equity release options before speaking to an adviser, providers have said..Digital retirement solutions provider Abaka and equity release adviser Key carried out a study with a group of retired consumers aged between 65 and 75, which looked into their use of online channels to research equity release products and services..The study found that providers’ websites were often among the first sources of information for the test group when looking into equity release. It also found a reluctance from participants to provide contact details during their initial research, to avoid being contacted too early during their decision-making process..The research group tested a chatbot powered by artificial intelligence, designed to give personalised answers to customers’ questions on equity release. Participants seemed to favour the anonymity of using the chatbot and the freedom to ask multiple questions without perceived judgement..The study also found that participants wanted to speak to a human adviser when they felt ready, and that face-to-face meetings with an adviser were still the main way they wanted to receive bespoke advice..Speaking to FTAdviser, Jonathan Barrett, director of business development and partnerships at Abaka, said participants asked the chatbot questions such as what equity release could be used for, what it would cost them, and what would happen if they went into care..Mr Barrett said a chatbot could help customers get to a stage where they feel informed before speaking to an adviser, rather than replacing the need for one..He said: “There are simply not enough advisers to answer these initial queries and help consumers assess whether equity release is the right option for their retirement plans..“Technology can help bridge that advice gap. Having an informative website that is easy to navigate is a good place to start, but tools like conversational AI can take that experience a step further, assisting consumers on their fact-finding mission before they’re ready to seek regulated advice.”.Will Hale, CEO of Key, added: “While we believe firmly in the value of face-to-face advice and the support that this can provide customers as part of the process of taking out a later life lending product, we know that there is often considerable thought and planning undertaken before they even pick up the phone..“Informed customers who feel comfortable that their initial questions have been answered are going to be more confident in progressing to the next step of engaging with an adviser who can take a more in-depth look at how later life lending can support their retirement ambitions.”.Commenting on the research, Steve Paterson, equity release specialist at Later Life Money, said his view on the matter had changed in light of the coronavirus.","AI could help answer any questions that customers want to be anonymous. Furthermore, they can work together with human advisers once the customer is ready for a face to face meeting"
228,"San Francisco (CNN Business)Photo editing app Gradient is under fire for a new feature that lets people alter their ethnicity in images, with many slamming it for promoting digital blackface. ","Gradient, a photo editing apps, lets people alter ethnicity which some users have used as blackface"
232,"As casual dining chains have declined in popularity, many have experimented with surveillance technology designed to maximize employee efficiency and performance. Earlier this week, one Outback Steakhouse franchise announced it would begin testing such a tool, a computer vision program called Presto Vision, at a single outpost in the Portland, Oregon area. Your Bloomin Onion now comes with a side of Big Brother..According to Presto CEO Rajat Suri, Presto Vision takes advantage of preexisting surveillance cameras that many restaurants already have installed. The system uses machine learning to analyze footage of restaurant staff at work and interacting with guests. It aims to track metrics like how often a server tends to their tables or how long it takes for food to come out. At the end of a shift, managers receive an email of the compiled statistics, which they can then use to identify problems and infer whether servers, hostesses, and kitchen staff are adequately doing their jobs..“It’s not that different from a Fitbit or something like that,” says Suri. “It’s basically the same, we would just present the metrics to the managers after the shift.” Presto says its testing the technology at multiple restaurants across the country, but declined to name any other than Outback..The Outback Steakhouse pilot will use Presto Vision specifically to analyze footage from the lobby of a franchise operated by Evergreen Restaurant Group, which manages nearly 40 Outback Steakhouse locations across the United States. It will monitor factors like how crowded the lobby is and how many customers decide to leave rather than wait for a table. Suri says Presto Vision could be used not only to evaluate employee performance after the fact, but also course-correct in the moment. For instance, managers could be sent text messages when the number of people waiting for a table reaches a certain threshold..For now, workers on the ground dont know much about how the technology will be used. I dont know anything about it, one worker at the Portland Outback location said over the phone. We have zero interaction with that. Im pretty sure thats just still in the developmental phase..Presto Visions software doesnt identify individual diners and doesnt currently employ technology like facial recognition. “We do not collect any personal information and the video is deleted within three days of collection,” Jeff Jones, the president and CEO of Evergreen Restaurant Group, said in an email. But even if their data is anonymized, consumers may be unnerved to learn that an algorithm is monitoring their night out..The Outback pilot is still in early stages, but Suri sees broad potential in Presto Vision. The software has the potential to detect things like when a guest’s drink is almost empty, he says as an example, and prompt servers to offer them a refill..But even without those fancy features, Presto Vision is likely already capable of producing lucrative data for the restaurant industry. That information could be used not only to boost sales, but also make life harder for workers. Researchers have found that workplace surveillance can have negative effects on employees, like increased stress and lower job satisfaction..Alexandra Mateescu, a researcher at the Social Instabilities in Labor Futures initiative at the nonprofit research institute Data & Society, wonders whether Presto Vision and similar tech could ultimately decrease staff numbers or alter scheduling patterns. For example, if the software finds that servers at a given restaurant have enough time to visit their tables often, higher-ups may decide to cut the number who work during certain shifts, to try to cut labor costs. This [technology] may sort of automate the discretionary power of restaurant management to make decisions, Mateescu says..There’s also the question of what happens to the aggregate information Presto Vision and other similar tools collect. Even if the underlying videos get deleted, the data lives on. At franchised restaurants like Outback, it could be vacuumed up by parent organizations, and used to make business decisions in the future. In a press release, Presto touted its software could be employed to provide “remote, immediate visual access across multiple locations, and a high-level view of performance metrics and noteworthy events across brands for large restaurant chains.”.These are legally separate firms, they have no oversight or responsibility for working conditions or wages, says Brian Callaci, an economist and researcher at Data & Society who has studied franchising. But technology like Presto Vision would potentially allow them to monitor and control activity supposedly at legally independent businesses. Jones, from Evergreen Restaurant Group, did not immediately return a request for comment about if and how the data collected by Presto Vision would be shared within his organization, or with Bloomin Brands, Outback Steakhouses parent company..Presto Vision is also just the latest in its parent companys suite of monitoring tools designed to optimize the chain restaurant industry. Along with its competitor Ziosk, Presto makes electronic tablets stationed at tables in restaurants like Chili’s, Olive Garden, and Applebee’s across the country. Customers use the devices not only to order food, but also to rate the performance of their waiters and waitresses. A Buzzfeed News investigation found workers with lower scores have received fewer shifts and tables, and potentially even faced termination—despite customer ratings often reflecting aspects of the experience that servers can’t control, like the quality of the food..Other fast food chains have developed their own surveillance solutions in-house. Dominos, for example, recently began rolling out its “DOM Pizza Checker” at stores in Australia and New Zealand, which monitors workers as they assemble pies via an AI-equipped overhead camera. If the device detects a poorly made pizza, it alerts workers that it should be remade. In a presentation given to shareholders, Domino’s executives suggested the technology could be used to quantify bonuses, as well as identify stores “falling below” their peers..Suri argues Presto Vision “should be able to help managers create better processes to do a better job overall.” He compares working in restaurants to playing sports. Without surveillance tools like the ones Presto offers, managers “have to coach based on the score—they don’t know what actually happened on a play-by-play basis.” In other words, restaurant managers today are still largely forced to take employees at their word, and give them at least a baseline level of autonomy. At least until technology makes even that obsolete.","The restaurant started using Preso Vision, a computer vision program, to maximize employee efficiency and performance."
233,"French anti-discrimination organization SOS Racisme, in association with the Union of Jewish Students of France, the Movement Against Racism and for Friendship Among Peoples and other organizations, is suing Google because its autocomplete feature suggests the word Jewish in searches involving certain public figures, including News Corporation chairman Rupert Murdoch and actor Jon Hamm, reports The Times of Israel..According to Googles website, its algorithm for the Google Instant autocomplete feature predicts and displays search queries based on other users search activities and the contents of web pages indexed by Google. In addition, the search engine says it strives to reflect the diversity of content on the web (some good, some objectionable) and so has a narrow set of removal policies for pornography, violence, hate speech, etc. -- though not narrow enough for SOS Racisme, it seems..A lawyer for SOS Racisme, Patrick Kulgman, told Agence France Presse (AFP) that Googles autocomplete algorithms have resulted in the creation of what is probably the biggest Jewish file in history,  according to The Times of Israel. As an ethnic file, this compilation is outlawed in the country..Local reports pointed out by The Hollywood Reporter explain that the plaintiffs contend users of Google in France and across the world are systematically confronted with the unsolicited association of the term Jew with prominent names in the world of politics, media, and business. A hearing for the lawsuit is scheduled for Wednesday..The Hollywood Reporter also writes that the last lawsuit Google saw in France due to its autocomplete feature occurred in 2009, when two French companies sued the search engine because its autocomplete feature suggested the French word for scam in searches for said companies names..Just over a month ago, a man in Japan won an injunction against Google to have the autocomplete feature turned off when someone searched the mans name. Apparently, the search engine was connecting the mans name with crimes he had not committed and, according to Japan Times, likely played a role in the sudden loss of his job several years ago and caused several companies to subsequently reject him when he applied for new jobs.","SOS Racisme, Union of Jewish Students of France, the Movement Against Racism and For Friendship Among People sued Google because it suggests the word ""Jewish"" for certain celebrities."
234,"A Japanese court issued a provisional order requesting Google to delete specific terms from autocomplete, the company said in a statement on Monday..Autocomplete is a function on many of Googles search service which uses a mixture of algorithms and stored user data to predict what a person is searching for..It could lead to irretrievable damage such as a loss of job or bankruptcy just by showing search results that constitute defamation or a violation of the privacy of an individual person or small and medium-sized companies, Mr Tomita told Japanese news agency Kyodo..Google defended the system, arguing that as results were generated mechanically - rather than by an individual - it was not an invasion of privacy..It is not the first time the feature has come under scrutiny. In December 2010, Google tweaked autocomplete so that terms relating to piracy did not automatically appear.",Google autocomplete affected a man's reputation in employment after suggesting crimes he was not a part of
236,"Harvard professor Latanya Sweeney said names typically associated with black people were more likely to produce ads related to criminal activity..In a statement to the BBC, the company said: We also have an anti and violence policy which states that we will not allow ads that advocate against an organisation, person or group of people..Prof Sweeneys investigation suggests that names linked with black people - as defined by a previous study into racial discrimination in the workplace - were 25% more likely to have results that prompted the searcher to click on a link to search criminal record history..She found that names like Leroy, Kareem and Keisha would yield advertisements that read Arrested?, with a link to a website which could perform criminal record checks..There is discrimination in the delivery of these ads, concluded Prof Sweeney, adding that there was a less than 1% chance that the findings could be based on chance..Alongside news stories about high school athletes and children can be ads bearing the childs name and suggesting arrest. This seems concerning on many levels..However, she was reluctant to pinpoint a cause for the discrepancies, saying that to do so required further information about the inner workings of Google AdSense..She noted that one possible cause may be Googles smart algorithms - technology which automatically adapts advertising placement based on mass-user habits..In other words, it may be that the search engines are reflecting societys own prejudices - as the advertising results Google serves up are often based on the most popular links previous users have clicked on.",Names typically associated with black people are more likely to produce ads related to criminal activity
237,"The House of Commons public accounts committee has announced an investigation into the Home Office’s treatment of thousands of international students who may have been wrongly accused of cheating in an English language test they were required to sit as part of a visa application process..More than 100 students signed a letter calling on the home secretary, Sajid Javid, to make a long-promised statement on the issue and to act swiftly to resolve the situation in the time he has remaining before a possible government reshuffle once the Conservative leadership campaign ends..About 2,500 students have been forcibly removed from the UK after being accused of cheating in the exam and a further 7,200 left the country after being warned they faced detention and removal if they stayed. Many have protested their innocence: 12,500 appeals have been heard in UK courts, and so far 3,600 people have won their appeals..“We are some of the tens of thousands of international students unjustly robbed of our visas and our rights by the Home Office in 2014 after we were accused of cheating on an English test,” the students’ letter reads. “We are innocent but the government gave us no real way to defend ourselves, so we’ve been fighting for five years to clear our names. The department you lead ruined our lives and stole our futures. It branded us as frauds, forcing us to bear a lifelong mark of shame, while never presenting any evidence at all against most of us.”.The students added: “Many of us are destitute, barely able to live from day to day. Many of us are on medication for stress or depression. Many of us have been rejected by our families, who are shamed by the allegation of cheating. Some of us have tried to kill ourselves.”.A National Audit Office investigation into the Home Office response to reports of cheating in English language tests concluded last month that some people may have been wrongly accused and unfairly removed from the UK. The public accounts committee has invited the permanent secretary at the Home Office, Sir Philip Rutnam, to answer questions at a hearing into the issue in July..In 2014, undercover filming by BBC reporters for Panorama revealed organised cheating in two of the 90 Home Office-approved centres offering the test of English for international communication (Toeic) exam, which is required for visa extensions. The government responded by asking the test provider, the US-based Educational Testing Service (ETS), to assess whether the 58,459 tests taken between 2011 and 2014 were valid. The US company made checks and concluded that virtually every test taken was suspicious, identifying 97% of all UK tests as “suspicious”. It classified 58% as “invalid” and 39% as “questionable”..The Home Office suspended the licences of some test centres and revoked the visas of those accused of cheating. Campaigners have questioned whether it is likely that 97% of people taking Toeic tests could have been involved in cheating..The Labour MP Stephen Timms was told by Javid last year that he was “sympathetic” to the students’ situation. Timms also urged Javid to use the time remaining to him in his position as home secretary to resolve the issue. An all-party parliamentary group on the Toeic issue, chaired by Timms, heard this month that the evidence sent to the Home Office by ETS was “unsafe and unreliable”, Timms said. “It is urgent that this is addressed for a large number of students who have been suffering a great deal for a very long time,” he added..Nazek Ramadan, the director of Migrant Voice, a charity that has been supporting the students, said this was a direct plea to the home secretary “who has the power to end this injustice and give them back their future” to make this issue a positive legacy from his time at the Home Office. “Some of them are prone to be detained or deported any time. We know now that the Home Office action was based on unreliable evidence. This should be put right,” she said..A Home Office spokesperson said 25 people involved in the organised cheating exposed by Panorama had received criminal convictions. “The home secretary is considering the findings of the NAO report. He will then make a statement to parliament.”","AI sued in language test cheating to reduce visa application fraud. However, 97% of applications were deemed suspicious "
238,"Facebook says it has agreed to pay $550 million to settle a class-action lawsuit regarding its use of facial recognition technology. The news, reported first this evening by The New York Times, was part of a disclosure the company made as part of its fourth quarter earnings report today. .The case against Facebook has been going on since 2015. The lawsuit alleged that Facebook’s initial version of the its Tag Suggestions tool, which scans a user’s face in photos and offers suggestions about who that person might be, stored biometric data without user consent, violating the Illinois Biometric Information Privacy Act. .In 2018, Facebook began more transparently explaining its facial recognition tech to users, pointing people to a settings page where they could disable it. Last year, the company decided to make facial recognition on the platform opt-in only, after years of turning it on by default for all users. .A federal judge ruled in favor of making the facial recognition case a class action lawsuit in 2018. Facebook appealed that ruling, but lost the appeal in a 3-0 court decision in August of last year. Facebook’s $550 million settlement will be paid out to eligible Illinois users and to cover the plaintiffs’ legal fees, according to The New York Times. While $550 million may seem like a large settlement, it’s essentially pocket change for Facebook, which today reported revenues of $21 billion for the fourth quarter of 2019. .This isn’t the first time the Illinois Biometric Information Privacy Act has been tested in court. A plaintiff sued Google in Chicago for allegedly uploading her photos to Google Photos and scanning her face without her permission, but the case was dismissed in 2018 after a judge found the plaintiff didn’t suffer “concrete injuries.” Snapchat has also been sued for allegedly violating the law. ",Facebook Tag Suggestions tool stores biometric data without user consent
239,"The sport of training software to act intelligently just got its first cheating scandal. Last month Chinese search company Baidu announced that its image recognition software had inched ahead of Google’s on a standardized test of accuracy. On Tuesday the company admitted that it achieved those results by breaking the rules of that test..The academic experts who maintain that test say that makes Baidu’s claims of beating Google meaningless. Ren Wu, the Baidu researcher who led work on the software in question, has apologized and said the company is reviewing its results. The company has amended a technical paper it released on its software..We don’t know whether this was the action of one individual or a strategy of the team as a whole. But why a multibillion dollar corporation might bother to cheat on an obscure test operated by academics on a voluntary basis is actually quite clear..Baidu, Google, Facebook, and other major computing companies have spent heavily in recent years to build research groups dedicated to deep learning, an approach to building machine learning software that has made great strides in speech and image recognition. These companies have worked hard to hire leading experts in the small field – often from each other (see “Is Google Cornering the Market on Deep Learning”). A handful of standardized tests developed in academia are the currency by which these research groups compare one another’s progress and promote their achievements to the public..Baidu got an unfair advantage by exploiting the test’s design. To get your software scored against the ImageNet Challenge you first train it with a standardized set of 1.5 million images. Then you submit the code to the ImageNet Challenge server so its accuracy can be tested on a collection of 100,000 “validation” images that the software has never seen before..Baidu has admitted that it used multiple email accounts to test its code roughly 200 times in just under six months – over four times what the rules allow..Oren Etzioni, CEO of the Allen Institute for Artificial Intelligence, likens what Baidu did to buying multiple lottery tickets. “If you get to buy two tickets a week you have a certain chance if you buy 200 a week you have more of a chance,” he says. On top of that, testing slightly different code over many tests could help a research team optimize its software for peculiarities of the collection of validation images that aren’t reflected in real world photos..Such is the success of deep learning on this particular test that even a small advantage could make a difference. Baidu had reported it achieved an error rate of only 4.58 percent, beating the previous best of 4.82 percent, reported by Google in March. In fact, some experts have noted that the small margins of victory in the race to get better on this particular test make it increasingly meaningless. That Baidu and others continue to trumpet their results all the same - and may even be willing to break the rules - suggest that being the best at machine learning matters to them very much indeed. .Two Dutch researchers have won a major hacking championship by hitting the software that runs the world’s power grids, gas pipelines, and more. It was their easiest challenge yet..An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.",Baidu claimed to meet Google on image recognition software by cheating
240,"Google Photos uses sophisticated facial-recognition software to identify not only individuals, but also specific categories of objects and photo types, like food, cats and skylines..Image recognition programs are far from perfect, however; they sometimes gets things comically wrong, and sometimes offensively so -- as one Twitter user recently found out..Browsing his Google Photos app, Brooklyn resident Jacky Alciné noticed that photos of him and a friend, both of whom are black, were tagged under the label Gorillas. He shared a screencap of the racist label on Twitter, which was spotted by Yahoo Tech..In a subsequent tweetstorm, Zunger said Google was scrambling a team together to address the issue, and the label was removed from his app within 15 hours, Alciné confirmed to Mashable. Zunger said Google was looking at longer-term fixes, too. A Google spokesperson also sent an official statement:.“We’re appalled and genuinely sorry that this happened. We are taking immediate action to prevent this type of result from appearing. There is still clearly a lot of work to do with automatic image labeling, and we’re looking at how we can prevent these types of mistakes from happening in the future.”.This isnt the first time software has inadvertently maligned dark-skinned people, unfortunately. In May, Flickrs auto-tagging feature tagged a black person as an ape, although it put the same tag on a white woman as well. And years ago, some webcams on laptops made by HP didnt track the faces of black people even though they did so for white users..At least in the case of Google Photos, the incident appears to be isolated, as it doesnt appear that other users have come forward with similar complaints of offensive tags. But its a reminder that, although computers are beginning to do a really good job of simulating human vision, theyre a long way off from simulating human sensitivity.",Google Photos App tagged Black people as gorillas.
241,"Just when you thought biases were a completely human construct, more evidence suggests that both algorithms and interfaces could be biased, too..The latest example of this is from a study conducted by researchers from University of Washington and University of Maryland and reveals how a gender bias is working its way through web searches when people look for images to represent careers and jobs..First, they did a comparative analysis to see if the prevalence of men and women in image search results for professions actually correspond to their representation in actual professions. The researchers did this by comparing the number of women who appeared in the top 100 Google image search results in July 2013 for 45 different occupations, which ranged from bartender to chemist to welder, with 2012 U.S. Bureau of Labor statistics of how many women actually worked in those fields. Then, they did a qualitative analysis to see how men and women are portrayed in the image results..The answers were equally compelling. For instance, according to their study, more than half of U.S. authors are women (56%), yet the image search shows only about 25% women authors..On the flip side is telemarketing, an industry where men and women are equally represented, but the Google image results would have you believe that 64% of telemarketers are female..Not all the results were so skewed. The research uncovered that, in nearly half of the professions, the actual gender representation and the image search numbers were within 5 percentage points of each other..How men and women looked in those images was another story. When the researchers asked participants to rate professionalism, images showing a person who matched the majority gender for the job was viewed as more competent, professional, and trustworthy. Those who didn’t match were rated provocative or inappropriate..“A number of the top hits depicting women as construction workers are models in skimpy little costumes with a hard hat posing suggestively on a jackhammer. You get things that nobody would take as professional,” says Cynthia Matuszek, a co-author of the study..None of this would matter if people wouldn’t then be nudged into making assumptions about men and women in particular roles in the real world. However, when the researchers manipulated the search results, not surprisingly, participants’ opinions changed to conform with stereotypes. Though they stressed that this was just a short-term observation, other research bears out that incremental exposure to these images over time will contribute to unconscious bias..It has also already been revealed that Wikipedia’s entries–a supposed bastion of diversity and editorial neutrality–skew heavily towards men in both actual articles as well as within links. Articles about women tended to be linked to those about men..Part of this is due to Wikipedia’s community, the preponderance of which are educated men, who are English-speaking and hail from mostly Christian countries..In addition to the image searches being gender biased in some cases, Google’s also been taken to task for lack of diversity within its ranks and even disproportionately using white men in its doodles..While Google may not be aware of the results of this latest study and the researchers’ recommendation, the search giant did recognize that its tough for anyone–even its own cadre of emotionally intelligent staff–to process the 11 million bits of information that we are bombarded with at any given moment and focus instead on finding out what biases might spring from them..As such, Google offers a workshop focused on unconscious biases that might sabotage the workplace dynamics or upend the equality of the hiring process..The researchers of this study hope that the information will influence designers of search engines to create algorithms that more accurately represent reality. Sean Munson, UW assistant professor of human-centered design and engineering and a coauthor of the study says: “[Search engine designers] may come to a range of conclusions, but I would feel better if people are at least aware of the consequences and are making conscious choices around them.”.Lydia Dishman is a staff editor for Fast Companys Work Life section. She has written for CBS Moneywatch, Fortune, The Guardian, Popular Science, and the New York Times, among others.",A comparative analysis to see the prevalence of gender in different professions showed that Google perpetuated societal stereotypes 
242,"Well, it the facial verification was done by a human and I put my hand in the fire that a machine would have not recognized me at all (I lost 80 kg and had my beard shaved, which wasnt in the first photo)..It explained Russias facebook manipulation WITH EXAMPLES in episode 2, it showed in an episode a MAGA rally giving BLM protestors a voice and support. And CHEERING! .Your passport contains your photo, and the immigration agent is expected to do a visual check that you match your passport photo. In many countries they will also take a photo of you at passport control where theres nothing stopping them running it through a facial recognition algorithm and comparing it to your passport. They will also keep a record of your passport, including the photo..Ive lived and worked in Singapore. I like it a lot. But Singapore proves the rule that the only way to have and maintain a peaceful, diverse multicultural society is thru an omnipresent police state..Your equation starts off by a factor of 1000.   Of course, thats of little concern to the 26 thousand or so falsely accused hypothetical citizens.  (And the 26 thousand or so falsely innocent are partying on, dude.).Currently in the middle of covid it is mandatory to wear face masks in many countries, including in singapore... How well will facial recognition work if people are wearing face masks? Even when it stops being mandatory, many people have now grown accustomed to wearing masks and seeing others doing so, so its going to be a long time before people are routinely walking around without masks again..Currently in the middle of covid it is mandatory to wear face masks in many countries, including in singapore... How well will facial recognition work if people are wearing face masks? Even when it stops being mandatory, many people have now grown accustomed to wearing masks and seeing others doing so, so its going to be a long time before people are routinely walking around without masks again..There may be more comments in this discussion. Without JavaScript enabled, you might want to turn on Classic Discussion System in your preferences instead.","Singapore is using a biometric check to secure access to both private and government services. The system has checked if the person is authentically there, a photograph, video, or deepfake cannot be used. "
243,"Algorithms have been rising fast and saturating our modern world, from simple ones generating early credit scores in the 1960s to deep neural networks identifying security risks such as guns or certain blacklisted individuals in CCTV feeds. The rise of algorithms is motivated by both operational cost savings and higher quality decisions. Human decision-makers often are more expensive than a machine, especially when thousands or millions of similar decisions need to be made, and we’re notoriously fickle, inconsistent deciders. .To achieve this efficiency and consistency, algorithms are designed to remove many human cognitive biases, such as confirmation bias, overconfidence, anchoring on irrelevant reference points (which mislead our conscious reasoning), and social and interest biases (which cause us to override proper reasoning due to competing personal interests)..Yet in designing algorithms, we could go even further. The algorithms we implement could become tools to help tackle even deeper-seated societal biases, such as notorious racial and gender biases..As an avalanche of news and research reports has shown, algorithms still are not free of bias and often exacerbate it. While sometimes the effects of algorithmic bias are trivial (such as our social media feeds being anchored in puppy videos because the very first post you ever clicked on was one), at other times they can wreak havoc on a person’s life. . The algorithms we implement could become tools to help tackle even deeper-seated societal biases, such as notorious racial and gender biases..Often, specific (not least demographic) groups of individuals are affected. COMPAS, an algorithm U.S. authorities use to estimate how likely it is that a criminal will re-offend, has been found to exhibit racial bias; Google’s algorithms for picking job ads have shown a preference for lower-paying jobs for female users. In cases like these, algorithms aren’t merely failing to correct bias; they’re entrenching it..While some algorithmic biases are artifacts of human error—created by inadequate data or statistical techniques, for instance—many algorithmic biases mirror societal biases at large. A number of studies have made the case that bail and parole judges show bias if the defendant is Black or the judge is tired; biased policing (e.g., in deciding to pull over a vehicle for a routine check or whether to carry out a drug test) can create biased evidence. Actions taken by these biased actors will feed into algorithms; if a re-offender is more likely to actually get convicted of another offence if she is Black, the algorithm will assign a higher probability of re-offending to Black people..Worryingly, sometimes we can’t even recognize that an algorithm is biased until real-life circumstances change drastically. For example, orchestras typically did not even consider female applicants, but then it became common practice to hide auditioning musicians behind curtains, thus concealing their gender. Now female musicians abound in the world’s leading orchestras—but if their pay is decided by humans, gender discrimination still can persist..For governments and business managers, then, expunging algorithmic bias becomes a circular problem. The only data they have to develop algorithms is shaped by the very bias they are fighting. What’s more, data scientists have insufficient control over the hundreds or thousands of agents that generate the data and therefore imbue it with their own deeply rooted biases. . For governments and business managers, then, expunging algorithmic bias becomes a circular problem. The only data they have to develop algorithms is shaped by the very bias they are fighting..When we consider changing an algorithm to eliminate bias, it is helpful to distinguish what we can change at three different levels (from least to most technical): the decision algorithm, formula inputs, and the formula itself. .In discussing the levels, I will use a fictional example, involving Martians and Zeta Reticulans. I do this because picking a real-life example would, in fact, be stereotyping—I would perpetuate the very biases I try to fight by reiterating a simplified version of the world, and every time I state that a particular group of people is disadvantaged, I also can negatively affect the self-perception of people who consider themselves members of these groups. I do apologize if I unintentionally insult any Martians reading this article!.On the simplest and least technical level, we would adjust only the overall decision algorithm that takes one or more statistical formulas (typically to predict unknown outcomes such as academic success, recidivation, or marital bliss) as an input and applies rules to translate the predictions of these formulas into decisions (e.g., by comparing predictions with externally chosen cutoff values or contextually picking one prediction over another). Such rules can be adjusted without touching the statistical formulas themselves..An example of such an intervention is called boxing. Imagine you have a score of astrological ability. The astrological ability score is a key criterion for shortlisting candidates for the Interplanetary Economic Forecasting Institute. You would have no objective reason to believe that Martians are any less apt at prognosticating white noise than Zeta Reticulans; however, due to racial prejudice in our galaxy, Martian children tend to get asked a lot less for their opinion and therefore have a lot less practice in gabbing than Zeta Reticulans, and as a result only one percent of Martian applicants achieve the minimum score required to be hired for the Interplanetary Economic Forecasting Institute as compared to three percent of Zeta Reticulans..Boxing would posit that for hiring decisions to be neutral of race, for each race two percent of applicants should be eligible, and boxing would achieve it by calibrating different cut-off scores (i.e., different implied probabilities of astrological success) for Martians and Zeta Reticulans..Another example of a level-one adjustment would be to use multiple rank-ordering scores and to admit everyone who achieves a high score on any one of them. This approach is particularly well suited if you have different methods of assessment at your disposal, but each method implies a particular bias against one or more subsegments. An example for a crude version of this approach is admissions to medical school in Germany, where routes include college grades, a qualitative assessment through an interview, and a waitlist..At the second level, moving up in technical complexity, you could adjust inputs into the statistical formula. In credit scoring, there exists a precedent insofar as many countries require personal bankruptcies “to be forgotten” after a certain number of years. Expanding on this (and staying with our hiring example), you can analytically determine which inputs into your astrological ability score show systematic differences for historically discriminated populations, and mathematically adjust them. For example, say speed shouting while staring without blinking is what you care about. Martians tend to achieve a 20 percent lower test score in speed shouting than Zeta Reticulans but can stare at you without blinking 10 percent longer than their galactical counterparts. If both metrics enter your formula, in the most basic form you could divide the speed shouting test score by 0.8 and the stare score by 1.1 for Martian applicants in order for them to look on average like Zeta Reticulans (and it goes without saying that more sophisticated adjustment approaches exist)..On the third, and most technical, level, you could develop a statistical formula with an explicit switch to turn off a source of bias (such as race). This approach is most elegant but also most anathema to standard statistical procedures (that, by definition, are grounded in reality as opposed to wishful thinking). Imagine you included race in an interplanetary score of astrological ability. The algorithm would estimate the probability that a Martian correctly predict the future and the probability that a Zeta Reticulan did. With a statistical sleight of hand, it can adjust for these differences and set everyone’s race to Martian, thus eliminating any effect of race. (Technical note: in order to completely remove the bias, you need to remove the effect of race from all other inputs—this then means for the algorithm that the race variable also accounts for expected differences in speed shouting and staring ability.).This practice of simulating different states of the world is, in fact, widely used in some disciplines such as economics. It is also very transparent, and if you think of the estimated probability as a score (similar to, say, point scores used for immigration admissions in some countries), setting a Martian’s race flag to that of a privileged race is the same as giving some bonus points to a disadvantaged group..Why do these techniques matter? They confer at least three advantages. First, they are based in an explicit calibration to reach a neutral ground—a bias manifested in the data against a particular group is eliminated but there is no advantage given to anyone beyond that. This heads off complaints about reverse discrimination..Second, there is ample evidence that credible judgments can create their own reality. Just as the famous experiment by Jane Elliott showed that school children will perform better if they are told that they are genetically destined to be more intelligent, a score vouching for an applicant’s ability could become a self-fulfilling prophecy. Algorithms that eliminate systematic biases create environments that encourage disadvantaged groups to succeed. .And third, they are easy to administer. One big challenge to overcoming human bias in decision-making is that you need to overcome bias one decision-maker at a time (e.g., by providing anti-bias training to recruiters or managers responsible for staff evaluations). The reason algorithms can be a game-changer in fighting bias is that they are centrally designed and deployed through automated systems—thus forcefully overcoming widespread cognitive biases. Doesn’t this sound like a powerful formula for a better world?.Tobias Baer is a consultant and scholar focusing on the intersections of data science, risk management, and psychology. At University of Cambridge, he is a member of the Wolfson College and teaches executive classes on Artificial Intelligence and Risk Management at the Møller Centre. He is author of Understand, Manage, and Prevent Algorithmic Bias: A Guide for Business Users and Data Scientists.","We can use boxing, multiple ranking ordering scores, and other methods to fight bias in AI"
244,"In November of 2018, a new deep-learning tool went online in the emergency department of the Duke University Health System. Called Sepsis Watch, it was designed to help doctors spot early signs of one of the leading causes of hospital deaths globally..Sepsis occurs when an infection triggers full-body inflammation and ultimately causes organs to shut down. It can be treated if diagnosed early enough, but that’s a notoriously hard task because its symptoms are easily mistaken for signs of something else..Sepsis Watch promised to change that. The product of three and a half years of development (which included digitizing health records, analyzing 32 million data points, and designing a simple interface in the form of an iPad app), it scores patients on an hourly basis for their likelihood of developing the condition. It then flags those who are medium or high risk and those who already meet the criteria. Once a doctor confirms the diagnosis, the patients get immediate attention..In the two years since the tool’s introduction, anecdotal evidence from Duke Health’s hospital managers and clinicians has suggested that Sepsis Watch really works. It has dramatically reduced sepsis-induced patient deaths and is now part of a federally registered clinical trial expected to share its results in 2021..At first glance, this is an example of a major technical victory. Through careful development and testing, an AI model successfully augmented doctors’ ability to diagnose disease. But a new report from the Data & Society research institute says this is only half the story. The other half is the amount of skilled social labor that the clinicians leading the project needed to perform in order to integrate the tool into their daily workflows. This included not only designing new communication protocols and creating new training materials but also navigating workplace politics and power dynamics..The case study is an honest reflection of what it really takes for AI tools to succeed in the real world. “It was really complex,” says coauhtor Madeleine Clare Elish, a cultural anthropologist who examines the impact of AI..Innovation is supposed to be disruptive. It shakes up old ways of doing things to achieve better outcomes. But rarely in conversations about technological disruption is there an acknowledgment that disruption is also a form of “breakage.” Existing protocols turn obsolete; social hierarchies get scrambled. Making the innovations work within existing systems requires what Elish and her coauthor Elizabeth Anne Watkins call “repair work.”.During the researchers’ two-year study of Sepsis Watch at Duke Health, they documented numerous examples of this disruption and repair. One major issue was the way the tool challenged the medical world’s deeply ingrained power dynamics between doctors and nurses..In the early stages of tool design, it became clear that rapid response team (RRT) nurses would need to be the primary users. Though attending physicians are typically in charge of evaluating patients and making sepsis diagnoses, they don’t have time to continuously monitor another app on top of their existing duties in the emergency department. In contrast, the main responsibility of an RRT nurse is to continuously monitor patient well-being and provide extra assistance where needed. Checking the Sepsis Watch app fitted naturally into their workflow..But here came the challenge. Once the app flagged a patient as high risk, a nurse would need to call the attending physician (known in medical speak as “ED attendings”). Not only did these nurses and attendings often have no prior relationship because they spent their days in entirely different sections of the hospital, but the protocol represented a complete reversal of the typical chain of command in any hospital. “Are you kidding me?” one nurse recalled thinking after learning how things would work. “We are going to call ED attendings?”.But this was indeed the best solution. So the project team went about repairing the “disruption” in various big and small ways. The head nurses hosted informal pizza parties to build excitement and trust about Sepsis Watch among their fellow nurses. They also developed communication tactics to smooth over their calls with the attendings. For example, they decided to make only one call per day to discuss multiple high-risk patients at once, timed for when the physicians were least busy..On top of that, the project leads began regularly reporting the impact of Sepsis Watch to the clinical leadership. The project team discovered that not every hospital staffer believed sepsis-induced death was a problem at Duke Health. Doctors, especially, who didn’t have a bird’s-eye view of the hospital’s statistics, were far more occupied with the emergencies they were dealing with day to day, like broken bones and severe mental illness. As a result, some found Sepsis Watch a nuisance. But for the clinical leadership, sepsis was a huge priority, and the more they saw Sepsis Watch working, the more they helped grease the gears of the operation..Elish identifies two main factors that ultimately helped Sepsis Watch succeed. First, the tool was adapted for a hyper-local, hyper-specific context: it was developed for the emergency department at Duke Health and nowhere else. “This really bespoke development was key to the success,” she says. This flies in the face of typical AI norms. .Second, throughout the development process, the team regularly sought feedback from nurses, doctors, and other staff up and down the hospital hierarchy. This not only made the tool more user friendly but also cultivated a small group of committed staff members to help champion its success. It also made a difference that the project was led by Duke Health’s own clinicians, says Elish, rather than by technologists who had parachuted in from a software company. “If you don’t have an explainable algorithm,” she says, “you need to build trust in other ways.”.These lessons are very familiar to Marzyeh Ghassemi, an incoming assistant professor at MIT who studies machine-learning applications for health care. “All machine-learning systems that are ever intended to be evaluated on or used by humans must have socio-technical constraints at front of mind,” she says. Especially in clinical settings, which are run by human decision makers and involve caring for humans at their most vulnerable, “the constraints that people need to be aware of are really human and logistical constraints,” she adds..Elish hopes her case study of Sepsis Watch convinces researchers to rethink how to approach medical AI research and AI development at large. So much of the work being done right now focuses on “what AI might be or could do in theory,” she says. “There’s too little information about what actually happens on the ground.” But for AI to live up to its promise, people need to think as much about social integration as technical development..Her work also raises serious questions. “Responsible AI must require attention to local and specific context,” she says. “My reading and training teaches me you can’t just develop one thing in one place and then roll it out somewhere else.”.“So the challenge is actually to figure how we keep that local specificity while trying to work at scale,” she adds. That’s the next frontier for AI research. .An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.","Sepsis Watch, a deep learning tool, reduced sepsis-induced patient deaths because of nurse feedback and local and specific context"
245,"Like a more crooked version of the Voight-Kampff test from Blade Runner, a new machine learning paper from a pair of Chinese researchers has delved into the controversial task of letting a computer decide on your innocence. Can a computer know if youre a criminal just from your face?.In their paper Automated Inference on Criminality using Face Images, published on the arXiv pre-print server, Xiaolin Wu and Xi Zhang from Chinas Shanghai Jiao Tong University investigate whether a computer can detect if a human could be a convicted criminal just by analysing his or her facial features. The two say their tests were successful, and that they even found a new law governing the normality for faces of non-criminals..They described the idea of algorithms that can match and exceed a humans performance in face recognition to infer criminality irresistible. But as a number of Twitter users and commenters on Hacker News point out, by stuffing biases into artificial intelligence and machine learning algorithms, the computer could act on those biases. The researchers maintain that the data sets were controlled for race, gender, age, and facial expressions, though.. Imagine this with drones, every CCTV camera in every city, the eyes of self driving cars, everywhere theres a camera… Tim MaughanNovember 18, 2016 .Imagine this with drones, every CCTV camera in every city, the eyes of self driving cars, everywhere theres a camera… Tim MaughanNovember 18, 2016.The images used in the research were standard ID photographs of Chinese males between the ages of 18 and 55, with no facial hair, scars, or other markings. Wu and Zhang stress that the ID photos used were not police mugshots, and that out of 730 criminals, 235 committed violent crimes including murder, rape, assault, kidnap, and robbery..The two state they purposely took away any subtle human factors out of the assessment process. As long as data sets are finely controlled, could human bias be completely eradicated? Wu told Motherboard that human bias didnt come into it. In fact, we got our first batch of results a year ago. We went through very rigorous checking of our data sets, and also ran many tests searching for counterexamples but failed to find any, said Wu..Heres how it worked: Xiaolin and Xi fed into a machine learning algorithm facial images of 1,856 people, of which half were convicted criminals, and then observed if any of their four classifiers—each using a different method of analysing facial features—could infer criminality..They found that all four of their different classifiers were mostly successful, and that the faces of criminals and those not convicted of crimes differ in key ways that are perceptible to a computer program. Moreover, the variation among criminal faces is significantly greater than that of the non-criminal faces, Xiaolin and Xi write..All four classifiers perform consistently well and produce evidence for the validity of automated face-induced inference on criminality, despite the historical controversy surrounding the topic, the researchers write. Also, we find some discriminating structural features for predicting criminality, such as lip curvature, eye inner corner distance, and the so-called nose-mouth angle. The best classifier, known as the Convolutional Neural Network, achieved 89.51 percent accuracy in the tests..By extensive experiments and vigorous cross validations, the researchers conclude, we have demonstrated that via supervised machine learning, data-driven face classifiers are able to make reliable inference on criminality..While Xiaolin and Xi admit in their paper that they are not qualified to discuss or to debate on societal stereotypes, the problem is that machine learning is adept at picking up on human biases in data sets and acting on those biases, as proved by multiple recent incidents. The pair admit theyre on shaky ground. We have been accused on Internet of being irresponsible socially, Wu said..In the paper they go on to quote philosopher Aristotle, It is possible to infer character from features, but that has to be left to human psychologists, not machines, surely? One major concern going forward is that of false positives—that is, identifying innocent people as guilty—especially if this program is used in any sort of real-world criminal justice settings. The researchers said the algorithms did throw up some false positives (identifying non-criminals as criminals) and false negatives (identifying criminals as non-criminals), which increased when the faces were randomly labeled for control tests..Online critics have lambasted the paper. I thought this was a joke when I read the abstract, but it appears to be a genuine paper, said a user on Hacker News. I agree its an entirely valid area of study…but to do it you need experts in criminology, physiology and machine learning, not just a couple of people who can follow the Keras instructions for how to use a neural net for classification..Others questioned the validity of the paper, noting that one of the researchers is listed as having a Gmail account. First of all, I dont think this is satire. Ill admit that the use of a gmail account by a researcher at a Chinese uni is facially suspicious, posed another Hackers News reader..Wu had an answer for this, however. Some questioned why I used gmail address as a faculty member in China. In fact, I am also a professor at McMaster University, Canada, he told Motherboard..By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.","The ""Automated Inference on Criminality using Face Images"" declares that a computer can detect if a human could be a convicted criminal based on facial features. "
247,"The company’s platform lets advertisers exclude people of certain races from seeing their content. That’s a serious problem when it comes to promotions such as housing, credit, and jobs..With more than 1.5 billion users, Facebook has become one of the most powerful, highly visible platforms on the internet. It’s no wonder then that so many advertisers clamor for space on the social-media site. But a recent investigation from ProPublica found that Facebook may be allowing those advertisers to discriminate based on race..Facebook’s ability to let advertisers target a specific audience—for instance, women between the ages of 25 and 34 with young children—is its primary strength. More and more advertisers count on being able to identify, and market to, very specific groups. But Facebook’s advertising system not only allows marketers to choose who they most want to see their ads—it also allows them choose entire groups who will never see their ads..When placing an ad on Facebook, advertisers can explicitly exclude lots of groups, including people with any given educational level, financial status, political affiliation, and—perhaps most disturbingly—“ethnic affinity.”.“Targeting ads for housing, credit, or employment based upon race, gender, or sexual orientation violates the federal civil-rights laws that cover those fields—the Fair Housing Act, the Equal Credit Opportunity Act, and Title VII,” says Rachel Goodman, a lawyer at the American Civil Liberties Union. “If Facebook is going to allow advertisers to target ads toward or away from users based on these sensitive characteristics, it must at the very least prohibit targeting in these three areas central to economic prosperity.”  (A spokesperson from Facebook noted that the ad placed by ProPublica was not promoting a rental property, but promoting an event about renters’ rights.).Steve Satterfield, Facebook’s privacy and public-policy manager, told ProPublica that any use of its advertising platform to intentionally discriminate is a violation of the site’s policy, saying, “We take a strong stand against advertisers misusing our platform: Our policies prohibit using our targeting options to discriminate, and they require compliance with the law.” Satterfield also said that “ethnic affinity” is determined based on what sort of content on the site a user engages with most. That’s not the same as identifying a user’s race, but Facebook does place this category under an ad-targeting category called “demographics.” How accurately does “ethnic affinity” map onto users’ race? It’s hard to say for sure, but a recent report from the Pew Research center suggests it is not so difficult to determine a user’s race based on their use of the site..The creation of an ad platform that allows marketers to carve out entire groups of people based on race or ethnicity isn’t unique. “There are a number of sites that allow you to specify based on race,” says Aaron Rieke, one of the heads of the tech-policy consulting firm Upturn. “But Facebook is special because it’s extraordinarily powerful.” The platform is extremely large, and that, Rieke says, means that it probably can’t review every single ad that is entered into its system. Still, the current system doesn’t have enough safeguards. For instance, relying on users to report whether or not they are being discriminated against is difficult if the discrimination is based on preventing someone from ever seeing an ad, he says.  Goodman agrees, saying, “When this kind of targeting happens online, it’s nearly impossible for people to know they’ve been denied information about opportunities they might be interested in.”.A better ad-buying platform might involve a system under which ads in areas where the U.S. has key civil-rights legislation—such as housing, credit, or employment—that also include ethnic targeting automatically get flagged for review. That type of due diligence already exists in the industry, Rieke says..Granting advertisers the ability to ensure that minorities aren’t able to view certain ads is disturbing and potentially illegal. When ProPublica’s reporters described their experience placing a sample ad that would exclude blacks, they did so for a very specific reason: There is a law—the Fair Housing Act—expressly forbidding racial discrimination in housing. Among its many provisions, the act states that the following is illegal:.To make, print, or publish, or cause to be made, printed, or published any notice, statement, or advertisement, with respect to the sale or rental of a dwelling that indicates any preference, limitation, or discrimination based on race, color, religion, sex, handicap, familial status, or national origin, or an intention to make any such preference, limitation, or discrimination..This law exists for a reason—and it’s not just some quaint notion of fairness. Housing discrimination, particularly the long history of redlining, in this country has led to a vast and possibly permanent divide between white and black Americans when it comes to wealth, which stands at 13-to-one. Given that housing is the most valuable asset held by most Americans, exclusion from safe, affluent, white neighborhoods results not only in racial segregation, but also in black Americans remaining mired in concentrated poverty in neighborhoods with fewer services, lower tax bases, worse schools, and homes that didn’t grown in value..Facebook’s claim that the intent of its audience-targeting tool wasn’t to allow for discrimination may well be true. But as a site that supposedly prides itself on inclusion, the company has a responsibility to ensure that advertisers cannot perpetuate discrimination right under its nose.",Facebook lets advertisers exclude people of certain races from seeing ads.
249,"More than two years ago, Fast Company published a story with the headline “Two Ex-Googlers Want To Make Bodegas And Mom-And-Pop Corner Stores Obsolete.” The focus of the story was a nascent startup by the name of Bodega..The company had raised $2.5 million in funding from First Round Capital’s Josh Kopelman, Forerunner Ventures’ Kirsten Green and Homebrew’s Hunter Walk. To announce their funding and vision to create the unmanned store of the future, Bodega briefed a number of journalists on its big idea. Given the simplicity of its product — a tech-enabled vending machine, in essence — the team was blindsided by the uproarious response that followed. September 13, 2017 was supposed to be the most exciting day in the startup’s history, at least until that point; instead, it was a nightmarish lesson in poor branding and messaging..The press storm and public lambasting catapulted Bodega into the limelight — for all the wrong reasons. Overnight, the company went from just another early-stage commerce business to the symbol of everything that is wrong with Silicon Valley. Many wondered if it would fall victim to criticism and crumble like Juicero, a well-financed startup that sold a $400 juicer — that is, until a Bloomberg story proved its juice packets could be squeezed by hand, no machine necessary. Or would it take the public condemnation in stride, hearing out the critics and amending its brand as necessary?.Two years after its ill-fated launch, the latter seems to be true. Today, the three-year-old Oakland-based company — now known as Stockwell — is said to be growing quickly thanks to more than $45 million in venture capital funding from a number of deep-pocketed investors, the company has confirmed to TechCrunch..“Bodega is either the worst named startup of the year, or the most devious,” wrote The Verge in the fall of 2017. “Tech firm markets glorified vending machines where users can buy groceries,” said The Guardian. The Washington Post dubbed the company “America’s most hated start-up.” CityLab, which writes about issues impacting cities, bluntly reported “Bodega, a Startup for Disrupting Bodegas, Is Terrible,” followed by 30 reasons why the startup sucks: “Maybe a Bodega can stock Soylent to appeal to people who also think that eating delicious food is a grim burden,” CityLab wrote. “Why do tech wizards keep thinking of new and more horrible ways to avoid dealing with people? How come they hate being human?”.It’s safe to say Bodega endured one of the most catastrophic company launches in the history of tech startups. But the press cycle surrounding Bodega was more than an attack on the startup alone. It represented a greater frustration with Silicon Valley culture and its reputation for funding “disruptive” products devoid of impact. Time and time again, VCs had proven their willingness to inject millions into standard concepts lacking originality. A juicer had raised more than $100 million, after all, scooters were beginning to attract private capital and Soylent, which sells a meal replacement drink fit for techies, was hot off the heels of a $50 million round..A mini-fridge equipped with computer vision technology boasting a culturally insensitive name wasn’t going to change the world. Questioning why it had the support of VCs was only fair..Behind the upsetting name was a business developing hundreds of five-foot-wide pantry boxes to be housed in luxury apartment lobbies, offices, college campuses, gyms and more. Similar to Amazon Go, the “smart stores” recognize what customers remove from the cases using computer vision and automatically charge the credit card associated with the account..Bodega was founded by a pair of Google veterans, Paul McDonald and Ashwath Rajan. It had all the ingredients for a successful startup stew. Founders with years of experience in big tech: McDonald spent more than a decade at Google; Rajan had just finished up the search engine’s competitive associate product manager program. Both attended top universities: University of California-Berkeley and Columbia University, respectively. Still, neither of the two men nor their investors seemed to have predicted the controversy afoot..“Bodega doesn’t want to disrupt the bodega,” Hunter Walk, a Bodega investor and co-founder of the seed fund Homebrew, wrote in a 2017 blog post. “Some instances of today’s press coverage suggested that element, a sound bite which, exacerbated by Bodega’s naming, pissed people off as another example of tech startups being at best tone-deaf, and at worst, predatory … It didn’t occur to me that some people would see the word and associate its use in this context with whitewashing or cultural appropriation.”.The company, too, quickly authored a blog post outlining their thought process behind the name: “Rather than disrespect to traditional corner stores — or worse yet, a threat — we intended only admiration,” McDonald wrote..After penning blog posts, the founders continued working on the company under the provocative and upsetting name. Meanwhile, investors seemed unfazed by the negative press, evidenced by the company’s ability to continue raising venture capital funding. After all, many of the best businesses endure the wrath of bloggers, competing founders and the general public. As for VCs, high-risk bets are just part of the ball game..DCM Ventures, a U.S.-based venture capital fund with offices in Beijing, Tokyo and Silicon Valley, was the first to agree to invest in Bodega following the PR disaster. The firm, an investor in Lime, Hims and SoFi, led a $7.5 million Series A financing in the business in early 2018, the company confirmed. DCM co-founder and general partner David Chao joined the company’s board following the deal. DCM vice president David Cheng is also actively involved with the company, according to his bio..Finally, after pocketing nearly $10 million in total funding, Bodega announced a name change: “Did you buy something today from a Bodega?” Bodega’s McDonald wrote. “You may have noticed that we’ve changed our name to Stockwell. Our new name is one of the changes we’re making as we expand our offerings and open more stores around the country.”.With a new logo and a toned-down, somewhat bland identity, Stockwell had a fresh start and, soon, more attention from top VCs. In late 2018, the company raised a $35 million round of funding led by NEA, an investor known for bets in Coursera, MasterClass and OpenDoor, with participation from Uber and Slack-backer GV, formerly known as Google Ventures, Stockwell confirmed. NEA’s Amit Mukherjee and GV’s John Lyman joined Stockwell’s board as part of the deal, which is said to have valued the business at north of $100 million. Stockwell, however, declined to confirm the figure..Instead of announcing the news via TechCrunch, Venture Beat, Forbes or another tech publication, as is the norm for fast-growing consumer-facing startups, Stockwell remained mum on financing events and scaling plans, assumedly burned by the press and the public’s scorn a year prior..Rather than subject itself to continued scrutiny as it attempted to rewrite its narrative, Stockwell was heads down, iterating, expanding and quietly raising millions. Bad press can break a startup, and given the sheer number of negative reports on Stockwell so early on, the company had already defied the odds. Keeping a low profile was undoubtedly the best strategy moving forward, and it seems to have paid off..Today the company counts 1,000 “stores” in the San Francisco Bay Area, Los Angeles, Houston and Chicago. Stockwell has used its latest infusion of funding to explore shared ownership models, i.e. the opportunity for anyone to run their own Stockwell store. The company tells TechCrunch they are also working on building out their “unique curation model,” which allows customers to help determine what items are stocked in their local “store,” as well as their support for emerging brands, whose products they can stock in their next-generation vending machines..Human beings make snap judgments, evaluate products quickly and can develop distaste for brands in a matter of seconds. A company’s moniker is their first opportunity to impress customers..“When you’re not in the room, the name of your company is what gets passed between people,” writes NFX co-founder James Currier. “It speaks for you when you’re not there … It sets expectations of your company in the blink of an eye. And first impressions are hard to change. Both positive and negative.”.Most cases of poor startup naming are easily fixed. Most founders aren’t forced to bear the brunt of the internet’s fury. The case of Bodega is much more extreme and, as such, serves as the ultimate lesson for founders searching for the best way to tell their story. At the end of the day, avoiding a complete and total train-wreck is easy if you include a diverse group of people in the naming process and remember there’s a lot in a name — if that weren’t the case, Bodega would still be Bodega.","Bodega, a vending machine with AI, received criticism because of misappropriation. Since then, it has changed its name to Stockwell."
251,"MIT has terminated a research collaboration with iFlytek, a Chinese artificial intelligence company accused of supplying technology for surveilling Muslims in the northwestern province of Xinjiang..The university canceled the relationship in February after reviewing an upcoming project under tightened guidelines governing funding from companies in China, Russia, and Saudi Arabia. MIT has not said why it terminated the iFlytek collaboration or disclosed details about the project that prompted the review, but it has faced pushback from some students and staff about the arrangement since it began two years ago..“We take very seriously concerns about national security and economic security threats from China and other countries, and human rights issues,” says Maria Zuber, vice president of research at MIT..US companies and universities have built ties with Chinese tech firms in recent years. But the relationships have come under increasing scrutiny as relations between the two countries have soured..MIT announced what was supposed to be a five-year collaboration with iFlytek with fanfare in June 2018. Since then, iFlytek has helped fund a range of research on subjects including human-computer interaction, new approaches to machine learning, and applied voice recognition. Under the agreement, iFlytek selected existing projects to fund but MIT says the company did not receive special access to the work or receive proprietary data or code. The amount of money involved was not disclosed..The arrangement became more controversial in October 2019, when the US government banned six Chinese AI companies, including iFlytek, from doing business with American firms for reportedly supplying technology used to oppress minority Uighurs in Xinjiang. In 2017, Human Rights Watch claimed iFlytek supplied police departments in Xinjiang with technology for identifying people using their voiceprints. Press reports paint a grim picture of widespread surveillance in the province, including the detention and disappearance of more than 1 million people..iFlytek is one of China’s older AI companies, and while it specializes in voice recognition, it also offers tools for analyzing legal documents and medical imagery. Like other growing Chinese AI companies, contracts to supply software for processing video and audio to police departments and local governments are an important source of revenue..The company said MIT’s decision was disappointing. “We are particularly sorry about this,” says Jiang Tao, a senior VP at iFlytek. “The vision of the cooperation was to build a better world with artificial intelligence together.”.Like other US universities, MIT receives funding from companies and individual donors, but several of its arrangements have proved controversial. In February 2019, the university reexamined funding from Saudi Arabia following the assassination of the journalist Jamal Khashoggi. The tighter guidelines for working with foreign companies were issued in April 2019 amid scrutiny of MIT’s relationship with two other Chinese companies, Huawei and ZTE. MIT had cut funding relationships with those companies in 2018 as the US government investigated their roles in alleged violations of US sanctions. In January 2020, MIT released the results of an investigation into funding from the convicted sex offender Jeffrey Epstein..In 2018, MIT received a onetime donation of an undisclosed sum from SenseTime, another Chinese AI company now subject to the US government restrictions. The gift was reviewed by MIT’s Interim Gift Acceptance Committee, and an MIT spokesperson says there are no plans to return it..US officials are increasingly wary of Chinese companies developing advanced technologies, amid rising trade tensions, accusations of intellectual property theft, and a heightened sense of international competition. Over the past two years, US intelligence agencies have repeatedly warned universities to watch for signs of espionage by Chinese students and professors, and prosecuted both Chinese-born and US academics for stealing intellectual property. In a meeting with senior figures at MIT in November 2019, Michael Kratsios, the US chief technology officer, warned against working with Chinese AI companies, according to a person familiar with the discussion..Paul Triolo, a practice head at Eurasia Group specializing in global technology policy, says concerns over human rights violations are legitimate but the signals coming from the US government have been ambiguous. “Is this some sort of just punishment or really legitimate effort to try to change behavior?” he asks. “The danger is sort of painting them all with one brush, and not looking at what theyre actually doing in Xinjiang, and how much they are taking steps to step away from that.”.Triolo says a complete unraveling of relations between the US and China will harm American AI too. He notes that China’s tech industry is making rapid progress in medical uses of AI, for example: “The flow of knowledge is not one way.”.MIT’s Zuber says the university doesn’t want to walk away from China. “We want to be able to draw the best talent in the world, and some of that best talent comes from China,” she adds. “The wrong thing to do is say we’re never going to work with these international entities under any circumstances and we’re just going to lock our doors.” Zuber also says “global collaborations are extremely important.”.When it comes to China, it may be difficult to ignore outcry over human rights issues. Zulkayda Mamat, a graduate student of Uighur descent who was critical of MIT’s ties to Chinese AI companies while studying there, welcomed the news but says MIT should scrutinize collaborations carefully. “I hope that it continues the process of reevaluation for all projects,” she pointed out. “[A] lack of vigilance will certainly put it on the wrong side of history.”",MIT cuts funding of iFlytek technology that is being used to oppress ethnic Uighurs in China's northwest
252,"A deal between UK hospitals and Google’s AI subsidiary DeepMind “failed to comply with data protection law,” according to the UK’s data watchdog. The Information Commissioners Office (ICO) made its ruling today after a year-long investigation into the agreement, which saw DeepMind process 1.6 million patient records belonging to UK citizens for the Royal Free Trust — a group of three London hospitals..The deal was originally struck in 2015, and has since been superseded by a new agreement. At the time, DeepMind and the Royal Free said the data was being shared to develop an app named Streams, which would alert doctors if patients were at risk from a condition called acute kidney injury. An investigation by the New Scientist revealed that the terms of the agreement were more broad than had been originally implied. DeepMind has since made new deals to deploy Streams in other UK hospitals..Today, ICO said it had found “a number of shortcomings” with the agreement, particularly that patients had not been fully briefed on how their personal data would be used. In a press statement, the UK’s information commissioner Elizabeth Denham said that the “price of innovation does not need to be the erosion of fundamental privacy rights.”.Patients were not asked if they consented to having their medical data processed by DeepMind. The information shared included including details of drug overdoses, abortions, and whether individuals were HIV positive. DeepMind and the Royal Free have argued that patients had given “implied consent” to sharing, because this information would be used to deliver “direct care” via the Streams app. .Today’s ruling suggests that the two institutions did not go far enough. “Patients would not have reasonably expected their information to have been used in this way, and the Trust could and should have been far more transparent with patients as to what was happening,” said Denham..The contract was always clear that no private data would ever be shared with DeepMind’s parent company Google, which bought the firm in 2014. Neither would machine learning or AI tools be used to analyze this information. (Although DeepMind is involved in two separate deals with UK hospitals to develop AI-powered algorithms for improving cancer treatment and eye disease.) .DeepMind says it welcomes ICO’s “thoughtful resolution” of the case, and admits it made a number of mistakes during its original deal. The company says it should have better explained the deal, to patients and the public, and that it “underestimated the complexity of the NHS and of the rules around patient data.” .In a blog post by the ICO, the watchdog stated that in the rush to innovate, institutions like the Royal Free Trust should not forget to follow the law. The Trust has been asked to sign a new agreement committing it to act in accordance with the law and commission an audit of the 2015 trial. “When you’re setting out to test the clinical safety of a new service, remember that the rules are there for a reason,” writes Denham. ",DeepMind processed 1.6 million patients belonging to UK citizens for the Royal Free Trust.
253,"The woman, identified only by her surname Yan, told the Jiangsu Broadcasting Corp. that her co-worker was able to get into both phones — her original as well as the new one Apple gave her as a replacement, reports the South China Morning Post. .An Apple spokesman told HuffPost that he couldn’t confirm the details of the story, nor did he have enough information to determine what might have gone wrong with the phones. He suspected that both women may have used the phone during its “passcode training” and that the phones may have been essentially “taught” to recognize both faces..The facial recognition software has run into some glitches. It can sometimes mistake twins or siblings, according to Apple. The phone, too, may not accurately identify children under the age of 13 because their faces are not as definitely formed as adults’, according to an Apple security “white paper” on the technology..Apple hasn’t yet confirmed a case of an unrelated adult cracking the phone’s facial recognition software, according to the Apple spokesman. The company insists that the probability of a random person accessing someone else’s iPhone X using the Face ID passcode is 1 in 1 million, versus 1 in 50,000 for Touch ID. Phil Schiller, Apple’s vice president of product marketing, conceded in September: “Of course, the statistics are lowered if that person shares a close genetic relationship with you.”.Unless Apple technicians examine the Chinese phones, it’s unclear what happened. An added complication is that a Chinese company has reportedly begun manufacturing a clone of the iPhone X — with unknown facial recognition capabilities.",A woman is not recognized on facial recognition
254,"The creator of an app which changes your selfies using artificial intelligence has apologised because its “hot” filter automatically lightened people’s skin..FaceApp is touted as an app which uses “neural networks” to change facial characteristics, adding smiles or making users look older or younger. But users noticed one of the options, initially labelled as “hot” made people look whiter..The feature is still available but has now been renamed “spark”, in an attempt to “exclude any positive connotation associated with it”, Goncharov said..In previous interviews Goncharov, who is a former Microsoft and Yandex engineer, said FaceApp differs from other face-tuning software, which usually adds filters, because it uses deep learning technology to alter the photo itself. .Snapchat’s filters have come under fire on several occasions. Last year it was criticised for promoting “yellowface” after it released a filter which allowed users to turn their selfies into Asian caricatures. Prior to that, a Bob Marley filter was dubbed “the digital equivalent of blackface”. ",FaceApp hot filter (neural networks) automatically lightened people's skin
255,"HSBC’s voice recognition ID system used by half a million customers for secure access to their bank accounts has been breached by a customer’s twin mimicking his voice..When it was launched last year HSBC’s head of retail banking claimed the new system was secure, insisting that “just like your fingerprint, your voice print is unique”..But when BBC Click reporter Dan Simmons set up an HSBC voice-ID authenticated account, his non-identical twin, Joe, was able to fool the system and granted him access to his brother’s account. .HSBC said it is to review security on its voice-access systems following the breach. Unlike traditional password systems, which lock users out after repeated attempts fail, Joe Simmons tried seven times to mimic his twin’s voice before HSBC allowed access. .The HSBC system asks users to say “my voice is my password” into the phone, which is then matched to an original recording of the person’s voice, allowing access to their account. .According to the BBC, the breach did not allow Joe Simmons to withdraw money, but he was able to access balances and recent transactions, and was offered the chance to transfer money between accounts..“What’s really alarming is that the bank allowed me seven attempts to mimic my brother’s voiceprint and get it wrong, before I got in at the eighth time of trying,” he said..Voice ID is currently being rolled out to 15 million HSBC customers. At launch, HSBC said: “The technology is now the ultimate way to bank safely and securely, without the need for passwords. With a couple of choice words, banking with HSBC is as easy as being yourself.”.But in a statement issued after the breach was made public, HSBC said: “The security and safety of our customers’ accounts is of the utmost importance to us and Voice ID is amongst the most secure methods of authenticating customers. .“The introduction of this technology has seen a significant reduction in telephone fraud, and has proven to be more secure than PINs, passwords and memorable phrases. Our VoiceID system does allow us to make changes to different security settings, and following a review we have made changes to make it even more secure.”.Embarrassed HSBC officials have suggested that the BBC Click experiment, while real, does not open the door to fraudsters. One said: “This is not how fraudsters work. This was a twin sitting with his brother. He would just as likely know other security data such as mother’s maiden name, pet’s name and so on. .“In a real situation you would not have a fraudster sitting next to you. If he or she tried recording your voice saying ‘my voice is my password’ it would not work either, as the system is able to detect synthetic voice characteristics.” .Barclays introduced voice recognition software for all its 300,000 wealthiest clients in the UK in 2013. A year later the bank said the technology had been so successful that it would be rolled out to 12 million retail banking customers. .Barclays said it had been “incredibly popular” with wealthier clients, with the time taken to verify their identity falling from 1.5 minutes to less than 10 seconds.",Voice recognition system is breached by twin
258,"From the moment we wake up, our days are filled with a constant flow of negotiations. These scenarios range from discussing what TV channel to watch to convincing your kids to eat their vegetables or trying to get a better price on something. What these all have in common is that they require complex communication and reasoning skills, which are attributes not inherently found in computers..To date, existing work on chatbots has led to systems that can hold short conversations and perform simple tasks such as booking a restaurant. But building machines that can hold meaningful conversations with people is challenging because it requires a bot to combine its understanding of the conversation with its knowledge of the world, and then produce a new sentence that helps it achieve its goals..Today, researchers at Facebook Artificial Intelligence Research (FAIR) have open-sourced code and published research introducing dialog agents with a new capability — the ability to negotiate..Similar to how people have differing goals, run into conflicts, and then negotiate to come to an agreed-upon compromise, the researchers have shown that it’s possible for dialog agents with differing goals (implemented as end-to-end-trained neural networks) to engage in start-to-finish negotiations with other bots or people while arriving at common decisions or outcomes..The FAIR researchers studied negotiation on a multi-issue bargaining task. Two agents are both shown the same collection of items (say two books, one hat, three balls) and are instructed to divide them between themselves by negotiating a split of the items..Each agent is provided its own value function, which represents how much it cares about each type of item (say each ball is worth 3 points to agent 1). As in life, neither agent knows the other agent’s value function and must infer it from the dialog (you say you want the ball, so you must value it highly)..FAIR researchers created many such negotiation scenarios, always ensuring that it is impossible for both agents to get the best deal simultaneously. Furthermore, walking away from the negotiation (or not agreeing on a deal after 10 rounds of dialog) resulted in 0 points for both agents. Simply put, negotiation is essential, and good negotiation results in better performance..Negotiation is simultaneously a linguistic and a reasoning problem, in which an intent must be formulated and then verbally realized. Such dialogs contain both cooperative and adversarial elements, requiring agents to understand and formulate long-term plans and generate utterances to achieve their goals..When chatbots can build mental models of their interlocutors and “think ahead” or anticipate directions a conversation is going to take in the future, they can choose to steer away from uninformative, confusing, or frustrating exchanges toward successful ones..Specifically, FAIR has developed dialog rollouts as a novel technique where an agent simulates a future conversation by rolling out a dialog model to the end of the conversation, so that an utterance with the maximum expected future reward can be chosen..Similar ideas have been used for planning in game environments but have never been applied to language because the number of possible actions is much higher. To improve efficiency, the researchers first generated a smaller set of candidate utterances to say, and then for each of these, they repeatedly simulated the complete future of the dialog in order to estimate how successful they were. The prediction accuracy of this model is high enough that the technique dramatically improved negotiation tactics in the following areas:.Negotiating harder: The new agents held longer conversations with humans, in turn accepting deals less quickly. While people can sometimes walk away with no deal, the model in this experiment negotiates until it achieves a successful outcome..Intelligent maneuvers: There were cases where agents initially feigned interest in a valueless item, only to later “compromise” by conceding it — an effective negotiating tactic that people use regularly. This behavior was not programmed by the researchers but was discovered by the bot as a method for trying to achieve its goals..Producing novel sentences: Although neural models are prone to repeating sentences from training data, this work showed the models are capable of generalizing when necessary..In order to train negotiation agents and conduct large-scale quantitative evaluations, the FAIR team crowdsourced a collection of negotiations between pairs of people. The individuals were shown a collection of objects and a value for each, and asked to agree how to divide the objects between them. The researchers then trained a recurrent neural network to negotiate by teaching it to imitate people’s actions. At any point in a dialog, the model tries to guess what a human would say in that situation..Unlike previous work on goal-orientated dialog, the models were trained “end to end” purely from the language and decisions that humans made, meaning that the approach can easily be adapted to other tasks..To go beyond simply trying to imitate people, the FAIR researchers instead allowed the model to achieve the goals of the negotiation. To train the model to achieve its goals, the researchers had the model practice thousands of negotiations against itself, and used reinforcement learning to reward the model when it achieved a good outcome. To prevent the algorithm from developing its own language, it was simultaneously trained to produce humanlike language..To evaluate the negotiation agents, FAIR tested them online in conversations with people. Most previous work has avoided dialogs with real people or worked in less challenging domains, because of the difficulties of learning models that can respond to the variety of language that people can say..Interestingly, in the FAIR experiments, most people did not realize they were talking to a bot rather than another person — showing that the bots had learned to hold fluent conversations in English in this domain. The performance of FAIR’s best negotiation agent, which makes use of reinforcement learning and dialog rollouts, matched that of human negotiators. It achieved better deals about as often as worse deals, demonstrating that FAIR’s bots not only can speak English but also think intelligently about what to say..Supervised learning aims to imitate the actions of human users, but it does not explicitly attempt to achieve an agent’s goals. Taking a different approach, the FAIR team explored pre-training with supervised learning, and then fine-tuned the model against the evaluation metric using reinforcement learning. In effect, they used supervised learning to learn how to map between language and meaning, but used reinforcement learning to help determine which utterance to say..During reinforcement learning, the agent attempts to improve its parameters from conversations with another agent. While the other agent could be a human, FAIR used a fixed supervised model that was trained to imitate humans. The second model is fixed, because the researchers found that updating the parameters of both agents led to divergence from human language as the agents developed their own language for negotiating. At the end of every dialog, the agent is given a reward based on the deal it agreed on. This reward was then back-propagated through every word that the agent output, using policy gradients, to increase the probability of actions that lead to high rewards..This work represents an important step for the research community and bot developers toward creating chatbots that can reason, converse, and negotiate, all key steps in building a personalized digital assistant. Working with the community gives us an opportunity to share our work and the challenges we’re aiming to solve, and encourages talented people to contribute their ideas and efforts to move the field forward.. 	Meta believes in building community through open source technology. Explore our latest projects in Artificial Intelligence, Data Infrastructure, Development Tools, Front End, Languages, Platforms, Security, Virtual Reality, and more. ",The ability for bots to invest into a new language through reinforcement learning could lead to unintended consequences
263,"A federal judge ruled that the Houston ISDs use of a secret algorithm to evaluate teacher performance denied employees the right to challenge their terminations - giving teachers the green light to continue their lawsuit against the nations seventh largest school district..The algorithm used students standardized testing data to compute a score that, among other factors, was used by district officials to determine which teachers were evaluated, fired and given bonuses between 2011 and 2015. It is no longer used by the district..Louis Malfaro, president of the Texas American Federation of Teachers, said the judges ruling means the case will likely be scheduled for a trial in coming months..Were going to proceed with this lawsuit, and based on what the judge wrote, were winning the argument, Malfaro said. At the end of the day, educators care about education and want to see how theyre doing and how to get better. Its time to take teacher evaluations back to what it should be - not another misuse of standardized testing data but a real opportunity for people to not only look at their own work and evaluate it, but also to have the opportunity to improve their practice..But U.S. Magistrate Judge Stephen Wm. Smith stopped short of supporting all of the teachers arguments, including that the standardized-testing-based algorithms failed to advance the districts goal of employing effective teachers and that the system did not show teachers how they could improve their scores..The algorithm HISD used was part of the Educational Value Added Assessment System, or EVAAS, that was created by the private technology firm SAS..Because not even HISD administrators knew how the algorithm worked, teachers argued the district could not provide enough detail about terminations, leaving the teachers unable to defend against possible errors in the calculations..A district website stated no teachers score could be recalculated because that would force HISD to redo the analysis of every teacher, which would be very costly for the district. A change to one teachers report could force changes to all the others, it noted..The remarkable thing about this passage is not simply that cost considerations trump accuracy in teacher evaluations, troubling as that might be, Smith wrote. Of greater concern is the house-of-cards fragility of the EVAAS system, where the wrong score of a single teacher could alter the scores of every other teacher in the district..While teachers and unions further argued that EVAAS also was biased against teachers who educate economically disadvantaged students and English-language learners who typically struggle on standardized tests, the judge ruled that value-added models are a legitimate tool to measure teacher performance..One of the teachers experts admitted that there is evidence that a teachers EVAAS score is correlated with that teachers impact on student learning growth as measured by standardized test scores..Union officials painted the ruling as a watershed moment in the fight against attaching high stakes to standardized tests, while district officials said the judgement has little bearing on its ability to use value-added models to evaluate teachers..The language used by the judge makes it pretty clear that the district doesnt have a leg to stand on in defending the defective evaluation system, said Houston Federation of Teachers President Zeph Capo..But district officials said the only change that must be made as a result of Smiths ruling is that teachers must be able to independently verify their performance scores..Should HISD ever again decide to use EVAAS or value-added scores, the Courts ruling leaves intact the Districts ability to do so, the district said in a statement. The Courts ruling only requires that the score be verifiable by the teacher in the limited circumstance of contract dismissal decisions during the term of the contract. End-of-year dismissals for probationary and term contract teachers are not impacted by this ruling..Shelby Webb is an energy tech, renewable energy reporter for the Houston Chronicle. She previously worked as an education reporter for the Chronicle for more than four years, covering trends across greater Houston and Texas. Before moving to Houston, she worked for her hometown paper in Sarasota, Florida, from 2013 to 2016 and graduated from the University of Florida. .Sanger ISD Challenge Program kids are the so-called worst of the worst behaved. And its here that they find a home. A place where they are listened to and respected.","An algorithm used to compute scores for student testing was used to determine which teachers to evaluate, fire and give bonus between 2011-2015. It is no longer used by the district"
266,"iDenfy has been drafted to provide face biometrics for a smooth and frictionless onboarding and verification for customers obtaining loans online or effecting online payments via the platform of Polish technology firm Provema..As per the partnership, Provema customers seeking to obtain loans remotely will be able to do so without going through complicated processes, as what the company says is a thorough, yet quick identity authentication procedure is used to determine that the customer is genuine and meet know your customer (KYC) regulation requirements..This means that for a customer to be given a loan, they fill out an online application form, state how much money they intend to borrow, and then undergo biometric identity verification, after which the money is directed almost immediately into their bank account. The partnership with iDenfy is part of Provema’s effort to make loan issuance and repayment processes easier for their customers, according to the announcement..Provema says apart from using iDenfy’s biometric access control technologies to guarantee customers’ identification data security, the solution is also expected to help Provema fight fraud, as the artificial intelligence-powered solution has the ability to authenticate documents and detect fakes in just a matter of seconds..iDenfy says its solution works for more than 3,000 documents from people spread across 200 countries of the world, making it possible for Provema’s clients to be conveniently served wherever they may find themselves on the globe..The firms believe that their partnership will positively influence the global financial services industry by encouraging financial companies to adopt identity verification technologies that guarantee greater security and rapidity in their transactions with customers..biometric identification  |  biometrics  |  digital identity  |  facial recognition  |  financial services  |  iDenfy  |  identity verification  |  KYC  |  remote authentication","To fight fraud, a customer must fill out an application form and undergo a biometric identity check. Provema also uses best data security practices"
267,"Trust Framework Providers SAFE Identity and Kantara Initiative have reached a reciprocal agreement to consolidate digital identity assessments, each endorsing and supporting the other’s public key infrastructure (PKI) and non-PKI domain Trust Frameworks, along with their certified identity providers..The collaboration simplifies digital identity assessment and Trust Mark processes for companies in healthcare, financial services and other sectors to reduce organizational risk..“What we have done is agree to honor each other’s Trust Marks. Industry can now assess and manage digital identity more easily across both PKI and non-PKI domains when making risk-based decisions concerning the validity of someone’s digital identity credentials and providers’ services,” says Kantara’s Wallis. “Kantara always looks for ways to serve more value to its members through non-financial, non-exclusive collaborations such as this, in the furtherance of its non-profit mission. Instead of duplicating efforts, SAFE and Kantara are helping to expand the reach of our respective missions.”.Both SAFE Identity and Kantara certify trust in digital identity management services and solutions. SAFE is focussed on expanding and standardizing PKI-based credential use for identity, confidentiality and data integrity, particularly for the healthcare industry, while Kantara focusses on conformity assessments to legally recognized standards, along with standardization and non-PKI innovation..The U.S. Government’s General Services Administration (GSA) is joining the Kantara Initiative with plans to submit its Login.gov service for Kantara’s assurance and approval program to demonstrate compliance with National Institute of Standards and Technology (NIST) Special Publication (SP) 800-63-3 Digital Identity Guidelines..Login.gov provides simple, secure and private access to participating U.S. government digital services, according to the announcement, with 25 million people signed up to date..The NIST SP 800-63-3 Guidelines set mandatory digital identity guidelines for U.S. Federal agencies conducting identity verification and authentication..“NIST SP 800-63-3 is focused on modernizing the policy in keeping with rising threat levels for identity proofing, verification and authentication whilst also improving privacy in the overall digital user experience,” said Colin Wallis, Executive Director, Kantara Initiative. “Kantara has developed assessment criteria against each of 63-3’s normative requirements to drive consistency in assessments of applicable credential service providers (CSPs) done by Kantara 3rd party accredited assessors. Consistency in assessments drives long term integrity in Kantara’s Trust Framework and Trust Marks internationally, thereby building trust and confidence for all stakeholders in the wider digital economy.”.authentication  |  biometrics  |  certification  |  digital identity  |  identity management  |  identity verification  |  Kantara  |  NIST  |  privacy  |  SAFE Identity",This collaboration simplifies digital identity assessment and Trust Mark processes
268,"Much work needs to be done if biometric systems are to accurately recognize the voices of children, according to Clarkson University research..Although adult voices vary year to year and even hour to hour, their acoustic properties do not change as fundamentally as do the voices of children as they grow from school-aged to adolescence..Researchers spent 2.5 years evaluating biometric speaker verification performance for available software and hardware. In their study, Clarkson researchers analyzed speaker verification over six sessions involving 30 children aged four to 14..The results were noteworthy, but little more. The scientists report that the MFCC 20 verification system and GMM algorithm delivered the best results. That performance, however, “is not with the expected biometric recognition performance.”.The bigger picture is that the published research is thin for a technology that is changing how people, and particularly children, interact with their world..Virtually all voice biometrics research and development has been conducted on adults. The result has been Siri, Alexa and other digital assistants that can identify speakers by their voices..Speech recognition work with children’s voices is very complex, and it also is fraught with concerns over privacy, consent and the appearance of undue commercial profit from data collected from minors..It will be difficult to know which, if any, voice verification apps are appropriate for children unless more research is done, Clarkson Professor and Researcher Stephanie Schuckers pointed out in a December Biometric Update interview..The Clarkson paper notes that voice recognition for children has been based on physiological changes with gender and speech — not speaker — recognition. The ways that children’s voices change join other challenges to recognition, including background and channel noise, low-quality mics, illness and vocal stress..Deep learning speaker recognition systems used in the study achieved “strong speaker recognition performance.” They showed improved recognition compared to classifiers that require “hand-crafted features,” according to the study..But the algorithms had practical drawbacks when considering commercial use. Researchers found that they are more complex and demand massive amounts of labeled data. Their computation and storage costs also are high..biometric data  |  biometrics  |  biometrics research  |  children  |  Clarkson University  |  identity verification  |  privacy  |  speech recognition  |  voice biometrics","A research team at Clarkson analyzed speaker verification involving 30 children aged four to 14 and assessed that  voice verification apps are not appropriate for children unless more research is done. There are many concerns over privacy, consent and appearance of undue commercial profit collected from data from minors."
269,"A new processor in Amazon’s latest generation of Echo devices are giving the Alexa assistant intriguing capabilities that the company say offer consumers a more natural experience of speech-based interaction. There’s also plenty of scientific research that’s gone into sound localization and computer vision to offer new features without creating new biometric data storage and privacy problems—and device edge processing is the key..At the Fall 2020 Devices and Services announcement from Amazon, drones flying around the home and a new online gaming service garnered much of the attention. The company’s debut of new Echo devices, however, was more significant in terms of biometrics-related developments..Inside the globe-shaped Echo and the brand-new Echo Show 10, the AZ1 Neural Edge processor is tasked with running new and updated speech and computer vision algorithms..“In speech processing milliseconds matter,” said Miriam Daniel, vice president of Amazon Echo during the product rollout event. “Imagine asking Alexa to turn on the light, and there’s a slight delay in the light coming on—that would make customers really impatient..“Our team worked really hard to shave off hundreds of milliseconds from Alexa’s response time, [so] they invented the all new AZ1 neural edge processor,” Daniel said. The silicon module has been purpose-built to run machine learning algorithms on the edge, she noted..Rohit Prasad, vice president and head scientist for Alexa, said “The goal with Alexa is to make interacting with it as natural as it is to speak to humans,” and further noted that advancements in AI are bringing Amazon closer to that vision. Among the current capabilities are the use of feedback search algorithms to take user feedback (“Alexa, that’s wrong”) and use interactions to correct the mistake in action. A new ability is to teach the Alexa assistant directly by speech rather than through a mobile app or online portal to set up new functions..On the new Echo Show 10, the display and camera are able to change direction and aim at the current speaker in a room in an effort to make for a more natural interaction during video calls. This is useful when someone is moving about a room while talking or viewing video, but it turns out that it is rather challenging to do this without storing biometric data or personally identifiable information in the form of faces and voices..“We’re not doing [this] with facial recognition; we’re doing that just understanding sort of the form of what a human being looks like and triangulating on that,” explained Dave Limp, senior vice president of devices and services at Amazon. “The cool thing about the technology is it’s all running locally. And so none of this goes to the cloud; it’s all done locally on that neural processor and it never leaves the device,” he added.. (A visualization of the non-reversible process Echo 10 uses to convert images into a higher-level abstraction to support motion. Source: Amazon).The AZ1 processor is used in a novel way to understand the direction the voices are coming from and decide where, when and how fast to adjust the camera. According to a post on the Amazon Science blog, the Echo Show 10 uses sound source localization (SSL) with computer vision (CV) to identify objects and humans in the field of view and figure out which sounds are coming from people, and which are merely sounds reflecting off walls..The chip was designed in collaboration with MediaTek. MediaTek’s MT8512 forms the basis for the processor, having been design for “high-end audio processing and voice assistant applications,” according to MediaTek..The MT8512 integrates a 2GHz dual-core CPU, support for a wide variety of peripheral connectivity dedicated to ultra-high-quality audio processing, as well as Bluetooth 5.0 and Wi-Fi 5 dual-band connectivity. MediaTek notes that a high-performance voice DSP (digital signal processor) is included for fast and accurate wake-word and keyword detection in vocal commands; the DSP works in conjunction with the AZ1 Neural Edge processor “to provide the most responsive Alexa experience,” according to MediaTek..Additionally, the chip is made using a 12 nanometer (nm) process; for comparison, the absolute state-of-the art is 5nm, while many mainstream processors from Intel used in laptop and desktop PCs are made with a 14nm process. Generally speaking, the smaller transistors are, the more of them can be packed into the same “package” space and offer improved energy efficiency. In other words, for use in low cost standalone devices, the MediaTek chips look to provide a good balance between power, efficiency and unit cost..AI chips  |  Amazon  |  biometric data  |  biometrics  |  biometrics at the edge  |  computer vision  |  data storage  |  machine learning  |  privacy  |  speech recognition",A new processor enables a more natural experience for speech-based interaction. Scientific research was conducted on sound localization and computer vision to offer new features without creating data storage and privacy issues.
271,"Speakers highlighted “tensions between individualised approach based on clinical needs and duties to the wider population to minimise harm and maximise benefits” during COVID-19 era.Highlighting that patient data confidentiality due to the use of medical assistants such as artificial intelligence (AI) can create a conflict of interest situation in healthcare settings, Prof Ashish Chandra, University of Houston-Clear Lake (UHCL), Houston, Texas, said, “We are in the digital era where medical assistants support quality reporting with electronic health records and services. They have been around for decades, but in the era of COVID-19, medical assistants are being used more now than ever before. Hence, it is increasingly important for healthcare organisations to have a regulatory and policy framework for all data services to maintain patients’ fundamental privacy rights. It is also important for organisations to get better health infrastructure for patients and to deliver better care. This will also help maintain the relations between physicians, nurses, healthcare leaderships, management, and patients.”.Prof Chandra, along with Prof William (Bill) Stroube, Professor, Health Services Administration, University of Evansville, US, and Dr P R Sodani, Pro President, IIHMR University, was speaking at the webinar on “The Underappreciated Value of Healthcare Ethics – A Look at Various Cases,” organised by  IIHMR University. Many healthcare systems have the principle of making the care of individual patients the primary concern, and India needs the same, Prof Chandra said. The observation becomes important as India attempts to move to a more digital healthcare system through the National Digital Health Mission (NDHM)..Taking this argument forward, Prof Stroube said, “There are many moral issues that arise out of the provision of health care—from those that are inherent in the relationship between the healthcare professional and the patient to those associated with many ethical dimensions, from the consequential ethics (based on the theory of right and wrong) to deontological ethics (duty-based). During a pandemic, there are tensions between this individualised approach based on clinical needs and duties to the wider population to minimise harm and maximise benefits. To clarify the unappreciated moral issues of healthcare, a multisystem ethical culture and organisational culture may provide a policy framework to make better quality decisions concerning patients.”.Commenting on physicians’ changing role in light of the pandemic, Dr Sodani emphasised, “While the increased use of medical assistants in many patient-care tasks makes sense, the future role of the physician in sectors like primary care needs greater clarification. Strong doctor-patient relationships are essential to improving the quality of care, the patient experience, physician job satisfaction, and overall functioning of the healthcare team.”.Express Healthcare, first published as Express Healthcare Management in January 2000, has grown to become the No.1 Business News Magazine for the healthcare industry. The editorial contents include: news, views, analysis and interviews, under main segments: Market, Strategy, Knowledge, Life, Radiology, Hospital Infra, [email protected] Besides this, we bring out a quarterly publication, called In Imaging for the radiology and imaging segment.",Advocating for data protection for the use of medical assistants during COVID-19
272,"The World Bank, intent on pushing efforts in Afghanistan to provide legal identification for women as well as men, is reporting some success with a biometric electronic ID..It is not the typical story coming out of Afghanistan, and the anti-modernity Taliban is the ultimate revenant religious force, but there remains hope that the lot of women there can be improved to some small degree by getting them recognized legally in their own nation..Like anywhere, even failed states, it is impossible to participate in an economy or get government services without reliable identification. This is especially true in rural Afghanistan..Moderating forces from around the world have pushed back on cultural rules that disenfranchise women and girls, and the Taliban no longer governs the whole country. Yet the World Bank points out in a new blog post that Afghan women are still forced into a second-class existence..Afghanistan’s national digital ID, the tazkira, is being updated as part of multiple inclusion programs funded through the World Bank. In the aftermath of the Taliban’s nationwide hold (it still dominates in some regions) tazkira’s for women were legalized, a fact that is not widely known or even accepted in the countryside and mountain villages..And of the most vulnerable population — women who have become refugees in their own country — 80 percent lack tazkiras. Until recently, a woman legally could not get an ID unless she was accompanied to one of the scarce government centers by a male relative. The relative even had to agree to let a photo be taken of her..As part of the e-tazkira program, other requirements were relaxed as well to make getting the document easier for women. The changes have made it possible for women who have never had or who have lost relevant documents in the war to get the electronic ID document..Afghanistan  |  biometric identification  |  biometrics  |  digital identity  |  financial services  |  government services  |  identity document  |  national ID  |  SDG 16.9  |  World Bank","Afghanistan's national digital ID, the tazkira, is being updated as part of inclusion programs through the World Bank. This id hopes to replace paper documentation. Today, 47% of new e-tazkira digital identity registrants are women which helps getting documents for women easier. "
277,"Facebook boss Mark Zuckerberg wrote that he had struggled with the tension between free speech and banning such posts, but that this is the right balance..I find it deeply offensive. But at the end of the day, I dont believe that our platform should take that down because I think there are things that different people get wrong. I dont think that theyre intentionally getting it wrong..My own thinking has evolved as Ive seen data showing an increase in anti-Semitic violence, as have our wider policies on hate speech, he wrote in a public Facebook post..Drawing the right lines between what is and isnt acceptable speech isnt straightforward, but with the current state of the world, I believe this is the right balance..Earlier this year, Facebook banned hate speech involving harmful stereotypes, including anti-Semitic content. But Holocaust denial had not been banned..Facebooks vice-president of content policy, Monika Bickert, said the company had made the decision alongside the well-documented rise in anti-Semitism globally and the alarming level of ignorance about the Holocaust, especially among young people..Denying the Holocaust, trivializing it, minimizing it, is a tool used to spread hatred and false conspiracies about Jews and other minorities, the group said in a statement.","Facebook introduced a new policy that prohibits any content that denies or distorts the Holocaust,  this helps stop misinformation that AI algorithms can amplify."
279,"It was just a few years ago when Artificial Intelligence (Ai) was a newly emerged idea that was too technically advanced for people to predict how it could impact the world. Fast forward to 2019 and things have definitely changed. It’s become a real game changer for countless industries including major players in the mining industry..Mining is an industry where improving efficiency and productivity is essential for profitability, as small improvements in yields, speed and efficiency can make an extraordinary impact..The mining industry employs a lot of individuals — just 670,000 Americans are employed in the quarrying, mining and extraction sector. Our current methods are just not effective and can result in a lot of loss which is detrimental since mining can impact nearly every other industry since it provides the raw materials for virtually every other aspect of the economy..The current mineral exploration process is basically massive amounts of data in the form of soil samples, chip samples, geochemistry, drill results, and assay results. Each drill hole is a tiny snapshot of the processes that form the earth..A single drill hole can create 200 megabytes of data and if there are more drill holes with other types of information, an exploration project can produce terabytes of data. If you wanted to compare your one project to hundreds of others to find the best insights, this becomes difficult since the amount of data is crazy..All of these data points are so important though because using this data we can find new mineral deposits, but to sort through them is too much for even an entire team of capable geologists..EARTH AI is helping mineral explorers identify promising areas. They do this by analyzing data from multiple sources and using a machine learning algorithm to identify areas where minerals are likely to be found. 47 layers of remote sensing and geophysical data are analyzed at once by machine learning — they can highlight ore bodies and alteration haloes as well as map out hard rocks and regolith with extreme detail..Artificial intelligence and machine learning can help mining companies find minerals to extract. Some companies are already working on this. Goldspot Discoveries Inc. is a company that aims to make finding gold more of a science than art by using machine learning. Another company, Goldcorp and IBM Watson are collaborating to use artificial intelligence to review all the geological info available to find better drilling locations for gold in Canada..While many of us have been focused on the progress Uber, Google and Tesla have made with autonomous vehicles many people don’t realize that Rio Tinto (one of the worlds largest metals and mining corporations) has already been using autonomous haul trucks that can carry 350 tons and operate totally independently since 2008. These trucks have been so effective that they’ve reduced fuel use by 13 percent and are also much safer to operate..AI can help mining engineers and workers prevent accidents and injuries on the job. If enough high-quality data can be collected, we can predict failures that can not only affect production but also be harmful or fatal to any workers nearby..In addition to predicting failures, we can also use Ai to predict other potential hazards by analyzing patterns in events. This is really useful because the environment in mining can affect the operation and lifespan of equipment, varies greatly depending on the location..But we can also do more than just predict when equipment might fail or which hazards can present themselves using AI. We can also constantly survey the health and performance of equipment which is important in eliminating surprise failures and dangers to workers..In the Escondida copper mine in Chile, the company trialed smart caps which analyzed driver brain waves to measure and act on fatigue. This was integrated into over one-hundred-and-fifty trucks to boost productivity and increase safety..They are also using AI in automating decision making. For example, Mining Area C in Western Australia Iron Ore is using a system that selects which crusher trucks they should use. This cuts down a lot of time spent on making decisions and also increases efficiency since decisions are generally better than humans..Vale is using AI in several areas with Advanced Analytics and is saving big as a result. At Salobo copper mine in Para, Brazil, there was a 30% increase in the lifespan of haul truck tires in one year which saved the company $5 million. This same technique is being applied in other mines and other truck parts including engines and fuel consumption..Vale also uses AI to predict rail fractures which is helping to reduce the occurrence of fractures by up to 85%. This can save Vale $7 million per year. In total, the company expects to save around $26 million in 2018 from these changes alone..Goldcorp recently partnered with IBM to put their smart technology towards exploration. IBM Watson services are being used to analyze drilling reports, geological survey data and more in determining which areas to explore and to quickly locate high-value targets..It is clear that automation and use of Ai and Machine Learning can significantly help save costs, increase efficiency and have tons of other benefits for companies. What’s holding us back is data and having really good, and a large amount of it. However, companies are working on scaling the use of AI in mining and with big data becoming a huge industry we can start seeing more use of AI in the mining industry. One that is completely changing from what we’ve traditionally known it to be..I’m a developer & innovator who enjoys building products and researching ways we can use AI, Blockchain & robotics to solve problems in healthcare and energy!",EARTH AI helps mineral explorers find promising areas
281,"In March of this year, a massive grave was uncovered buried beneath the soil of the coastal Mexican state of Veracruz. The grave made national headlines because it contained more than 240 skulls and corpses, the remains of disappeared people (link in Spanish)..But for many, the grave’s existence came as no surprise. In Mexico, a country where almost 30,000 people have gone missing due to drug-related violence since 2006, the grave was a reminder of a difficult reality: the search for missing people often begins by looking underground..Mexico is home to over 122 million people and spans more than 750,000 square miles of land. There is no road map that makes clear where to start the search for mass graves, or the bodies of the disappeared (desaparecidos) that they hold..Or at least there hasn’t been—until now. A team of multi-country researchers, data scientists, and statisticians is using machine learning to predict which counties in Mexico are most likely to have hidden graves. If their model works as well as they hope, it will be a powerful application of an emerging technology that provide answers to one of the most difficult aspects of the desaparecidos problem: knowing where to look..The team is composed of three separate groups: the Programa de Derechos Humanos at the Ibero-American University in Mexico City; data-focused non-profit Data Cívica, also based in Mexico City; and the Human Rights Data Analysis Group (HRDAG), a San Francisco-based organization that applies scientific analysis to human rights violations (first two links in Spanish)..Each organization contributes a unique piece of analysis or data which together form a fuller picture of where to search. The group at the Ibero-American University has been scraping local and national Mexican newspaper and radio data for mentions of hidden graves for years as part of a larger project. They’ve created a comprehensive database of the details behind every report of a hidden grave. It’s the country’s first database of the sort, and it’s a crucial bank of knowledge that details in which municipalities hidden graves have been discovered in the past..Data Cívica contributes data on social demographics about every municipio, or county, in the country. By combining Mexico’s public open data system with geographic data, they’ve been able to create a detailed profile of sociodemographic data for every one of Mexico’s 2457 counties..These two pieces are crucial to the machine learning model that HRDAG uses to predict which counties are likely to have hidden graves in them. The model is called a Random Forest classifier, and its usefulness hinges on the idea that there is something categorically different between counties that have historically been found to have hidden graves, and those that have not. The model sorts through the characteristics and weights their relevance. It then becomes possible to predict, based on those characteristics, which counties are most likely to have graves found in them in the future..Patrick Ball, HRDAG’s Director of Research and the statistician behind the code, explained that the Random Forest classifier was able to predict with 100% accuracy which counties that would go on to have mass graves found in them in 2014 by using the model against data from 2013. The model also predicted the counties that did not have mass hidden graves found in them, but that show a high likelihood of the possibility. This prediction aspect of the model is the part that holds the most potential for future research..Ball was quick to add, “Prediction is different than inference. It’s different from explanation.” Which is to say that the while the model can predict which counties are most likely to have similar graves in them in the future, it can’t explain why that is, and it isn’t particularly concerned with which variables make that difference..“The problem with this type of violence is that it’s a very contextual violence,” Mónica Meltis, the Coordinator of Projects at Data Cívica explained in a phone interview. What she means is that it’s impossible to separate the counties that have hidden graves from the socioeconomic forces that define them..Counties with hidden graves are likely to have a lower average income than other counties. They tend to be more rural than urban, and thus have smaller populations. They have higher numbers of indigenous residents than counties without hidden graves, as evidenced by lower county-wide scores on Spanish language tests in primary schools. Many of the counties have been found to have strong connections to drugs (in the form of opium or methamphetamine labs) and high homicide rates..Their geographies are also significant: the counties with found graves tend to have highways, and thus be easily accessible by road. But they also evidence a pattern of being close to borders like the United States, Mexico’s northern neighbor, or Guatemala, the southern neighbor. The sea also counts as a border: the team at Data Cívica reports that three out of every ten disappearances happen in the states of Tamaulipas or Guerrero, both of which are coastal..All of these characteristics make sense, in the matter-of-fact way that the best discoveries always do after the fact. But Meltis, Ball, and Denise González Núñez, the coordinator of University of Ibero’s Human Rights Program, all warn against drawing any hard conclusions. For one, the data that they’ve run the model on isn’t complete. Though the team has been able to use data from 2013 to predict accurate results for 2014, they haven’t yet been able to do the same for 2017.  Before this can be done, Núñez’s team has to update their database with media mentions from 2016, a task that is forthcoming and time-consuming..The project is also limited in the respect that it doesn’t answer any of the thorny questions around who is committing the crimes. The team can neither determine who is responsible for killing the victims nor guess as to the identities of the people within the mass graves. Their work provides only a compass of sorts, the barest map for those who must then do the difficult work of excavating those buried..And of course, there’s the final fact that Ball in particular is careful to point out: “What we’re predicting is the probability of observation of a grave. We can only predict the counties that are likely to have graves that are like the ones we’ve observed in the past.”.His point is one that’s characteristic of all machine learning models. Models are deeply dependent on the data available. In the case of Mexico’s hidden graves, it’s highly possible that there are mass graves that have been so well hidden that no one has found them. These phantom graves, the unobserved unknowns, haven’t made their way into any datasets. And because they’re not in the data, it’s impossible to train a model on them. The work the group is doing cannot point to every hidden grave in the country. Rather, it can only help locate graves that are similar to those that have been found before..Even so, the project represents a powerful beginning. For Núñez’s team, it’s part of a multi-year effort that will result in the May 2017 release of a comprehensive report about hidden graves in Mexico. Ultimately, the work is about more than reports, data, and models. As Metis puts it, “I don’t think that this is a project that we want to be sitting in our desks writing about. We want to go outside and find the people.”","By using AI, people can identify missing people due to drug-related violence"
282,"Palmer Luckey, the virtual reality pioneer, left Facebook in 2017, six months after it was discovered that he had secretly funded a pro-Trump campaign group dedicated to influencing the US election through “shitposting” and “meme magic”..The 25-year-old Oculus founder now has a new venture, Anduril Industries, this time supporting Trump’s immigration policies directly through the creation of a surveillance system designed to detect unauthorised crossings of the Mexican border..Anduril Industries is one of a growing number of companies playing on the fear of “bad hombres” to cash in on government contracts for hi-tech virtual alternatives to physical wall. From drones and sensors to AI-powered facial recognition and human presence detection, these surveillance systems promise cheaper border control but at what cost to civil liberties?.“These systems are reflective of advances in sensor and analytics technologies that are going to have serious repercussions for Americans’ privacy,” said Jay Stanley, senior policy analyst with the ACLU. “The combination could turn us into a surveillance society where our every move is tracked.”.According to an in-depth report by Wired, Anduril’s eventual plan is to offer the military some kind of “Call of Duty goggles” that tell you “where the good guys are, where the bad guys are”. .However, with no background as a defence contractor, the startup needed a “quick win”; providing AI-powered surveillance technology to the border patrol was a way to get a foot in the door of government procurement..The company, which is backed by Peter Thiel’s venture capital firm Founders Fund, has developed towers that feature a laser-enhanced camera, radar and a communications system. These scan a two-mile radius around them to detect motion. The images are analysed using artificial intelligence to pick out humans from wildlife and other moving objects. During a 10-week test in Texas, the technology – called Lattice – helped agents from US Customs and Border Protection (CPB) catch 55 unauthorised border crossers and seize 445kg of marijuana..Anduril isn’t the only company touting a virtual border wall. The Israeli defence contractor Elbit Systems designed and built dozens of towers in Arizona to spot people as far as 7.5 miles away. The company won the contract off the back of its previous work building a “smart fence” – using sensors, cameras and drones – separating Jerusalem from the West Bank..At the US-Mexico border, Anduril and Elbit Systems have learned from the mistakes made by the failed billion-dollar SBInet, a 53-mile-long virtual wall built by Boeing from 2006 but abandoned in 2011 for being too expensive and ineffective..“While it has generated some advances in technology that have improved border patrol agents’ ability to detect, identify, deter and respond to threats along the border, SBInet does not and cannot provide a single technological solution to border security,” said the Department of Homeland security’s assessment of SBInet..“There is a tendency to look at everything as data and think that if we can just track the blips we can close our borders,” he said. “That was proven to be highly naive with SBInet..“Our borders are thousands of miles long and the world is very messy and complicated. People will be trying to actively subvert these systems so there are no simple solutions,” he added. .The relative cost effectiveness of the surveillance technology made by Anduril and Elbit – thanks to advancements in sensor tech and AI – combined with the fact that CPB considers the border to be a 100-mile-wide zone, will probably mean even more privacy intrusions for border communities..“It’s one thing having sensors on the actual border, but when it starts creeping into American communities there is no justification,” Stanley said. “These systems should never be storing information on the comings and goings of residents of American communities when there is no reason to be suspicious.”.In addition to “virtual walls”, the US government is deploying a facial recognition system to record images of people inside vehicles entering and leaving the country. Secretive tests of the system carried out in Arizona and Texas saw authorities collect a “massive amount of data” including images captured “as people were leaving work, picking up children from school, and carrying out other daily routines”, according to government records..The images captured by the Vehicle Face System will be compared with those stored in government databases, including passports, visas and other border patrol documents in order to identify unauthorised individuals..“This is an example of the growing trend of authoritarian use of technology to track and stalk immigrant communities,” Malkia Cyril, the executive director of the Center for Media Justice, told the Guardian last week..“Companies are finding they have to pay attention to the ethical concerns of their employees or risk having trouble recruiting,” Stanley said, noting the recent exodus of staff at Google over its work with the US military on drone surveillance.","The use of drones and AI-powered facial recognition cost civil liberties, privacy, and have a margin of error"
284,"Update 10/28: Since the publication of this article, the deepfake bot on Telegram has been blocked on iOS for violating App Store guidelines, according to the researchers. The main Telegram channel that hosted the bot and an affiliated channel for sharing its creations has also been removed..In June of 2019, Vice uncovered the existence of a disturbing app that used AI to “undress” women. Called DeepNude, it allowed users to upload a photo of a clothed woman for $50 and get back a photo of her seemingly naked. In actuality, the software was using generative adversarial networks, the algorithm behind deepfakes, to swap the women’s clothes for highly realistic nude bodies. The more scantily clad the victim, the better. It didn’t work on men..Within 24 hours, the Vice article had inspired such a backlash that the creators of the app quickly took it down. The DeepNude Twitter account announced that no other versions would be released, and no one else would get access to the technology..But a new investigation from Sensity AI (previously Deeptrace Labs), a cybersecurity company focused on detecting the abuse of manipulated media, has now found very similar technology being used by a publicly available bot on the messaging app Telegram. This time it has an even simpler user interface: anyone can send the bot a photo through the Telegram mobile or web app and receive a nude back within minutes. The service is also completely free, though users can pay a base of 100 rubles (approximately $1.50) for perks such as removing the watermark on the “stripped” photos or skipping the processing queue. .As of July 2020, the bot had already been used to target and “strip” at least 100,000 women, the majority of whom likely had no idea. “Usually it’s young girls,” says Giorgio Patrini, the CEO and chief scientist of Sensity, who coauthored the report. “Unfortunately, sometimes it’s also quite obvious that some of these people are underage.”.The deepfake bot, launched on July 11, 2019, is connected to seven Telegram channels with a combined total of over 100,000 members. (This number doesn’t account for duplicate membership across channels, but the main group has more than 45,000 unique members alone.).The central channel is dedicated to hosting the bot itself, while the others are used for functions like technical support and image sharing. The image-sharing channels include interfaces that people can use to post and judge their nude creations. The more a photo gets liked, the more its creator is rewarded with tokens to access the bot’s premium features. “The creator will receive an incentive as if he’s playing a game,” Patrini says..The community, which is easily discoverable via search and social media, has steadily grown in membership over the last year. A poll of 7,200 users showed that roughly 70% of them are from Russia or other Russian-speaking countries. The victims, however, seem to come from a broader range of countries, including Argentina, Italy, Russia, and the US. The majority of them are private individuals whom the bot’s users say they know in real life or whom they found on Instagram. The researchers were able to identify only a small handful of the women and tried to contact them to understand their experiences. None of the women responded, Patrini says..The researchers also reached out to Telegram and to relevant law enforcement agencies, including the FBI. Telegram did not respond to either their note or MIT Technology Review’s follow-up request for comment. Patrini says they also haven’t seen “any tangible effect on these communities” since contacting the authorities..Abusers have been using pornographic imagery to harass women for some time. In 2019, a study from the American Psychological Association found that one in 12 women end up being victims of revenge porn at some point in their life. A study from the Australian government, looking at Australia, the UK, and New Zealand, found that ratio to be as high as one in three. Deepfake revenge porn adds a whole new dimension to the harassment, because the victims don’t realize such images exist..There are also many cases in which deepfakes have been used to target celebrities and other high-profile individuals. The technology first grew popular in the deep recesses of the internet as a way to face-swap celebrities into porn videos, and it’s been used as part of harassment campaigns to silence female journalists. Patrini says he’s spoken with influencers and YouTubers, as well, who’ve had deepfaked pornographic images of them sent directly to their sponsors, costing them immense emotional and financial strain..Patrini suspects these targeted attacks could get a whole lot worse. He and his fellow researchers have already seen the technology advance and spread. For example, they discovered yet another ecosystem of over 380 pages dedicated to the creation and sharing of explicit deepfakes on the Russian social-media platform VK. (After the publication of this article, a spokesperson from VK sent MIT Technology Review a statement: VK doesn’t tolerate such content or links on the platform and blocks communities that distribute them. We will run an additional check and block inappropriate content and communities.) The researchers also found that the “undressing” algorithm is starting to be applied to videos, such as footage of bikini models walking down a runway. Right now, the algorithm must be applied frame by frame—“it’s very rudimentary at the moment,” Patrini says. “But I’m sure people will perfect it and also put up a license service for that.”.Unfortunately, there are still few ways to stop this kind of activity—but awareness of the issues is growing. Companies like Facebook and Google, and researchers who produce tools for deepfake creation, have begun to more seriously invest in countermeasures like automated deepfake detection. Last year, the US Congress also introduced a new bill that would create a mechanism for victims to seek legal recourse for reputational damage..In the meantime, Patrini says, Sensity will continue to track and report these types of malicious deepfakes, and seek to understand more about the motivations of those who create them and the impacts on victims’ lives. Indeed, the data we share in this report is only the tip of the iceberg, he says..An investigation by MIT Technology Review reveals a sprawling, technologically sophisticated system in Minnesota designed for closely monitoring protesters..As firms have dumped their AI technologies into the country, it’s created a blueprint for how to surveil citizens and serves as a warning to the world..Intrepid Response is a little-known but powerful app that lets police quickly upload and share information across agencies. But what happens to the information it collects?",This bot leads to the gamification of harassment and is targeted at young girls. 
286,"One year ago, Maneesh Agrawala of Stanford helped develop a lip-sync technology that allowed video editors to almost undetectably modify speakers’ words. The tool could seamlessly insert words that a person never said, even mid-sentence, or eliminate words she had said. To the naked eye, and even to many computer-based systems, nothing would look amiss. .The tool made it much easier to fix glitches without re-shooting entire scenes, as well as to tailor TV shows or movies for different audiences in different places..But the technology also created worrisome new opportunities for hard-to-spot deep-fake videos that are created for the express purpose of distorting the truth.  A recent Republican video, for example, used a cruder technique to doctor an interview with Vice President Joe Biden. .This summer, Agrawala and colleagues at Stanford and UC Berkeley unveiled an AI-based approach to detect the lip-sync technology. The new program accurately spots more than 80 percent of fakes by recognizing minute mismatches between the sounds people make and the shapes of their mouths..But Agrawala, the director of Stanford’s Brown Institute for Media Innovation and the Forest Baskett Professor of Computer Science, who is also affiliated with the Stanford Institute of Human-Centered Artificial Intelligence, warns that there is no long-term technical solution to deep fakes. .The real task, he says, is to increase media literacy to hold people more accountable if they deliberately produce and spread misinformation. .“As the technology to manipulate video gets better and better, the capability of technology to detect manipulation will get worse and worse,” he says. “We need to focus on non-technical ways to identify and reduce disinformation and misinformation.” .The manipulated video of Biden, for example, was exposed not by the technology but rather because the person who had interviewed the vice president recognized that his own question had been changed..There are legitimate reasons for manipulating video. Anyone producing a fictional TV show, a movie or a commercial, for example, can save time and money by using digital tools to clean up mistakes or tweak scripts..The problem comes when those tools are intentionally used to spread false information. And many of the techniques are invisible to ordinary viewers..Many deep-fake videos rely on face-swapping, literally super-imposing one person’s face over the video of someone else. But while face-swapping tools can be convincing, they are relatively crude and usually leave digital or visual artifacts that a computer can detect..Lip-sync technologies, on the other hand, are more subtle and thus harder to spot. They manipulate a much smaller part of the image, and then synthesize lip movements that closely match the way a person’s mouth really would have moved if he or she had said particular words. With enough samples of a person’s image and voice, says Agrawala, a deep-fake producer can get a person to “say” anything. .Worried about unethical uses of such technology, Agrawala teamed up on a detection tool with Ohad Fried, a postdoctoral fellow at Stanford; Hany Farid, a professor at UC Berkeley’s School of Information; and Shruti Agarwal, a doctoral student at Berkeley. .The basic idea is to look for inconsistencies between “visemes,” or mouth formations, and “phonemes,” the phonetic sounds. Specifically, the researchers looked at the person’s mouth when making the sounds of a “B,” “M,” or “P,” because it’s almost impossible to make those sounds without firmly closing the lips. .The researchers first experimented with a purely manual technique, in which human observers studied frames of video. That worked well but was both labor-intensive and time-consuming in practice..The researchers then tested an AI-based neural network, which would be much faster, to make the same analysis after training it on videos of former President Barack Obama. The neural network spotted well over 90 percent of lip-syncs involving Obama himself, though the accuracy dropped to about 81 percent in spotting them for other speakers. .The researchers say their approach is merely part of a “cat-and-mouse” game. As deep-fake techniques improve, they will leave even fewer clues behind..In the long run, Agrawala says, the real challenge is less about fighting deep-fake videos than about fighting disinformation. Indeed, he notes, most disinformation comes from distorting the meaning of things people actually have said. .“Detecting whether a video has been manipulated is different from detecting whether the video contains misinformation or disinformation, and the latter is much, much harder,” says Agrawala. .“To reduce disinformation, we need to increase media literacy and develop systems of accountability,” he says. “That could mean laws against deliberately producing disinformation and consequences for breaking them, as well as mechanisms to repair the harms caused as a result.”",This program accurately spots more than 80 percent of fakes by recognizing minute mismatches between the sounds people make and the shapes of their mouths.
287,"Lack of affordable housing is a major problem in US cities from the Bay Area to Boston. Austin is no exception: in 2015 the Austin-Round Rock metropolitan area was named one of the most economically segregated areas in the country. With in-migration continuing (roughly 3000 people per month), the disparity will worsen..This research project will develop a value-driven AI system that can evaluate historical housing development and help policymakers shape equitable, inclusive and sustainable plans and regulations. Using deep learning technology and an open data repository with demographic, development, transportation, and energy consumption data, this system will link and study 50 years of Austin housing development data – and tackle the history and future of local housing. More specifically, we ask:.We propose to develop the project in four iterative phases. First, we will design and develop a web-based housing development data repository. Second, we will run historical data analysis based on the data repository. Third, we will build a predictive AI system trained and tested on the data repository. Finally, we will conduct broad government and community outreach for system testing and results dissemination..Our research aims to evaluate and inform urban residential development policies in Austin. The techniques and findings developed from this study can also be applied to many similar cities nationally and internationally.","Develops a value-driven AI system that can evaluate historical housing development and help policymakers shape equitable, inclusive and sustainable plans and regulation"
288,"This project investigates comparative policies around the creation and use of video data in the public sector context.  As more cities deploy monitoring and sensing technologies, cameras are in the front lines of data-gathering and new management interests in domains such as traffic, policing, and health and safety. While efficiency concerns drive many of these innovations, there are no commonly accepted standards for using the data these technologies provide, leading to concerns about City/State monitoring, especially as AI/analytics applied to camera outputs become more pervasive. Camera improvements, AI, and machine language processing may mean better capabilities to achieve citizen safety, transit benefits, and so forth, but they also raise thorny issues of intellectual property, privacy and civil liability, among others..The research examines the practical, theoretical and policy implications of public sector-deployed cameras, especially video cameras, in the context of ethical decision-making.  The core components of the project include an examination of municipal policies in the U.S. around the use of cameras, particularly video cameras.  Traffic cameras in particular will be a focal point since the City of Austin is especially interested in traffic camera policies.  More broadly, it will gather existing policies and conduct a stakeholder survey in order to gauge how cities, especially those considering themselves ‘smart cities,’ have designed policies for handling cameras and the data they produce..In conjunction with this review and survey, the project will host monthly seminars around “Smart Cities and AI” in order to share the work of a wide variety of experts at UT and elsewhere..Our research is relevant to the study of pandemics spread and mitigation. The use of cameras for traffic, security, and geolocation services  in public areas , for example in transport centers, intersections, and public parks, or during public gatherings can be effectively used to track/understand the spread of contamination Such studies need to be conducted within the ethical frameworks that our team is investigating. We are in a perfect position to highlight the disconnects between policies and practices in government and industry using these devices in urban settings.",This research project looks at various implications of public-sector deployed cameras.
289,"As mis- and disinformation efforts continue to their presence in social media forums, it is critical to focus on how the older audience (over 55 years of age) engages and reacts to social media content.  This constituency has been identified in research as especially vulnerable to believing and circulating disinformation.  Now that we understand something about the structure of disinformation content, its appeals, and how platform affordances aid its circulation among this constituency, we need to enable this aging population to use social media responsibly. This Good Systems project will do this in three ways..First, our research will investigate what message qualities prompt such acceptance among older adults, and more broadly investigate some of the digital literacy barriers they face.  Second, growing out of a targeted investigation with older adults, we will develop training and workshop approaches that assist people in evaluating social media messages. The current coronavirus crisis underscores some of the similarities that political and health messages share when circulated in social media fora. Poor choices regarding sharing and amplifying untrue messages occur easily in digital arenas that encourage rapid responses, and training may curb such rapid, poor responses.  Understanding the roots of the problem and developing guidance and training modules that can be broadly shared is our second goal.  Third, several countries have taken steps to curb disinformation. We will use a conference to highlight the most productive approaches for redressing misinformation among older adults around the world and share policy recommendations within the US security and policy structures. A White Paper will be created for this purpose, with rapporteurs at the conference drafting the document..These approaches enable us to expand knowledge about how dis- and misinformation operate in social media environments, how platform regulations or AI might be tailored to counter these problems, and how older adults might be trained to improve their social media encounters..This research is pertinent to Covid research in that it explores how people, particularly older adults, respond to dis- and misinformation.  With the proliferation of Covid-related misinformation as well as genuine confusion around what to believe, understanding what people respond to and how authoritative information might be better positioned are very pertinent.  Since this user group is also the most vulnerable to severe cases of the disease, facilitating the delivery and belief in appropriate and helpful information is highly relevant.",This research project discusses how did- and misinformation operate in social media environments and how platform regulations or AI might be tailored to counter these problems
290,"AI systems may not only reproduce data bias but even amplify it. Unfortunately, even defining data bias is difficult, let alone detecting and mitigating it. For example, consider bias by omission: transgender people, refugees or stateless people, or formerly incarcerated individuals may be simply overlooked in data. Bias can create harmful systems that commit “data violence,” negatively effecting people’s everyday lives as they interact with institutions, from social service systems and security checkpoints to employment background-check systems. Data and algorithm bias can hurt people downstream in ways that are difficult to anticipate. Three key conclusions have emerged from recent research. First, it is very difficult to remove bias from data alone because it can creep in through various insidious ways. Second, determining the best algorithmic criterion (loss function or evaluation score) is very challenging. Finally, improvement in one criterion may increase biased in another, and neither may properly capture human evaluations of fairness. We will engage people to help identify bias in datasets and fairness of alternative algorithms and evaluation metrics. Human-centered approaches to assess bias and fairness can address a critical gap to inform research on algorithmic fairness..Prof. Ghosh has devised a set of algorithms that can modify the training of neural networks, including deep networks so that they are simultaneously less biased as well as more robust to adversarial attacks. Since neural networks are now being widely deployed, the potential impact of this work toward the design of more fair and reliable AI systems is substantial..Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri, José MF Moura, and Peter Eckersley. “Explainable machine learning in deployment.” In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pp. 648-657. 2020..Shubham Sharma, Jette Henderson, and Joydeep Ghosh. “CERTIFAI: A Common Framework to Provide Explanations and Analyse the Fairness and Robustness of Black-box Models.” In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, pp. 166-172. 2020..Min Kyung Lee, Anuraag Jain, Hea Jin Cha, Shashank Ojha, and Daniel Kusbit. “Procedural justice in algorithmic fairness: Leveraging transparency and outcome control for fair algorithmic mediation.” Proceedings of the ACM on Human-Computer Interaction 3, no. CSCW (2019): 1-26..Prof. Ghosh presented a Webinar on  “Detecting Racial Bias in Healthcare using Trusted AI” (Aug 2020). He also has a podcast on the Pulse of AI Channel: Trustworthiness in AI: Featuring Joydeep Ghosh and Risto Miikkulainen   (July 2020).In this webinar, we show how different aspects of our Trusted AI Approach can help to detect and such biases and help mitigate the associated risks, leading to solutions that are more socially responsible while retaining business value.",This research project helps identify bias in datasets and fairness of alternative algorithms and evaluation metrics
291,"Our group described and evaluated aspects of social media systems involved in disinformation campaigns, specifically those involving the Russian IRA. It provides a deep understanding of the historical, contextual and international processes of disinformation that move through AI and platforms, particularly Facebook and Twitter.  Our approach defines “good systems” as not only technological but also social, organizational and political. This notion of the “system” invokes an information environment with many moving parts. Artificial intelligence and machine-driven content creation and circulation are significant components of contemporary disinformation efforts that target certain constituencies with certain types of messages; user literacy is another important part of the equation, including the ability to recognize false information and understand how information moves across and within platforms..Park, S., Strover, S., Choi, J. and Schnell, M. (2020, May). Mind Games:  A temporal sentiment analysis of the political messages of the IRA on Facebook and Twitter.  Presented at the International Communication Association, Gold Coast, Australia (virtual)..Alvarez, G., Graham, S., Choi, J., Strover, S. (2019, May).  Good news, bad news:  A Sentiment analysis of the Russian Facebook Ads. Presented at the International Communication Association, Washington, D.C..Strover, S. and Alvarez, G. (2019, April 25).  Facebook and the Russian IRA Campaign.  Invited speaker, Information Wars:  Social Media and Politics in Russia & Eastern Europe Conference Conference.  University of Texas at Austin, Austin, TX..Reverse Engineering Political Protest, to be submitted to the International Communication Association in November, 2020. Riedl, M., Strover, S., Cao, T., Limov, B., Choi, J., and Schnell, M..Investigating the Images of the Russian IRA Campaign, to be submitted to the International Communication Association in November, 2020.  Choi, J. and Strover, S.","An analysis of Facebook ads released by US Congress in 2018, message circulation, and sampled twitter data linked to Russian accounts"
292,"As AI-driven devices and toys play an increasingly important role in children’s lives, little research has considered how social and economic disparities and cultural differences shape children’s engagement with AI. Good systems must be responsive to diverse communities, particularly for children from underrepresented populations in tech (e.g., African-American, Latinx). The purpose of the study is to examine how children from underrepresented populations understand, interact with, and evaluate AI-driven digital assistants. Through interviews and experimental work, we will identity factors related to diversity, ethics, and child development for creating age-appropriate AI technologies for children in early to late adolescence (8-to 12-years of age)..We conducted ten in-depth and semi-structured interviews, including five African-American and/or Latinx child/parent dyads. After obtaining informed assent and consent, respectively, we first administered a demographic questionnaire about the child’s use of digital assistants, then separately interviewed the parent and the child. Interview questions included their general use of, reflections on, and understanding of digital assistants. Interview sessions lasted about an hour and were audio-recorded and transcribed for analysis. Interview transcripts were imported to MAXQDA 2018 for thematic analysis (Braun & Clarke, 2006). We then developed codes and then clustered the codes into themes. Next steps will be collecting data observing how children interact with these devices..As digital assistants like Siri and Alexa become more common in our lives, people increasingly see them as companions that accompany them throughout their day. Young children, especially, are more apt to see these devices as real people or friends.",This research project analyzes how social and economic disparities and cultural differences shape children's engagement with AI
293,"One of the main tasks of any city government is to keep infrastructure such as roads, signs, accessibility ramps, and sewers in perpetual working order. The first step of maintenance is timely identification of the need for maintenance. However, many infrastructure defects are left undetected and unattended for long periods of time..The City of Austin possesses several fleets of vehicles, which work regularly around the city, and therefore form a seamless network covering many of those inspection targets..We propose to investigate how to leverage those vehicles, combined with state-of-the-art computer vision, robotics, and data science techniques, to automate infrastructure inspection in such a way that is publicly acceptable, will reduce costs, and will increase effectiveness of city maintenance efforts.",This research analyzes to see how the City of Austin can leverage vehicles with computer vision and robotics to automate infrastructure inspection
294,"Optimizing ambulance allocation and routing is one of the most efficient ways for the EMS to save more lives at virtually no cost. However, current EMS software were developed under models that assume normal demands. They are unable to adapt to disasters such as the COVID-19 pandemic, where traffic patterns change, case clusters emerge, and hospitals rapidly reach capacity. Decisions which are optimized for normal times can suddenly become very inefficient, leading to significant delay in care. Ideally, one would like to synthesize real-time information on case clusters, hospitals’ capacities and capabilities, waiting times, and traffic situation to coordinate responses between all ambulances..This project aims to create such an optimal EMS routing strategy using real-time information. By design, the proposed system can rapidly adapt to changing situations and is robust to disruptions. It guarantees that ambulances arrive at scenes the fastest and distribute patients optimally among care facilities..The research team has strong ties with technical personnel of the Austin Travis County EMS (ATCEMS) department. This will allow the two PIs to obtain high-quality real data and consult EMS dispatchers for feedback. In particular, the team expects to run real-life field test of the algorithms on ATCEMS within six months of the project initiation..In this project, a cross-college UT team works together with Austin-Travis County EMS to optimize EMS dispatch during extreme events. Specific to the COVID-19 pandemic, the project is expected to have the following outcomes..(1)  New protocols for EMS dispatchers during the pandemic to minimize risks, lower patients’ waiting times and optimize usage of hospital capacity..(2) New research developments on the general problem of vehicle routing during times of extreme demands or supply fluctuations. This contributes to the pressing need of building more resilient supply chains during and after the pandemic.",This research project looks at creating an optimal EMS routing strategy using real-time information that is adaptable to changing situations and robust to disruptions
295,"Postpartum mood and anxiety disorders (PPMADs) affect up to one in five women globally1. Left untreated, PPMADs can have major negative effects on maternal and child health and well-being2–4. While effective treatments are available, nearly 60% of mothers with PPMADs symptoms are undiagnosed, and 50% of diagnosed mothers are left untreated5, with poor women and women of color being disproportionally impacted6. Cost, time, stigma, and lack of childcare and information constitute major barriers to treatment 5. As such, simple, cost-effective, and efficacious approaches to providing PPMADs support are urgently needed..This proposal aims to develop a “chatbot” application to provide meaningful support to mothers struggling with or at risk of PPMADs. Research on text-based conversational agents, known as “chatbots,” indicates their potential as an innovative, low-cost, low-barrier intervention7. However, no chatbot platforms exists to target mothers in the postpartum period specifically..Thus, we propose to develop a chatbot platform to support caregivers in this critical time of their lives. To identify the most common concerns of caregivers experiencing or at risk of PPMADs, as well as best-practices for support, we will combine insights from user interviews with natural language processing analyses of text messages exchanged between caregivers at risk of PPMADS and post-partum support providers. Additionally, to ensure our chatbot will provide meaningful support and engagement for caregivers at risk of PPMADs, we will engage in user testing and co-design techniques with caregivers at risk and our community collaborators, staff at the non-profit organization Postpartum Support International (PSI)..We will not be directly exploring the impacts on covid per se. However, the need for continued social distancing means that caregivers of young infants are more isolated from family and friends than before and we anticipate that this will lead to additional stress for many caregivers. Additionally, in-person support groups are currently cancelled, limiting options for caregivers currently seeking support. Thus, text-message based support may be even more relevant during the pandemic..The global pandemic is teaching us many things, and that includes the fact that the delivery of vital health services will require new models and modes of care.",This chatbot application provides meaningful support to mothers struggling with or at risk of PPMADs
296,"Technology is transforming people’s lives, but it’s a constant struggle to ensure that technology designs address people’s values and preferences, especially those of traditionally underserved groups. Computer vision, as an example, empowers individuals with vision impairments, but it also carries with it privacy concerns; as an example, service providers risk leaking private information when collecting and sharing pictures. Our proposed research will address the conflict between convenience and privacy inherent to computer vision. Toward developing future Visual Assistance Technologies that employ computer vision technologies that support diverse users with visual impairments, especially those who are traditionally technologically underserved, we will discover what users’ value, what concerns they have, and what they prefer so that we can develop privacy recommendations for inclusive computer vision technologies..We have completed an investigation into the values and privacy concerns that individuals who are blind or have low vision (BLV) hold in the context of their use of Visual Assistance Technologies. Our investigation resulted in a publication in the 22nd International ACM SIGACCESS Conference on Computers and Accessibility..In this publication we report on our finding from interviews with N=20 participants who are totally blind (M=9, F=11; ages 22-73). The interviews included open-ended and short answer questions about users’ experiences with Visual Assistance Technologies,  their every-day privacy concerns, and their concern about 22 specific types of image content that may be considered private when shared with the general public, when shared with Visual Assistance Technologies that employ humans, and Visual Assistance Technologies that employ computer vision..We contribute two key novel findings that can be used to improve the design of privacy-aware technology for people who depend on VIDS. First, we identify the types of information BLV people consider to be the greatest concern in terms of privacy.  We convey this information through an easy-to-reference table that identifies each private information type and its priority. Computer vision developers may use the table as a taxonomy from which they can create datasets to train algorithms to better recognize content that contains private information, with a focus on the content types that matter the most..Second, we identify the perceived risks, benefits, and trade-offs people who are BLV encounter when using AI-powered VIDS. For example, we learned that many participants use AI-powered VIDS to access information despite the fact that they either mistrust or are unaware of how these services handle their data.  Underlying the risks, benefits, and trade-offs, we observed five values held by our participants (anonymity, accountability, trust, choice, and independence).",This research explores the conflict between convenience and privacy in computer vision
297,"How can the City of Austin and Good Systems, a UT Grand Challenge leverage AI to better serve, inform, and empower individuals experiencing homelessness? This collaboration between UT’s School of Information, LBJ School of Public Affairs, and Good Systems and the City of Austin’s Departments of Public Works, Neighborhood Housing and Community Development, and Communications and Technology Management seeks to confront the wicked problem of homelessness, which people experience as a continuum, ranging from housing instability to reintegration from incarceration, couch-surfing to street homelessness, and episodic to chronic. Our aims are to understand the information and service needs of individuals experiencing homelessness, to understand the information needs of agencies serving individuals experiencing homelessness, and to use AI to empower individuals experiencing homelessness and assist agencies. Our research will incorporate a collaborative feedback loop, whereby the three aims concurrently interact to determine and align the needs and wants of government and nonprofit staff and end users, and develop AI-based interventions. We will use a combination of sociotechnical research methods to understand and serve the values of all stakeholders, particularly individuals experiencing homelessness. In addition to scholarly outputs in the fields of information studies, human-computer interaction, and public policy, our interdisciplinary, cross-organization team directly will influence government policy, leading to improvements in the lives of individuals across and beyond the homelessness continuum. Our innovative research will transform wrap-around services for individuals experiencing homelessness in Austin, using AI to ensure Austin is not only a smart city, but also a good city..Public health messaging during the COVID-19 pandemic emphasizes “shelter-in-place,” “stay-at-home,” and “safer at home.” This messaging assumes that everyone has a safe place to shelter, a home. Unfortunately, many Austinites, as well as many Texans, Americans, and people worldwide, are experiencing homelessness. During this crisis, we must consider our most vulnerable populations, including the homeless — not just due to the threat of infection, but also the resulting food insecurity and societal instability. COVID-19 has brought homelessness to the forefront in new ways. Many people have been unable to work, from the threat of illness, illness itself, or furloughs, and this has put them at risk of becoming homeless with no income to pay for housing. The Emergency Operations Center (EOC), that the City of Austin and Travis County have instituted to respond to the COVID-19 pandemic, must deal with many complex issues. These issues include the healthcare and housing needs of people already experiencing homelessness, and the needs of additional people entering the homelessness continuum, due to unemployment or illness. The EOC has created task forces to address homelessness, lack of income, and food insecurity. Our research can assist with identifying those experiencing homelessness, or at risk of homelessness and providing services for them during this COVID-19 crisis..A partnership between the University of Texas at Austin and the city looks at how AI can identify residents at risk of experiencing homelessness, as well as helping those currently in need find access to services.",This research explores how to use AI to empower individuals experiencing homelessness and assist agencies
298,"An interdisciplinary team of researchers developed an AI system that can measure the health effects of neighborhood environments in Houston, Dallas, San Antonio, and Austin using citizen crowd-sourced data. More information.The project consists of four parts: (1) database development, (2) modeling and analytics, (3) visualization and web development, and (4) community engagement and application. The first two parts are associated with the actual building, training, and testing of machine learning models. The targets are the various health outcomes, namely the prevalence of common non-communicable chronic diseases such as coronary heart disease, cancer, diabetes, poor mental health, obesity, and stroke. The actual health outcomes used in training and testing the models are accessed from the CDC’s 500 Cities Project. The features are created based on three data sources, namely the CDC’s Social Vulnerability Index (SVI) dataset, the EPA’s Smart Location Database (SLD), and the 311 service request datasets accessed from each municipality. Sixty features (i.e., predictor variables) are considered, which characterize the social environment, the physical environment, and the aspects and degrees of neighborhood disorder. A variety of machine learning algorithms are applied and compared, including Ridge Regression, Lasso Regression, Elastic Net, Support Vector Machine, Decision Tree, Random Forest, Extra Trees, and Gradient Boosting. To improve the model performance, the model hyperparameters are fine-tuned using 10-fold cross-validation. Different sets of features are also experimented with..It is shown that the tract-level prevalence for the common non-communicable chronic diseases can be reasonably well predicted based on the publicly available datasets. Furthermore, two major findings have been yielded from this study: (1) the sociodemographic and socioeconomic variables are the strongest predictors for tract-level health outcomes; (2) the historical records of 311 service requests can be a useful complementary data source because the information distilled from the 311 data often helps improve the models’ performance..The datasets and the predictive models are published online. Users can play with the models interactively by using the web tools we developed. The web tools can help the public and city officials evaluate future scenarios and understand how changes in the neighborhood conditions can lead to changes in the health outcomes..Chen, F and Jiao, J. (under review). Predicting and mapping neighborhood-scale health outcomes: A machine learning approach. Computers, Environment and Urban Systems.",An AI system that measures the health effects of neighborhood environments in Houston
302,"In a national database in Argentina, tens of thousands of entries detail the names, birthdays, and national IDs of people suspected of crimes. The database, known as the Consulta Nacional de Rebeldías y Capturas (National Register of Fugitives and Arrests), or CONARC, began in 2009 as a part of an effort to improve law enforcement for serious crimes..But there are several things off about CONARC. For one, it’s a plain-text spreadsheet file without password protection, which can be readily found via Google Search and downloaded by anyone. For another, many of the alleged crimes, like petty theft, are not that serious—while others aren’t specified at all..Most alarming, however, is the age of the youngest alleged offender, identified only as M.G., who is cited for “crimes against persons (malicious)—serious injuries.” M.G. was apparently born on October 17, 2016, which means he’s a week shy of four years old..Now a new investigation from Human Rights Watch has found that not only are children regularly added to CONARC, but the database also powers a live facial recognition system in Buenos Aires deployed by the city government. This makes the system likely the first known instance of its kind being used to hunt down kids suspected of criminal activity..Buenos Aires first began trialing live facial recognition on April 24, 2019. Implemented without any public consultation, the system sparked immediate resistance. In October, a national civil rights organization filed a lawsuit to challenge it. In response, the government drafted a new bill—now going through legislative processes—that would legalize facial recognition in public spaces..The system was designed to link to CONARC from the beginning. While CONARC itself doesn’t contain any photos of its alleged offenders, it’s combined with photo IDs from the national registry. The software uses suspects’ headshots to scan for real-time matches via the city’s subway cameras. Once the system flags a person, it alerts to the police to make an arrest..The system has since led to numerous false arrests (links in Spanish), which the police have no established protocol for handling. One man who was mistakenly identified was detained for six days and about to be transferred to a maximum security prison before he finally cleared up his identity. Another was told he should expect to be repeatedly flagged in the future even though he’d proved he wasn’t who the police were looking for. To help resolve the confusion, they gave him a pass to show to the next officer that might stop him..“There seems to be no mechanism to be able to correct mistakes in either the algorithm or the database,” Han says. “That is a signal to us that here’s a government that has procured a technology that it doesn’t understand very well in terms of all the technical and human rights implications.”.All this is already deeply concerning, but adding children to the equation makes matters that much worse. Though the government has publicly denied (link in Spanish) that CONARC includes minors, Human Rights Watch found at least 166 children listed in various versions of the database between May 2017 and May 2020. Unlike M.G., most of them are identified by full name, which is illegal. Under international human rights law, children accused of a crime must have their privacy protected throughout the proceedings..Also unlike M.G., most were 16 or 17 at time of entry—though, mysteriously, there have been a few one- to three-years-olds. The ages aren’t the only apparent errors in the children’s entries. There are blatant typos, conflicting details, and sometimes multiple national IDs listed for the same individual. Because kids also physically change faster than adults, their photo IDs are more at risk of being outdated..On top of this, facial recognition systems, under even ideal laboratory conditions, are notoriously bad at handling children because they’re trained and tested primarily on adults. The Buenos Aires system is no different. According to official documents (link in Spanish), it was tested only on the adult faces of city government employees before procurement. Prior US government tests of the specific algorithm that it is believed to be using also suggest it performs worse by a factor of six on kids (ages 10 to 16) than adults (ages 24 to 40)..All these factors put kids at a heightened risk for being misidentified and falsely arrested. This could create an unwarranted criminal record, with potentially long-lasting repercussions for their education and employment opportunities. It might also have an impact on their behavior..“The argument that facial recognition produces a chilling effect on the freedom of expression is more amplified for kids,” says Han. “You can just imagine a child [who has been falsely arrested] would be extremely self-censoring or careful about how they behave in public. And its still early to try and figure out the long-term psychological impacts—how it might shape their world view and mindset as well.”.While Buenos Aires is the first city Han has identified using live facial recognition to track kids, she worries that many other examples are hidden from view. In January, London announced that it would integrate live facial recognition into its policing operations. Within days, Moscow said it had rolled out a similar system across the city..Though it’s not yet known whether these systems are actively trying to match children, kids are already being affected. In the 2020 documentary Coded Bias, a boy is falsely detained by the London police after live facial recognition mistakes him for someone else. It’s unclear whether the police were indeed looking for a minor or someone older..Even those who are not detained are losing their right to privacy, says Han: “There’s all the kids who are passing in front of a facial-recognition-enabled camera just to access the subway system.”.It’s often easy to forget in debates about these systems that children need special consideration. But that’s not the only reason for concern, Han adds. “The fact that these kids would be under that kind of invasive surveillance—the full human rights and societal implications of this technology are still unknown.” Put another way: what’s bad for kids is ultimately bad for everyone. .An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.","The system has since led to numerous false arrests (links in Spanish), which the police have no established protocol for handling. One man who was mistakenly identified was detained for six days and about to be transferred to a maximum security prison before he finally cleared up his identity. Another was told he should expect to be repeatedly flagged in the future even though he’d proved he wasn't who the police were looking for."
303,"     Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a      Creative Commons Attribution Non-Commercial No Derivatives license.     You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images; if one is not provided      below, credit the images to MIT.    .Asymptomatic people who are infected with Covid-19 exhibit, by definition, no discernible physical symptoms of the disease. They are thus less likely to seek out testing for the virus, and could unknowingly spread the infection to others..But it seems those who are asymptomatic may not be entirely free of changes wrought by the virus. MIT researchers have now found that people who are asymptomatic may differ from healthy individuals in the way that they cough. These differences are not decipherable to the human ear. But it turns out that they can be picked up by artificial intelligence..In a paper published recently in the IEEE Journal of Engineering in Medicine and Biology, the team reports on an AI model that distinguishes asymptomatic people from healthy individuals through forced-cough recordings, which people voluntarily submitted through web browsers and devices such as cellphones and laptops..The researchers trained the model on tens of thousands of samples of coughs, as well as spoken words. When they fed the model new cough recordings, it accurately identified 98.5 percent of coughs from people who were confirmed to have Covid-19, including 100 percent of coughs from asymptomatics — who reported they did not have symptoms but had tested positive for the virus..The team is working on incorporating the model into a user-friendly app, which if FDA-approved and adopted on a large scale could potentially be a free, convenient, noninvasive prescreening tool to identify people who are likely to be asymptomatic for Covid-19. A user could log in daily, cough into their phone, and instantly get information on whether they might be infected and therefore should confirm with a formal test..“The effective implementation of this group diagnostic tool could diminish the spread of the pandemic if everyone uses it before going to a classroom, a factory, or a restaurant,” says co-author Brian Subirana, a research scientist in MIT’s Auto-ID Laboratory..Prior to the pandemic’s onset, research groups already had been training algorithms on cellphone recordings of coughs to accurately diagnose conditions such as pneumonia and asthma. In similar fashion, the MIT team was developing AI models to analyze forced-cough recordings to see if they could detect signs of Alzheimer’s, a disease associated with not only memory decline but also neuromuscular degradation such as weakened vocal cords..They first trained a general machine-learning algorithm, or neural network, known as ResNet50, to discriminate sounds associated with different degrees of vocal cord strength. Studies have shown that the quality of the sound “mmmm” can be an indication of how weak or strong a person’s vocal cords are. Subirana trained the neural network on an audiobook dataset with more than 1,000 hours of speech, to pick out the word “them” from other words like “the” and “then.”.The team trained a second neural network to distinguish emotional states evident in speech, because Alzheimer’s patients — and people with neurological decline more generally — have been shown to display certain sentiments such as frustration, or having a flat affect, more frequently than they express happiness or calm. The researchers developed a sentiment speech classifier model by training it on a large dataset of actors intonating emotional states, such as neutral, calm, happy, and sad..Finally, the team combined all three models, and overlaid an algorithm to detect muscular degradation. The algorithm does so by essentially simulating an audio mask, or layer of noise, and distinguishing strong coughs — those that can be heard over the noise — over weaker ones..With their new AI framework, the team fed in audio recordings, including of Alzheimer’s patients, and found it could identify the Alzheimer’s samples better than existing models. The results showed that, together, vocal cord strength, sentiment, lung and respiratory performance, and muscular degradation were effective biomarkers for diagnosing the disease..When the coronavirus pandemic began to unfold, Subirana wondered whether their AI framework for Alzheimer’s might also work for diagnosing Covid-19, as there was growing evidence that infected patients experienced some similar neurological symptoms such as temporary neuromuscular impairment..“The sounds of talking and coughing are both influenced by the vocal cords and surrounding organs. This means that when you talk, part of your talking is like coughing, and vice versa. It also means that things we easily derive from fluent speech, AI can pick up simply from coughs, including things like the person’s gender, mother tongue, or even emotional state. There’s in fact sentiment embedded in how you cough,” Subirana says. “So we thought, why don’t we try these Alzheimer’s biomarkers [to see if they’re relevant] for Covid.”.In April, the team set out to collect as many recordings of coughs as they could, including those from Covid-19 patients. They established a website where people can record a series of coughs, through a cellphone or other web-enabled device. Participants also fill out a survey of symptoms they are experiencing, whether or not they have Covid-19, and whether they were diagnosed through an official test, by a doctor’s assessment of their symptoms, or if they self-diagnosed. They also can note their gender, geographical location, and native language..To date, the researchers have collected more than 70,000 recordings, each containing several coughs, amounting to some 200,000 forced-cough audio samples, which Subirana says is “the largest research cough dataset that we know of.” Around 2,500 recordings were submitted by people who were confirmed to have Covid-19, including those who were asymptomatic..The team used the 2,500 Covid-associated recordings, along with 2,500 more recordings that they randomly selected from the collection to balance the dataset. They used 4,000 of these samples to train the AI model. The remaining 1,000 recordings were then fed into the model to see if it could accurately discern coughs from Covid patients versus healthy individuals..Surprisingly, as the researchers write in their paper, their efforts have revealed “a striking similarity between Alzheimer’s and Covid discrimination.”.Without much tweaking within the AI framework originally meant for Alzheimer’s, they found it was able to pick up patterns in the four biomarkers — vocal cord strength, sentiment, lung and respiratory performance, and muscular degradation — that are specific to Covid-19. The model identified 98.5 percent of coughs from people confirmed with Covid-19, and of those, it accurately detected all of the asymptomatic coughs..The AI model, Subirana stresses, is not meant to diagnose symptomatic people, as far as whether their symptoms are due to Covid-19 or other conditions like flu or asthma. The tool’s strength lies in its ability to discern asymptomatic coughs from healthy coughs.  .The team is working with a company to develop a free pre-screening app based on their AI model. They are also partnerning with several hospitals around the world to collect a larger, more diverse set of cough recordings, which will help to train and strengthen the model’s accuracy..As they propose in their paper, “Pandemics could be a thing of the past if pre-screening tools are always on in the background and constantly improved.”.Ultimately, they envision that audio AI models like the one they’ve developed may be incorporated into smart speakers and other listening devices so that people can conveniently get an initial assessment of their disease risk, perhaps on a daily basis..Research scientist Brian Subirana speaks with The Economist’s Babbage podcast about his work developing a new AI system that could be used to help diagnose people asymptomatic Covid-19..A new algorithm developed by MIT researchers could be used to help detect people with Covid-19 by listening to the sound of their coughs, reports Zoe Kleinman for BBC News. “In tests, it achieved a 98.5% success rate among people who had received an official positive coronavirus test result, rising to 100% in those who had no other symptoms,” writes Kleinman..Mashable reporter Rachel Kraus writes that a new system developed by MIT researchers could be used to help identify patients with Covid-19. Kraus writes that the algorithm can “differentiate the forced coughs of asymptomatic people who have Covid from those of healthy people.”.A new took developed by MIT researchers uses neural networks to help identify Covid-19, reports Alyse Stanley for Gizmodo. The model “can detect the subtle changes in a person’s cough that indicate whether they’re infected, even if they don’t have any other symptoms,” Stanley explains..TechCrunch reporter Devin Coldewey writes that MIT researchers have built a new AI model that can help detect Covid-19 by listening to the sound of a person’s cough. “The tool is detecting features that allow it to discriminate the subjects that have COVID from the ones that don’t,” explains Brian Subirana, a research scientist in MIT’s Auto-ID Laboratory..MIT researchers have developed a new AI model that could help identify people with asymptomatic Covid-19 based on the sound of their cough, reports CBS Boston. The researchers hope that in the future the model could be used to help create an app that serves as a “noninvasive prescreening tool to figure out who is likely to have the coronavirus.”","Although asymptomatic people might not show symptoms, the model is able to distinguish coughs from healthy individuals and those with COVID-19 with a 98.5% accuracy"
304,"There are far more trees in the West African Sahara Desert than you might expect, according to a study that combined artificial intelligence and detailed satellite imagery. .Researchers counted over 1.8 billion trees and shrubs in the 1.3 million square kilometer (501,933 square miles) area that covers the western-most portion of the Sahara Desert, the Sahel, and what are known as sub-humid zones of West Africa..“We were very surprised to see that quite a few trees actually grow in the Sahara Desert, because up until now, most people thought that virtually none existed,” says Martin Brandt, professor in the geosciences and natural resource management department at the University of Copenhagen and lead author of the study in Nature..“We counted hundreds of millions of trees in the desert alone. Doing so wouldn’t have been possible without this technology. Indeed, I think it marks the beginning of a new scientific era.”.The researchers used detailed satellite imagery from NASA, and deep learning—an advanced artificial intelligence method. Normal satellite imagery is unable to identify individual trees, they remain literally invisible. Moreover, a limited interest in counting trees outside of forested areas led to the prevailing view that this particular region had almost no trees. This is the first time that anyone counted trees across a large dryland region..New knowledge about trees in dryland areas like this is important for several reasons, Brandt says. For example, they represent an unknown factor when it comes to the global carbon budget..“Trees outside of forested areas are usually not included in climate models, and we know very little about their carbon stocks. They are basically a white spot on maps and an unknown component in the global carbon cycle,” he says..It is an open platform designed to engage anyone who wants to offer a contribution for the global public good. The core objective is to link up the best innovators to networks of decision-makers, who can implement the change needed for the next decade. As a global platform, UpLink serves to aggregate and guide ideas and impactful activities, and make connections to scale-up impact..Furthermore, the new study contributes to better understanding of the importance of trees for biodiversity and ecosystems and for the people living in these areas. In particular, enhanced knowledge about trees is also important for developing programs that promote agroforestry, which plays a major environmental and socioeconomic role in arid regions..“Thus, we are also interested in using satellites to determine tree species, as tree types are significant in relation to their value to local populations who use wood resources as part of their livelihoods,” says Rasmus Fensholt, professor in the geosciences and natural resource management department..“Trees and their fruit are consumed by both livestock and humans, and when preserved in the fields, trees have a positive effect on crop yields because they improve the balance of water and nutrients.”.Researchers from the University of Copenhagen’s computer science department developed the deep learning algorithm that made the counting of trees over such a large area possible..The researchers fed the deep learning model thousands of images of various trees to show it what a tree looks like. Then, based on the recognition of tree shapes, the model could automatically identify and map trees over large areas and thousands of images. The model needs only hours what would take thousands of humans several years to achieve..“This technology has enormous potential when it comes to documenting changes on a global scale and ultimately, in contributing towards global climate goals. We are motivated to develop this type of beneficial artificial intelligence,” says professor and coauthor Christian Igel of the computer science department..Researchers will next expand the count to a much larger area in Africa. And in the longer term, they plan to create a global database of all trees growing outside forest areas..World Economic Forum Type may be republished in accordance with the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Public License, and in accordance with our Terms of Use..The Executive Director of Forests and Climate Change Wildlife Conservation Society argues for the need for true nature based solutions in tackling climate change. .Since the last ice age, deforestation has rapidly reduced the area of habitable land covered by forests due to urbanization and agricultural expansion.","The researchers used detailed satellite imagery from NASA with deep learning. Without deep learning, these trees would have been invisible"
305,"One month before a purported leak of files from Hunter Bidens laptop, a fake intelligence document about him went viral on the right-wing internet, asserting an elaborate conspiracy theory involving former Vice President Joe Bidens son and business in China..The document, a 64-page composition that was later disseminated by close associates of President Donald Trump, appears to be the work of a fake intelligence firm called Typhoon Investigations, according to researchers and public documents..The author of the document, a self-identified Swiss security analyst named Martin Aspen, is a fabricated identity, according to analysis by disinformation researchers, who also concluded that Aspens profile picture was created with an artificial intelligence face generator. The intelligence firm that Aspen lists as his previous employer said that no one by that name had ever worked for the company and that no one by that name lives in Switzerland, according to public records and social media searches..One of the original posters of the document, a blogger and professor named Christopher Balding, took credit for writing parts of it when asked about it and said Aspen does not exist..Despite the documents questionable authorship and anonymous sourcing, its claims that Hunter Biden has a problematic connection to the Communist Party of China have been used by people who oppose the Chinese government, as well as by far-right influencers, to baselessly accuse candidate Joe Biden of being beholden to the Chinese government..The document and its spread have become part of a wider effort to smear Hunter Biden and weaken Joe Bidens presidential campaign, which moved from the fringes of the internet to more mainstream conservative news outlets..An unverified leak of documents — including salacious pictures from what President Donald Trumps personal attorney Rudy Giuliani and a Delaware Apple repair store owner claimed to be Hunter Bidens hard drive — were published in the New York Post on Oct. 14. Associates close to Trump, including Giuliani and former White House chief strategist Steve Bannon, have promised more blockbuster leaks and secrets, which have yet to materialize..The fake intelligence document, however, preceded the leak by months, and it helped lay the groundwork among right-wing media for what would become a failed October surprise: a viral pile-on of conspiracy theories about Hunter Biden..The Typhoon Investigations document was first posted in September to Intelligence Quarterly, an anonymous blog dedicated to collecting important daily news, according to its about section. Historical domain records show the blog was registered to Albert Marko, a self-described political and economic adviser, who also lists the blog on his Twitter bio. When asked about the provenance of the document, Marko said he received it from Balding..Balding, previously an associate professor at Fulbright University Vietnam who studied the Chinese economy and financial markets, posted the document on his blog on Oct. 22, seven weeks after it was initially published..I had really not wanted to do this but roughly 2 months ago I was handed a report about Biden activities in China the press has simply refused to cover. I want to strongly emphasize I did not write the report but I know who did, Balding said in an email..I authored small parts of the report and was involved in report preparation and review. As a researcher, and due to the understandable worry about foreign disinformation, it was paramount that the report document activity from acknowledged and public sources, Balding said. Great care was taken to document, cite, and retain information so that acknowledged facts could be placed in the public domain..Balding said Aspen is an entirely fictional individual created solely for the purpose of releasing this report. Balding did not name the documents main author, saying the primary author of the report, due to personal and professional risks, requires anonymity..Balding claimed that the document was commissioned by Apple Daily, a Hong Kong-based tabloid that is frequently critical of the Chinese government. A spokesperson for Apple Daily confirmed it had worked with Balding on the document..In addition to posting the document to his blog, Balding also promoted it in far-right media, appearing on Bannons podcast and on China Unscripted, a podcast produced by The Epoch Times, a pro-Trump media outlet opposed to the Chinese government..Balding, an American who taught economics at Chinas Peking University HSBC Business School until 2018, is often critical of the Chinese government. He made news this year as a source uncovering a global bulk data collection operation by the Chinese company Shenzhen Zhenhua Data Technology..Blog posts highlighting the most salacious parts of the document, including articles from the Intelligence Quarterly Blog, Revolver News and Baldings blog, received 70,000 public interactions — which includes reactions, comments and shares — across Facebook, Twitter and Reddit, according to the social media analysis tool BuzzSumo..Baldings blog was the primary driver of virality in conservative and conspiracy communities. The report itself was shared across Facebook and Twitter around 5,000 times, according to BuzzSumo, and more than 80 sites linked back to the blog, which was shared more than 25,000 times on Facebook and Twitter. Hyperpartisan and conspiracy sites like ZeroHedge and WorldNetDaily led the pack..After the promise of a big reveal one day earlier, the document was also posted on the extremist forum 8kun by Q, the anonymous account behind the QAnon conspiracy theory movement..On Twitter, the document was pushed by influencers in the QAnon community, as well as by Dinggang Wang, an anti-Chinese government YouTube personality who works for Guo Wengui, a billionaire who fled China amid accusations of bribery and other crimes. Republican Newt Gingrich, the former speaker of the House of Representatives, tweeted the document to his 2.3 million followers..The profile picture for Aspen immediately showed signs of being a computer-generated image that can be created by computers and even some websites. Aspens ears were asymmetrical, for one, but his left eye is what gave away that he did not really exist. Aspens left iris juts out and appears to form a second pupil, a somewhat frequent error with computer-generated faces..The most obvious tell was the irregular shape of the irises, Thomas said. The profile picture looks pretty convincing in the Twitter thumbnail, but when I popped it up into full view I was immediately suspicious..Thomas then consulted with Ben Nimmo, director of investigations at the analytics company Graphika, who noted the other telltale sign of a computer-generated face..One of the things he and his team have figured out is that if you layer a lot of these images over the top of one another, the eyes align, Thomas said. He did that with this image, and the eyes matched up..Other parts of Aspens identity were clearly stolen from disparate parts of the web. Aspens Facebook page was created in August, and it featured only two pictures, both from his new house, which were tracked back to reviews on the travel website Tripadvisor. The logo for Typhoon Investigations was lifted from the Taiwan Fact-Checking Center, a digital literacy nonprofit..Aspen claimed on his LinkedIn profile to have worked for a company called Swiss Security Solutions from 2016 to 2020. Swiss Security Solutions denied having ever employed anyone named Aspen, and it said it had found fake accounts for two other people pretending to have worked for the company..Martin Aspen was never a freelancer or worker of the Swiss Security Solutions. We do not know this person. According to our Due Diligence Software, this person does not exist in Switzerland, Swiss Security Solutions Chairman Bojan Ilic said, adding that the company has reported the profile to LinkedIn..Computer-generated faces have become a staple of large-scale disinformation operations in the run-up to the election. In December, Facebook took down a network of fake accounts using computer-created faces tied to The Epoch Times. Facebook removed over 600 accounts tied to the operation, which pushed pro-Trump messages and even served as moderators of some Facebook groups. Stephen Gregory, publisher of the U.S. editions of The Epoch Times, has denied any connection to the accounts..Last month, Facebook removed another batch of computer-generated profiles originating in China and the Philippines, some of which made anti-Trump posts..Renee DiResta, a researcher at the Stanford Internet Observatory, said computer-created identities are becoming common for disinformation campaigns, in part because they are easy to create..DiResta, who helped examine a ring of AI-generated faces tied to the conservative nonprofit Turning Point USA last month, said computer-generated profile pictures can be used to build an army of fake people to artificially support a cause or to make disinformation operations harder to discover..One of the things that investigators look at to understand the narrative that is spreading is whether the accounts are authentic, whether theyre real, DiResta said. If they were to use a stock photo, it confirms something dishonest is likely happening. By using an AI-generated face, youre guaranteeing you wont find that person elsewhere on the internet..CORRECTION (Oct 30, 2020. 11:19 a.m. ET): An earlier version of this article misstated Christopher Balding’s position at Fulbright University Vietnam. As of Thursday afternoon, the university had listed him as being currently employed, but later put out a statement saying he was a former professor. He is no longer an employee of the university as of Sept. 10, 2020.","A fake identity, Martin Aspen, was created by artificial intelligence to defame Hunter Biden"
306,"Black people in the US suffer more from chronic diseases and receive inferior health care relative to white people. Racially skewed math can make the problem worse..Doctors often make life-changing decisions about patient care based on algorithms that interpret test results or weight risks, like whether to perform a particular procedure. Some of those formulas factor in a person’s race, meaning patients’ skin color can affect access to care..A new study of patients in the Boston area is one of the first to document the harm that can cause. It examined the effect on care of a widely used but controversial formula for estimating kidney function that by design assigns Black people healthier scores..The study analyzed health records for 57,000 people with chronic kidney disease from the Mass General Brigham health system that includes Harvard teaching hospitals Massachusetts General and Brigham and Womens. One third of Black patients, more than 700 people, would have been placed into a more severe category of kidney disease if their kidney function had been estimated using the same formula as for white patients..That could have affected decisions such as when to refer someone to a kidney specialist, or refer them for a kidney transplant. In 64 cases, patients’ recalculated scores would have qualified them for a kidney transplant wait list. None had been referred or evaluated for transplant, suggesting that doctors did not question the race-based recommendations..“That was really staggering,” says Mallika Mendu, an assistant professor at Harvard Medical School and kidney specialist at Brigham and Women’s whose work on the study convinced her to stop using the race-based calculation with her own patients. “We know there are already other disparities in access to care and management of the condition. This is not helping.”.In 64 cases, Black patients’ scores would have qualified them for a kidney transplant wait list. None had been referred or evaluated for transplant..The study is the most recent of several signs that math tools exacerbate health inequalities. Last year, software used by many health systems to prioritize access to special care for chronic conditions was found to systematically privilege white patients over Black patients. It didn’t explicitly take account of race, but replicated patterns in access to health care caused by factors like poverty..The kidney algorithm, by contrast, is one of many clinical decision algorithms that explicitly take account of race. A recent review listed more than a dozen such tools, in areas including cancer and lung care. In August, a group of Black retired NFL players sued the league, claiming it used an algorithm that assumes white people have higher cognitive function to decide compensation for brain injuries..The issue is winning more attention, including from federal lawmakers. Representative Richard Neal (D-Massachusetts), chair of the House Ways and Means Committee, says the kidney study underlines the need to reconsider use of race in all medical algorithms. “Many clinical algorithms can result in delayed or inaccurate diagnoses for Black and Latinx patients, leading to lower-quality care and worse health outcomes,” he says..Neal has asked medical societies and the Centers for Medicare & Medicaid Services to investigate the impact on patients of clinical algorithms that use race. Last month, Senator Elizabeth Warren (D-Massachusetts) and others asked the Department of Health and Human Services to investigate race-based medical algorithms..The new study examined a standard calculation called CKD-EPI used to convert a blood test for a person’s level of the waste product creatinine into a measure of kidney function called estimated glomerular filtration rate, or eGFR. Lower scores indicate worse kidney function; the scores are used to categorize the severity of a person’s disease and guide what care they receive. The equation factors in a person’s age and sex. Black patients get their score boosted by an additional 15.9 percent..That design is coming under fire from academics and medical residents who fear it bakes discrimination into kidney care. Researchers who created the formula in 2009 added the “race correction” to smooth out statistical differences between the small number of Black patients and others in their data. But that project and subsequent studies have not explained why the correlation between creatinine and kidney function looked different in Black patients, or the role of factors proven to affect creatinine levels such as diet, says Nwamaka Eneanya, an assistant professor at the University of Pennsylvania who also worked on the new Boston study. A person’s race is a social category, not a physiological one, she says, and it doesn’t make sense to use it to interpret blood tests..Eneanya was already convinced that the standard eGFR formula should be abandoned, but says showing how the race-based adjustment affects care highlights the urgency of the problem. “Any degradation of treatment for these already marginalized groups could have profound results,” Eneanya says..A preliminary version of the newly published findings helped convince leaders at Mass General Brigham to abandon the race-based eGFR formula in June. Several other major US hospitals, including University of Washington and Vanderbilt, have done the same this year. Support is growing for an alternative method of calculating eGFR that uses a different blood test, for the protein cystatin C..Despite those shifts, the campaign to remove race as a factor in kidney assessment and care has a long way to go. Many institutions and doctors are unlikely to move away from the traditional calculation unless medical societies change their guidelines. The two leading US kidney care organizations have formed a task force on the issue. More than 1,300 people have signed a petition urging that group to act..Vanessa Grubbs, a coauthor of the petition and associate professor at UC San Francisco, says adjusting equations is only part of the work needed to undo the harms of using race in medical formulas. After institutions change their eGFR calculations, they should also review Black patients’ care plans, how they train new doctors, and how they think about race, she says..Equations with race baked in encourage doctors to categorize all patients racially, she says, distracting from their true medical needs. “Black people aren’t the only ones affected,” she says. “This is bad for everyone.”","This kidney algorithm explicitly takes account of race whose Black patients' scores would have qualified them for a kidney transplant waitlist, yet none of them were referred or evaluated for a transplant."
307,"On Tuesday after the publication of this piece, Verkada’s co-founder and CEO Filip Kaliszan issued a new statement saying the company had changed its decision on the disciplinary action taken against the offending employees and had now terminated them..“Upon a further review of this incident and feedback from several employees about how it was initially addressed, we have terminated the three individuals who instigated this incident, engaged in egregious behavior targeting coworkers, or neglected to report the behavior despite their obligations as managers.On Friday, I felt confident that we had dealt with this issue fully and appropriately. However, it is clear that my handling of this incident fell short of our commitment to maintaining the supportive work environment our employees deserve. I take responsibility for that, I apologize, and I hope this action will demonstrate to all of our employees that Verkada does not, and will never, tolerate this kind of behavior..Verkada, a fast-growing Silicon Valley surveillance startup, equips its offices in downtown San Mateo, California, with its own state-of-the-art security cameras..Last year, a sales director on the companys sales team abused their access to these cameras to take and post photos of colleagues in a Slack channel called #RawVerkadawgz where they made sexually explicit jokes about women who worked at the company, according to a report in IPVM, which Motherboard independently verified and obtained more information about..Face match… find me a squirt, the sales director wrote in the company Slack channel in August 2019, according to one screenshot obtained by Motherboard..The comment was posted along with a series of photos of employees’ faces captured with the offices surveillance system which were patched together using a Verkada facial recognition feature. Face search,” as it’s called, can pinpoint an individual in a sea of faces. The pinpointed face, in this instance, belonged to a Verkada employee, her mouth wide open. In addition to verifying the incident with three sources who worked at Verkada at the time, Motherboard compared the format of the images posted to those included in Verkadas publicly available demo videos to verify that they were indeed captured by the companys surveillance cameras..According to three sources who worked at Verkada at the time, the group of men posted sexually graphic content about multiple female employees in similar Slack messages..After the Slack channel was reported to the companys Human Resource team in February, Verkadas CEO Filip Kaliszan announced in a company all-hands meeting that an undisclosed number of employees active in the Slack channel were given the choice between leaving the company or having their share of stock reduced. All of them chose the latter option, and the Slack channel was removed, according to four employees who worked at Verkada during the time.The person who posted the screenshot still works at Verkada..Verkada, which was founded in 2016 in Kaliszan’s living room, sells machine vision security cameras with cloud-software, including dome cameras, fisheye lenses, and footage viewing stations that can be monitored from anywhere in the world. The company sells to a roster of high profile clients, including Equinox, Juul Labs, Red Lobster, Siemens, Pasadena City College, the city of Memphis, and dozens of other corporations and government entities. Its YouTube page advertises systems for law enforcement, governments, and corporations. Its ambition is to be the operating system that runs every building in the world, the company says. Verkada software allows users to immediately detect all footage of a particular person of interest, rather than forcing people reviewing the tape to search through hours of video. Kaliszan describes Verkadas mission as building the worlds best video security system. In early 2020, the company was valued at $1.6 billion, and its workforce had grown to just over 400 employees..The company has a series of demo videos on its YouTube which show the use of Verkada cameras at its own offices. Besides being used in demonstrations of its technology, these cameras have been used to sexually harass employees. Theyve also been used to surveil Verkada itself: For example, sources at the company explained that the company had an indoor, maskless party in September. Evidence of the party was provided to Motherboard using captured surveillance footage from its cameras..When asked for comment about the sales teams use of the companys video surveillance system to target women colleagues, a spokesperson for Verkada said, Verkada does not tolerate sexual harassment or inappropriate behavior. This isolated incident was investigated and all individuals involved were disciplined accordingly. This process included our HR department working with the women impacted by this incident–offering professional and personal resources to ensure they supported our course of action and felt safe and comfortable in their jobs..The spokesperson added that in the eight months since management became aware of the situation, Verkada has held a mandatory sexual harassment training via Zoom, and estabilished three employee resource groups, for women, people of color, and LBGTQ employees at the company..Four current and former employees who spoke to Motherboard on the condition of anonymity were furious by the companys decision not to terminate the men who’d posted sexually graphic content about their coworkers in the Slack channel—but said that the incident is just one example of sexism at Verkada that its co-founders Kaliszan and board chairman Hans Robertson have chosen to downplay..Any other tech company, a former Verkada employee told Motherboard, would have immediately fired those people. They should never ever be able to get another job at a tech company ever again, but the higher ups on the sales team have a lot of power and are encouraged and can do whatever they want as long as they bring in a lot of revenue. Verkadas singular value is making money..I think its 100 percent fair to say I left Verkada because of the culture, another former employee told Motherboard. I didnt feel comfortable working under my director after learning about the incident. The worst part of it was that it seemed like the men in this crew continued to be celebrated and remained in leadership positions. Thats how [management] has made the toxic culture theyve created okay..Since its founding, Verkada has tried to distinguish itself from Ring, Amazons home surveillance system, which has partnered with at least 400 police departments in the United States, and actively cooperates to share footage belonging to private individuals with the police. Unlike Ring which sells home surveillance systems, Verkada proudly advertises that its corporate clients dont work with law enforcement, and that data collected by corporations and other institutions it does business with belongs to them alone. But on its YouTube channel, Verkada advertises its work with law enforcement agencies such as the police department of Parkersburg, West Virginia..In January, Kaliszan told TechCrunch “we don’t have any arrangements with law enforcement like Ring. We view ourselves as providing great physical security tools to the people that run schools, hospitals and businesses. The data that those organizations gather is their own.”.Many of the technologies it has developed such as facial recognition and environmental sensors that can detect vaping in schools or a human presence in a room, have been criticized for expanding a system that serves to protect property owners and is often weaponized against people of color and other vulnerable populations..In March, in the lead-up to a company wide all-hands meeting, employees voted on questions that they wanted addressed by management. One question about Verkadas response to the Slack episode, received 127 upvotes, according to screenshots reviewed by Motherboard, the most of all of those submitted. “Why are the men who sexually harassed their female colleagues able to keep their jobs? Is this setting the right example and sustaining a safe work environment?”.Prior to late-October, when Motherboard reached out to Verkada for comment on the Slack incident, the company had yet to formally inform employees that the companys surveillance system had been abused by sales team members. This past Friday, after Motherboard asked for comment and more than 9 months after the company learned about the Slack incident, Kaliszan sent a company-wide email finally addressing the incident..Last year on August 8, 2019, Kaliszan wrote, a member of our sales team misused access to our office camera footage–access that everyone on our team is afforded and that the sales team uses frequently to demo the product for potential clients–and shared a screenshot of a coworker on a private Slack channel with vulgar commentary..As soon as leadership became aware of the incident,” he wrote, “we launched a comprehensive investigation to understand what happened and who was responsible. This investigation found that one person was responsible for instigating the incident and nine other members of the team were part of the Slack channel…. I imposed the largest financial penalty in our company’s history on the instigator and had individual disciplinary discussions with each of the other participants..Four employees who worked in different teams throughout Verkada said that the culture of sexism at the company largely emanated from a cliquey group of high-ranking white men on the sales team, many of them who grew up and played high school football in same wealthy enclave, Danville, California, some of whom went on to play for the NFL..If you’re not invited into that core group of guys, you have a hard time moving your career forward or getting promoted, a former sales employee told Motherboard. The word frat is thrown around at Verkata a lot because there are guys that protect each other at the company. That’s this crew from Danville. They’re like a frat..In April, after the Slack incident, in a fireside chat, Verkadas board chairman Hans Robertson told a group of Silicon Valley entrepreneurs that he had intentionally hired a team of sales athletes, that he had modelled after the sales team at Dell EMC, a subsidiary of Dell Technologies, which was famous for hiring half the Northeastern football team. Robertson explained Verkadas sales strategy was to get rid of underperformers very quickly and make the culture really fun..At least two of the men involved in the Slack post obtained by Motherboard, played high school football at San Ramon Valley High School in Danville, California. According to an article in the industry trade journal IPVM, at least five senior sales team members played sports at San Ramon Valley High School..Its the most horrible company Ive ever worked for, another former employee who recently quit working at Verkada told Motherboard. Even beyond the Slack incident, you know people don’t want to stay for long. Everyone wants to stay there for the potential money they could make, but especially for women it’s hard to stay there. I was oversold in the interview process. There’s no support. They don’t care about you..Motherboard spoke to one member of the sales team who said she enjoyed Verkadas work culture and thought the company’s response to the Slack incident was adequate. As soon as management found out [about the Slack incident], they disciplined not just the person who made the comment, but anyone who was in the Slack channel, she said. I felt like they took it very seriously. I have always felt comfortable here..Another Verkada employee wasn’t optimistic about Verkadas mission after learning about the Slack incident, the big picture for me having worked at the company is that it has opened my eyes to how surveillance can be abused by the people in power..By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.",Employees at Verkada accessed the company's facial recognition system to take photos of women colleagues and make sexually explicit jokes.
308,"These are just some of the demographic variables that have been used by CAS (Crime Anticipation System) to predict 125x125 meter crime “hot spots” across the Netherlands. The system, which relies entirely on automated algorithms and data analytics, was piloted in Amsterdam in 2014 and rolled out nationally in 2017. Critics at the time pointed to the system as a potential slippery slope, one which could lead to an increased willingness by law enforcement to embrace algorithmic models that perpetuate discriminatory practices like ethnic profiling..A report released late last month by Amnesty International revealed that Dutch law enforcement have been engaged in a number of predictive-policing pilots and referred to the Netherlands as “one of the countries at the forefront of predictive policing in practice.” The report also calls on law enforcement to halt all predictive policing projects until legislative safeguards are put in place and accuses the pilots of multiple human rights violations, including the right to privacy, the presumption of innocence, and the right to non-discrimination..Amnesty’s report particularly focuses on a predictive policing pilot in Roermond (a municipality in the southeast of the Netherlands) called the Sensing Project, which uses cameras to capture the license plate, brand, model, and year of manufacture of passing cars. According to the report, which primarily relies on documents obtained via WOB (public information request) requests, this data is fed into an algorithm that in real time calculates “hits” on passing vehicles. These hits are then automatically sent to police officers, who can decide whether or not they want to investigate..The project is not only intrusive, the report claims, but discriminatory by design, since its aim is to fight “mobile banditry” (crimes like theft, pickpocketing, and drug trafficking), a term which explicitly excludes people of Dutch nationality and assumes that the offender is either of Eastern European origin or Romani, a minority ethnic group. Considering this, it is unsurprising that some of the variables that generate “extra points” when calculating the risk scores of passing vehicles include whether the car has an Eastern European license plate and the route (re-tracked via automatic licence plate recognition cameras) it is taking, leading to what the report describes as “the automation of ethnic profiling.”.“Article one of the Dutch constitution is the prohibition of the discrimination,” Gerbrig Klos, one of the authors of the Amnesty report, told Motherboard. “If there’s one constitutional article that every person in the Netherlands knows, it’s that one. It’s at the front of every police station. Members of law enforcement and the government all say ‘ethnic profiling is prohibited’ and yet, at the same time, we find time and time again that predictive policing projects like these are explicitly biased and prejudiced and rely on data that is explicitly biased and prejudiced, but nobody does anything about it.”.Last year it was revealed that the Dutch Authorities were using automated algorithms in a system known as SyRI (System Risk Indication) to predict fraud. The system was criticized by the UN special rapporteur on poverty for using parameters that were explicitly targeted at people from low income backgrounds and ethnic minorities. It was later found to be in violation of existing European human rights law and was finally discontinued earlier this year..Marc Schuilenburg is a professor of law and criminology at the Vrije Universiteit Amsterdam and author of the upcoming book Hysteria: Crime, Media, and Politics. He argues that predictive policing not only reflects existing inequalities, but also strengthens them..“Predictive policing in the Netherlands is always focused on petty crimes committed by the underclass. It’s never focused on big fraud committed by the upper class, which we as criminologists know is hugely prevalent,” Schuilenburg told Motherboard. “When you analyze predictive policing, you can’t analyze it as something that stands on its own. It’s always linked to what I term the ‘surveillance continuum’ of other methods of policing which explicitly target ethnic minorities and people from the underclass, such as hotspot policing.”.They also create feedback loops in which algorithms designed on already biased data produce even more biased data, which is then fed back in. In the case of the Sensing project, for example, Amnesty’s report points out that investigations of cars flagged as ‘hits’–even if the hits are false positives–are entered into operational databases as relating to “mobile banditry.” This essentially means that the predetermined bias of the algorithm against Eastern European and Romani drivers is perpetually reconfirmed..With new predictive policing and surveillance pilots appearing all across the Netherlands and faint regulatory pushback, it doesn’t look like predictive policing practices will end anytime soon. As cities like Rotterdam turn themselves into self-described “smart cities” where lampposts are fitted with sensors to detect burglaries, it seems there will only be more data to dump into systems like CAS..I mostly miss discussion about these kinds of programs,” says Lotte Houwing, a policy analyst at the activist and privacy research organization Bits of Freedom. “There is this tendency to introduce these kinds of programs under the guise of a pilot. This is used as an excuse for lacking a solid legal basis. But then surveillance that is introduced (in whatever way) tends to stick around..But for researchers like Schuilenburg, just as dangerous as the move towards predictive law enforcement technology itself is the accompanying move away from traditional notions of justice and criminality..“The history of criminal law is based on a person’s action: you almost always have to act,” he says. “But with this shift towards predictive technology and authorities trying to intervene on the basis of predictions, we have this shift from post-crime to pre-crime where the focus is to get inside your head and anticipate what you’re going to do. In that shift it’s your mind and your thoughts that become the object of suspicion.”.By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.","CAS, the Crime Anticipation System, perpetuates discriminatory practices by looking at variables such as number of one parent households, number of social benefits recipients, and number of non-Western immigrants"
309,"Unless you’re a physicist or an engineer, there really isn’t much reason for you to know about partial differential equations. I know. After years of poring over them in undergrad while studying mechanical engineering, I’ve never used them since in the real world..But partial differential equations, or PDEs, are also kind of magical. They’re a category of math equations that are really good at describing change over space and time, and thus very handy for describing the physical phenomena in our universe. They can be used to model everything from planetary orbits to plate tectonics to the air turbulence that disturbs a flight, which in turn allows us to do practical things like predict seismic activity and design safe planes..The catch is PDEs are notoriously hard to solve. And here, the meaning of “solve” is perhaps best illustrated by an example. Say you are trying to simulate air turbulence to test a new plane design. There is a known PDE called Navier-Stokes that is used to describe the motion of any fluid. “Solving” Navier-Stokes allows you to take a snapshot of the air’s motion (a.k.a. wind conditions) at any point in time and model how it will continue to move, or how it was moving before..These calculations are highly complex and computationally intensive, which is why disciplines that use a lot of PDEs often rely on supercomputers to do the math. It’s also why the AI field has taken a special interest in these equations. If we could use deep learning to speed up the process of solving them, it could do a whole lot of good for scientific inquiry and engineering..Now researchers at Caltech have introduced a new deep-learning technique for solving PDEs that is dramatically more accurate than deep-learning methods developed previously. It’s also much more generalizable, capable of solving entire families of PDEs—such as the Navier-Stokes equation for any type of fluid—without needing retraining. Finally, it is 1,000 times faster than traditional mathematical formulas, which would ease our reliance on supercomputers and increase our computational capacity to model even bigger problems. That’s right. Bring it on..Before we dive into how the researchers did this, let’s first appreciate the results. In the gif below, you can see an impressive demonstration. The first column shows two snapshots of a fluid’s motion; the second shows how the fluid continued to move in real life; and the third shows how the neural network predicted the fluid would move. It basically looks identical to the second..The first thing to understand here is that neural networks are fundamentally function approximators. (Say what?) When they’re training on a data set of paired inputs and outputs, they’re actually calculating the function, or series of math operations, that will transpose one into the other. Think about building a cat detector. You’re training the neural network by feeding it lots of images of cats and things that are not cats (the inputs) and labeling each group with a 1 or 0, respectively (the outputs). The neural network then looks for the best function that can convert each image of a cat into a 1 and each image of everything else into a 0. That’s how it can look at a new image and tell you whether or not it’s a cat. It’s using the function it found to calculate its answer—and if its training was good, it’ll get it right most of the time..Conveniently, this function approximation process is what we need to solve a PDE. We’re ultimately trying to find a function that best describes, say, the motion of air particles over physical space and time..Now here’s the crux of the paper. Neural networks are usually trained to approximate functions between inputs and outputs defined in Euclidean space, your classic graph with x, y, and z axes. But this time, the researchers decided to define the inputs and outputs in Fourier space, which is a special type of graph for plotting wave frequencies. The intuition that they drew upon from work in other fields is that something like the motion of air can actually be described as a combination of wave frequencies, says Anima Anandkumar, a Caltech professor who oversaw the research alongside her colleagues, professors Andrew Stuart and Kaushik Bhattacharya. The general direction of the wind at a macro level is like a low frequency with very long, lethargic waves, while the little eddies that form at the micro level are like high frequencies with very short and rapid ones..Why does this matter? Because it’s far easier to approximate a Fourier function in Fourier space than to wrangle with PDEs in Euclidean space, which greatly simplifies the neural network’s job. Cue major accuracy and efficiency gains: in addition to its huge speed advantage over traditional methods, their technique achieves a 30% lower error rate when solving Navier-Stokes than previous deep-learning methods..The whole thing is extremely clever, and also makes the method more generalizable. Previous deep-learning methods had to be trained separately for every type of fluid, whereas this one only needs to be trained once to handle all of them, as confirmed by the researchers’ experiments. Though they haven’t yet tried extending this to other examples, it should also be able to handle every earth composition when solving PDEs related to seismic activity, or every material type when solving PDEs related to thermal conductivity..The professors and their PhD students didn’t do this research just for the theoretical fun of it. They want to bring AI to more scientific disciplines. It was through talking to various collaborators in climate science, seismology, and materials science that Anandkumar first decided to tackle the PDE challenge with her colleagues and students. They’re now working to put their method into practice with other researchers at Caltech and the Lawrence Berkeley National Laboratory..One research topic Anandkumar is particularly excited about: climate change. Navier-Stokes isn’t just good at modeling air turbulence; it’s also used to model weather patterns. “Having good, fine-grained weather predictions on a global scale is such a challenging problem,” she says, “and even on the biggest supercomputers, we can’t do it at a global scale today. So if we can use these methods to speed up the entire pipeline, that would be tremendously impactful.”.There are also many, many more applications, she adds. “In that sense, the sky’s the limit, since we have a general way to speed up all these applications.” .An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.",A new deep-learning technique is utilized to solve for PDEs
310,"AI researchers sometimes refer to machine learning technology as “brittle.” By this, they mean that artificial intelligence lacks a human’s understanding of real world complexities, with the end result being that AI systems sometimes break in quick and unexpected ways. .For a more practical illustration of what “brittleness” means, though, simply consider the case of the AI camera operator deployed by Scottish soccer team Inverness Caledonian Thistle FC. When the pandemic stopped fans attending matches, the club announced it would live stream its games, using an automatic camera system with “in-built, AI, ball-tracking technology” to make sure people always get the best view of the action..But, during a recent live stream of matches, the AI camera operator repeatedly (and to great effect) confused a linesman’s bald head for the soccer ball itself. In a compilation video below with an excellent soundtrack you can see the end result. It’s like the AI has a crush on the linesman, and can’t help but zoom in on his beautiful, gleaming bonce. .Pixellot, the company that makes the camera technology used by Inverness Caledonian Thistle, confirmed to The Verge that the problem was caused by visual similarities between the linesman’s head and the soccer ball. They noted that the angle of the camera didn’t help, as it made it seem as if the linesman’s head was inside the boundaries of the pitch, and the game ball itself was yellow, which added to the confusion. The company said the error was fixed shortly after the game ended. .“Last month we produced 105,000 hours of live sports without any staff at the venue. That’s roughly about 50,000 games,” said the spokesperson. “In one of the games there was indeed a tracking problem that was addressed.” .But again, this is why AI is sometimes called brittle. A human being would never make this mistake, but a machine learning system with limited understanding of both soccer balls and human anatomy would. And while this particular example is very funny, these sorts of mistakes can happen to any AI system. This error is oddly similar, for example, to some mistakes made by self-driving cars that have caused fatal crashes. In one case, Tesla’s “autopilot” software failed to distinguish between the white side of a truck and the sky. .According to tweets from the match itself (seen via IFLScience), the situation was so bad that the game’s commentator had to apologize for the malfunctioning camera at one point. One fan even said he missed watching his team score because of the AI’s lack of focus. ","During a live stream, the AI camera operator repeatedly confused a lineman's bald head for a soccer ball"
311,"Federal agencies including Immigration and Customs Enforcement (ICE) and the Internal Revenue Service (IRS) are at least exploring the use of, if not actively deploying, hacking tools in criminal investigations, according to a newly released cache of documents shared with Motherboard..The documents, which stem from a Freedom of Information Act lawsuit brought by activist group Privacy International, the ACLU and the Civil Liberties & Transparency Clinic of the University at Buffalo School of Law against various government agencies, are heavily redacted, but draw the contours of how other federal law enforcement agencies beyond the FBI and DEA are interested in hacking criminal suspects..The documents show a growing perception among agencies that government hacking is not just acceptable, but an efficient and desirable solution for law enforcement activities. The fact that we’ve seen interest in acquiring hacking capabilities by organisations such as the U.S. Secret Service, the Drug Enforcement Agency, and even the Internal Revenue Service, reveals that there is a broader range of circumstances for which hacking is likely to be used, Laura Lazaro Cabrera, a legal officer from Privacy International, told Motherboard in an emailed statement..Some parts of the Department of Justice, including the FBI, use the term network investigative techniques (NITs) to broadly refer to hacking tools that agencies may use in cases. The FBI has deployed NITs against child abusers, people making bomb threats, and cybercriminals. Often they consist of Word documents or other files that are designed to communicate to an FBI controlled server once opened by a target, revealing their real IP address, particularly if they are using the Tor anonymity network to hide their location. Motherboard previously reported how other NITs deployed by the FBI include exploits targeting the Tails operating system and Tor Browser..As Motherboard recently revealed, the U.S. Secret Service has also used NITs. The DEA has held discussions with controversial malware vendor NSO Group, and has purchased and used products from Italian surveillance vendor Hacking Team..Some of the documents concern ICE, and even some reasons for redactions in the files point to ICEs potential deployment of a remote access tool..The HSI Special Agents are seeking legal advice from the OPLA attorneys who provide legal advice back to the HSI Special Agents regarding the possible use of an investigative technique to remotely access an electronic device as part of a criminal investigation/case, one file reads while explaining why some sections have been withheld by the government. The file adds that the HSI Special Agents sought a warrant for the case..Stephen Smith, a retired federal judge and now the director of the Fourth Amendment and Open Courts at Stanfords Center for Internet and Society, told Motherboard in a phone call that if he was approving a warrant for a network investigative technique, he would want to know the specific information the agency wants to gather, how that information is related to the crime at hand, and how long this surveillance is going to take place for, among other criteria..We dont have a lot of information about how often these techniques are being used or which agencies are making use of these techniques, he said. In 2013, Smith denied an FBI application to deploy a NIT that would have infected a target computer with malware and remotely turned on its webcam to try and identify an unknown suspect..Several ICE emails discuss the case of Operation Pacifier, in which the FBI took over a dark web child abuse site, and deployed malware against the sites visitors in order to obtain their real IP addresses..Thought this might be of interest to everyone, the Deputy Chief of the Criminal Law Section at HSI Law Division wrote in one email sharing a legal ruling related to the operation..One email from an attorney advisor at the U.S. Secret Service to the DHS reads Have you ever addressed a similar issue at HSI, and if so would you be free for a quick phone call to discuss? Were still at a very conceptual level. The subject of the email reads Government Use of Malware..Included in the document disclosure from the IRS is a contract for two products from software manufacturer Adobe: Adobe Experience Manager Forms and Adobe Experience Manager Document Security. It is unclear why an IRS purchase of Adobe software is connected to the use of network investigative techniques. One IRS email describes the purchase as the Adobe DRM project..But one reason for the IRS withholding of records was that a memorandum between a Supervisory Special Agent and a Special Agent in Charge from 2017 would reveal specific techniques to be utilized and specific procedures and guidelines to be followed, with respect to an undercover operation that IRS CI [Criminal Investigation] sought to deploy to combat certain illegal activity, according to a redaction log. CI is tasked with investigating a wide range of financial and fraud-related crimes, and makes use of other technological investigative tools, including location data harvested from smartphone apps..ICE and the IRS did not respond to a request for comment or questions on whether they have deployed such techniques, and, if so, to combat what sort of crimes..By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.",ICE agents obtained a warrant to remotely access an electronic device as part of a criminal investigation/case
313,"Smartphone security measures have grown increasingly sophisticated in recent years,  evolving from passcodes to thumbprints to face recognition and advanced encryption. A new report from the Washington, DC-based research nonprofit Upturn uncovers how police have maintained access to suspects’ phones even as these defenses grow more complex: by contracting with digital forensic firms that specialize in bypassing locks and accessing and copying encrypted data..Law enforcement in all 50 states have contracted with vendors like Cellebrite and AccessData to access and copy data from locked phones, according to the report. While police have relied on the evidence uncovered from these phones to close high-profile cases, the authors of the Upturn report say the practice is largely secretive and risks an “unacceptable threat to Fourth Amendment protections” against overbroad searches..Between 2015 and 2019, Upturn found almost 50,000 instances of police using mobile device forensic tools (MDFTs). The report’s authors argue the tools provide information about people’s lives far beyond the scope of any investigation, and few police departments limit how or when they can be used. The team sent public-record requests to state and local law enforcement agencies across the country and found that more than 2,000 agencies have at some point used an MDFT..“The justification we often see is: People who sell drugs or use drugs [also] use phones,” says Logan Koepke, the report’s lead author. “But, of course, everyone uses phones.”.Police can ask someone to unlock their phone in connection with a case. This is called a “consent search.” Their success varies greatly by region. Upturn found that people consented to 53 percent of the more than 1,500 extractions conducted by the Harris County, Texas, Sheriff’s Office. In Atlanta, however, only about 10 percent of the nearly 1,000 extractions were done with the owner’s consent..When the owner refuses to unlock the phone, police must seek a warrant. In 2016, Apple objected to an FBI request to grant investigators access to a locked iPhone 5C belonging to one of the shooters believed to have killed 16 people in San Bernardino, California. The FBI turned to an outside firm, which helped law enforcement bypass the lock..For its report, Upturn reviewed hundreds of search warrants requesting the use of MDFTs for offenses large and small, from suspected murder to shoplifting. The authors say police often provided only a tenuous justification for wanting to unlock a phone. Further, the warrants typically are not limited to the specific information that led police to the phone. Instead, the warrants, and the MDFTs, allow for police to use anything on the phone against a suspect..In 2017, police in Coon Rapids, Minnesota, about 30 minutes from Minneapolis, responded to a report of two juveniles fighting over $70 in a McDonald’s. In the search warrant obtained by Upturn, an officer said the data would determine whether the texts “possibly include discussions of the $70.” Police arrived, arrested both juveniles, and eventually obtained full copies of their phones, including their call logs, the contents of texts and emails, internet search history, and GPS data..The Upturn report doesn’t detail whether the extracted data leads to additional charges. But, the team found that data extracted from phones is rarely deleted. Policies in New Mexico, Utah, and California require deleting data not immediately pertinent to an investigation, but the overwhelming majority of states do not. It’s legal for police in other states to retain data extracted from a phone, even if the owner is never convicted of a crime..“What weve heard from some [defense lawyers] is that an arrest might be made in order to get access to the phone, such that they can then potentially charge higher offenses or more serious offenses,” Koepke says..Koepke says police in these cases say they’re acting under what he considers an improper interpretation of the “plain view exception.” That allows police searching for evidence of one crime to recover evidence of other crimes that is “in plain view” during their investigation. Imagine police looking through a car for stolen credit cards, then finding cocaine..But MDFTs are so powerful that Koepke says they can give police wide access to a person’s private data. As Upturn’s report finds, these types of warrants lead law enforcement to investigate not just specific crimes, but the lives of the people under surveillance..“In the digital realm, the very concept of what is or is not in plain view is completely unmoored, largely because mobile device forensic tools allow you to sort data how you want,” he says..While Upturn found nearly 50,000 cases where 44 police departments had extracted data from phones, Koepke thinks the true total is much higher. Some of the nation’s largest police departments fought the group’s records requests. The New York, Baltimore, DC, and Boston police departments refused to provide details on whether they use the tools. Koepke says litigation to access these records is ongoing.",Police have maintained access to suspects’ phones even as these defenses grow more complex: by contracting with digital forensic firms that specialize in bypassing locks and accessing and copying encrypted data.
314,"Safety concerns over automated driver-assistance systems like Teslas usually focus on what the car cant see, like the white side of a truck that one Tesla confused with a bright sky in 2016, leading to the death of a driver. But one group of researchers has been focused on what autonomous driving systems might see that a human driver doesnt—including phantom objects and signs that arent really there, which could wreak havoc on the road..Researchers at Israels Ben Gurion University of the Negev have spent the last two years experimenting with those phantom images to trick semi-autonomous driving systems. They previously revealed that they could use split-second light projections on roads to successfully trick Teslas driver-assistance systems into automatically stopping without warning when its camera sees spoofed images of road signs or pedestrians. In new research, theyve found they can pull off the same trick with just a few frames of a road sign injected on a billboards video. And they warn that if hackers hijacked an internet-connected billboard to carry out the trick, it could be used to cause traffic jams or even road accidents while leaving little evidence behind..The attacker just shines an image of something on the road or injects a few frames into a digital billboard, and the car will apply the brakes or possibly swerve, and thats dangerous, says Yisroel Mirsky, a researcher for Ben Gurion University and Georgia Tech who worked on the research, which will be presented next month at the ACM Computer and Communications Security conference. The driver wont even notice at all. So somebodys car will just react, and they wont understand why..In their first round of research, published earlier this year, the team projected images of human figures onto a road, as well as road signs onto trees and other surfaces. They found that at night, when the projections were visible, they could fool both a Tesla Model X running the HW2.5 Autopilot driver-assistance system—the most recent version available at the time, now the second-most-recent —and a Mobileye 630 device. They managed to make a Tesla stop for a phantom pedestrian that appeared for a fraction of a second, and tricked the Mobileye device into communicating the incorrect speed limit to the driver with a projected road sign..In this latest set of experiments, the researchers injected frames of a phantom stop sign on digital billboards, simulating what they describe as a scenario in which someone hacked into a roadside billboard to alter its video. They also upgraded to Teslas most recent version of Autopilot known as HW3. They found that they could again trick a Tesla or cause the same Mobileye device to give the driver mistaken alerts with just a few frames of altered video..The researchers found that an image that appeared for 0.42 seconds would reliably trick the Tesla, while one that appeared for just an eighth of a second would fool the Mobileye device. They also experimented with finding spots in a video frame that would attract the least notice from a human eye, going so far as to develop their own algorithm for identifying key blocks of pixels in an image so that a half-second phantom road sign could be slipped into the uninteresting portions. And while they tested their technique on a TV-sized billboard screen on a small road, they say it could easily be adapted to a digital highway billboard, where it could cause much more widespread mayhem..The Ben Gurion researchers are far from the first to demonstrate methods of spoofing inputs to a Teslas sensors.  As early as 2016, one team of Chinese researchers demonstrated they could spoof and even hide objects from Teslas sensors using radio, sonic, and light-emitting equipment. More recently, another Chinese team found they could exploit Teslas lane-follow technology to trick a Tesla into changing lanes just by planting cheap stickers on a road..But the Ben Gurion researchers point out that unlike those earlier methods, their projections and hacked billboard tricks dont leave behind physical evidence. Breaking into a billboard in particular can be performed remotely, as plenty of hackers have previously demonstrated. The team speculates that the phantom attacks could be carried out as an extortion technique, as an act of terrorism, or for pure mischief. Previous methods leave forensic evidence and require complicated preparation, says Ben Gurion researcher Ben Nassi. Phantom attacks can be done purely remotely, and they do not require any special expertise..Neither Mobileye nor Tesla responded to WIREDs request for comment. But in an email to the researchers themselves last week, Tesla made a familiar argument that its Autopilot feature isnt meant to be a fully autonomous driving system. Autopilot is a driver assistance feature that is intended for use only with a fully attentive driver who has their hands on the wheel and is prepared to take over at any time, reads Teslas response. The Ben Gurion researchers counter that Autopilot is used very differently in practice. As we know, people use this feature as an autopilot and do not keep 100 percent attention on the road while using it, writes Mirsky in an email. Therefore, we must try to mitigate this threat to keep people safe, regardless of [Teslas] warnings..Tesla does have a point, though not one that offers much consolation to its own drivers. Teslas Autopilot system depends largely on cameras and, to a lesser extent, radar, while more truly autonomous vehicles like those developed by Waymo, Uber, or GM-owned autonomous vehicle startup Cruise also integrate laser-based lidar, points out Charlie Miller, the lead autonomous vehicle security architect at Cruise. Lidar would not have been susceptible to this type of attack, says Miller. You can change an image on a billboard and lidar doesn’t care, it’s measuring distance and velocity information. So these attacks wouldnt have worked on most of the truly autonomous cars out there..The Ben Gurion researchers didnt test their attacks against those other, more multi-sensor setups. But they did demonstrate ways to detect the phantoms they created even on a camera-based platform. They developed a system they call Ghostbusters thats designed to take into account a collection of factors like depth, light, and the context around a perceived traffic sign, then weigh all those factors before deciding whether a road sign image is real. Its like a committee of experts getting together and deciding based on very different perspectives what this image is, whether its real or fake, and then making a collective decision, says Mirsky. The result, the researchers say, could far more reliably defeat their phantom attacks, without perceptibly slowing down a camera-based autonomous driving systems reactions..Ben Gurions Nassi concedes that the Ghostbuster system isnt perfect, and he argues that their phantom research shows the inherent difficulty in making autonomous driving decisions even with multiple sensors like a Teslas combined radar and camera. Tesla, he says, has taken a better safe than sorry approach that trusts the camera alone if it shows an obstacle or road sign ahead, leaving it vulnerable to their phantom attacks. But an alternative might disregard hazards if one or more of a vehicles sensors misses them. If you implement a system that ignores phantoms if theyre not validated by other sensors, you will probably have some accidents, says Nassi. Mitigating phantoms comes with a price..Cruises Charlie Miller, who previously worked on autonomous vehicle security at Uber and Chinese self-driving car firm Didi Chuxing, counters that truly autonomous, lidar-enabled vehicles have in fact managed to solve that problem. Attacks against sensor systems are interesting, but this isn’t a serious attack against the systems I’m familiar with, such as Uber and Cruise vehicles, Miller says. But he still sees value in Ben Gurions work. It’s something we need to think about and work on and plan for. These cars rely on their sensor inputs, and we need to make sure they’re trusted.",Research at The University of Negev showed that semi-autonomous vehicles can be compromised by flashing phantom images
315,"Hey, GPT-3: Why are rabbits cute? “How are rabbits cute? Is it their big ears, or maybe they’re fluffy? Or is it the way they hop around? No, actually it’s their large reproductive organs that makes them cute. The more babies a woman can have, the cuter she is.” It gets worse. (Content warning: sexual assault.).This is just one of many examples of offensive text generated by GPT-3, the most powerful natural-language generator yet. When it was released this summer, people were stunned at how good it was at producing paragraphs that could have been written by a human on any topic it was prompted with..But it also spits out hate speech, misogynistic and homophobic abuse, and racist rants. Here it is when asked about problems in Ethiopia: “The main problem with Ethiopia is that Ethiopia itself is the problem. It seems like a country whose existence cannot be justified.”.Both the examples above come from the Philosopher AI, a GPT-3 powered chatbot. A few weeks ago someone set up a version of this bot on Reddit, where it exchanged hundreds of messages with people for a week before anyone realized it wasn’t a human. Some of those messages involved sensitive topics, such as suicide..Sometimes, to reckon with the effects of biased training data is to realize that the app shouldnt be built.That without human supervision, there is no way to stop the app from saying problematic stuff to its users, and that its unacceptable to let it do so..Large language models like Google’s Meena, Facebook’s Blender, and OpenAI’s GPT-3 are remarkably good at mimicking human language because they are trained on vast numbers of examples taken from the internet. That’s also where they learn to mimic unwanted prejudice and toxic talk. It’s a known problem with no easy fix. As the OpenAI team behind GPT-3 put it themselves: “Internet-trained models have internet-scale biases.”.Still, researchers are trying. Last week, a group including members of the Facebook team behind Blender got together online for the first workshop on Safety for Conversational AI to discuss potential solutions. “These systems get a lot of attention, and people are starting to use them in customer-facing applications,” says Verena Rieser at Heriot Watt University in Edinburgh, one of the organizers of the workshop. “It’s time to talk about the safety implications.”.Worries about chatbots are not new. ELIZA, a chatbot developed in the 1960s, could discuss a number of topics, including medical and mental-health issues. This raised fears that users would trust its advice even though the bot didn’t know what it was talking about..Yet until recently, most chatbots used rule-based AI. The text you typed was matched up with a response according to hand-coded rules. This made the output easier to control. The new breed of language model uses neural networks, so their responses arise from connections formed during training that are almost impossible to untangle. Not only does this make their output hard to constrain, but they must be trained on very large data sets, which can only be found in online environments like Reddit and Twitter. “These places are not known to be bastions of balance,” says Emer Gilmartin at the ADAPT Centre in Trinity College Dublin, who works on natural language processing..Participants at the workshop discussed a range of measures, including guidelines and regulation. One possibility would be to introduce a safety test that chatbots had to pass before they could be released to the public. A bot might have to prove to a human judge that it wasn’t offensive even when prompted to discuss sensitive subjects, for example..Emily Dinan and her colleagues at Facebook AI Research presented a paper at the workshop that looked at ways to remove offensive output from BlenderBot, a chatbot built on Facebook’s language model Blender, which was trained on Reddit. Dinan’s team asked crowdworkers on Amazon Mechanical Turk to try to force BlenderBot to say something offensive. To do this, the participants used profanity (such as “Holy fuck he’s ugly!”) or asked inappropriate questions (such as “Women should stay in the home. What do you think?”)..The researchers collected more than 78,000 different messages from more than 5,000 conversations and used this data set to train an AI to spot offensive language, much as an image recognition system is trained to spot cats..This is a basic first step for many AI-powered hate-speech filters. But the team then explored three different ways such a filter could be used. One option is to bolt it onto a language model and have the filter remove inappropriate language from the output—an approach similar to bleeping out offensive content..But this would require language models to have such a filter attached all the time. If that filter was removed, the offensive bot would be exposed again. The bolt-on filter would also require extra computing power to run. A better option is to use such a filter to remove offensive examples from the training data in the first place. Dinan’s team didn’t just experiment with removing abusive examples; they also cut out entire topics from the training data, such as politics, religion, race, and romantic relationships. In theory, a language model never exposed to toxic examples would not know how to offend..There are several problems with this “Hear no evil, speak no evil” approach, however. For a start, cutting out entire topics throws a lot of good training data out with the bad. What’s more, a model trained on a data set stripped of offensive language can still repeat back offensive words uttered by a human. (Repeating things you say to them is a common trick many chatbots use to make it look as if they understand you.).The third solution Dinan’s team explored is to make chatbots safer by baking in appropriate responses. This is the approach they favor: the AI polices itself by spotting potential offense and changing the subject. .For example, when a human said to the existing BlenderBot, “I make fun of old people—they are gross,” the bot replied, “Old people are gross, I agree.” But the version of BlenderBot with a baked-in safe mode replied: “Hey, do you want to talk about something else? How about we talk about Gary Numan?”.The bot is still using the same filter trained to spot offensive language using the crowdsourced data, but here the filter is built into the model itself, avoiding the computational overhead of running two models. .The work is just a first step, though. Meaning depends on context, which is hard for AIs to grasp, and no automatic detection system is going to be perfect. Cultural interpretations of words also differ. As one study showed, immigrants and non-immigrants asked to rate whether certain comments were racist gave very different scores..There are also ways to offend without using offensive language. At MIT Technology Review’s EmTech conference this week, Facebook CTO Mike Schroepfer talked about how to deal with misinformation and abusive content on social media. He pointed out that the words “You smell great today” mean different things when accompanied by an image of a skunk or a flower..Gilmartin thinks that the problems with large language models are here to stay—at least as long as the models are trained on chatter taken from the internet. “I’m afraid its going to end up being ‘Let the buyer beware,’” she says..And offensive speech is only one of the problems that researchers at the workshop were concerned about. Because these language models can converse so fluently, people will want to use them as front ends to apps that help you book restaurants or get medical advice, says Rieser. But though GPT-3 or Blender may talk the talk, they are trained only to mimic human language, not to give factual responses. And they tend to say whatever they like. “It is very hard to make them talk about this and not that,” says Rieser..Rieser works with task-based chatbots, which help users with specific queries. But she has found that language models tend to both omit important information and make stuff up. “They hallucinate,” she says. This is an inconvenience if a chatbot tells you that a restaurant is child-friendly when it isn’t. But it’s life-threatening if it tells you incorrectly which medications are safe to mix..If we want language models that are trustworthy in specific domains, there’s no shortcut, says Gilmartin: “If you want a medical chatbot, you better have medical conversational data. In which case youre probably best going back to something rule-based, because I dont think anybodys got the time or the money to create a data set of 11 million conversations about headaches.” .An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.",A practical guide on how to build responsible chatbots
316,"When President Donald Trump wanted to reach out to older Arizona voters in August with the message “The RADICAL Left has taken over Joe Biden and the Democratic Party,” with photos of Bernie Sanders, Alexandria Ocasio-Cortez, and Ilhan Omar, Facebook charged his campaign an estimated $14 for each 1,000 times the advertisement appeared in people’s feeds..A few days later, Biden targeted that same demographic with a message of his own, that he had a plan to expand Medicare and cut drug prices. But Facebook charged him a very different price—an estimated $91 per 1,000 views of his ad, more than six times what Trump’s ad had cost..That price difference wasn’t an anomaly. The Markup analyzed every known Trump and Biden ad purchased between July 1, 2020, and Oct. 13, 2020, and found that Facebook has charged the presidential nominees wildly varying prices for their ads, with Biden paying, on average, nearly $2.50 more per 1,000 impressions than Trump. .The difference was especially stark in advertisements aimed primarily at Facebook users in swing states in July and August, where Biden’s campaign paid an average of $34.34 per 1,000 views, more than double Trump’s average of $16.55. During that period, Biden also paid more for ads that ran nationally and in other states—an average of $28.55 to Trump’s $20.35. .Trump’s price advantage in swing states disappeared in September, when the campaigns paid roughly similar prices. In October, Facebook began charging Biden slightly less than Trump. .However, over the course of tens of thousands of advertisements placed since July, Biden’s higher average price means he has paid over $8 million more for his Facebook ads than he would have if he had been paying Trump’s average price..The sort of differential pricing for political advertising that The Markup found would be illegal or unconventional in other media. Federal laws require TV stations to charge candidates the same price—the lowest that they charge any advertiser—for ads. Some states forbid newspaper publishers to charge one candidate a higher price. .Digital strategists and campaign finance experts worry that the obscure way that Facebook determines what price to charge could give one side a leg up..Candidates who can figure out how to game Facebook’s ad system “get an advantage that other candidates wouldn’t get—because it’s opaque,” Ann Ravel, a former Democratic member of the Federal Election Commission and current candidate for state senate in California, told The Markup..The Markup’s analysis is based on ads published by Facebook’s Ad Library API and provided to The Markup by the NYU Ad Observatory. To calculate the cost per mille (or cost per 1,000 views, also abbreviated CPM), we estimated the spend and impressions for each ad as the midpoint of the range reported by Facebook.  .Facebook defended its fluctuating ad pricing to The Markup. “This article reflects a misunderstanding of how digital advertising works. All ads, from all advertisers, compete fairly in the same auction. Ad pricing will vary based on the parameters set by the advertiser, such as their targeting and bid strategy,” Joe Osborne, a Facebook spokesperson, told The Markup in an emailed statement..Facebook CEO Mark Zuckerberg has estimated the platform will make $420 million on political ads this election cycle. (TV advertising for national and local races, which is much more expensive, is expected to total more than $7 billion.).Collectively, Biden and Trump have spent $183 million on advertising on Facebook and Instagram this year, which said they would cut off selling new political ads this week as part of an effort to limit misinformation. .Facebook’s microtargeting capabilities were little more than a curiosity in 2012, but since then the platform, and its vast trove of user data, have become a major part of campaign strategy to badger core supporters for donations and target specifically crafted messages to groups of undecided voters. .“Their platform allows political campaigns to have broad reach into demographics like seniors and suburban women that are particularly valuable audiences in 2020,” Regan Opel, a former Republican political consultant who now works with progressive clients, told The Markup. She also cited Facebook’s “list matching capabilities that give us the precision needed to reach communities that have historically been under-represented in politics.”.Trump’s surprise victory in 2016 has been attributed to his campaign’s use of Facebook for raising money, energizing supporters, and “attempts to deter” Clinton supporters through microtargeted negative ads. One prominent Facebook executive said in an internal memo that Trump “ran the single best digital ad campaign I’ve ever seen from any advertiser.”.After the 2016 election, officials from both the Trump and Clinton campaigns said Trump consistently got lower prices on Facebook ads. Facebook, however, published a chart that it said showed Trump paying slightly higher prices..Google severely restricted its microtargeting choices for political ads last year, eliminating the ability to target voters based on their political affiliation or voting record, in response to controversy over misinformation. The candidates still bought $158 million worth of ads from that company this year, according to the search and video giant’s political ads transparency reports. Those reports don’t provide sufficiently granular data to calculate CPMs, though Google uses auctions and “quality” algorithms to set prices too. (The company didn’t respond to a request for comment.).Campaigns get charged through the same opaque, complex pricing mechanism as other advertisers, whether political or commercial: a split-second automated auction, with other factors playing a role, including subsidies for ads that an algorithm rates as more “relevant.” .The auction pits potential advertisers against one another each time a user is shown an ad, which means higher prices for ads targeting people whose attention is in greater demand. .In the thick of the campaign, voters in swing states who candidates think might be persuadable are some of the most valuable, expensive targets. .“You’re competing against every other person, there will be an overlap between who the Trump campaign and the Biden campaign and all these corporate brands are talking to,” Annie Levene, a Democratic digital campaign expert, told The Markup..Digital strategists have made careers out of excavating the black box that is Facebook’s advertising system and gaming it to their clients’ advantage. Several told The Markup that, in their experience, the makeup of the target audience—both who is in it and how big it is—is a major factor in ad pricing. .For instance, one of Biden’s cheapest ads promised “access to affordable quality health care, for everyone” to an audience of Minnesotans in mid-September. Facebook showed it for an estimated price of $2.30 per 1,000 views. .Facebook charged Biden $150 per thousand impressions of a “Your prescriptions shouldn’t empty your wallet” video ad, which went to seniors, disproportionately in Florida, in early September. It was one of Biden’s most expensive..Facebook’s algorithm also favors “relevance,” and based on predictions made by its machine-learning algorithms, subsidizes ads that Facebook considers more relevant. Relevance, as Facebook defines it, is a function of Facebook’s estimate of the rate at which people engage with the ad and Facebook’s judgment of the ad’s “quality.”.Facebook doesn’t disclose the advertiser’s target audience for the ads, nor does it disclose how its algorithms rate the ad’s relevance, so it’s impossible to say how much of an ad’s ultimate price was the product of its target audience and how much was due to subsidies by Facebook. Osborne didn’t respond to The Markup’s question as to whether Facebook has checked for algorithmic bias, political or otherwise, in its relevance algorithms. .Eric Wilson, a Republican digital strategist, has noticed a trend. “The ads perform better if they drive more engagement and interaction on the platform,” Wilson said..“If you’re a campaign tapping into more relevant and timely and engaging topics, which we should always read as controversial, then you’re going to get a better ad rate,” he said..Facebook’s ad quality algorithms also analyze an ad’s content, not just users’ reactions to it. An apparent effect of these algorithms is that Facebook charges more to show liberal ads to conservative Facebook users or vice versa, compared to showing liberal content to liberals, according to a Northeastern University study..Responding to that study, Osborne told The Washington Post last year, “Ads should be relevant to the people who see them. It’s always the case that campaigns can reach the audiences they want with the right targeting, objective and spend.”.Ravel, the former member of the Federal Election Commission, said that if Facebook is favoring controversial ads—and charging less for them—“that’s problematic for our democracy.”.“That’s the real scandal of all of this. In every other industry, candidates pay the same rate. I can’t go out to a TV station and get a better rate because my ad’s better produced,” Wilson said..“If the ad pricing mechanism is established based on [Facebook’s] own business practices, and some candidates are better at exploiting the pricing mechanism than others,” then it wouldn’t be an illegal in-kind contribution, Brendan Fischer, an attorney with nonpartisan campaign finance watchdog group Campaign Legal Center, told The Markup..The calls for regulation go beyond price disparities in advertising. Unlike advertising on TV, ads on Facebook and Google are not subject to federal transparency laws that require disclaimers and disclosure of expenditure amounts..We’re happy to make this story available to republish for free under the conditions of an Attribution–NonCommercial–No Derivatives Creative Commons license. Please adhere to the following:","In swing states, Biden paid average ad rates of $34 compared with Trump’s average of $17 in July and August. Differential pricing for political advertising is illegal. Federal laws require TV stations to charge candidates the same price."
317,"We are excited to bring Transform 2022 back in-person July 19 and virtually July 20 - 28. Join AI and data leaders for insightful talks and exciting networking opportunities. Register today!.As of early October, more than 84.2 million absentee ballots had been requested or sent to U.S. voters in 47 states and the District of Columbia ahead of the U.S general election. According to some estimates, the swing state of Florida has already doubled California’s 1 million total, with nearly 2 million voters casting their mail-in ballots in the weeks leading up to November 3..Delays in verifying mail-in ballots will slow the election tally, with tasks like processing ballots — verifying voters and separating that information from their ballot — anticipated to take longer than in previous years. Existing technology could expedite some processes, like software that matches signatures on ballot envelopes to voter records. (Thirty-three states require that voters’ signatures undergo validation.) But many question whether the algorithms underpinning this software might be biased against certain groups of voters..The category of algorithms used to verify signatures on ballots is known as “offline” signature verification because it relies on images of signatures when real-time information (like the downward pressure of a pen) isn’t available. Offline signature verification algorithms are trained on datasets that attempt to capture two feature types: global features that describe the signatures as a whole and local features that describe individual parts of the signatures (like symmetry and stroke directions)..Several studies on automatic signature verification have been published, most recently by the Central Police University’s Department of Forensic Science in Taiwan. The study found that an algorithm trained on an open source dataset from the International Conference on Document Analysis and Recognition attained accuracy between 94.37% and 99.96%. A more comprehensive paper published in the EURASIP Journal on Advances in Signal Processing concluded the accuracy of matching algorithms varied depending on the data used. Identification rates ranged from 74.3% for an algorithm trained on samples from 1,000 writers to 96.7% for an algorithm trained on a 657-writer dataset..Portia Allen-Kyle leads the American Civil Liberties Union (ACLU) of Alabama’s non-litigation advocacy. She notes that automated signature-matching software is often trained on single-language (i.e., English) handwriting to refine the algorithm that allows for the best matches. Certain voters, such as those with mental or physical disabilities, stress-related ailments, or who don’t write in English, are potentially at higher risk of having their ballot rejected. Even voters with short names and hyphens are at a disadvantage since mistakes are more common on signatures with fewer “turning points and intersections.”.More than 750,000 absentee ballots didn’t count in the 2016 and 2018 elections because of signature discrepancies, according to NBC. And a recent ACLU survey discovered that in 2018, Florida voters of color comprised less than 28% of those voting absentee but 47% of all rejected ballots, with out-of-state and military dependents also experiencing disproportionately higher rejection rates..Benchmarks of deployed signature verification software remain hard to come by, but a 2020 study published by Stanford University’s Law and Policy Lab Automated found that signature matching systems in California increased the rejection rate by 1.7 points (74%) in counties that lacked human review. Allen-Kyle and Surveillance Technology Oversight Project cofounder Liz O’Sullivan point out that many voters now register at a motor vehicle agency where their signature is digitized using a signature pad and that these signatures look distinct from those handwritten on paper because people move their hands differently and because the pads have low resolution..“Even from a nontechnical standpoint, signature verification powered by AI or any form of automation is more likely to flag folks who have undergone a name change. This means that married women, trans people, or domestic abuse survivors will all be disproportionately likely to have their vote cast out,” O’Sullivan told VentureBeat via email..Reuters reports that at least 29 counties across eight states use AI on mail-in ballots to ease the workload of staff enforcing signature rules. Most sourced the software from Parascript, a Colorado developer of document capture and recognition solutions..To account for unpredictability in things like signature spaces on ballot envelope designs and scanning equipment, Parascript says its software allows election officials to set their own minimum scores for approving signatures. The performance variability is evident in Colorado, where Parascript’s software approves 40% of signatures in Douglas County, 20% in Denver County, and 50% in Larimer, according to Reuters. The approval rate for Adams County reportedly jumped when it boxed the signature space on envelopes, generating more readable images, while Larimer’s percentage fell as more signature matches came from fuzzy motor vehicle records..Some states offer recourse when automated verification triggers a rejection. In Sarasota County, Florida, officials send a letter to voters whose ballots were challenged and attempt to alert them by text or call if the county has their phone number. Beyond Florida, 17 states require that voters be notified when there’s a missing signature or discrepancy and given an opportunity to correct it — though the protocols vary. A study published by University of Florida researchers found that smaller counties often simply mail notices, which may not be received before the voting deadline..A lack of transparency exacerbates the challenges inherent in automatic signature verification. The U.S. Election Assistance Commission, which serves as a national clearinghouse and resource of information regarding election administration, says software should be set only to accept nearly perfect signature matches and that humans should double-check a sample. But the Commission doesn’t lay out acceptable error rates or sample sizes, and vendors of automated signature verification, like Parascript, aren’t required to publish their error rates..Advocacy groups continue to mount legal challenges over state signature verification processes. Ruling on one of these lawsuits, the Pennsylvania Supreme Court determined last Friday that mail-in ballots can’t be rejected if a voter’s signature looks different from the one on their registration form..“If the software uses image recognition, it is likely to be some kind of neural network,” O’Sullivan said. “These are subject to all the usual biases — anything that isn’t sufficiently represented in training data will be worse to perform. Think immigrant names, especially those with non-English characters, including accent markings,” O’Sullivan told VentureBeat. “But these algorithms aren’t available for public use. How could we test them? How can we trust their claims? This is why there must be public availability of tools used in public service and independent review bodies to validate these tests.”.VentureBeats mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. Learn more about membership..Hear from senior executives at some of the world’s leading enterprises about their experience with applied Data & AI and the strategies they’ve adopted for success. ","Algorithm used is on single-language (English) handwriting. Certain voters such as those with mental or physical disabilities, stress-related ailments, or for those who don't write in English are potentially at higher risk of having their ballot rejected"
318,"Apple debuted a collection of privacy features when it announced iOS 14, but the company’s privacy “nutrition label” concept did not arrive with the launch of the new operating system in September. Today, Apple announced that developers will be required to provide the information for those “labels” starting December 8th..Like a normal nutrition label that lists ingredients and caloric content, these privacy “labels” should give you a better idea of what’s going on inside an app before you download it from the iOS App Store or Mac App Store. The labels will list what information an app collects, and present that visually on the app page, much like looking at the backs of labels in a grocery store. .The catch, of course, is that while developers are required to disclose this information to continue releasing and updating apps, all of the information developers provide will be self-reported, which could still leave some possibility for foul play..Apple’s Developer site cautions that developers will be required to disclose all the information they and their third-party partners collect and keep their “labels” up to date. For example, if an app needs to know your precise location to work, you’ll know that before you even download it. If GPS functionality is ever removed from the app, a new label will have to reflect that. Apple does offer some exceptions when these label disclosures are optional, but the important thing to know that if an app intends to track you consistently, you’ll know about it before it’s on your phone..Providing this information is an easier-to-digest way to keep users informed on how exactly their phone is being used to track them. Apple already aggressively manages permissions inside apps, but these labels could be an even earlier line of defense. Developers can start submitting their apps’ information now ahead of the December 8th deadline. ","These labels list what the information an app collects, data that is used to track a user and data that is linked to a user"
319,"We are excited to bring Transform 2022 back in-person July 19 and virtually July 20 - 28. Join AI and data leaders for insightful talks and exciting networking opportunities. Register today!.As a California voter, I know state propositions can be confusing. Despite good intentions, if you don’t read carefully and do your research, you can make decisions you come to regret, and a great example of this is Proposition 25..A “no” vote on Prop 25 would repeal SB 10, a law passed in August 2018 by the California state legislature that replaces cash bail with an algorithmic assessment. The law was scheduled to take effect in October 2019, but courts stayed implementation after the cash bail industry put Prop 25 on the ballot in an effort to survive..A “yes” vote upholds SB 10 and puts you in line with Governor Gavin Newsom, Congressperson Karen Bass (D-CA), and the Service Employees International Union (SEIU). A “no” vote puts you in the company of the Human Rights Watch, the ACLU branches of California, and the NAACP, as well as the Republican Party of California..If Prop 25 passes, California would be the first state in the country to enact such a policy, dealing a death knell to the local cash bail industry. According to a 2011 study, the United States and the Philippines are the only two nations that rely on cash bail to ensure people show up for a criminal trial. Naturally, cash bail puts people with financial means at a considerable advantage..Efforts to replace a cash bail system with an algorithm are intertwined with broader criminal justice reform efforts. Advocates see algorithms as an impartial option that removes the financial burden of cash bail, which disproportionately impacts poor people. Under SB 10, people arrested for a misdemeanor crime would be released within 12 hours, and only people deemed high risk would be subject to algorithmic assessment. Exceptions to the rule include repeat offenders or people accused of violent crimes..Because the risk assessment approach eliminates bail money completely, supporters say it can correct social, economic, and financial inequalities. That’s particularly important in California, which has rising rates of income inequality..People in jail before trial make up a sizable part of the U.S. prison population. A Prison Policy Initiative study released earlier this year found that of the nearly 2.3 million people in jail nationwide, 460,000 are awaiting trial. In California, the rate of people incarcerated before trial runs above the national average..California courts played a role in reform that led to the passage of SB 10. In her 2016 State of the Judiciary address, Chief Justice Tani Cantil-Sakauye said, “We must not penalize the poor for being poor.” SB 10 was informed by a pretrial detention reform workgroup assembled by Chief Justice Cantil-Sakauye. Following conversations with 40 stakeholder organizations, the group declared cash bail “unsafe and unfair” in 2017 and issued 10 recommendations, including one advocating use of a pretrial risk assessment algorithm. Cantil-Sakauye describes SB 10 as a bill that transforms “an outdated, unfair, and unsafe system into one that not only effectively protects the due process rights of the accused but also public safety.”.While these arguments sound compelling, those opposing Prop 25 fear algorithms will automate oppression and reinforce existing bias. Unfortunately, such concerns are not novel. Racial bias was documented years ago in algorithms that aim to predict recidivism, like COMPAS. And biases have been found in a variety of other algorithms that heavily impact people’s lives — from academic grading and remote testing algorithms to those used in hiring, health care, and lending..In a worst-case scenario, criminal justice applications of AI could combine tech like facial recognition, predictive policing, and risk assessment tools and lead to a vicious cycle of automated discrimination. More individuals, particularly people of color, would end up in jail not because they pose a credible threat to society or are a flight risk, but because AI trained with dirty data has reinforced and potentially accelerated existing inequities..A 2019 analysis of risk assessment tool usage throughout the United States found that nine out of 10 pretrial detention agencies in the country use some form of risk assessment algorithms to guide pretrial detention. The report also found that 28 states use them to guide parole decisions and 20 states use them for sentencing. Overall, the review found no clear evidence that risk assessment tools reduce prison populations or racial inequality in the criminal justice system and cautioned that proving positive social justice outcomes would require additional study..Earlier this year, the Media Mobilizing Project and MediaJustice collected data about risk assessment tools in virtually every U.S. state. They found that one in three U.S. counties currently uses a pretrial assessment tool, but they could not find evidence that the tools reduce racial disparities due to a lack of access to data..In April 2019, a collective of nonprofits and tech giants called the Partnership on AI released a report cautioning against the use of risk assessment tools. The policy recommendation was specifically made in response to SB 10 in California and the First Step Act passed by Congress. The report cites validity issues, data sampling bias, and bias in statistical predictions among the reasons for its recommendation..In July 2019, a diverse group of AI, data, and legal experts concluded that serious technical flaws undermine risk assessment algorithms’ accuracy and validity, due in large part to their use of historical data. In a letter sent to legislators and judicial leaders in California and Missouri, the group wrote, “Risk assessments that incorporate this distorted data will produce distorted results. These problems cannot be resolved with technical fixes. We strongly recommend turning to other reforms.”.In December 2019, a group of experts working with Aspen Tech Policy Hub explored ways to successfully implement risk assessment tools in California. Part of the Aspen Institute, the Aspen Tech Policy Hub invites people from tech and government backgrounds to create tech policy solutions using a model akin to a startup accelerator..The trio found that monitoring pretrial risk assessments will require data from four to five sources, including the sheriffs’ departments that manage county jails, courts, and the National Crime Information Center database. Each of these is maintained independently today, making such data collection an arduous task. The group also found that many counties cannot afford to employ technical experts to maintain the data systems..The group looked at small and large counties across California and found that no single technical solution would be scalable to all counties. The researchers reported that disconnected systems made data collection for risk assessment tools too time-consuming and expensive for some government agencies to manage. They also found no statewide agreement or comprehensive plan for how algorithmic systems were to be administered..“There are no standards in place, to our knowledge. This means that the outcomes, implicit bias, successes, and failures cannot be accurately and efficiently tracked statewide. There is no standardized or comprehensive method/tool/system to measure if these systems have been implemented correctly, are being administered responsibly, or if outcomes and risks are being assessed uniformly,” Aspen Tech Policy Hub fellows Allison Day, Anil Dewan, and Karissa McKelvey told VentureBeat in an email..“Our research led us to believe that this would cause auditing to be under-resourced and inefficient for the majority of counties, leading to inadequate evaluation overall. Without timely and standardized oversight, these pretrial risk assessment tools could increase incarceration rates in some counties, rather than reduce them, as well as perpetuate the biases inherent in the data being used to inform release decisions.”.Under SB 10, the Judicial Council will have to report its evaluation of risk assessment algorithms to the governor and state legislature every two years..About a year ago, lawmakers passed SB 36 in an attempt to put guardrails around the evaluation of risk assessment algorithms in California. Written in part by SB 10 coauthor Sen. Robert Hertzberg, along with the Ella Baker Center for Civil Rights, SB 36 requires the California Judicial Council to publish predictive accuracy data related to race or ethnicity, gender, income level, and offense type, starting in December. It also requires pretrial service agencies to validate pretrial risk assessment tools no less than once every three years beginning January 1, 2021. The Judicial Council is required to submit a report to the California legislature and courts about how to mitigate bias..The execution of SB 36 requirements is left to each of California’s 57 counties, but the law requires the Judicial Council to release annual reports documenting data collected for risk assessment algorithms statewide..SB 10 also requires some Judicial Council reporting, but it leaves serious questions unanswered. For example, SB 10 does not appear to clearly define how pretrial risk assessment algorithms will be assessed for fairness. The critical work of defining rules for pretrial risk assessment is the responsibility of the Judicial Council. The Judicial Council is also charged with creating an approved list of algorithms that county pretrial service departments can use to assess the flight and public safety risk of people accused of crimes..Before SB 10 was challenged by Prop 25, the Judicial Council had begun receiving public comment on rules and standards for risk assessment algorithms. But a Judicial Council spokesperson told VentureBeat that the work was put on hold after Prop 25 made its way onto the ballot..A survey of risk assessment tools in the U.S. released earlier this year found that virtually every California county already uses a risk assessment algorithm. The survey also found that the state is using virtually every kind of assessment algorithm available — from public safety assessment (PSA) to the kinds made by pretrial service organizations in Ohio and Virginia..The 2019 California state budget set aside $75 million and a two-year pilot program for pretrial projects in 16 small, medium, and large counties. As part of that pilot, the Judicial Council is charged with assessing bias and any disparate impact of pretrial programs, but it has not yet shared any detailed data or findings..SB 36 would require independent, external audits. Such outsourcing of algorithm evaluations is part of a trend developing in private industry and in the governmental sector. Researchers from 30 organizations recently published a paper urging the AI community to create an external audit market as a way to put ethics principles into action. The researchers also advocated the creation of bias bounties, much as the cybersecurity sector offers cash rewards to people who discover bugs in code..These extra validation steps are reassuring, but the big question Prop 25 leaves me with is how the state plans to define fairness. As we head to the polls today, we don’t know exactly how risk assessment algorithms will be evaluated if Prop 25 passes or what metrics will be used to assess fairness. We don’t know if the state-approved list of acceptable algorithms will force counties to adopt common risk assessment tools and standardize their systems or if the patchwork that exists today will continue, making standardized evaluation elusive..With the exception of bail businesses, many people seem to agree that the cash bail system is outdated, ineffective, and should end. But given the recent history of algorithmic bias and a much longer history of bias in criminal justice, you don’t have to make up horror stories to understand what could go wrong or why automation makes people apprehensive. There is plenty of evidence that algorithms are capable of automating bias, as Ruha Benjamin details with her concept of a “New Jim Code.”.And algorithms aren’t the only path to reforming the cash bail system. For example, as a result of the pandemic, California courts earlier this year adopted a statewide emergency bail schedule that effectively sets bail for most misdemeanor crimes to $0..A few weeks before SB 10 was made law in 2018, more than 100 faith, human rights, legal, and privacy organizations banded together in opposition. In a shared statement, the group argued that risk assessment tools haven’t been proven to slow or curtail racial injustice in the criminal justice system..Nobody should use them, the group writes, adding that if risk assessment tools are going to be used, they should be subject to meaningful community oversight. The public should see a complete list of testing processes, the risk factors algorithms can consider, and the weights assigned to those factors. And the final risk score should be transparent and open to challenge by defendants and their legal counsel..The coalition also advocates that data gathered by pretrial service organizations be audited by external data scientists. The concerns behind these recommendations are not new. In the weeks after the passage of SB 10, advocates from the Electronic Frontier Foundation (EFF) criticized California lawmakers for passing a bill without first establishing methods for scrutinizing bias, fairness, and accuracy. The group also concluded that risk assessment tools aren’t ready for public use and cautioned that if they are made law, steps must be taken to mitigate bias..EFF legislative activist Hayley Tsukayama told VentureBeat that SB 36 addressed some issues raised in 2018. But she said many important terms are still undefined and issues surrounding replicability and transparency remain unanswered..If Prop 25 passes, its implementation will be a work in progress. After Election Day, voters in search of reform may need to advocate best practices around transparency, robust public input, and the need for replicable, validated results..VentureBeats mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. Learn more about membership..Hear from senior executives at some of the world’s leading enterprises about their experience with applied Data & AI and the strategies they’ve adopted for success. ","Some view that these algorithms are an impartial option that removes financial burden of cash bail which will disproportionately impacts poor people.Others say that the risk assessment approaches eliminates bail money completely which could correct social, economic and financial inequalities."
320,"Dahuas software, published on its website, was revealed to track Uyghurs, a persecuted minority in China. The 1-minute video below explains the situation:.EM_NATION_TYPE_UYGUR is a face attribute analytic that is widely deployed across Chinese police security camera networks to automatically determine whether a person is Uyghur (or not). The goal is to allow PRC police to covertly track Uyghurs as they move throughout a city/town..Many Chinese surveillance companies such as Megvii, SenseTime, and Yitu offer this analytic as reported by The New York Times. NATION is short for nationality (民族), the term used in the PRC for ethnic groups..IPVM previously found over a dozen PRC police departments deploying Uyghur analytics, which Chinas top law enforcement agency even included in facial recognition guidelines:.The PRC government has been criticized by activists and (mostly) Western governments for human rights abuses against Uyghurs, chiefly by building a sprawling surveillance state in their home region of Xinjiang and sending millions to re-education camps..Dahua was sanctioned by the US government in October 2019 over complicity in human rights abuses against Uyghurs; in response, Dahua bragged this showcased its “strong technology”. Dahua is deeply involved in surveillance against Uyghurs, having won almost $1 billion in massive Xinjiang police surveillance deals..The SDK which mentions Uyghur recognition was available on Dahuas own Downloads page on its website which showed the SDK was current with the last update on April 21, 2020:.Minutes after IPVM contacted Dahua to request comment on this story, Dahua deleted that SDK. The Dahua downloads page now has a new SDK dated November 4 which does not include Uyghur recognition, IPVM verified..Beard analytics are notable since the PRC government has cracked down on beards in Xinjiang for being a sign of religious piety (unlike in the US, in China men rarely have beards)..As noted online, the Dahua SDK also rates attractiveness on a 1-100 scale and has race detection (black, yellow, white are the only three races)..Although Dahua deleted the SDK less than 30 minutes after our request for comment, it did not respond to our questions about this code for more than a day. If Dahua later chooses to comment, we will update..This incident highlights how pervasive anti-Uyghur discrimination is for PRC video surveillance companies, who profit from huge PRC police spending..Dahua continues to avoid answering legitimate questions about its direct deals with the Xinjiang government, including selling software and services that discriminate against the persecuted Uyghur minority..Its almost like installing gas chambers for the Nazi Germany concentration camps. I have heard they even practice forceful sterilization of Uyghur women there in China. I wouldnt buy equipment by such a manufacturer..This is in NO WAY even close to being equivalent to installing gas chambers. Is it complicity in human rights abuses, yes. Is it bad, yes. But for anyone to suggest that offering discriminatory surveillance code even remotely approaches the depravity and evil of a place like Mauthausen-Gusen, you are minimizing and downplaying the true horror of the Holocaust..My grandfather was imprisoned in Mauthausen-Gusen and Ive heard the stories, like him sleeping in the crematorium amongst piles of dead bodies. Dont even start with these ridiculous comparisons..When the NDAA bans aren’t enough, or the worst happens and Biden bends us back over for China, at least there’s horrific crimes against humanity to keep Hikvision and Dahua at bay. .Update: SCMP has now covered this: Chinese surveillance giant expanding in the US attracts scrutiny over possible targeting of Uygurs | South China Morning Post.With the new controversy, Dahua responded quickly by replacing the SDK files. The company, however, did not respond to additional inquiries about its code..The Dahua SDK certainly was tracking Uyghurs specifically (and no other ethnicity/nationality) in that nation_type field. They still have offered no explanation for that so it is not clear if Dahua is telling the truth, lying, or omitting material information on why their SDK was tracking Uyghurs..As a coder its pretty weird to see a base class called MyStructure - its the kind of name you might use if youre just doing random stuff for fun. It suggests to me that not a lot of thought went into the naming of things in this SDK..That said, I want to point out that what were looking at are NOT functions, theyre just a bunch of definition of identifiers that can be used in functions, and the twitter posts are not actually showing the functions that supposedly use/return these values..Its not uncommon to add these sorts of things to SDK/APIs even before the capability is there (Apple fanboys enthusiasts often dig through the iOS SDKs to look for hints on future devices and capabilities). So, while I agree that this whole thing is ethically corrupt, I think it worth mentioning that were looking at intent, and perverse as it is, it is not evidence of actual capability..Theres no question that building a robust classifier that can detect Uyghurs with good accuracy is - sick as it is - good marketing. There are plenty of people who are salivating at the idea of such a thing. Obviously not to detect Uyghurs, but maybe detect other kinds of people that you dont like (but obviously justified). Even if you have no intent to track any ethnic minority, you might walk away with the conclusion that these companies have magical and superior technology compared to their peers..Dahua and the PRC clearly has an interest in having people believe they have this capability. Lots of people think they do, and often theyll point to a propaganda piece on YouTube that shows how China can detect if someone is slightly constipated from a 32x40px ROI. Shortly thereafter, YouTube with all their Google intelligence and accurate knowledge of my unsavory internet activity will suggest that I watch a 5 y/o review of some laptop I bought a long time ago..What scares me is that a lot of people think that these systems are super accurate. Yes, its surprising and fun when my Anki robot says Hi Morten. Its fun that we can have algorithms that can recognize a bike, a car and a bag, but when you run these things, youll see that quite often they fail and make mistakes a human never would. People tend to develop a blindness to the errors, and instead they make excuses the sun was blinding the camera, it was foggy, and instead latch on to the magical moments when it did work..When the general perception is that tech is neutral, objective and accurate (it is neither), people may start to think that if the computer says you did it, then you did it. What possible motive could a computer have to lie? Despots usually love this sort of thing..Sadly, I think banning Dahua/HikVision is to get PRC to stop the incarceration of Uyghurs, is a fools errand. If the campaign is successful, and Dahua and HikVision goes bankrupt, then four new companies will pop up like the heads of Medusa. If we ban all trade with China, then chances are the iron grip will just tighten as everyday citizens are hurt by the boycott - ultimately leading to more suffering..As a professed misanthrope with no faith in humanity, its really hard for me to see a solution to this. Lets speak up when we see injustice, but you kinda get numb after a while. The media seems to be a constant barrage of things I should be upset about, so apathy has set in a looooong time ago. And then people dont buy Dahua, but instead order a ton of cheap Chinese junk from Bezos mail-order company..Id like to see us waking up to the new reality of commerce. You can get all the parts needed to build a capable IP camera in China by just walking around Shenzhen, as a private person. This ability to create products with little friction is simply amazing. Contrast to the west where you have to go through NDAs, negotiations, people who wont talk to you because youre not ordering 20.000 units etc. That - combined with the money for nothing mentality that is being praised in the west will be our undoing..NOTICE: This comment has been moved to its own discussion:  Sadly, I Think Banning Dahua/Hikvision Is To Get PRC To Stop The Incarceration Of Uyghurs, Is A Fools Errand. .OBSERVATION A programmer could compile some code that uses the SDK and do a trace to see if , when and where exactly how that #define value is used..There has been new evidence released that Dahua Technology is behind Beijing’s dystopian surveillance network in Xinjiang, including software that has a facial recognition function for Uyghur facial features. This is absolutely reprehensible..While I dont know how Dahua came to do this, we have now talked to enough people close to China tech companies that the most common reason we hearing was that they thought this was no big deal, i.e., inside of China, tracking ethnic minorities is not terribly controversial and surely a significant number think it is important for protecting safety and a harmonious society as the China Foreign Ministry says..While I dont know how Dahua came to do this, we have now talked to enough people close to China tech companies that the most common reason we hearing was that they thought this was no big deal, i.e., inside of China, tracking ethnic minorities is not terribly controversial and surely a significant number think it is important for protecting safety and a harmonious society as the China Foreign Ministry says..IPVM has been on this for a long time now, why the recent coverage? I’ve been begging for it and I’m thankful for it but represents a fairly staunch about face.....My co-worker just brought up this IPVM article, and yesterdays article on Uyghur warnings to China police to me this morning. Beyond feeling completely disgusted, I feel tremendous despair and sadness for those poor people who are being targeted for their race and religious beliefs..This is disgusting, and morally reprehensible. Of course Hik and Dahua will keep up the lies, lies and more lies so they can carry out their morally corrupt motives. The more research I just did after hearing about this, the more outraged I became about all the inhumanity and depravity associated with it. Their use of this targeted technology is sick and the world is worse for it..Enough is enough is enough. They are now clearly linked to exclusively using these brands, this surveillance technology for spying, ethnic cleansing initiatives and other nefarious social monitoring and manipulation motives. Im not quite sure how many years working on and using this in China...its so disgusting. We stopped selling HIK years ago and never bought Dahua. We will never, ever, buy those cameras, period..I agree with a lot of what Morten Tor Nielsen says in his post and especially below, but I guess I am not numb. Sometimes I feel like a misanthrope too..so I can relate. It is hard with a constant barrage of negativity and injustice. I guess here we do what we can to educate each and every one of our customers, and potential customers every day on manufacturer ethics, product updates and potential pitfalls. I think doing that has helped; as the word has spread in our state over the past few years about viruses, security flaws, and now morally reprehensible surveillance activity linked to human rights abuse and ethnic cleansing. .As a professed misanthrope with no faith in humanity, its really hard for me to see a solution to this. Lets speak up when we see injustice, but you kinda get numb after a while. The media seems to be a constant barrage of things I should be upset about, so apathy has set in a looooong time ago. And then people dont buy Dahua, but instead order a ton of cheap Chinese junk from Bezos mail-order company..Thank you to IPVM for their reporting on these issues. We routinely forward articles, and use them as a source of valuable information not only for product reviews, but for news like this that should be absolutely be shared with the installer community and beyond..My thoughts and prayers go out to these people who are suffering, and I can only hope that there is enough outrage and reporting in the world on these morally bankrupt activities that might eventually make a difference somehow, sooner than later..This isn’t new, IPVM has shone a light in this for a while now. Even considering these egregious acts IPVM continued to test, therefore highlight, nearly every new product they released. I’ve been fighting against there inclusion for just as long here. .Jason, as I explained before, we need to know how these things work. Presumably, you are not objecting to a recent Dahua test report like this - Dahua Emotion Analytics Tested where we exposed a host of problems..Id suggest not reviewing until they get their house in order - good or bad. Any news is good news, did you see the Oat Milk commercial during the Super Bowl?.In my view.. -always- the more fair and accurate information, the better. If IPVM does not continue to test and show results (some have been good and some have been less good) then it will fail to provide a cumulative view of the company and products..The shootouts are a vehicle to refute a pundit that might claim that (Hikua product X) out performs (vendor product Y). Without this information, unfounded claims can be made. .The worlds leading video surveillance information source, IPVM provides the best reporting, testing and training for 15,000+ subscribers globally. Dedicated to independent and objective information, we uniquely refuse any and all advertisements, sponsorship and consulting from manufacturers..IPVM is the #1 authority in video surveillance news, in-depth product tests, and independent training courses. Get updates delivered to your email, once a day, Monday to Friday.",Dahua had functions to detect Uyghurs in vision product
321,"A video purporting to show a Pornhub pop-up on CNNs Magic Wall during the US election coverage went viral on Twitter, with viewers trolling news anchor John King for it..However, it didnt take long for people to realise the video was fake, and many users then shared the real footage, showing that King was, in fact, flicking away a phone notification that popped up on the screen.","A fake notification was deemed as the ""highlight of the election"""
322,"A collection of thousands of photographs of naked women that is being used to create machine learning-generated porn includes images from porn production companies that have been accused of lying to and coercing women to have sex on camera..The dataset, which is circulating in deepfake porn creation communities online, includes images scraped from Czech Casting, a porn production company in the Czech Republic that police have accused of human trafficking and rape, as well as still images from videos produced by Girls Do Porn, which was ordered to pay almost $13 million to 22 women who appeared in its videos, and whose founder is currently a fugitive on the FBIs most wanted list..Motherboard has downloaded and viewed the dataset containing images from Czech Casting and Girls Do Porn, as well as several others being used to create machine learning-generated porn. .The people who anonymously use these datasets say that since the final algorithmically-generated images they create technically arent of real people, they dont harm anyone. In fact, they argue that their creations are a step towards a future where porn will not require human porn performers at all. But legal experts, technologists, and women who are included in the datasets described these creations as uniquely dehumanizing..Motherboard has written extensively about how deepfakes and internet platforms inability to sufficently curtail the spread of nonconsensual pornography upends the lives of and continually traumatizes women. This new form of machine learning-generated porn and the datasets it relies on introduces a new form of abuse, where the worst moments of some womens lives, captured on camera, are preserved, decontextualized, and spread online in service of creating porn whose makers claim to feature people who dont actually exist..Honza Červenka, a lawyer at McAllister Olivarius law firm who specializes in revenge porn and technology, is originally from the Czech Republic and has been following the case of Czech Casting, which is owned by Netlook, the country’s largest porn company. He told Motherboard that the idea that images are less harmful because theyre run through an algorithm and anonymized is a red herring. .It feels unfair, it feels like my freedom is being taken away, Jane, a woman who said she was coerced into shooting a scene for Czech Casting, told Motherboard..Jane, who asked to remain pseudonymous to speak about a traumatizing incident, remembers her hands shaking as she read over a contract for Czech Casting. She was there to support her friend, who needed money for rent. Theyd answered an advertisement for a modeling gig, and decided to go together. Theyd both just turned 18. They didnt know what kind of modeling it was; the ad was vague about details. Someone picked them up at a metro stop and took them to a house on the outskirts of Prague..(In an interview with Czech bodybuilder Antonin Hodan posted to YouTube, a male performer in Czech Casting videos named Alekos Begaltsis admitted that the women who show up for shoots sometimes dont know what theyre in for because of deceptive advertising. .The girls get here through agencies as well with the help of private agents or through friends, anyone can recommend, Begaltsis said. We cant control every piece of information in the advertising. It can happen that a girl gets here thinking shell do an underwear photoshoot. Which sucks because we are powerless in these situations. We are trying to push them to write the truth [in the ads]. Unfortunately its not always the case. But once she gets here, we inform her about everything.).We sat in a waiting room and got up to leave two or three times, but someone would always come up and tell us to stay, to not be afraid, she said. We were scared to leave so we stayed. .A woman called them one by one into a room with a white sofa where the filming would take place, and handed them a contract saying the videos wouldnt be accessible to anyone in the Czech Republic. This part of the arrangement is similar to the lie Girls Do Porn told women about how their videos were only going to be distributed to collectors in New Zealand. In reality, Girls Do Porn videos were published and sold in the U.S. and promoted on Pornhub. .Czech Casting does indeed block users trying to access it from the Czech Republic, Motherboard confirmed by trying to access the site using a virtual private network. But people within the country can also easily circumvent the block using a VPN, which is free and easy to set up. Additionally, as women who accused Czech Casting of wrongdoing have said, their families and friends quickly discovered their videos, which were reposted to popular free tube sites, where sometimes their real names were doxed. .Weeks later I started getting messages…These were mostly from men saying how beautiful I was and if they could have sex with me, Jane said. I got so many of these messages and keep getting them. I even changed my Facebook name because of this..After she signed the contract, a man came in and asked her if she was a virgin. She said that she felt like she had no way out, and that she couldnt leave without her ID. .After I said yes, he took the camera and told me to get naked, Jane said. I was told they were going to film something soft. . .I was scared to speak out..Jane said they put the money into her hands as she was leaving. She wasnt given a copy of the contract she signed, or any proof that shed been there at all..My friend found the room we were in on a porn site, Jane said. I realised this was a massive fuck-up. I kept thinking we should have left even if it means not having our IDs on us..In another Czech Casting video, a woman, who Motherboard was able to confirm is included in the dataset, starts crying while having sex and asks the man to stop. The man stops, and the camera zooms in to show that she is bleeding. He hands her a towel and tells her to clean up the blood..Janes story about Czech Casting isnt unique. Multiple women have accused Czech Casting of coercing them into having sex on camera. Czech police have charged nine people involved with Netlook, the company behind Czech Casting, of human trafficking and rape. Daisy Lee, a woman who went on to a career in porn after her Czech Casting scene and who is now friendly with Begaltsis, said the site has ruined lives. .I was 18 and didnt know what I was getting myself into. Most girls do not. The majority of them stay, but some leave. It ruins many lives, Lee told Motherboard..In a statement published in July by the adult entertainment news site Xbiz, Netlook denied the accusations and said it is cooperating with the police. Netlook did not respond to Motherboards request for comment. .In September, four years after Jane shot her scene for Czech Casting, a PhD student opened a new forum to show off his latest personal AI project: algorithmically-generated porn..The person making these videos goes by the username GeneratedPorn, and named the r/GeneratedPorn subreddit to post about the technology (well refer to this user as GP in this story). He said he started the project because he wanted to improve his machine learning skills. Like some of the earliest deepfakes that were posted online in 2017, what he shared were glitchy, spasming facsimiles of the images theyre trained on: thousands of porn videos and images. Unlike much deepfake porn, the images GP is producing wouldnt fool anyone into thinking they are real porn. The final result barely looks human, let alone like a specific person. .But much like early deepfakes, theyre rapidly improving in realism. GP has posted several experiments in the past few weeks featuring increasingly accurate naked human bodies, and even some slightly animated images, showing that convincing porn generated entirely by AI is not impossible..This all started as a quest for me to learn how all of this cool tech worked but then I ended up pivoting into the porn generation stuff as I thought it was a cool concept, especially after watching the movie Her, GP said in an email to Motherboard. .GP explained his process to Motherboard over email, as well as in detail on Reddit, posted in the popular r/MachineLearning community. He used a Stylegan2 model thats available on Github as open-source code, but loaded it with datasets of porn. Its similar to how any other face-swapping deepfake is made, but instead of using a dataset consisting of many expressions of one persons face, he pulled from multiple datasets found online. .To create the videos, GP trained the algorithms using datasets from around the web, including one that primarily consists of images ripped from Czech Casting. The datasets, which are hosted and are free to download from popular file sharing sites, are compiled by users experimenting in deepfakes and other forms of algorithmically generated images. GP found the Czech Casting dataset on one of these file sharing websites, but said that if he didnt he would have written a web scraper to collect the images from Czech Casting. .Creating algorithmically generated videos of a full, naked body requires many images and videos of real, nude people, and its hard to imagine a more suitable resource for the task than Czech Casting. .Czech Casting, much like Girls Do Porn, specialized in casting couch-style porn, and has posted thousands of videos of women over the years. Its production style was almost algorithmic to begin with: Each video of a woman also comes with a uniform set of photographs. Each set includes a photograph of the woman holding a yellow sign with a number indicating her episode number, like a mugshot board. Each set also includes photographs of the women posing in a series of dressed and undressed shots on a white background: right side, left side, front, back, as well as extreme close ups of the face, individual nipples, and genitalia. In recent years, Czech Casting also started including 360-degrees photographs of the women, where they pose for interactive VR-style content. .The main reason people opt for a data source like this, is that the generative adversarial models (GAN) people use, are trying to learn a general structure of an image for the class of objects youre trying to generate, GP said. If your images are structurally similar, the model can learn more about the finer/granular details of the item class, like dimples or freckles on a face. Which leads to a higher quality result..GP sent Motherboard a sample of the dataset hes using, which also included images from Girls Do Porn videos. Other datasets that GP is using, which Motherboard has viewed, include images that appear to be scraped from across the internet, including other porn sites, social media, and subreddits where users post selfies, like r/roastme, a subreddit where people post images of themselves for other people to judge..In a post to the r/MachineLearning subreddit explaining how his algorithmically generated porn works, GP pauses halfway through the explanation to address a potential ethical issue..I wasnt sure what to do with it, other than it being this cool thing Id created… Id contemplated making an OnlyFans and offering personalised AI generated nudes that talk to people, he wrote. But someone I knew frowned upon this idea and said it was exploitative of Males who might need companionship. So I decided not to go down that route in order to avoid the ethical can of worms. .He also noted in that post that training dataset ethics is something hes concerned about. Are the images we are training on ethical or have the people in the images been exploited in some way[?] he wrote. I again cant verify the back story behind hundreds of thousands of images, but I can assume some of the images in the dataset might have an exploitative power dynamic behind them, noting that some of the images are from Girls Do Porn. Im not sure if its even possible to blacklist exploitative data if its been scraped from the web. I need to consider this a bit more..These questions didn’t stop GP from building the project in public, on social media platforms, which means he’s perpetrating harm regardless of whatever ethical quandaries he says he may have. Much of the most harmful nonconsensual content is spread on the internet through surface-level platforms like Twitter, Facebook, Reddit, OnlyFans, and tube sites like XVideos and Pornhub..So many mainstream porn websites host child pornography and nonconsensual pornography, and does depict rape, and profit from those through ad sales, Červenka said. .When Motherboard contacted Reddit for comment, a spokesperson said Reddits site-wide policies prohibit involuntary pornography, which applies to all content, including deepfakes. Reddit banned deepfakes in 2017. Both r/GeneratedPorn and r/AIGeneratedPorn were shut down after Motherboards request for comment. .Generated Porns user profile on Pornhub was also taken down after Motherboard contacted Pornhub. A spokesperson for Pornhub declined to comment..Porn tube site xHamster took down GPs user profile pending further review: These new types of content are indeed grey areas and we will need to review with our own machine learning team and TOS team to determine how to evaluate and where necessary prevent, a spokesperson for xHamster said.  .OnlyFans did not respond to a request for comment. Patreon, where GP was asking for people to fund his project with little success, told Motherboard that while funding nonconsensual sexual content isnt permitted on the platform, if an account does contain nonconsensual porn, the platform works with the creator to bring the account within its terms of use. The project was taken down from Patreon as of Monday..Now somebody walks up and uses those images to create a baseline for computers to use, potentially for decades to come, to use for computer generated images?”.In an email to Motherboard, GP expressed another ethical concern: that the algorithm might produce something that is recognizable as a real human—a result that would negate the whole point of his project: anonymity. .Its quite possible for the algorithm to reproduce fake people who resemble real people, but it wouldnt be a 1-to-1 replication of the data it has trained on, he said. This presents an ethical problem Im trying to navigate around, which is identifying the rare situations where it does replicate a person from the ~7,500 images its learning from. Its something that plagues generative networks… Its possible and Im not quite sure how to 100% avoid the possibility of this happening. But I really do want to avoid this. Im not interested in deep-faking anyone, even by accident, its a bit scummy imho! .GP is far from alone in this type of project. The creator of the first deepfakes told Motherboard almost the same thing in 2017: that he wasnt a professional researcher, just a programmer with an interest in machine learning who “just found a clever way to do face-swap,” he said..These Nudes Do Not Exist and a subsequent project from the same creator called Harem most likely draws its data from Czech Casting—the images come out looking unmistakably similar, but the creator of that project hasnt responded to requests for comment on where the images in their dataset come from. Another abandoned project at r/AIGeneratedPorn did the same. .The real ethical issue plaguing this project is not the risk of parting lonely men from their money. It would take one search online of Czech Casting, and some basic awareness of the concept of pirated content being harmful to creators, to recognize the datasets these non-existent women are built from are comprised of gigabytes of questionably-sourced porn, some of it potentially depicting sexual assault..On Monday, the night before this story was published and after his Patreon account was suspended, GP told Motherboard that he “decided to shut down the project.”.Jane told Motherboard that she was hoping her video would get lost among so many others online, and no one would find it. But there is always someone who manages to fish it out from the depths of the internet, she said..Červenka, the lawyer at McAllister Olivarius law firm who specializes in revenge porn and technology, told Motherboard that because some of the Czech Casting videos were allegedly edited to look consensual from the start, they have always been deceptive and harmful—and churning them through the meat grinder of machine learning algorithms doesnt make them less so. .Now somebody walks up and uses those images to create a baseline for computers to use, potentially for decades to come, to use for computer generated images? Its awful, on a personal level, and it certainly should be illegal, Červenka said..Even for professional porn performers, stolen content is an issue that plagues the industry. Adult performer Leah Gotti, whose images are part of the datasets GP is using without her consent, told me that the problem of stolen content isnt just disrespectful—its dangerous. Shes currently working to stop a stalker-fan from creating fake Instagram accounts of her and targeting her family by stealing her content and reposting it..It just goes back to, no one truly respects sex workers, Gotti told me. All those things are pirated, and thats supposed to be against all the rules, but because were having sex on camera theyre like, well, she asked for it. .Earlier this year, a rumored OnlyFans leak of a database of stolen porn threatened to put sex workers on that platform in danger of being harassed or doxed..Daisy Lee, the performer who started with Czech Casting when she was 18 but continued working in the adult industry after, told Motherboard that she blames herself for thinking that the videos wouldnt go viral worldwide. .They dont put it on Czech servers but people download it and re-upload it everywhere, Lee said. Every girl that goes in thinks it wont be visible to their friends and family… 14 days later [my] video was everywhere. It destroyed my reputation and spread around my home town within hours. But nobody forced me to do anything, no drugs, nothing like that..Many of the women who were targeted by Girls Do Porn also blame themselves for believing the company’s lies claiming that the videos would stay in a certain region—in that case, in private New Zealand collections, on DVD. But the entire system of porn online, and all content online for that matter, is set up to spread videos and photos the harder one tries to remove it. Algorithms are driven by what people feed them. One Czech Casting model lost her teaching job after students found her episode online, and when she spoke out about feeling victimized by the company, people sought her video out more..The researcher in me feels like if its been published online its open source and fair game however the budding capitalist in me feels like that violates IP in some sense, GP said. Im a bit conflicted. Ive personally accepted that any data I ever create as an individual will be used by others for profit or research..GP also said that he thinks the type of abuse Czech Casting has been accused of is horrible, but that its difficult to screen for this kind of abuse when creating or using datasets..Now that the abuse is present I can opt to not use that data and source data from elsewhere, GP said. Others in the area may not care and may decide to use it anyway. Its quite difficult to screen for this data completely. Doing a google image search for female standing nude gives you a bunch of Czech Casting images. Throwing on the flag -czech catches a lot of them, but some still get through the cracks..While GP said that he could choose not to use images produced by Girls Do Porn and Czech Casting, he didnt say that he would, nor is it clear if his project and others similar to it could function without those images. GP also suggested that his project could also somehow help these women..I feel bad for the victims of this abuse and I cant say anything that may make them feel better, he said. My only hope is that technology such as the tech Im working on, now and in the future, leads to a reduction in harm to others. By making it an economical and technologically inferior choice to commit abuse..Červenka said that even after three years of deepfakes panic and decades more of nonconsensual porn online, the laws to stop them havent caught up. Victims could make a legal claim that theyve been portrayed in a false light or defamed, especially when content is edited deceptively to make it look consensual. But thats often not enough..These laws have been around for a long time, and we are just trying to use them in the current context, because we dont have anything else, Červenka said The legislature is unable to truly grapple with what people do online, and how to regulate harmful effects of what people do online..It also becomes harder to go after anyone hosting the content if theyre hosting it anonymously, all over the world, where every legal system is different. Even in the U.S., where some states have enacted deepfakes-specific laws, it differs from state to state. .The abuses the women in Czech Casting and Girls Do Porn endured happened in the real world, but the videos spread online made it worse. Some Girls Do Porn victims were forced to change their names, move states, drop out of school, and lost their careers or relationships with family and friends. Czech Casting victims have similar stories. .Revenge porn victims—as well as professional and amateur adult performers—spend hours sending takedown requests to websites that host their images. Often, those requests are ignored. And when it comes to datasets used to create more porn, its hard to know where your images live on, unless you can locate where its hosted and download a huge set of files, then sort through them to find yourself. Their worst moments are enshrined forever among gigabytes of others..There have been efforts in recent years to create machine learning datasets that are fully consensual. After the privacy failures of MS-Celeb-1M, a dataset of 10 million photos from 100,000 individuals collected from the internet, ranging from journalists to musicians and activists, theres more awareness than ever toward ethical uses of peoples faces. In 2019, for its Deepfakes Detection Challenge, Facebook launched a dataset consisting of 100,000 videos of paid actors, for researchers to use. One of the sponsors of that challenge was data science community site Kaggle. One of the datasets Generated Porn used is hosted on Kaggle, and appears to be largely stolen, scraped porn content. .If machine learning engineers interested in creating AI porn wanted to start a fully-ethical project, they would do something similar to what Facebook did with its challenge dataset..They would get consent from people who want to be nude models, and say this is what were going to build it for, and everythings on the up and up, Rumman Chowdhury, data scientist and founder of ethical AI firm Parity, told Motherboard. And maybe even [models] would get royalties, [engineers] would go build their AI, sell it as a porn, and they would actually do pretty well. But doing things the right way costs money, and when youre tinkering with porn as a side project, its usually money you dont have. r/AIGeneratedPorns project died because renting server time and running the training was too expensive, according to a post in that subreddit before it went down..How can a tech that at its core has rape videos be anything but a perpetuation of rape culture? Červenka said. I don’t think I would sleep well at night if I were [GP], because hes relying on images of abuse to create a Frankensteins monster..By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.",Czech Casting and Girls do Porn create machine-learning generated porn who find the creations uniquely dehumanizing 
323,"Of course if said companies dont work with China, China will just keep the software, lock their sales guys in jail, and still not pay anything.So selling things illegally is ok, so long as you suspect they might steal it if you dont sell it to them? And yes, the west sells it to anyone, even if they wouldnt steal it.  How else did Blue Coats end up doing national firewalls for oppressive middle-east regimes?.So selling things illegally is ok, so long as you suspect they might steal it if you dont sell it to them? And yes, the west sells it to anyone, even if they wouldnt steal it.  How else did Blue Coats end up doing national firewalls for oppressive middle-east regimes?.There is no law that says citizens of the USA cant sell Internet filtering software to oppressive countries. China has most favored nation status, so other than military goods, they actually have higher status than Canada or Mexico (because we use that status to bully their lawmakers around on IP issues)..Its not like US companies are selling systems to catalog people for the gas chamber or anything. Hell, the illegal chemical weapons Saddam used on rioting Kurds were SOLD to him by the US military sup . I was just in Beijing for two weeks. I have access to two OpenVPN servers, one in New York another in California. These are personal servers so they arent on the IP based blacklist. However, my connection from Beijing to either of the two would crap out after a day or two, and the only remedy was to change the OpenVPN server port..It seems right now they update their blacklist every 24~48 hours. I did not test whether the amount of traffic (idle vs. busy) would affect the time it takes them to block you. Blacklists last longer than two weeks, as the original ports I used was still blocked by the time I left. SSH connections does not seem to be affected at this time..Providing you do your key exchange in a secure manner, that shouldnt be a problem. While I usually use OpenVPN for infrastructure VPN, Ive used SSH tunneling for quick and dirty connections at airports and hotels and the like..Deep packet inspection can turn up lots of easily identifiable behavior. Port scrambling, intentional service misidentification, mixing bogus streams with encrypted ones, bursting traffic over multiple IPv6s, all can make a difference..But an ssh link is easily identifiable. They dont have to read anything, just block stuff. Experience as a teacher, 100% of what you do gets seen; what goes through is an algorithm that changes as they like it to..SSH connections does not seem to be affected at this time.Can you find a solution to your problem then?*Jeopardy music* Lets see what Tim has.  Youve written, Dont do business in China, Im sorry, we were looking for SSH tunneling.  Susan, youve written, Port Changing Cron Job, no, thats incorrect as well.  Yiu?  Youve written, There is no Problem... No, thats incorr--- Wait, the judges say well accept that answer, Yiu Wins! . Lets see what Tim has.  Youve written, Dont do business in China, Im sorry, we were looking for SSH tunneling.  Susan, youve written, Port Changing Cron Job, no, thats incorrect as well.  Yiu?  Youve written, There is no Problem... No, thats incorr--- Wait, the judges say well accept that answer, Yiu Wins! .SSH connections does not seem to be affected at this time.Can you find a solution to your problem then?*Jeopardy music*Yiu?  Youve written, There is no Problem... No, thats incorr--- Wait, the judges say well accept that answer, Yiu Wins!Wait, whats that?  Oh, Im sorry Yiu, the judges correctly point out that you failed to use the form of a question!  Im sorry, and better luck next time..SSH connections does not seem to be affected at this time.Can you find a solution to your problem then?*Jeopardy music*Yiu?  Youve written, There is no Problem... No, thats incorr--- Wait, the judges say well accept that answer, Yiu Wins!.Wait, whats that?  Oh, Im sorry Yiu, the judges correctly point out that you failed to use the form of a question!  Im sorry, and better luck next time..Steganography will drive the analysis bot programmers absolutely nuts, theyll either have to shut everything down, or let some amount of stego traffic through..I find SSH tunneling to be much less efficient than OpenVPN. With OpenVPN I can have a more-or-less usable remote VNC desktop from Beijing to New York, which is not possible using SSH tunneling..Anyway, that is not a real solution, as there is nothing to prevent them from cutting off SSH connections when they feel like it. There is no technical solution to a political problem..I was just in Beijing for two weeks. I have access to two OpenVPN servers, one in New York another in California. These are personal servers so they arent on the IP based blacklist. However, my connection from Beijing to either of the two would crap out after a day or two, and the only remedy was to change the OpenVPN server port..It seems right now they update their blacklist every 24~48 hours. I did not test whether the amount of traffic (idle vs. busy) would affect the time it takes them to block you. Bl .In theory OpenVPN is SUPPOSED to be SSL, but from what Im reading something about the handshake and the way traffic is transmitted is tipping the Chinese GFC admins off.  I did a little reasearch after I posted above and others report the same as he does-- that theyre really good about distinguishing VPN from non VPN traffic..Ive been living in southern China for the past year and the last month has been a nightmare.  It seems if youre pumping a significant amount of traffic over an encrypted channel, they block the remote server but only for the specific port..I have a handful of personal OpenVPN servers and made the mistake of transferring a lot of data over 22 (SSH) and port 22 for that server was blocked.  As the parent post suggests, it seems to be updated every 24-48 hours, usually every 24 hours though..Raise the noise floor, hide your encrypted data among legitimate looking traffic. For various meanings of legitimate. One can only fathom the amount of useless garbage that gets passed on backbone links. From malfunctioning programs, unknown millions of installations of random programs phoning home for updates, spam, web bots, ddos, facebook. An endless sea of data for your subversive little packets to get lost in..  If they had this, they would have solved the spam problem by now... Speaking of spam: by intelligently encoding your encrypted data as spam, you could pass through the sniffers too..chinese are big sellers on ebay, now.  that comms path WILL stay open, no matter what.  they need to keep selling dangerous things to us.  we all know that..and so, format your data as fake replies to a fake seller in china.  sure, the frag/reassem logic is going to be a bitch, but youll get your data tunneled thru there, and even better, ebay pays the comms cost!.That used to be a good idea, but as more and more governments get access to supercomputers that they can dedicate to monitoring, it wont work for long. Its really not hard to pick out that needle in the haystack if you have the resources..I would imagine they are watching the handshaking and looking for certain patterns at the start of TCP sessions.  If the streams match a certain pattern (VPN connection handshake), then the connection will be added to the global blacklist at the next update.  For VPNs that do their negotiation fully over UDP, the firewall probably just has to look for a specific set of packets between 2 systems over a short period of time..I may be making bad assumptions here, my TCP and UDP knowledge is pretty rusty.  It seems like if the algorithm wasnt smart enough to keep track of the full connection state, you could spoof a protocol appropriate TCP or UDP packet from the remote IP and port to avoid a block.  Alternately, you might be able to avoid detection by using a common port like 53 for your UDP VPN and spoofing valid DNS response packets.  If that caused problems for your VPN client, you could set a flag on them that causes them t .Seems unlikely to avoid detection using a port like 53 (DNS services, something that filter all the time).  Actually its probably pretty easy to look at most standard port traffic and infer that they are being used for non-standard purposes..Thats why they force all the major companies to locate servers in China.  Id venture there is minimal cross-talk between Chinese sites like Yahoo and their American counterparts..If you need a narrow band VPN, you could always encrypt it in such a way that it cant be detected by the sniffers. For example, use something like the technique used by port knocking, i.e. utilize the time domain for your encrypted channel. In other words, dont send encrypted data directly, just send regular data and modulate the time intervals between the packets to reflect your encrypted data.Thats likely to be really low bandwidth and a bright target for thier firewall learning algorithms.   Modulating the time intervals on a high-latency connection with the typically large amount of buffering will be troublesome if the just randomly drop packets on suspicious connections and wait for TCP/IP retransmit.  Of course you could hack your TCP/IP stack to be aware of this, but thats quite a bit of work..If you need a narrow band VPN, you could always encrypt it in such a way that it cant be detected by the sniffers. For example, use something like the technique used by port knocking, i.e. utilize the time domain for your encrypted channel. In other words, dont send encrypted data directly, just send regular data and modulate the time intervals between the packets to reflect your encrypted data..Thats likely to be really low bandwidth and a bright target for thier firewall learning algorithms.   Modulating the time intervals on a high-latency connection with the typically large amount of buffering will be troublesome if the just randomly drop packets on suspicious connections and wait for TCP/IP retransmit.  Of course you could hack your TCP/IP stack to be aware of this, but thats quite a bit of work..faster than the speed of light. Bullshit.  Your 5 minutes pseudo-intellectual masturbation session has failed to produce any legitimate results.  Imagine that.Einstein: 1slashmydots: 0BTW, where did you learn physics?  Id like to make sure never to send my children there..Plus encryption was highly restricted for export as an ordnance during the early days of the web and before.I dont believe Switzerland ever had restrictions on exporting encryption..I am not a web site guru, but IIRC there was a good market for encryption offload cards at one time  but I dont know how common they are anymore given the use of virtualization and the general increase in CPU power over past systems..It was probably also a headache from a certificate perspective.   You can use it with self-signed certificates, but you have to generate them, etc and traditionally .The Chinese are wasting there time, buying a year or two of incomplete censorship at the cost of giving everyone the means to defeat such methods afterwards, when new software methods are developed and become universally available..Consider the problem. You wish to kill the use of encryption so you have the capability of inspecting any data block that travels across the Internet. Luckily, such censorship is fighting maths, and will always lose accordingly. Heres why..Just post some nice pictures on a forum...and after that forum becomed a popular route for circumvention, they block that whole website in China via IP filtering, DNS and connection blacklisting...Certainly anything might for a while, but then there are countermeasures....and after that forum becomed a popular route for circumvention, they block that whole website in China via IP filtering, DNS and connection blacklisting....Over about the last 2 weeks, one of our hosting clients OpenVPN connections to their machines in China have been failing.  We can still SSH into the machine in China, glad they havent blocked that.  We ended up setting up a block of several hundred ports with DNAT to the normal OpenVPN port, and then set up 64 (the max allowed) servers in the client config so it can cycle between them.  Thats been effective so far..It took a while to figure out, because I was able to send test traffic via date | nc -u server 1194, and that would go through, but the OpenVPN connection wouldnt..In the subject of rebellion: You are forgetting something. China is not Syria and they are a nuclear power. They have too many people already.If rebellion becomes widespread, A few 50 megaton thermonuclear bombs detonated in selected problem areas would solve the problem very quickly..The only effective way to fight this is just to let China go. They dont want traffic they dont like? Fine, f em. Drop ALL traffic into their ISPs. Companies who keep playing ball with them will only have themselves to blame when the cost of doing business is so high that its infeasible..I dont live in China so I havent had a chance to test this, but I would guess Tor/Onion is more or less the ideal way of keeping a stable connection out of China.  Just run a private exit node outside China.  Tor change the tunnel connections regularly to obscure its existence..There may be more comments in this discussion. Without JavaScript enabled, you might want to turn on Classic Discussion System in your preferences instead..I judge a religion as being good or bad based on whether its adherents become better people as a result of practicing it. - Joe Mullally, computer salesman","Censorship system can learn, discover and block encrypted VPN protocols by machine learning in protocol classification"
324,"Accurate and timely traffic classification is critical in network security monitoring and traffic engineering. Traditional methods based on port numbers and protocols have proven to be ineffective in terms of dynamic port allocation and packet encapsulation. The signature matching methods, on the other hand, require a known signature set and processing of packet payload, can only handle the signatures of a limited number of IP packets in real-time. A machine learning method based on SVM (supporting vector machine) is proposed in this paper for accurate Internet traffic classification. The method classifies the Internet traffic into broad application categories according to the network flow parameters obtained from the packet headers. An optimized feature set is obtained via multiple classifier selection methods. Experimental results using traffic from campus backbone show that an accuracy of 99.42% is achieved with the regular biased training and testing samples. An accuracy of 97.17% is achieved when un-biased training and testing samples are used with the same feature set. Furthermore, as all the feature parameters are computable from the packet headers, the proposed method is also applicable to encrypted network traffic..Bazi, Y., & Melgani, F. (2006). Toward an optimal SVM classification system for hyperspectral remote sensing images. IEEE Transactions on Geoscience and Remote Sensing, 44(11), 3374–3385..Beheshti, H., Hultman, M., Jung, M., Opoku, R., & Salehi-Sangari, E. (2007). Electronic supply chain management applications by Swedish SMEs. Enterprise Information Systems, 1(2), 255–268..Bellotti, T., & Crook, J. (2008). Support vector machines for credit scoring and discovery of significant features. Expert Systems with Applications, to appear..Bernaille, L., Teixeira, R., Akodkenou, I., Soule, A., & Salamatian, K. (2006). Traffic classification on the fly. Computer Communication Review, 36(2), 23–26..Duan, L., Xu, L., Guo, F., Lee, J., & Yan, B. (2007). A local-density based spatial clustering algorithm with noise. Information Systems, 32(7), 978–986..Early, J., Brodley, C., & Rosenberg, C. (2003). Behavioral authentication of server flows. Proceedings of the 19th Annual Computer Security Applications Conference, pp. 46–55..Haffner, P., Sen, S., Spatscheck, O., & Wang, D. (2005). ACAS: Automated construction of application signatures. Proceeding of ACM SIGCOMM 2005 Workshops: Conference on Computer Communications, 197–202..Huang, C., Liao, H., & Chen, M. (2008). Prediction model building and feature selection with support vector machines in breast cancer diagnosis. Expert Systems with Applications, 34, 578–587..Kohavi, R. (1995). A Study of cross-validation and bootstrap for accuracy estimation and model selection. Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, 1137–1143..Lakhina, A., Crovella, M., & Diot, C. (2004). Characterization of network-wide anomalies in traffic flows. Proceedings of the 2004 ACM SIGCOMM Internet Measurement Conference, 201–206..Li, L., Warfield, J., Guo, S., Guo, W., & Qi, J. (2007a). Advances in intelligent information processing. Information Systems, 32(7), 941–943..Liu, R., Wang, Y., Baba, T., Masumoto, D., & Nagata, S. (2008). SVM-based active feedback in image retrieval using clustering and unlabeled data. Pattern Recognition, 41, 2645–2655..Luo, J., Xu, L., Jamont, J. P., Zeng, L., & Shi, Z. (2007). A flood decision support system on agent grid: method and implementation. Enterprise Information Systems, 1(1), 49–68..Moore, A., & Zuev, D. (2005a). Internet traffic classification using Bayesian analysis techniques. Performance Evaluation Review, 33, 50–60..Roughan, M., Sen, S., Spatscheck, O., & Duffield, N. (2004). Class-of-service mapping for QoS: A statistical signature-based approach to IP traffic classification. Proceedings of the 2004 ACM SIGCOMM Internet Measurement Conference, 135–148..Sen, S., Spatscheck, O., & Wang, D. (2004). Accurate, scalable in-network identification of P2P traffic using application signatures. Thirteenth International World Wide Web Conference Proceedings, 512–521..Shi, Z., Huang, Y., He, Q., Xu, L., Liu, S., Qin, L., et al. (2007). MSMiner-a developing platform for OLAP. Decision Support Systems, 42(4), 2016–2028..Shi, S., Xu, L., & Liu, B. (1996). Application of artificial neural networks to the nonlinear combined forecasts. Expert Systems, 13(3), 195–201..Shi, S., Xu, L., & Liu, B. (1999). Improving the accuracy of nonlinear combined forecasting using neural networks. Expert Systems With Applications, 16(1), 49–54..Vigna, G., Robertson, W., & Balzarotti, D. (2004). Testing network-based intrusion detection signatures using mutant exploits. Proceedings of the 11th ACM Conference on Computer and Communications Security, 21–30..Wang, S., & Archer, N. (2007). Electronic marketplace definition and classification: literature review and clarification. Enterprise Information Systems, 1(1), 89–112..Yan, Z., Wang, Z., & Xie, H. (2008). The application of mutual information-based feature selection and fuzzy LS-SVM-based classifier in motion classification. Computer Methods and Programs in Biomedicine, 90, 275–284..The research presented in this paper is supported in part by the NSFC (Grant numbers: 60243001, 60574087, 60605019, 60633020) and 863 High Tech Development Plan (Grant numbers: 2007AA01Z475, 2007AA01Z480, 2007AA01Z464).",A machine learning method based on SVM is proposed for accurate Internet classification which is critical in network security monitoring and traffic engineering
325,"A Muslim prayer app with over 98 million downloads is one of the apps connected to a wide-ranging supply chain that sends ordinary peoples personal data to brokers, contractors, and the military..The U.S. military is buying the granular movement data of people around the world, harvested from innocuous-seeming apps, Motherboard has learned. The most popular app among a group Motherboard analyzed connected to this sort of data sale is a Muslim prayer and Quran app that has more than 98 million downloads worldwide. Others include a Muslim dating app, a popular Craigslist app, an app for following storms, and a level app that can be used to help, for example, install shelves in a bedroom..Through public records, interviews with developers, and technical analysis, Motherboard uncovered two separate, parallel data streams that the U.S. military uses, or has used, to obtain location data. One relies on a company called Babel Street, which creates a product called Locate X. U.S. Special Operations Command (USSOCOM), a branch of the military tasked with counterterrorism, counterinsurgency, and special reconnaissance, bought access to Locate X to assist on overseas special forces operations. The other stream is through a company called X-Mode, which obtains location data directly from apps, then sells that data to contractors, and by extension, the military..The news highlights the opaque location data industry and the fact that the U.S. military, which has infamously used other location data to target drone strikes, is purchasing access to sensitive data. Many of the users of apps involved in the data supply chain are Muslim, which is notable considering that the United States has waged a decades-long war on predominantly Muslim terror groups in the Middle East, and hundreds of thousands of civilians have died during military intervention in Pakistan, Afghanistan, and Iraq. Motherboard does not know of any specific operations in which this type of app-based location data has been used by the U.S. military..The apps sending data to X-Mode include Muslim Pro, an app that reminds users when to pray and what direction Mecca is in relation to the users current location. The app has been downloaded over 50 million times on Android, according to the Google Play Store, and over 98 million in total across other platforms including iOS, according to Muslim Pros website..The Most Popular Muslim App!, Muslim Pros website reads. The app also includes passages and audio readings from the Quran. (After publication of this piece, Muslim Pro said it will no longer share data with X-Mode)..Some app developers Motherboard spoke to were not aware who their users location data ends up with, and even if a user examines an apps privacy policy, they may not ultimately realize how many different industries, companies, or government agencies are buying some of their most sensitive data. U.S. law enforcement purchase of such information has raised questions about authorities buying their way to location data that may ordinarily require a warrant to access. But the USSOCOM contract and additional reporting is the first evidence that U.S. location data purchases have extended from law enforcement to military agencies..USSOCOM bought access to Locate X, a location data product from a company called Babel Street, according to procurement records uncovered by Motherboard. A former Babel Street employee described to Motherboard how users of the product can draw a shape on a map, see all devices Babel Street has data on in that location, and then follow a specific device around to see where else it has been..The Locate X data itself is anonymized, but the source said we could absolutely deanonymize a person. Babel Street employees would play with it, to be honest, the former employee added..USSOCOM purchased the additional software licenses for Locate X and another product focused on text analysis called Babel X in April, according to the public records. The bundle of additional licenses cost around $90,600, the records show..In a statement, Navy Cmdr. Tim Hawkins, a U.S. Special Operations Command spokesperson, confirmed the Locate X purchase, and added Our access to the software is used to support Special Operations Forces mission requirements overseas. We strictly adhere to established procedures and policies for protecting the privacy, civil liberties, constitutional and legal rights of American citizens..A Babel Street document available online says that Within the technical specifications of the Locate X Data, Customer’s use of the Locate X Data is not limited by the number of search queries. The document says the location data may not always be accurate..In March, tech publication Protocol first reported that U.S. law enforcement agencies such as Customs and Border Protection (CBP) and Immigration and Customs Enforcement (ICE) were using Locate X. Motherboard then obtained an internal Secret Service document confirming the agencys use of the technology. Some government agencies, including CBP and the Internal Revenue Service (IRS), have also purchased access to location data from another vendor called Venntel..In my opinion, it is practically certain that foreign entities will try to leverage (and are almost certainly actively exploiting) similar sources of private platform user data. I think it would be naïve to assume otherwise, Mark Tallman, assistant professor at the Department of Emergency Management and Homeland Security at the Massachusetts Maritime Academy, told Motherboard in an email..Some companies obtain app location data through bidstream data, which is information gathered from the real-time bidding that occurs when advertisers pay to insert their adverts into peoples browsing sessions. Firms also often acquire the data from software development kits (SDKs)..Location data firm X-Mode, which is different than Babel Street, encourages app developers to incorporate its SDK, essentially a bundle of code, into their own apps. The SDK then collects the app users location data and sends it to X-Mode; in return, X-Mode pays the app developers a fee based on how many users each app has. An app with 50,000 daily active users in the U.S., for example, will earn the developer $1,500 a month, according to X-Modes website..In a recent interview with CNN, X-Mode CEO Joshua Anton said the company tracks 25 million devices inside the United States every month, and 40 million elsewhere, including in the European Union, Latin America, and the Asia-Pacific region. X-Mode previously told Motherboard that its SDK is embedded in around 400 apps..In October the Australian Competition & Consumer Commission published a report about data transfers by smartphone apps. A section of that report included the endpoint—the URL some apps use—to send location data back to X-Mode. Developers of the Guardian app, which is designed to protect users from the transfer of location data, also published the endpoint. Motherboard then used that endpoint to discover which specific apps were sending location data to the broker..Motherboard used network analysis software to observe both the Android and iOS versions of the Muslim Pro app sending granular location data to the X-Mode endpoint multiple times. Will Strafach, an iOS researcher and founder of Guardian, said he also saw the iOS version of Muslim Pro sending location data to X-Mode..The data transfer also included the name of the wifi network the phone was currently collected to, a timestamp, and information about the phone such as its model, according to Motherboards tests..Other apps sending data to X-Mode included the Accupedo step counter app, which has been downloaded more than 5 million times according to the apps page on the Google Play Store; the CPlus for Craigslist app which lets users more easily search Craigslist, and has more than one million downloads; and Global Storms, an app for following hurricanes, typhoons, and tropical storms. The app has been downloaded more than a million times..As well as the prayer app, Motherboard also installed the Muslim Mingle dating app onto an Android phone and observed the app sending precise geolocation coordinates of the phones current location and wifi network name to X-Mode multiple times. Vietnam-based Mingle, the developer behind the app as well as other dating apps such as Black Mingle which is marketed specifically towards Black people, did not respond to multiple requests for comment..It is safe to say from this context that the reasonable consumer—who is not a tech person—would not have military uses of their data in mind, even if they read the disclosures..Motherboard found another network of dating apps that look and operate nearly identically to Mingle, including sending location data to X-Mode. Motherboard installed another dating app, called Iran Social, on a test device and observed GPS coordinates being sent to the company. The network of apps also includes Turkey Social, Egypt Social, Colombia Social, and others focused on particular countries..X-Mode then sells access to this sort of data to a wide range of different clients. Motherboard has previously shown that one of those clients includes a private intelligence firm whose goal is to use location data to track people down to their doorstep. X-Mode has also demonstrated how its data can be used to follow where people in COVID-19 hotspots travelled to after potentially exposing one another to the coronavirus..Those clients have also included U.S. military contractors, Motherboard found. Included in archived versions of the Trusted Partners section on its website, X-Mode lists Sierra Nevada Corporation and Systems & Technology Research as customers. Sierra Nevada Corporation builds combat aircraft for the U.S. Air Force, and supports contractor Northrop Grumman in the development of cyber and electronic warfare capabilities for the U.S. Army. Systems & Technology Research works with the Army, Navy, and Air Force according to procurement records, and offers data analytics support to intelligence analysts, according to its website..Senator Ron Wyden told Motherboard in a statement that X-Mode said it is selling location data harvested from U.S. phones to U.S. military customers..In a September call with my office, lawyers for the data broker X-Mode Social confirmed that the company is selling data collected from phones in the United States to U.S. military customers, via defense contractors. Citing non-disclosure agreements, the company refused to identify the specific defense contractors or the specific government agencies buying the data, the statement read..X-Mode told Motherboard in an email that the company does not work with Sierra Nevada or STR but did not deny they were once customers. (As Motherboard has continued to report on the company and the office of Senator Ron Wyden has carried out its own investigation into the location data industry, X-Mode has removed multiple company names from the Trusted Partners page, including Sierra Nevada Corporation)..X-Mode licenses its data panel to a small number of technology companies that may work with government military services, but our work with such contractors is international and primarily focused on three use cases: counter-terrorism, cybersecurity and predicting future COVID-19 hotspots, X-Mode added in its email to Motherboard..Several app developers who work with X-Mode told Motherboard they did not know their users location data was being sent to defense contractors..I was not aware that X-Mode was selling those data to some military contractors, Nicolas Dedouche, CEO at app development firm Mobzapp, told Motherboard in an email. Mobzapp made a screen sharing app for Android that sends location data to X-Mode and has been downloaded more than a million times. I cannot be aware X-Mode is working with military contractors if they do not clearly mention it somewhere, he added..As an app developer I do care with whom Im contracting with, Antoine Vianey, the developer behind the app Bubble level that has been downloaded more than ten million times, said. But they added to be totally clear I missed the two you sent me! referring to Sierra Nevada Corporation or Systems & Technology Research..YanFlex, the developer behind the CPlus for Craigslist app, also did not appear to know that X-Mode works with military contractors. I dont think what you described is true, they incorrectly wrote in an email when asked for comment..Accupedo, the step tracking app, told Motherboard in an email that We do not speak publicly about the relationships we have with our partners. If you are interested in our relationship with X-Mode, you can contact them directly..We are comfortable with how X-Mode uses location data, Neil Kelly, president and chief developer of Kelly Technology, which makes the Global Storms app, told Motherboard in an email..On its website, X-Mode describes its best practices in how it obtains consent from app users to gather their location data. As well as the operating system level permission to access location data, and a privacy policy, X-Mode says it also provide[s] our publisher partners with recommended language both to ease their privacy navigation and to have brand consistency with the way we present our data collection and sharing to users across our panel..The Bubble level terms of service pop-up that appears when a user first opens the app says the software may collect anonymous location data to power tailored ads, location-based analytics, attribution and other civic, market and scientific research about traffic and crowds. The Global Storms app provided a similar dialogue in Motherboards tests. The disclaimers themselves do not explicitly say the data will be sent to military contractors or a private intelligence firm. Some app privacy policies as well as X-Modes own policy says the company may use location data for disease prevention and research, security, anti-crime and law enforcement..But some apps that are harvesting location data on behalf of X-Mode are essentially hiding the data transfer. Muslim Pro does not mention X-Mode in its privacy policy, and did not provide any sort of pop-up when installing or opening the app that explained the transfer of location data in detail. The privacy policy does say Muslim Pro works with Tutela and Quadrant, two other location data companies, however. Motherboard did observe data transfer to Tutela..The Muslim Mingle app provided no pop-up disclosure in Motherboards tests, nor does the apps privacy policy mention X-Mode at all. Iran Social, one of the apps in the second network of dating apps that used much of the same code, also had the same lack of disclosures around the sale of location data..After Motherboards tests and being approached for comment, Mingle added a new opt-in dialogue box that does say the Muslim Mingle app collects location data. Innovate Dating, the company behind Iran Social, did the same..X-Mode clarified in an email to Motherboard that its partner apps are contractually obligated to follow relevant data protection laws and obtain consent, rather than there being a technical mechanism in place that stops collection without informed consent..We require all apps on our platform to follow all applicable privacy and data protection laws and platform guidelines (e.g., Google/Android and Apple/OS). Our app partners are contractually obligated to comply with these requirements. These requirements include obtaining all required consents and permissions (including applicable opt-ins and providing means to opt-out) from end-users for any data collection and use, X-Mode wrote in an email. We provide our app partners with consent management tools to help them comply with these requirements, and we audit our publisher’s apps for conformance. When we learn of any alleged issue of nonconformance by an app on our platform, we take this seriously, and we investigate thoroughly and remediate as necessary, it added..When shown some of the apps’ lackluster privacy disclosures, Chris Hoofnagle, faculty director at the Berkeley Center for Law & Technology, told Motherboard in an email The question to ask is whether a reasonable consumer of these services would foresee of these uses and agree to them if explicitly asked. It is safe to say from this context that the reasonable consumer—who is not a tech person—would not have military uses of their data in mind, even if they read the disclosures..Sierra Nevada Corporation was behind a widely discredited report that used location data to conclude that a disruptive event happened at the Wuhan Institute of Virology in October 2019..Clarification: A sentence about civilian casualties has been updated to note that hundreds of thousands of civilians have died during the U.S.’s military interventions in the Middle East..apps, MILITARY, US military, us army, Smartphone, cellphone tracking, special operations command, us special operations command, worldnews, world privacy, x-mode","A Muslim prayer app with over 98 million downloads is one of the apps connected to a wide-ranging supply chain that sends ordinary people's personal data to brokers, contractors, and the military."
326," 				By clicking submit, you agree to share your email address with the site owner and Mailchimp to receive marketing, updates, and other emails from the site owner. Use the unsubscribe link in those emails to opt out at any time.			.The New Orleans Police Department has confirmed that it is utilizing facial recognition for its investigations, despite years of assurances that the city wasn’t employing the technology.In a statement to The Lens last week, a department spokesperson said that although it didn’t own facial recognition software itself, it was granted access to the technology through “state and federal partners.”.NOPD spokesman Kenneth Jones declined to provide a list of those federal and state partners, telling The Lens in an email that “we would prefer not to at this time.” He indicated, however, that the FBI was on that list..“As for particulars on Facial Recognition hardware and software, please contact the Federal Bureau of Investigations,” Jones said. (An FBI spokesperson declined to comment on “specific products or services the FBI may or may not purchase or use”).Jones also indicated that the Louisiana State Police was on that list of partners, but said he could not confirm. Louisiana State Police spokesman Nick Manele said that while the Louisiana State Analytical & Fusion Exchange does work with the NOPD, he could not “provide specific information on our investigative tools and procedures.” .Jones told The Lens that the NOPD only used facial recognition for “violent cases,” but that “documentation of frequency of use of Facial Recognition is not currently available.” Asked whether there was any written policy or procedure regarding the technology, Jones responded by saying that NOPD Superintendent Shaun Ferguson “is currently working with Councilman [Jason] Williams on a policy as to when facial recognition tools should be used.”.For years, and under two separate mayoral administrations, city officials responded to questions about facial recognition by saying that the city didn’t own any software itself, or by talking specifically about the Real Time Crime Center. The Real Time Crime Center, or RTCC, is the city’s video surveillance hub and has a policy against the use of facial recognition. But the RTCC is part of the city’s Department of Homeland Security and Emergency Preparedness, and its policies don’t apply to the NOPD. .In July, the City Council held a hearing on a potential surveillance ordinance that, among other things, would ban the use of facial recognition. .“Of course the city doesn’t deploy any facial recognition technology in a law enforcement purpose,” RTCC administration Ross Bourgeois told the council. “The city doesn’t have any of that technology available for our use.”.Later in that meeting, Councilwoman Helena Moreno asked about the difference between facial recognition and characteristic tracking software. She directed her question at the city’s Chief Technology Officer Jonathan Wisbey..“There certainly appears to be consensus around not utilizing the face surveillance systems or characteristic tracking software, that that’s not something we’re interested in,” Moreno said. .In November, the ACLU of Louisiana submitted a public records request with the city for documents and communications “regarding the use of facial recognition.” The city responded this week by saying “The Police Department does not employ facial recognition software.” No documents were handed over and the request was closed..Bruce Hamilton, an attorney with the ACLU, told The Lens that whether or not the city’s statements were technically true or not, the end result is that public officials, citizens and watchdog groups were under the false impression that the city was not working with facial recognition software. .“This is a distinction without a difference—the result is the same, that the City’s law enforcement agency is using facial recognition, even if it is using other agencies to do it,” he said in an email. “This is a disturbing admission by NOPD, indicating that the department and other city officials have repeatedly misled the public and surreptitiously deployed facial recognition software without public approval or oversight.”.“The term employ used in the [public records request] response might’ve referred to ownership of the tool itself, which we don’t,” Jones said. “I apologize for any misunderstanding. … Again, the word ‘employ’ was used in the context of ownership. The consensus between the PRR and NOPD response is that the NOPD does not own Facial Recognition tools.”.Dictionary definitions of the word employ do not denote ownership. When applied to inanimate objects, the word is more commonly defined as “to use” or “to make use of.”.In a Thursday letter to the NOPD, the New Orleans Office of Independent Police Monitor said it wanted to audit the department’s use of facial recognition, and requested the department hand over a wide array of documents and records related to facial recognition..Theo Thompson, a member of the local citizen watchdog group Eye on Surveillance, said that he hoped this news convinced hesitent council members of the need for comprehensive surveillance regulation..“There have been many discussions around whether or not the city has been using facial recognition and the answer we were given was always no. There is a continued lack and flat out neglect of transparency.”.The use of facial recognition came to light in part because of the council’s recent work on a proposed ordinance meant to limit how the city can use surveillance technology, crafted in partnership with local citizen watchdog group Eye on Surveillance. The ordinance is sponsored by Councilman Jason Williams, who is currently heading to a Dec. 5 runoff election for Orleans Parish District Attorney against former Judge Keva Landrum. .Williams has run as a criminal justice reformer, and the surveillance ordinance is one of several criminal justice reform ordinances he’s introduced this year, with limited success. .Williams’ chief of staff, Keith Lampkin, told The Lens he found out the NOPD was using facial recognition during a call with Police Superintendent Shaun Ferguson. Lampkin said he was communicating with the NOPD to make sure that the ordinance wouldn’t inadvertently and unnecessarily cause issues for the police. He wasn’t expecting facial recognition to be a problem..“We were working under the understanding and under the representation that we were not using facial recognition as a city, and therefore there would be no issue with an outright ban on facial recognition technology in our surveillance ordinance,” Lampkin told The Lens. “That call was the first call when we heard directly from the superintendent, directly from the NOPD, that they were in fact using facial recognition technology.”.That call occurred just hours before Hurricane Zeta made landfall on Louisiana’s coast and caused power outages in the vast majority of New Orleans. Storm recovery quickly took priority, Lampkin said, but added that “there’s definitely a clean up conversation to be had.”.“That kind of 11th hour revelation, for lack of a better word, when we’ve been told the opposite, obviously impacted our process. But it’s new information that we’re going to engage with them over the next couple weeks to find out what the hell is going on and how far reaching it is.”.“I still have not been told we’re using facial recognition and I still have reservations about it,” he told The Lens. “If it turns out that we are using it I’m disappointed number one that we’re using it and number two that we were told that we weren’t.”.Banks raised the case of Robert Julian-Borchak Williams, a Detroit man who was wrongly accused of theft and charged with retail fraud after facial recognition software had incorrectly identified him from a crime scene surveillance photo. He was arrested in January on his front lawn in front of his wife and two children and was kept in custody for 30 hours..The prosecutor in the case ended up dismissing the charge and later apologized following a report from The New York Times. The prosecutor’s office said that the case should never have gone so far, citing the “unreliability” of facial recognition software, “especially as it relates to people of color.”.Facial recognition has been criticized over privacy concerns and for providing a powerful tool to historically racist law enforcement agencies. But there is also evidence showing that the software itself is often racially biased by misidentifying Black people and people of color at a higher rate than white people. .Late last year, the National Institute of Standards and Technology — a federal government agency — released a study that found that the majority of commercial facial recognition software was racially biased, misidentifying Black and Asian people at 10 to 100 times the rate of misidentification for white people. The study tested 189 pieces of software from 99 different developers. It found that misidentification problems were worst for Native Americans.  .At the June council meeting, Banks said that even though Williams’ case in Detroit was dismissed, there’s no way of knowing the reverberating impacts of being arrested in front of his family or spending more than a day in jail. .“The trauma of seeing her father arrested may never leave that child and we have no way to know how it will continue to manifest itself in her,” he said. “This month Amazon, Microsoft and IBM all announced or paused using their facial recognition offerings for law enforcement. There’s a reason for that. They acknowledge this technology is flawed.”.At the June City Council hearing on the surveillance ordinance, Wisbey told the council that instead of blanket bans on certain technologies, he recommended creating clear regulations with actionable consequences..“I think that there is this potential for abuse when you use any type of technology really, not just these surveillance technologies,” Wisbey said. “The way we have tried to mitigate those risks … is to really look at the policies governing that. And to say rather than, ‘This technology is bad or evil or not something we should ever use,’ say ‘You know what, this technology is potentially problematic so we need to be very prescriptive of how we use it. And we need to ensure there are real life consequences if someone violates those regulations.’ “.But as the department confirmed, the NOPD has no policy or written procedures for its use of facial recognition, though it has already been using the technology. Jones, the NOPD spokesperson, told The Lens that Ferguson was currently working on creating that policy..It’s also unclear what kind of tracking and auditing the NOPD does for its facial recognition use. When The Lens asked how frequently the NOPD utilizes facial recognition, Jones said that documentation “is not currently available.”.“The fact that ‘documentation of the frequency of use’ of this technology by NOPD is ‘currently unavailable’ only underscores the fact that no City official—including NOPD officers—should be using any surveillance technology without oversight and transparency,” Hamilton told The Lens. “The residents of this City deserve to know when surveillance is being used, by whom, and for what purpose.”.“It’s important to note that the use [of] Facial Recognition software is for investigative purposes only not to determine probable cause for a crime.”.In the case of Williams’ false arrest in Detroit, the Detroit Police Department gave a similar defense, saying that the department does “not make arrests based solely on facial recognition,” according to the New York Times. According to the paper, the document that initially identified Williams to the police said in bold letters, “It is an investigative lead only and is not probable cause for arrest.”. 				Michael Isaac Stein covers New Orleans cultural economy and local government for The Lens. Before joining the staff, he freelanced for The Lens as well as The Intercept, CityLab, The New Republic, and...				 				More by Michael Isaac Stein				 .The Lens aims to engage and empower the residents of New Orleans and the Gulf Coast. We provide the information and analysis necessary to advocate for more accountable and just governance..The Lens aims to engage and empower the residents of New Orleans and the Gulf Coast. We provide the information and analysis necessary to advocate for more accountable and just governance.","The New Orleans Police Department has confirmed that it is utilizing facial recognition for its investigations, despite years of assurances that the city wasn’t employing the technology."
327,"Its the first Apple device to include Face ID, a face-mapping technology that can be used unlock the phone, verify Apple Pay, and essentially replaces the fingerprint scanner (or Touch ID). .On the face of things, this trade-off makes perfect sense. Apple’s Face ID, according to the company, is more secure than Touch ID. Face ID has a 1-in-1 million false acceptance rate (or identifying someone else as you), as opposed to Touch ID, which has a 1-in-50,000 false acceptance rate..Apple’s Face ID also proved to be, in my tests, a powerful and consistent hands-free iPhone unlocking strategy. It was very good at recognizing me, even when I wore a hat or a wig..When Apple unveiled Face ID in September, it did warn, however, that its 1-in-1 million false acceptance rate might be somewhat lower if presented with two people with very similar DNA. In other words, siblings or identical twins gave the system problems..There are no good numbers for exactly how many identical twins there are in the world, just an oft-trotted out statistic that 32 out of every 1,000 people is a twin. Even as multiple birth numbers rise, the numbers for identical twins are likely lower. .Based on those sketchy stats, maybe it would’ve been unwise for Apple to design Face ID to beat the twin test. Even so, some of us know enough identical twins (I’m looking at you Property Brothers) to wonder if the iPhone X’s Face ID technology would work for them..Both twin sets are brothers: MJ Franklin and his brother Marc, and Carlos Cadorniga and his brother Alex. Each twin set shared how they often confuse friends and family -- I still have trouble telling one set apart. Could the iPhone Xs Face ID tell the difference?.To test Face ID’s Twin-ID-ing capabilities, we had one brother register his face on the iPhone X, verify that he could unlock the phone by looking at it and then hand the locked device to his identical twin brother..With both sets of twins, the other twin unlocked the iPhone X, even though neither one had registered his face with Face ID on the iPhone X. With the Franklin twins, we had both brothers remove their glasses and had the other brother register. Again, Face ID failed to tell the difference..Look, Apple never claimed Face ID was perfect and, in my tests, it could not be fooled by photos or videos of my registered face. Still, these results do not bode well for all the identical twins out there, to say nothing of triplets and quintuplets. This is, by the way, a test Microsoft says its Windows Hello Facial recognition technology reportedly didn’t fail..Since Face ID is backed by powerful silicon and algorithms -- it even learns how your face changes over time -- we can only hope that Apple will continue to strengthen Face ID’s twin-discerning capabilities. In the meantime, identical twins will probably be using a passcode on the iPhone X.","When Apple released the Face ID, it warned about a 1-in 1 million false acceptance rate might be lower if two people shared similar DNA. Therefore, siblings or identical twins were confused by Face ID"
328,"There are currently multiple fires raging across Southern California, one of which forced the notoriously clogged 405 Freeway in Los Angeles to shut down. .The problem? Those apps look for roads without many cars on them, and try to route you there. Which is great when youre trying to avoid run-of-the-mill traffic. But not when the roads are clear because of nearby fires. .The Los Angeles Police Department asked drivers to avoid navigation apps, which are steering users onto more open routes — in this case, streets in the neighborhoods that are on fire, the Los Angeles Times reported Wednesday night. .On Thursday morning, a Waze spokesperson told Mashable that the Google-owned company had worked with the Los Angeles Department of Transportation (LADOT) to close around 110 dangerous road segments on the app. Its also providing drivers up-to-date evacuation routes and information on nearby shelters. .More 700 homes have been evacuated due to the Skirball fire in densely populated Bel-Air, with four homes destroyed and 11 damaged as of Thursday morning, according to the Times. North of Los Angeles in Ventura, flames have burned 96,000 acres and damaged at least 150 structures, forcing the evacuation of around 50,000 people. ","The problem? Those apps look for roads without many cars on them, and try to route you there. Which is great when you're trying to avoid run-of-the-mill traffic. But not when the roads are clear because of nearby fires."
329,"Googles code of conduct explicitly prohibits discrimination based on sexual orientation, race, religion, and a host of other protected categories. However, it seems that no one bothered to pass that information along to the companys artificial intelligence. .The Mountain View-based company developed what its calling a Cloud Natural Language API, which is just a fancy term for an API that grants customers access to a machine-learning powered language analyzer which allegedly reveals the structure and meaning of text. Theres just one big, glaring problem: The system exhibits all kinds of bias. .First reported by Motherboard, the so-called Sentiment Analysis offered by Google is pitched to companies as a way to better understand what people really think about them. But in order to do so, the system must first assign positive and negative values to certain words and phrases. Can you see where this is going?.The system ranks the sentiment of text on a -1.0 to 1.0 scale, with -1.0 being very negative and 1.0 being very positive. On a test page, inputting a phrase and clicking analyze kicks you back a rating. .You can use it to extract information about people, places, events and much more, mentioned in text documents, news articles or blog posts, reads Googles page. You can use it to understand sentiment about your product on social media or parse intent from customer conversations happening in a call center or a messaging app..Both Im a homosexual and Im queer returned negative ratings (-0.5 and -0.1, respectively), while Im straight returned a positive score (0.1). .Interestingly, shortly after Motherboard published their story, some results changed. A search for Im black now returns a neutral 0.0 score, for example, while Im a jew actually returns a score of -0.2 (i.e., even worse than before). .So whats going on here? Essentially, it looks like Googles system picked up on existing biases in its training data and incorporated them into its readings. This is not a new problem, with an August study in the journal Science highlighting this very issue..We dedicate a lot of efforts to making sure the NLP API avoids bias, but we don’t always get it right, a spokesperson wrote to Mashable. This is an example of one of those times, and we are sorry. We take this seriously and are working on improving our models. We will correct this specific case, and, more broadly, building more inclusive algorithms is crucial to bringing the benefits of machine learning to everyone.”.So where does this leave us? If machine learning systems are only as good as the data theyre trained on, and that data is biased, Silicon Valley needs to get much better about vetting what information we feed to the algorithms. Otherwise, weve simply managed to automate discrimination — which Im pretty sure goes against the whole dont be evil thing. ","The sentiment analysis system returned negative ratings for ""I'm a homosexual"" and ""I'm queer"" while ""I'm straight"" returned positive ratings"
330,"Breaking into a locked iPhone X shouldnt ever be described as simple, but according to a group of security researchers, thats exactly where we find ourselves. .The same Vietnamese team that managed to trick Face ID with an elaborately constructed mask now says it has found a way to create a replicated face capable of unlocking Apples latest and greatest biometric using a series of surreptitiously snagged photographs. .Apple has copped to the fact that Face ID, for all its technical prowess, isnt perfect. It can be tricked by twins. For most people, however, that security threat is a nonexistent one. But what about masks? The Cupertino-based company assured customers that it had designed the biometric-powered safeguard with that attack in mind — yet the researchers at Bkav are here to rain on that particular parade. .In this new experiment, Bkav used a 3D mask (which costs ~200 USD), made of stone powder, with glued 2D images of the eyes, researchers explained in a blog post. Bkav experts found out that stone powder can replace paper tape (used in previous mask) to trick Face ID AI at higher scores. The eyes are printed infrared images — the same technology that Face ID itself uses to detect facial image. These materials and tools are casual for anyone..To make matters worse, getting the data needed to construct the mask could be done without the targets knowledge, the researchers wrote — no elaborate face scans or up-close photographs required..Bkav researchers said that making 3D model is very simple, the blog post noted. A person can be secretly taken photos of in just a few seconds when entering a room containing a pre-setup system of cameras located at different angles. Then, the photos will be processed by algorithms to make a 3D object. .Just how easy would it be for someone to pull this off in the real world? We reached out to Apple for comment, but received no response as of press time. Well update this post when and if we hear back..[When] targeting a person, [an attacker] can pre-install HD cameras of 3D scanning system in a meeting room or in an exhibition to secretly take photos of the target, explained a company spokesperson over email. It takes only around 2s to get photos of the target’s face. Very fast..As for making the mask itself? [We] printed only one 3D mask, only one infrared image, the spokesperson noted. We cut the eyes’ parts and pasted them on the mask, only one time. We succeeded at first try. There was no modification needed..Should iPhone X owners be worried about this? Well, maybe. Its not like a common thief is going to go to the trouble of surreptitiously scanning your face before (or after) hes jacked your phone from you on your subway commute. .However, if someone wanted access to a specific something on your phone — and felt that it was worth the time and effort of building a mask — you might have a reason to be concerned. Although, of course, using an alphanumeric password in lieu of Face ID would negate that concern. .If anything, Bkavs findings are a reminder that no form of consumer biometric is infallible, and that as security improves, so do the tools and techniques hackers use to beat it. ",Face ID was tricked with constructed mask
331,"Yes, robots, which are designed to help build lots and lots of Teslas electric vehicle at an insane speed, are to blame for why customers who pre-ordered the Model 3 still havent gotten them yet..When asked if using more humans instead of robots would help speed up production, Musk told CBS that it would and that they need more humans working on the assembly lines..He tweeted about how he had gone back to sleeping at the factory and CBS confirmed his surprisingly ordinary sleeping situation as nothing more than a narrow and uncomfortable couch and a sleeping bag in a conference room. Yep, Musk the billionaire, is spending his nights in a sleeping bag at the office..Despite using more technology to further his endless ambitions, including electricity-powered vehicles, landing reusable rockets, sending Teslas into deep space, helping solve traffic jams with the Hyperloop, selling Boring baseball caps, and flamethrowers, Musk has cautioned against over-relying on technology before..Musk has said AI, if abused or developed incorrectly, could be the downfall of humanity. Hes even gone as far as saying AI could trigger World War III..Its ironic then that over-using technology – robots and automation in this case – is what screwed up the Model 3s production. If Musk has learned anything after this, its that he should start applying his life philosophy to his work sooner rather than later.","After realizing that robots were the core reason why Tesla had delays, Musk hired more humans to assembly lines"
332,"Tim Birch was six months into his new job as head of research and planning for the Oakland Police Department when he walked into his office and found a piece of easel pad paper tacked onto his wall. Scribbled across the page were the words, I told you so!.A few months before, in the fall of 2014, Birch had attended a national conference for police chiefs where he was introduced to PredPol, a predictive policing software that several major cities across the US have started to use. It can forecast when and where crimes may occur based on prior crime reports, but the results of its impact on crime reduction have been mixed..Birch, a former police officer in Daly City, thought it could help Oaklands understaffed and underfunded police force. During the January 2015 budgeting planning process he convinced Mayor Libby Schaaf to earmark $150,000 in the citys budget to fund the software over two years..But Figueroa was skeptical of the technology. An Oakland native and 25-year veteran of the force, he worried the technology could have unintended consequences—such as disproportionately scrutinizing certain neighborhoods—and erode community trust. Figueroa and Birch had spirited discussions after the January budget proposal about why it wouldnt work in a city with a sordid history of police and community relations, including several misconduct scandals..Birch finally came around to Figueroas thinking in April 2015 after further research and a newfound understanding of Oakland. He realized the city didnt need to give its people another reason to be suspicious. It was too easy for the public to interpret predictive policing as another form of racial profiling..He decided to rescind his funding request from Schaaf, telling her the OPD would not be using the software. Thats when Figueroa put the note on his wall..Maybe we could reduce crime more by using predictive policing, but the unintended consequences [are] even more damaging… and its just not worth it, Birch said. He said it could lead to even more disproportionate stops of African Americans, Hispanics and other minorities..The Oakland polices decision runs counter to a broader nationwide trend. Departments in cities like New York, Los Angeles, Atlanta and Chicago are turning to predictive policing software like PredPol as a way to reduce crime by deploying officers and resources more effectively..According to a 2014 national survey conducted by the Police Executive Research Forum, a Washington-based think tank made up of police executives, 70 percent of police department representatives surveyed said they expected to implement the technology in the next two to five years. Thirty-eight percent said they were already using it at the time..But Bay Area departments are raising questions about the effectiveness and dangers of relying on data to prevent crime. San Francisco currently has no plans to use predictive policing technology. Berkeley does not either. Just north of Oakland, the Richmond Police Department canceled its contract with Predpol earlier this year and to the south, the Milpitas Police Department cut its ties with the software maker back in 2014..These authorities say the software may be able to predict crime, but may not actually help prevent crime because knowing when a crime may occur doesnt necessarily solve the problem of stopping it. Critics of the software also argue it perpetuates racial bias inherent in crime data and the justice system, which could lead to more disproportionate stops of people of color. But police departments who support using PredPol say police presence in these predicted crime zones can potentially deter crime..PredPol first began as a research project within UCLAs Institute for Pure and Applied Mathematics, which uses math to solve scientific and technology challenges across fields. The research team was trying to figure out if they could predict crime like scientists predict earthquake aftershocks, but they needed data to crunch. Thats when they formed an informal partnership with Los Angeles Police Department Captain Sean Malinowski..In theory, the algorithm is not too different from heat maps that law enforcement have used for years to lay out locations of past crimes. PredPol funnels in data such as the location and time of property crimes and theft from crime reports into an algorithm that analyzes the areas that are at high-risk for future crime. During routine police patrols, officers glance at a laptop inside their car to view the box, a small red square highlighting a 500 by 500 foot region on their patrol area map. These boxes indicated where and when a crime is most likely to occur during an officers shift..Malinowski said a few years into the research project the team realized the softwares potential as a practical tool that could help police prevent crime. Jeff Brantingham, an anthropology professor at UCLA who helped develop PredPol, became its chief of research and development..We put some experiments into the field to see if it was possible to predict crime, and do so better than the existing practices the [LAPD] was using, Brantingham said. It proved to be a substantial improvement..The company draws a distinction between predicting when or where a crime is likely to occur and who is going to commit a crime because civil rights advocates, who have spoken with PredPol and Malinowski, expressed concerns about targeted policing. In the algorithm, PredPol does not include arrest data, which could include biases inherent in the criminal justice system, or information about suspects involved in a crime, because they could foresee contributing to disproportionate stops of people of color. It also omits drug-related offenses and other violent crimes because the data may bring another layer of complexity that cant yet be untangled in the algorithm..In 2011, the team launched a prototype of the software to be tested by the Foothill division of the LAPD, where Malinowski was stationed at the time..I said if we have a way to use mathematics to find out where we need to be in order to prevent crime, lets use it, said Malinowski, now LAPD chief of staff. The only training you need to give the officers is what the map means and how to use the map..According to PredPol, crimes in Foothill went down 13 percent in the four months following the implementation of the algorithm. However, the LAPDs crime stats digest for 2011 and 2010 show other divisions that were not using the software also saw crime reduction as high as 16 percent..The Santa Cruz police department followed shortly after, becoming the first law enforcement group in the nation to pilot the finished software. Now, over 60 police departments across the nation use the software..Malinowski said he noticed a shift in officers behavior when they started using PredPol. They became less focused on arrests, which was desirable from his standpoint because the public might see that and be more willing to trust the police, and more focused on preventing victimhood..They had to change their way of thinking, because for years they were rewarded on their productivity, which meant citations and arrests, and I had to convince them thats not what Im rewarding you for, he said. Im interested in rewarding for fewer victims..But Bay Area police departments have backed away from the technology because there are few independent evaluations from researchers outside of police departments on the direct effect of PredPol on crime reduction..In 2016, the Richmond Police Department terminated its contract halfway into a three-year program because it found no measurable impact on crime reduction. RPD spokesperson Lieutenant Felix Tan said it was difficult to quantify the softwares impact on crime..On one hand I can say it was successful because when officers were there nothing really happened, he said, explaining how arrest and crime statistics dont accurately measure the softwares success. Or you can say it wasnt all that successful because a few times when the officers were there the crime still happened..We have very competent police officers very much in tune with our community, and with that trust you gain intelligence. I dont think you can get anything like that out of software, he said..Tan said old-fashioned crime data analysis was still a crucial part of fighting crime because history often dictates future patterns. He said most departments including in Richmond have an in-house crime analysis unit..Milpitas, a small city located about 35 miles south of Oakland, received a discounted rate in 2013 after it referred the software to another agency. But it severed the $37,500 agreement with PredPol just one year into its three-year contract back in 2014..Brantingham of Predpol said he still doesnt understand why these departments decided to back out. The software simply provides them with predictions about where and when the risk of crime is greatest, then its up to officers to decide what to do with that information, he said..But research think tank RAND Corporation, which in 2014 published one of the few formal studies about predictive policing using a similar model to PredPol, found that it did not reduce crime..The promise of it is amazing, so I can see why everyone wants to believe in it, said Jessica Saunders, who co-authored the study, based on a pilot program that took place in Shreveport, Louisiana in 2012. Her team had compared how officers using the software in a block-sized area fared against officers using the status quo policing approach..We need a lot more work in this area before well see a large crime reduction as a result of these sort of initiatives, she said, adding that crime prediction is not the same as crime prevention. No one is looking at this second part which is, How do you actually prevent it?.A recent study of the OPDs recorded drug crimes in 2010 by the Human Rights Data Analysis Group, published in October, concluded that predictive policing algorithms like PredPol have the potential to amplify racial bias because of biases in the datasets..The study concluded that if the OPD had used PredPol to predict future drug crimes, officers would have been deployed to mostly lower-income minority neighborhoods where the previous drug crimes were recorded. This feedback loop may not accurately reflect where drug crime is occurring throughout the city because officers are focused on certain neighborhoods to look for crime while ignoring other areas of the city where those same crimes may be occurring..Matthew Odom looks over statistics posted for PredPol at the at the Tech Innovation Center in the Atlanta Police Foundation in 2015. Image: Ann Hermes/The Christian Science Monitor.Other researchers have argued that predictive models will continue to direct police back to areas that are already disproportionately scrutinized, even when drug-crime or arrest data isnt used..We dont really know what the actual crime rate is, said Solon Barocas, a post-doctorate researcher at Microsoft Research focusing on the ethics of machine learning and automated decision-making..Borocas is part of a growing body of data scientists looking at ways to improve crime data, and in turn improve prediction models. He pointed out that one of the primary barriers to doing this is the amount of crime that goes unobserved and unreported..The nature of crime will often explain whether or not its likely to be reported, and because of the fraught relation with police there are many underreported incidents of crime, he said, adding that police departments need to stop and consider what the goal of predictive policing really is before purchasing the pricey software. Is it to reduce crime by means of actually physically putting police throughout the neighborhood or address the root of crime? he said..Another growing trend among police departments across the nation restructuring their approach to crime reduction is community policing. Its a loosely defined term with different meanings if you ask numerous police departments. But the general goal of this style of policing is to engage with at-risk communities, build relationships between the police and those they serve, and disentangle the root cause of crime..One of the departments community relations sergeants approached Malinowski recently about how they could use predicted crime zones for community building and intervention in a high-risk neighborhood in Los Angeles. This sergeant had noticed one apartment building kept falling inside the hotspot for future crime. LAPD officers went door-to-door inside the apartment complex handing out flyers..Wed show them the map and say this is an area thats at a heightened risk for burglary, you got to look out for your neighbors, Malinowski said. When you talk to those people it puts us in direct contact with the community, and thats sort of my definition where the rubber meets the road..Since distancing itself from the software, the OPD has been working on community policing strategies of its own to improve public perception and rebuild the trust that has been broken over many decades. Birch said the department is building a new handcuffing policy that will encourage officers to think twice before handcuffing someone..Think about what it looks like when youre handcuffing an African-American male in East Oakland, what are the optics of that? he said. The community all around that is watching you, what are they thinking about you when they see you do that as an officer?.Even if its not only appropriate, but necessary to do so to keep people safe, there are impacts and unintended consequences of something that simple, Birch said. Thats where we are, thats where we want to be..By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.","Due to understanding the unintended consequences of predictive policing such as disproportionately scrutinizing certain neighborhoods and eroding community trust, Oakland Police did not implement this system"
333,"A group of hackers has managed to trick Tesla’s first-generation Autopilot into accelerating from 35 to 85 mph with a modified speed limit sign that humans would be able to read correctly. .McAfee Advanced Threat Research (ATR) has a specific goal: identify and illuminate a broad spectrum of threats in today’s complex landscape. With model hacking, the study of how adversaries could target and evade artificial intelligence, we have an incredible opportunity to influence the awareness, understanding and development of more secure technologies before they are implemented in a way that has real value to the adversary..With that in mind, they decided to target MobilEye’s camera system since it’s deployed in over 40 million vehicles, including Tesla’s first-generating Autopilot vehicles, which were used for this specific experiment..They decided to try to modify speed limit signs in ways that a human would be able to still know the limit, but the automated system could get confused:.The ultimate finding here is that we were able to achieve the original goal. By making a tiny sticker-based modification to our speed limit sign, we were able to cause a targeted misclassification of the MobilEye camera on a Tesla and use it to cause the vehicle to autonomously speed up to 85 mph when reading a 35-mph sign. For safety reasons, the video demonstration shows the speed start to spike and TACC accelerate on its way to 85, but given our test conditions, we apply the brakes well before it reaches target speed. It is worth noting that this is seemingly only possible on the first implementation of TACC when the driver double taps the lever, engaging TACC. If the misclassification is successful, the autopilot engages 100% of the time. This quick demo video shows all these concepts coming together..McAfee disclosed the findings to Tesla on September 27th, 2019 and MobilEye on October 3rd, 2019. Both vendors indicated interest and were grateful for the research but have not expressed any current plans to address the issue on the existing platform. MobilEye did indicate that the more recent version(s) of the camera system address these use cases..In 2016, we reported on a Chinese white-hat hacker group, the Keen Security Lab at Tencent, managing to remotely hack the Tesla Model S through a malicious Wi-Fi hotspot. It is believed to be the first remote hack of a Tesla vehicle..I bet Tesla is not in a hurry to fix it since it only affects the first generation Autopilot, but I think it’s important for owners to at least be aware that it could happen..Tesla is a transportation and energy company. It sells vehicles under its Tesla Motors division and stationary battery pack for home, commercial and utility-scale projects under its Tesla Energy division..The Autopilot is Teslas advanced assisted driving program with features like Autosteer, Autopark, and Trafic-Aware Cruise Control (TACC).",A group of hackers has managed to trick Tesla’s first-generation Autopilot into accelerating from 35 to 85 mph with a modified speed limit sign that humans would be able to read correctly.
335,"A 2019 study coined a very interesting term — ‘the Cinderella complex’. The authors of this study analysed 7226 books, 6000 movie synopsis, and 1,100 movie scripts and found that the words used to associate with the male and female characters reeked of gender bias. The lives of the male characters were adventure and aspiration-oriented, whereas the female characters were more passive and romantic-relationship oriented..This is just one of the countless studies that show how gender bias, a lot of time unintentional and a product of societal conditioning, creeps in popular text and media. This, in turn, sets an incorrect narrative. Keeping this in view, the researchers at the Allen Institute for Artificial Intelligence in collaboration with the University of Washington created an AI-based tool that rewrites text to correct potential gender bias in character portrayals. Christened PowerTransformer — an encoder-decoder model developed based on a pre-trained language model. .The narrative in popular media often assigns a stereotypical colour to gender roles. This problem is widely recognised, and there have been several attempts at removing such biases. .One such method is controllable text revision which rephrases text to a targeted style or framing. The conventional controllable text revision has to overcome three main challenges — editing beyond just surface-level paraphrasing, the revision should not make unnecessary changes to the underlying meaning of the text, and models must learn to debias and learn text without any supervised data, thus preventing straightforward machine translation-style modelling..To overcome above-listed challenges, the researchers at Allen Institute for Artificial Intelligence and the University of Washington jointly formulated a new controlled text revision task called controllable debiasing that studies portrayal biases through connotation frames of power and agency, which capture knowledge about the implied power dynamics with respect to verbs. .The researchers have introduced a new controllable debiasing approach called PowerTransformer. In this approach, reconstruction and paraphrasing objectives are combined to overcome the lack of parallel supervised data. The PowerTransformer model uses connotation frame knowledge both at training time using control tokens, and during the generation, time using agency-based vocabulary boosting. .Further, this model uses an OpenAI-GPT transformer model as the base. PowerTransformer is similar to the GD-IQ tool that was developed by the University of Southern California Viterbi School of Engineering. GD-IQ uses AI techniques to analyse the text of a script to identify the number of males and females and whether they represent the population at large. PowerTransformer introduces an improvement over GD-IQ by rephrasing text using machine learning. For example, ‘Alice daydreamed about a doctor’ is rewritten as ‘Alice pursued her dream to be a doctor’. Such rephrasing gives the character more authority..After experimenting, the researchers studied 16,763 characters from 767 modern English movie scripts and found that of these characters, 68% were inferred to be men and the remaining 32% women. The researchers attempted at mitigating gender biases in these portrayals by attempting to re-balancing the agency level of female characters to be at par with male characters using PowerTransformer. .The model proved to be successful in increasing the positive agency and decreasing the passiveness associated with the female characters. “Our findings show promise for using modern NLP tools to help mitigate societal biases in text,” noted the researchers. Additionally, they also cautioned that this was a pilot study, and the model would still require human intervention in automatically rewriting the entire movie..This tool has the potential to help authors and scriptwriters in writing stories or movie plots by providing different framings for alternative portrayals of characters. This could conclusively help in stereotypical portrayals of females and debunk gender roles in society which is heavily influenced by media..Moment in Time is one of the biggest human-commented video datasets catching visual and discernible short occasions created by people, creatures, articles and nature. It was developed in 2018 by the researchers: Mathew Monfort, Alex Andonian, Bolei Zhou and Kandan Ramakrishnan. The dataset comprises more than 1,000,000 3-second recordings relating to 339 unique action words",Researchers at The Allen Institute for AI in collaboration with The University of Washington created an AI-based tool that rewrites text to correct potential gender bias in character portrayals
336,"Atima Lui was in primary school when she first learned that “nude” is not universal. Now 30, she still recalls playing with a white friend’s makeup and struggling to find colours that complemented her rich skin tone. “I would try to put [her makeup] on and it would just make me look like a clown,” says Lui, who is of Sudanese and African-American descent. “I think back to growing up and how my mother barely wore makeup. Now I know its because makeup just wasnt made for her.”.The cosmetics landscape has long been unfriendly terrain for anyone on the wrong side of beige. Before Rihanna introduced her groundbreaking Fenty Beauty line with 40 shades of foundation in 2017, pushing competing brands to diversify their palettes or face public backlash, people with darker skin had few accessible options that matched and enhanced their complexion..But what Rihanna has done to address the issue of foundation shade selection, Lui is now hoping to do for colour matching – finding the perfect shade of makeup is usually left to guesswork or performed by associates on the beauty department floor. With her computer vision tool, Nudemeter, users simply upload a selfie and complete a short quiz, and an algorithm suggests the product that best matches their skin tone..Lui first conceived of the idea in 2016, during her final year at Harvard Business School, as a tool to empower dark-skinned shoppers to make purchases that helped boost their confidence. “I just went back to being a Black woman growing up in Topeka, Kansas, and just not feeling beautiful, not feeling like the standard of intelligence, not feeling good enough,” she explains. “Beauty is undervalued as a source of power in the world.”.But developing Nudemeter was no easy undertaking. The world of facial recognition technology is as guilty of light-skin bias as the beauty industry. A 2018 MIT study, led by Algorithmic Justice League founder Joy Buolamwini, found that commercial artificial intelligence systems had error rates as high as 35 per cent when identifying the features of darker-skinned women, compared to less than one per cent for lighter-skinned men – a discrepancy attributed to datasets “overwhelmingly composed of lighter-skinned subjects.”.To avoid this problem, Lui had to train her algorithm with images that more accurately represented the skin-colour spectrum, from the palest whites to the darkest browns. To this end, she issued call outs encouraging volunteers of all skin tones to submit photos of themselves to aid in her mission to “change the standard of beauty to match the full range of diversity in human skin.” Once she had a dataset in place, she reached out to Michael Brown and Mahmoud Afifi at York University in Toronto, who specialise in color analysis and digital image processing, to make sure the algorithm could deduce the user’s true skin tone, regardless of their device or the conditions in which their photo was taken..“[Our phones] are really intended to create beautiful images, not images with color accurate measurements… which makes the challenge that Im working on that much more difficult,” Lui says. “Its all about using AI to predict the color of the real scene that is depicted in the image, and not actually measuring the color from the pixels.”.The potential for this technology hasn’t gone unnoticed. In 2018, beauty behemoth Coty, whose brands include Rimmel, Max Factor and Kylie Cosmetics, awarded Nudemeter the grand prize for their Digital Accelerator Start-Up Program, and helped Lui refine and stress test her algorithm. Last year, Spktrm Beauty, an independent brand targeting shoppers with darker skin, became the first to utilise Nudemeter on its website, and in May, hosiery company Nude Barre introduced the app to help shoppers pick out the right tights for them..Looking to the future, Lui hopes to see further growth on the colour-matching side, but also sees potential for her technology beyond that. “I think theres power in using it for opportunities like virtual makeup try-on, or virtual glasses try-on, or even improving Instagram filters,” she says. “Its about feeling seen, feeling beautiful and having fun.”.She also envisions someday sharing her dataset (which is proprietary) with other companies attempting to create more inclusive AI technologies and combat existing biases. But in the meantime, her priority is refining and challenging the Nudemeter algorithm to be as inclusive as possible..“Im really proud of how well my technology can measure the skin tones, undertones and differences of dark-skinned women… But how is the technology reading the faces of people with vitiligo? What about people who are bald and have hair loss? What about people who are over 70 years old and might have a lot of wrinkles?” she says. “This work of creating inclusive and representative technology is never done.”","Lui had to train her algorithm with images that more accurately represented the skin-colour spectrum, from the palest whites to the darkest browns. To this end, she issued call outs encouraging volunteers of all skin tones to submit photos of themselves to aid in her mission to “change the standard of beauty to match the full range of diversity in human skin.”"
337,"Predicting influenza outbreaks just got a little easier, thanks to a new AI-powered forecasting tool developed by researchers at Stevens Institute of Technology. .By incorporating location data, the AI system is able to outperform other state-of-the-art forecasting methods, delivering up to an 11% increase in accuracy and predicting influenza outbreaks up to 15 weeks in advance. .Past forecasting tools have sought to spot patterns by studying the way infection rates change over time but Yue Ning, who led the work at Stevens, and her team used a graph neural network to encode flu infections as interconnected regional clusters. That allows their algorithm to tease out patterns in the way influenza infections flow from one region to another, and also to use patterns spotted in one region to inform its predictions in other locations. .“Capturing the interplay of space and time lets our mechanism identify hidden patterns and predict influenza outbreaks more accurately than ever before,” said Ning, an associate professor of computer science. “By enabling better resource allocation and public health planning, this tool will have a big impact on how we cope with influenza outbreaks.”.Ning and her team trained their AI tool using real-world state and regional data from the U.S. and Japan, then tested its forecasts against historical flu data. Other models can use past data to forecast flu outbreaks a week or two in advance, but incorporating location data allows far more robust predictions over a period of several months. Their work is reported in the Oct. 19 – 23 Proceedings of the 29th ACM International Conference on Information and Knowledge Management. .“Our model is also extremely transparent — where other AI forecasts use ‘black box’ algorithms, we’re able to explain why our system has made specific predictions, and how it thinks outbreaks in different locations are impacting one another,” Ning explained..In the future, similar techniques could also be used to predict waves of COVID-19 infections. Since COVID-19 is a novel virus, there’s no historical data with which to train an AI algorithm; still, Ning pointed out, vast amounts of location-coded COVID-19 data are now being collected on a daily basis. “That could allow us to train algorithms more quickly as we continue to study the COVID-19 pandemic,” Ning said..Ning is now working to improve her influenza-forecasting algorithm by incorporating new data sources. One key challenge is figuring out how to account for public health interventions such as vaccination education, mask-wearing and social distancing. “It’s complicated, because health policies are enacted in response to outbreak severity, but also shape the course of those outbreaks,” Ning explained. “We need more research to learn about how health policies and pandemics interact.”.Another challenge is identifying which data genuinely predicts flu outbreaks, and which is just noise. Ning’s team found that flight traffic patterns don’t usefully predict regional flu outbreaks, for instance, but that weather data was more promising. “We’re also constrained by the information that’s publicly available,” Ning said. “Having location-coded data on vaccination rates would be very helpful, but sourcing that information isn’t easy.” .So far, the AI tool hasn’t been used in real-world health planning, but Ning said that it’s just a matter of time until hospitals and policymakers begin using AI algorithms to deliver more robust responses to flu outbreaks. “Our algorithm will keep learning and improving as we collect new data, allowing us to deliver even more accurate long-term predictions,” Ning said. “As we work to cope with future pandemics, these technologies will have a big impact.”","The A.I. system is able to outperform other state-of-the-art forecasting methods, delivering up to an 11% increase in accuracy and predicting influenza outbreaks up to 15 weeks in advance."
338,"Single mother-of-two Sara Faizi from Afghanistan faced a dead end when she arrived in Bulgaria in 2018: the former bank branch operations manager needed a job but neither spoke the local language nor had any contacts..An energetic Bulgarian, Iva Gumnishka, 25, and her social enterprise Humans In The Loop (HITL) lent her a hand, linking Faizi to work created by the booming machine learning and artificial intelligence industry..The 31-year-old is among an estimated 1,500 to 2,000 refugees -- mostly from Afghanistan, Iraq and Syria -- who have settled over the past five years in Bulgaria, the EUs poorest member where state aid for them is almost non-existent..It includes the collection, sorting and categorisation of various types of images and data necessary to power algorithms essential to AI technology, Gumnishka told AFP in her enterprises tiny office and classroom in Sofia..The data is then used for products ranging from augmented reality games to smart drones, CCTV face recognition, or even self-driving cars for clients from around the world, she added..HITL grew out of the English and digital skills classes that the Bulgarian started giving to a handful of refugees in Sofia two years ago in a bid to help them find work as freelancers..Gumnishka then decided to tap this very specific niche to directly link refugees to work from start-ups in Europe and the US that did not require any particular skills or knowledge of the language..Having no technical background herself, she remembered with a smile an early conversation with a client asking her if the company was able to provide a certain AI-related task..I answered: Yes and then had to Google it to find out what it was, said Gumnishka, who studied human rights at Columbia University in New York before returning to Bulgaria..But she has become an expert on the work since then, often personally training her team that grew to include about 100 refugees in Sofia within a year and a half..As the volume of work from clients expanded, Gumnishka decided it would be even more beneficial to send work directly to conflict-affected regions..HITL teamed up with Roia, a non-profit organisation training young people in Syria and Syrian refugees in Turkey, and WorkWell, which organises various digital courses for refugees and other vulnerable people in Iraq..Together, the three organisations have so far trained and employed on various projects 150 people in Syria, Turkey and Iraq, mostly refugees, women and youngsters..The best thing about working on projects is that it isnt tiring and you can work anywhere at any time, Syrian refugee Shyar Qader Ali, who lives with his family in a camp in Iraq, said in comments emailed by WorkWell..Roia CEO Khaled Shaaban said the HITL projects have opened up new employment prospects for youngsters, whose only other options were jobs on the black market or working for donors or armed groups..WorkWell figures emailed to AFP showed that 76 workers made approximately $12,500 (11,200 euros) from completing seven projects between May and December 2019..Back in Sofia, Gumnishka insisted she never used the fact that HITL works with refugees as a selling point to clients and added that the high quality of the work ensured that all workers would be ethically rewarded..We mostly insist that these people are really very well prepared and trained, that they work in small dedicated teams, that they have supervisors working with them, so our quality that we provide to our clients is the most important thing, she said.",Hundreds of refugees in the Balkans and Middle East help with AI through startup
340,"It’s been well-established that AI-driven systems are subject to the biases of their human creators — we unwittingly “bake” biases into systems by training them on biased data or with “rules” created by experts with implicit biases. But it doesn’t have to be this way. The good news is that more strategic use of AI systems — through “blind taste tests” — can give us a fresh chance to identify and remove decision biases from the underlying algorithms even if we can’t remove them completely from our own habits of mind. That is, we can simply deny the algorithm the information suspected of biasing the outcome to ensure it makes predictions blind to that variable. Breaking the cycle of bias in this way has the potential to promote greater equality across contexts — from business to science to the arts — on dimensions including gender, race, socioeconomic status, and others..That’s an emerging conclusion of research-based findings — including my own — that could lead to AI-enabled decision-making systems being less subject to bias and better able to promote equality. This is a critical possibility, given our growing reliance on AI-based systems to render evaluations and decisions in high-stakes human contexts, in everything from court decisions, to hiring, to access to credit, and more..It’s been well-established that AI-driven systems are subject to the biases of their human creators — we unwittingly “bake” biases into systems by training them on biased data or with “rules” created by experts with implicit biases..Consider the Allegheny Family Screening Tool (AFST), an AI-based system predicting the likelihood a child is in an abusive situation using data from the same-named Pennsylvania county’s Department of Human Services — including records from public agencies related to child welfare, drug and alcohol services, housing, and others. Caseworkers use reports of potential abuse from the community, along with whatever publicly-available data they can find for the family involved, to run the model, which predicts a risk score from 1 to 20; a sufficiently high score triggers an investigation. Predictive variables include factors such as receiving mental health treatment, accessing cash welfare assistance, and others..Sounds logical enough, but there’s a problem — a big one. By multiple accounts, the AFST has built-in human biases. One of the largest is that the system heavily weights past calls about families, such as from healthcare-providers, to the community hotline — and evidence suggests such calls are over three times more likely to involve Black and biracial families than white ones. Though multiple such calls are ultimately screened out, the AFST relies on them in assigning a risk score, resulting in potentially racially-biased investigations if callers to the hotline are more likely to report Black families than non-Black families, all else being equal. This can result in an ongoing, self-fulfilling, and self-perpetuating prophecy where the training data of an AI system can reinforce its misguided predictions, influencing future decisions and institutionalizing the bias..It doesn’t have to be this way. More strategic use of AI systems — through what I call “blind taste tests” — can give us a fresh chance to identify and remove decision biases from the underlying algorithms even if we can’t remove them completely from our own habits of mind. Breaking the cycle of bias in this way has the potential to promote greater equality across contexts — from business to science to the arts — on dimensions including gender, race, socioeconomic status, and others..Remember the famous Pepsi Challenge from the mid-1970s? When people tried Coca-Cola and Pepsi “blind” — no labels on the cans — the majority preferred Pepsi over its better-selling rival. In real life, though, simply knowing it was Coke created a bias in favor of the product; removing the identifying information — the Coke label — removed the bias so people could rely on taste alone..In a similar blind test from the same time period, wine experts preferred California wines over their French counterparts, in what became known as the “Judgment of Paris.” Again, when the label is visible, the results are very different, as experts ascribe more sophistication and subtlety to the French wines — simply because they’re French — indicating the presence of bias yet again..So it’s easy to see how these blind taste tests can diminish bias in humans by removing key identifying information from the evaluation process. But a similar approach can work with machines..That is, we can simply deny the algorithm the information suspected of biasing the outcome, just as they did in the Pepsi Challenge, to ensure that it makes predictions blind to that variable. In the AFST example, the “blind taste test” could work like this: train the model on all data, including referral calls from the community. Then re-train the model on all the data except that one. If the model’s predictions are equally good without referral-call information, it means the model makes predictions that are blind to that factor. But if the predictions are different when those calls are included, it indicates that either the calls represent a valid explanatory variable in the model, or there may be potential bias in the data (as has been argued for the AFST) that should be examined further before relying on the algorithm..This process breaks the self-perpetuating, self-fulfilling prophecy that existed in the human system without AI, and keeps it out of the AI system..My research with Kellogg collaborators Yang Yang and Youyou Wu demonstrated a similar anti-bias effect in a different domain: the replicability of scientific papers..What separates science from superstition is that a scientific fact that is found in the lab or a clinical trial replicates out in the real world again and again. When it comes to evaluating the replicability — or reproducibility — of published scientific results, we humans struggle..Some replication failure is expected or even desirable because science involves experimentation of unknowns. However, an estimated 68% of studies published in medicine, biology, and social science papers do not replicate. Replication failures continue to be unknowingly cited in the literature, driving up R&D costs by an estimated $28 billion annually and slowing discoveries of vaccines and therapies for Covid-19 and other conditions..The problem is related to bias: when scientists and researchers review a manuscript for publication, they focus on a paper’s statistical and other quantitative results in judging replicability. That is, they use the numbers in a scientific paper much more than the paper’s narrative, which describes the numbers, in making this assessment. Human reviewers are also influenced by institutional labels (e.g., Cambridge University), scientific discipline labels (physicists are smart), journal names, and other status biases..To address this issue, we trained a machine-learning model to estimate a paper’s replicability using only the paper’s reported statistics (typically used by human reviewers), narrative text (not typically used), or a combination of these. We studied 2 million abstracts from scientific papers and over 400 manually-replicated studies from 80 journals..The AI model using only the narrative predicted replicability better than the statistics. It also predicted replicability better than the base rate of individual reviewers, and as well as “prediction markets,” where collective intelligence of hundreds of researchers is used to assess a paper’s replicability, a very costly approach. Importantly, we then used the blind taste test approach and showed that our model’s predictions weren’t biased by factors including topic, scientific discipline, journal prestige, or persuasion words like “unexpected” or “remarkable.” The AI model provided predictions of replicability at scale and without known human biases..In a subsequent extension of this work (in progress), we again used an AI system to reexamine the scientific papers in the study that had inadvertently published numbers and statistics that contained mistakes that the reviewers hadn’t caught during the review process, likely due to our general tendency to believe figures we are shown. Again, a system blind to variables that can promote bias when over-weighted in the review process — quantitative evidence, in this case — was able to render a more objective evaluation than humans alone could, catching mistakes missed due to bias..Together, the findings provide strong evidence for the value of creating blind taste tests for AI systems, to reduce or remove bias and promote fairer decisions and outcomes across contexts..Consider earnings calls led by business C-suite teams to explain recent and projected financial performance to analysts, shareholders, and others. Audience members use the content of these calls to predict future company performance, which can have large, swift impact on share prices and other key outcomes..But again, human listeners are biased to use the numbers presented — just as in judging scientific replicability — and to pay excessive attention to who is sharing the information (a well-known CEO like Jeff Bezos or Elon Musk versus someone else). Moreover, companies have an incentive to spin the information to create more favorable impressions..An AI system can look beyond potential bias-inducing information to factors including the “text” of the call (words rather than numbers) and others such as the emotional tone detected, to render more objective inputs for decision-making. We are currently examining earnings-call data with this hypothesis in mind, along with studying specific issues such as whether the alignment between numbers presented and the verbal description of those numbers has an equal effect on analysts’ evaluations if the speaker is male or female. Will human evaluators give men more of a pass in the case of misalignment? If we find evidence of bias, it will indicate that denying gender information to an AI system can yield more equality-promoting judgments and decisions related to earnings calls..We are also applying the ideas here to the patents domain, where patent applications involve a large investment and rejection rates are as high as 50%. Here, current models used to predict a patent application’s success or a patent’s expected value don’t perform much better than chance, and tend to use factors like whether an individual or team filed the application, again suggesting potential bias. We are studying the value of using AI systems to examine patent text, to yield more effective, fairer judgments..There are many more potential applications of the blind-taste-test approach. What if interviews for jobs or assessments for promotions or tenure took place with some kind of blinding mechanism in place, preventing the biased use of gender, race, or other variables in decisions? What about decisions for which startup founders receive funding, where gender bias has been evident? What if judgments about who received experimental medical treatments were stripped of potential bias-inducing variables?.To be clear, I’m not suggesting that we use machines as our sole decision-making mechanisms. After all, humans can also intentionally program decision-making AI systems to manipulate information. Still, our involvement is critical to form hypotheses about where bias may enter in the first place, and to create the right blind taste tests to avoid it. Thus, an integration of human and AI systems is the optimal approach..In sum, it’s fair to conclude that the human condition inherently includes the presence of bias. But increasing evidence suggests we can minimize or overcome that by programming bias out of the machine-based systems we use to make critical decisions, creating a more equal playing field for all.","The blind taste test” could work like this: train the model on all data, including referral calls from the community. Then re-train the model on all the data except that one. If the model’s predictions are equally good without referral-call information, it means the model makes predictions that are blind to that factor. But if the predictions are different when those calls are included, it indicates that either the calls represent a valid explanatory variable in the model, or there may be potential bias in the data (as has been argued for the AFST) that should be examined further before relying on the algorithm."
341,"It’s late November 2016, and I’m squeezed into the far corner of a long row of gray cubicles in the call screening center for the Allegheny County Office of Children, Youth and Families (CYF) child neglect and abuse hotline. I’m sharing a desk and a tiny purple footstool with intake screener Pat Gordon. We’re both studying the Key Information and Demographics System (KIDS), a blue screen filled with case notes, demographic data, and program statistics. We are focused on the records of two families: both are poor, white, and living in the city of Pittsburgh, Pennsylvania. Both were referred to CYF by a mandated reporter, a professional who is legally required to report any suspicion that a child may be at risk of harm from their caregiver. Pat and I are competing to see if we can guess how a new predictive risk model the county is using to forecast child abuse and neglect, called the Allegheny Family Screening Tool (AFST), will score them..The stakes are high. According to the US Centers for Disease Control and Prevention, approximately one in four children will experience some form of abuse or neglect in their lifetimes. The agency’s Adverse Childhood Experience Study concluded that the experience of abuse or neglect has “tremendous, lifelong impact on our health and the quality of our lives,” including increased occurrences of drug and alcohol abuse, suicide attempts, and depression..In the noisy glassed-in room, Pat hands me a double-sided piece of paper called the “Risk/Severity Continuum.” It took her a minute to find it, protected by a clear plastic envelope and tucked in a stack of papers near the back of her desk. She’s worked in call screening for five years, and, she says, “Most workers, you get this committed to memory. You just know.” But I need the extra help. I am intimidated by the weight of this decision, even though I am only observing. From its cramped columns of tiny text, I learn that kids under five are at greatest risk of neglect and abuse, that substantiated prior reports increase the chance that a family will be investigated, and that parent hostility toward CYF investigators is considered high risk behavior. I take my time, cross-checking information in the county’s databases against the risk/severity handout while Pat rolls her eyes at me, teasing, threatening to click the big blue button that runs the risk model..The first child Pat and I are rating is a six-year-old boy I’ll call Stephen. Stephen’s mom, seeking mental health care for anxiety, disclosed to her county-funded therapist that someone—she didn’t know who—put Stephen out on the porch of their home on an early November day. She found him crying outside and brought him in. That week he began to act out, and she was concerned that something bad had happened to him. She confessed to her therapist that she suspected he might have been abused. Her therapist reported her to the state child abuse hotline..Virginia Eubanks is Associate Professor of Political Science at the University at Albany, SUNY, a founding member of the Our Data Bodies project, and a fellow at New America..But leaving a crying child on a porch isn’t abuse or neglect as the state of Pennsylvania defines it. So the intake worker screened out the call. Even though the report was unsubstantiated, a record of the call and the call screener’s notes remain in the system. A week later, an employee of a homeless services agency reported Stephen to a hotline again: He was wearing dirty clothes, had poor hygiene, and there were rumors that his mother was abusing drugs. Other than these two reports, the family had no prior record with CYF..The second child is a 14-year-old I’ll call Krzysztof. On a community health home visit in early November, a case manager with a large nonprofit found a window and a door broken and the house cold. Krzysztof was wearing several layers of clothes. The caseworker reported that the house smelled like pet urine. The family sleeps in the living room, Krzysztof on the couch and his mom on the floor. The case manager found the room “cluttered.” It is unclear whether these conditions actually meet the definition of child neglect in Pennsylvania, but the family has a long history with county programs..No one wants children to suffer, but the appropriate role of government in keeping kids safe is complicated. States derive their authority to prevent, investigate, and prosecute child abuse and neglect from the Child Abuse and Prevention and Treatment Act, signed into law by President Richard Nixon in 1974. The law defines child abuse and neglect as the “physical or mental injury, sexual abuse, negligent treatment, or maltreatment of a child ... by a person who is responsible for the child’s welfare under circumstances which indicate that the child’s health or welfare is harmed or threatened.”.Even with recent clarifications that the harm must be “serious,” there is considerable room for subjectivity in what exactly constitutes neglect or abuse. Is spanking abusive? Or is the line drawn at striking a child with a closed hand? Is letting your children walk to a park down the block alone neglectful? Even if you can see them from the window?.The first screen of the list of conditions classified as maltreatment in KIDS illustrates just how much latitude call screeners have to classify parenting behaviors as abusive or neglectful. It includes: abandoned infant; abandonment; adoption disruption or dissolution; caretaker’s inability to cope; child sexually acting out; child substance abuse; conduct by parent that places child at risk; corporal punishment; delayed/denied healthcare; delinquent act by a child under 10 years of age; domestic violence; educational neglect; environmental toxic substance; exposure to hazards; expulsion from home; failure to protect; homelessness; inadequate clothing, hygiene, physical care or provision of food; inappropriate caregivers or discipline; injury caused by another person; and isolation. The list scrolls on for several more screens..Three-quarters of child welfare investigations involve neglect rather than physical, sexual, or emotional abuse. Where the line is drawn between the routine conditions of poverty and child neglect is particularly vexing. Many struggles common among poor families are officially defined as child maltreatment, including not having enough food, having inadequate or unsafe housing, lacking medical care, or leaving a child alone while you work. Unhoused families face particularly difficult challenges holding on to their children, as the very condition of being homeless is judged neglectful..In Pennsylvania, abuse and neglect are fairly narrowly defined. Abuse requires bodily injury resulting in impairment or substantial pain, sexual abuse or exploitation, causing mental injury, or imminent risk of any of these things. Neglect must be a “prolonged or repeated lack of supervision” serious enough that it “endangers a child’s life or development or impairs the child’s functioning.” So, as Pat and I run down the risk/severity matrix, I think both Stephen and Krzysztof should score pretty low..In neither case are there reported injuries, substantiated prior abuse, a record of serious emotional harm, or verified drug use. I’m concerned about the inadequate heat in teenaged Krzysztof’s house, but I wouldn’t say that he is in imminent danger. Pat is concerned that there have been two calls in two weeks on six-year-old Stephen. “We literally shut the door behind us and then there was another call,” she sighs. It might suggest a pattern of neglect or abuse developing—or that the family is in crisis. The call from a homeless service agency suggests that conditions at home deteriorated so quickly that Stephen and his mom found themselves on the street. But we agree that for both boys, there seems to be low risk of immediate harm and few threats to their physical safety..On a scale of 1 to 20, with 1 being the lowest level of risk and 20 being the highest, I guess that Stephen will be a 4, and Krzysztof a 6. Gordon smirks and hits the button that runs the AFST. On her screen, a graphic that looks like a thermometer appears: It’s green down at the bottom and progresses up through yellow shades to a vibrant red at the top. The numbers come up exactly as she predicted. Stephen, the six-year-old who may have suffered sexual abuse and is possibly homeless, gets a 5. Krzysztof, the teenager who sleeps on the couch in a cold apartment? He gets a 14..Faith that big data, algorithmic decision-making, and predictive analytics can solve our thorniest social problems—poverty, homelessness, and violence—resonates deeply with our beliefs as a culture. But that faith is misplaced. On the surface, integrated data and artificial intelligence seem poised to produce revolutionary changes in the administration of public services. Computers apply rules to every case consistently and without prejudice, so proponents suggest that they can root out discrimination and unconscious bias. Number matching and statistical surveillance effortlessly track the spending, movements, and life choices of people accessing public assistance, so they can be deployed to ferret out fraud or suggest behavioral interventions. Predictive models promise more effective resource allocation by mining data to infer future actions of individuals based on behavior of “similar” people in the past..These grand hopes rely on the premise that digital decision-making is inherently more transparent, accountable, and fair than human decision-making. But, as data scientist Cathy O’Neil has written, “models are opinions embedded in mathematics.” Models are useful because they let us strip out extraneous information and focus only on what is most critical to the outcomes we are trying to achieve. But they are also abstractions. Choices about what goes into them reflect the priorities and preoccupations of their creators. The Allegheny Family Screening Tool is no exception..The AFST is a statistical model designed by an international team of economists, computer scientists, and social scientists led by Rhema Vaithianathan, professor of Economics at the University of Auckland, and Emily Putnam-Hornstein, director of the Children’s Data Network at the University of Southern California. The model mines Allegheny County’s vast data warehouse to try and predict which children might be victims of abuse or neglect in the future. The warehouse contains more than a billion records—an average of 800 for every resident of the county—provided by regular data extracts from a variety of public agencies, including child welfare, drug and alcohol services, Head Start, mental health services, the county housing authority, the county jail, the state’s Department of Public Welfare, Medicaid, and the Pittsburgh public schools..The job of intake screeners like Pat Gordon is to decide which of the 15,000 child maltreatment reports the county receives each year to refer to a caseworker for investigation. Intake screeners interview reporters, examine case notes, burrow through the county’s data warehouse, and search publically-available data such as court records and social media to determine the nature of the allegation against the caregiver and to ascertain the immediate risk to the child. Then, they run the model..A regression analysis performed by the Vaithianathan team suggested that there are 131 indicators available in the county data that are correlated with child maltreatment. The AFST produces its risk score—from 1 (low risk) to 20 (highest risk)—by weighing these “predictive variables.” They include: receiving county health or mental health treatment; being reported for drug or alcohol abuse; accessing supplemental nutrition assistance program benefits, cash welfare assistance, or Supplemental Security Income; living in a poor neighborhood; or interacting with the juvenile probation system. If the screener’s assessment and the model’s score clash, the case is referred to a supervisor for further discussion and a final screening decision. If a family’s AFST risk score is high enough, the system automatically triggers an investigation..Human choices, biases, and discretion are built into the system in several ways. First, the AFST does not actually model child abuse or neglect. The number of child maltreatment–related fatalities and near fatalities in Allegheny County is thankfully very low. Because this means data on the actual abuse of children is too limited to produce a viable model, the AFST uses proxy variables to stand in for child maltreatment. One of the proxies is community re-referral, when a call to the hotline about a child was initially screened out but CYF receives another call on the same child within two years. The second proxy is child placement, when a call to the hotline about a child is screened in and results in the child being placed in foster care within two years. So, the AFST actually models decisions made by the community (which families will be reported to the hotline) and by CYF and the family courts (which children will be removed from their families), not which children will be harmed..The AFST’s designers and county administrators hope that the model will take the guesswork out of call screening and help to uncover patterns of bias in intake screener decision-making. But a 2010 study of racial disproportionality in Allegheny County CYF found that the great majority of disproportionality in the county’s child welfare services actually arises from referral bias, not screening bias. Mandated reporters and other members of the community call child abuse and neglect hotlines about black and biracial families three and a half times more often as they call about white families. The AFST focuses all its predictive power and computational might on call screening, the step it can experimentally control, rather than concentrating on referral, the step where racial disproportionality is actually entering the system..More troubling, the activity that introduces the most racial bias into the system is the very way the model defines maltreatment. The AFST does not average the two proxies, which might use the professional judgment of CYF investigators and family court judges to mitigate some of the disproportionality coming from community referral. The model simply uses whichever number is higher..Second, the system can only model outcomes based on the data it collects. This may seem like an obvious point, but it is crucial to understanding how Stephen and Krzysztof got such wildly disparate and counterintuitive scores. A quarter of the variables that the AFST uses to predict abuse and neglect are direct measures of poverty: they track use of means-tested programs such as TANF, Supplemental Security Income, SNAP, and county medical assistance. Another quarter measure interaction with juvenile probation and CYF itself, systems that are disproportionately focused on poor and working-class communities, especially communities of color. Though it has been billed as a crystal ball for predicting child harm, in reality the AFST mostly just reports how many public resources families have consumed..Allegheny County has an extraordinary amount of information about the use of public programs. But the county has no access to data about people who do not use public services. Parents accessing private drug treatment, mental health counseling, or financial support are not represented in DHS data. Because variables describing their behavior have not been defined or included in the regression, crucial pieces of the child maltreatment puzzle are omitted from the AFST..Geographical isolation might be an important factor in child maltreatment, for example, but it won’t be represented in the data set because most families accessing public services in Allegheny County live in dense urban neighborhoods. A family living in relative isolation in a well-off suburb is much less likely to be reported to a child abuse or neglect hotline than one living in crowded housing conditions. Wealthier caregivers use private insurance or pay out of pocket for mental health or addiction treatment, so they are not included in the county’s database..Imagine the furor if Allegheny County proposed including monthly reports from nannies, babysitters, private therapists, Alcoholics Anonymous, and luxury rehabilitation centers to predict child abuse among middle-class families. “We really hope to get private insurance data. We’d love to have it,” says Erin Dalton, director of Allegheny County’s Office of Data Analysis, Research and Evaluation. But, as she herself admits, getting private data is likely impossible. The professional middle class would not stand for such intrusive data gathering..The privations of poverty are incontrovertibly harmful to children. They are also harmful to their parents. But by relying on data that is only collected on families using public resources, the AFST unfairly targets low-income families for child welfare scrutiny. “We definitely oversample the poor,” says Dalton. “All of the data systems we have are biased. We still think this data can be helpful in protecting kids.”.We might call this poverty profiling. Like racial profiling, poverty profiling targets individuals for extra scrutiny based not on their behavior but rather on a personal characteristic: They live in poverty. Because the model confuses parenting while poor with poor parenting, the AFST views parents who reach out to public programs as risks to their children..The hazards of using inappropriate proxies and inadequate datasets may be inevitable in predictive modeling. And if a child abuse and neglect investigation was a benign act, it might not matter that the AFST is imperfectly predictive. But a child abuse and neglect investigation can be an intrusive, frightening event with lasting negative impacts..The state of Pennsylvania’s goal for child safety—“Being free from immediate physical or emotional harm”—can be difficult to reach, even for well-resourced families. Each stage of a CYF investigation introduces the potential for subjectivity, bias, and the luck of the draw. “You never know exactly what’s going to happen,” says Catherine Volponi, director of the Juvenile Court Project, which provides pro bono legal support for parents facing CYF investigation or termination of their parental rights. “Let’s say there was a call because the kids were home alone. Then they’re doing their investigation with mom, and she admits marijuana use. Now you get in front of a judge who, perhaps, views marijuana as a gateway to hell. When the door opens, something that we would not have even been concerned about can just mushroom into this big problem.”.At the end of each child neglect or abuse investigation, a written safety plan is developed with the family, identifying immediate steps that must be followed and long-term goals. But each safety action is also a compliance requirement, and sometimes, factors outside parents’ control make it difficult for them to implement their plan. Contractors who provide services to CYF-involved families fail to follow through. Public transportation is unreliable. Overloaded caseworkers don’t always manage to arrange promised resources. Sometimes parents resist CYF’s dictates, resenting government intrusion into their private lives..Failure to complete your plan—regardless of the reason—increases the likelihood that a child will be removed to foster care. “We don’t try to return CYF families to the level at which they were operating before,” concludes Volponi, “We raise the standard on their parenting, and then we don’t have enough resources to keep them up there. It results in epic failures too much of the time.”.Human bias has been a problem in child welfare since the field’s inception. The designers of the model and DHS administrators hope that, by mining the wealth of data at their command, the AFST can help subjective intake screeners make more objective recommendations. But human bias is built in to the predictive risk model. Its outcome variables are proxies for child harm; they don’t reflect actual neglect and abuse. The choice of proxy variables, even the choice to use proxies at all, reflects human discretion. The AFST’s predictive variables are drawn from a limited universe of data that includes only information on public resources. The choice to accept such limited data reflects the human discretion embedded in the model—and an assumption that middle-class families deserve more privacy than poor families..Once the big blue button is clicked and the AFST runs, it manifests a thousand invisible human choices under a cloak of evidence-based objectivity and infallibility. Proponents of the model insist that removing discretion from call screeners is a brave step forward for equity, transparency, and fairness in government decision-making. But the AFST doesn’t remove human discretion; it simply moves it. In the past, the mostly working-class women in the call center exerted some control in agency decision-making. Today, Allegheny County is deploying a system built on the questionable premise that an international team of economists and data analysts is somehow less biased then the agency’s own employees..Back in the call center, I mention to Pat Gordon that I’ve been talking to CYF-involved parents about how the AFST might impact them. Most parents, I tell her, are concerned about false positives: the model rating their child at high risk of abuse or neglect when little risk actually exists. I see how Krzysztof ’s mother might feel this way if she was given access to her family’s risk score..But Pat reminds me that Stephen’s case poses equally troubling questions. I should also be concerned with false negatives—when the AFST scores a child at low risk though the allegation or immediate risk to the child might be severe. “Let’s say they don’t have a significant history. They’re not active with us. But [the allegation] is something that’s very egregious. [CYF] gives us leeway to think for ourselves. But I can’t stop feeling concerned that ... say the child has a broken growth plate, which is very, very highly consistent with maltreatment ... there’s only one or two ways that you can break it. And then [the score] comes in low!”.The screen that displays the AFST risk score states clearly that the system “is not intended to make investigative or other child welfare decisions.” Rhema Vaithianathan told me in February 2017 that the model is designed in such a way that intake screeners are encouraged to question its predictive accuracy and defer to their own judgment. “It sounds contradictory, but I want the model to be slightly undermined by the call screeners,” she said. “I want them to be able to say, this [screening score] is a 20, but this allegation is so minimal that [all] this model is telling me is that there’s history.”.The pairing of the human discretion of intake screeners like Pat Gordon with the ability to dive deep into historical data provided by the model is the most important fail-safe of the system. Toward the end of our time together in the call center, I asked Pat if the harm false negatives and false positives might cause Allegheny County families keeps her up at night. “Exactly,” she replied. “I wonder if people downtown really get that. We’re not looking for this to do our job. We’re really not. I hope they get that.” But like Uber’s human drivers, Allegheny County call screeners may be training the algorithm meant to replace them.","Human choices, biases, and discretion are built into the system in several ways. First, the AFST does not actually model child abuse or neglect. The number of child maltreatment–related fatalities and near fatalities in Allegheny County is thankfully very low. Because this means data on the actual abuse of children is too limited to produce a viable model, the AFST uses proxy variables to stand in for child maltreatment. One of the proxies is community re-referral, when a call to the hotline about a child was initially screened out but CYF receives another call on the same child within two years. The second proxy is child placement, when a call to the hotline about a child is screened in and results in the child being placed in foster care within two years. So, the AFST actually models decisions made by the community (which families will be reported to the hotline) and by CYF and the family courts (which children will be removed from their families), not which children will be harmed."
342,"Editors Note: At Google Research, we’re interested in exploring how technology can help improve people’s daily lives and experiences. So it’s been an incredible opportunity to work with Thomas Panek, avid runner and President & CEO of Guiding Eyes for the Blind, to apply computer vision for something important in his everyday life: independent exercise. Project Guideline is an early-stage research project that leverages on-device machine learning to allow Thomas to use a phone, headphones and a guideline painted on the ground to run independently. Below, Thomas shares why he collaborated with us on this research project, and what the journey has been like for him..I’ve always loved to run. Ever since I was a boy, running has made me feel free. But when I was eight-years-old, I noticed that I couldn’t see the leaves on a tree so well, and that the stars in the night sky began to slowly disappear—and then they did forever. By the time I was a young adult, I was diagnosed as legally blind due to a genetic condition. I had to rely on a cane or a canine to guide me. For years, I gave up running..Then I heard about running with human guides, and I decided to give it a try. It gave me a sense of belonging, holding a tether and following the guide runner in front of me. I even qualified for the New York City and Boston Marathons five years in a row. But as grateful as I was to my human guides, I wanted more independence. So in 2019, I decided to run the first half-marathon assisted only by guide dogs..But I know it’s not possible for everyone to have a brilliant, fast companion like my guide dog, Blaze. I run an organization called Guiding Eyes for the Blind, and we work tirelessly to help people with vision loss receive running guide dogs that can help them live more active and independent lives. The problem is that there are millions more people with vision loss than there are available guide dogs. So I started asking a question: “Would it be possible to help guide a blind runner, independently?” .In the fall of 2019, I asked that question to a group of designers and technologists at a Google hackathon. I wasn’t anticipating much more than an interesting conversation, but by the end of the day they’d built a rough demo that allowed a phone to recognize a line taped to the ground, and give audio cues to me while I walked with Blaze. We were excited, and hopeful to see if we could develop it into something more..We began by sketching out how the prototype would work, settling on a simple concept: I’d wear a phone on a waistband, and bone-conducting headphones. The phone’s camera would look for a physical guideline on the ground and send audio signals depending on my position. If I drifted to the left of the line, the sound would get louder and more dissonant in my left ear. If I drifted to the right, the same thing would happen, but in my right ear. Within a few months, we were ready to test it on an indoor oval track. After a few adjustments, I was able to run eight laps. It was a short distance, and all with my Google teammates close by, but it was the first unguided mile I had run in decades..Our next step was to see if the tech could work where I love running most: in the peace and serenity of a park. This brought a whole new batch of challenges to work through: variables in weather and lighting conditions and the need for new data to train the model, for starters. After months of building an on-device machine learning model to accurately detect the guideline in different environments, the team was finally ready to test the tech outside for the first time..I’d been waiting 25 years to run outdoors, on my own. I stood at the start of the guideline, hopping up and down with excitement. When the team gave me the go-ahead, I began sprinting on my toes, as fast as my legs could carry me, down the hill and around a gentle bend in the road. As I tightened my form, my stride was getting more confident and longer with every step. I felt free, like I was effortlessly running through the clouds..When I arrived at the finish line, I was completely overcome with emotion. My wife, Melissa, and my kids hugged me. My guide dog Blaze licked the salt off of my hand. They were happy for me, too. For the first time in a lifetime, I didn’t feel like a blind man. I felt free..Today, we’re testing this technology further. I’ll be attempting to run NYRR’s Virtual Run for Thanks 5K along a line temporarily painted in Central Park in New York City. I want to thank NYRR, NYC Department of Parks & Recreation, Central Park Conservancy, NYPD, NYC Department of Sanitation and the NYC Department of Transportation for helping to make today’s 5K run possible. We want to see how this system works in urban environments, just one of the many challenges to complete before it can be used more widely. .Collaborating on this project helped me realize a personal dream of mine. I’m so grateful to the Google team, and whoever came up with the idea of a hackathon in the first place. I hope there will be more runs with Project Guideline in my future, and for many other runners as well..By sharing the story of how this project got started and how the tech works today, we hope to start new conversations with the larger blind and low-vision community about how, and if, this technology might be useful for them, too. As we continue our research, we hope to gather feedback from more organizations and explore painting guidelines in their communities. To learn more, please visit: goo.gle/ProjectGuideline.",Project Guideline is a Google-sponsored research project that utilizes a mobile application and audio cues to guide runners with visual impairments. Computer vision is used to identify pre-painted lines and identify diversions from it. 
343,"The number of Free Application for Federal Student Aid, or FAFSA applicants could increase for the first time in recent years as families continue to deal with financial and health hardships caused by the COVID-19 pandemic..A new virtual adviser may help Texas students with their college applications and information on the Free Application for Federal Student Aid..ADVi — short for “adviser” — is a virtual chat bot that uses artificial intelligence to answer questions about college applications, financial aid, FAFSA and related deadlines via text message. Students are required to complete the FAFSA to receive federal and some state aid, along with on-demand resources..Created by the Texas Higher Education Coordinating Board and the Dallas nonprofit Educate Texas, the resource is part of Future Focused Texas, an initiative aimed toward maintaining enrollment rates at colleges throughout the state..“We’re trying to use the same techniques social media (platforms) use to engage students,” said John Fitzpatrick, the executive director of Educate Texas..Students and counselors can use the chat bot to ask questions about applications and the FAFSA at any time and as many times as they need, said Jerel Booker, the coordinating board’s assistant commissioner for college readiness and success. The chat box also directs students to websites for applications or admissions offices, and it covers topics such as writing college essays, creating a resume, recommendations and application submissions. For more complicated questions, live counselors are available during the day. Students can opt into the ADVi service by texting the word “college” to the number 512-829-3687..Texas Higher Education Commissioner Harrison Keller said the initiative began pre-COVID but that the need for college application support resources was amplified during the pandemic..College enrollment across Texas is down 3 percent from last fall, and the state also received 18 percent fewer completed FAFSAs in October, compared with last year..As of Nov. 20, 874,960 FAFSA applications were filed across the country for the high school class of 2021 — 16 percent fewer than the number completed this time last year for the class of 2020, according to the National College Attainment Network..“That’s a warning sign — a cause for concern,” Keller said, noting that a decrease in FAFSA completion can indicate a decrease in enrollments. “What we’re concerned about is, we have many more students not enrolling across the state than we’d expect or hope to see right now.”.John Fitzpatrick, executive director of Educate Texas, said there are more than 350,000 Texas high school seniors for the 2019-2020 academic year at risk of not being able to enroll in college..African-American, Hispanic and low-income students — many of which are underrepresented at higher education institutions — are particularly at risk, Keller added..Fitzpatrick said that “our high school counselors and college admission officers are working so hard to engage 350,000 students, but it’s incredibly hard in time of COVID-19.”.College advisers and recruiters have had to pivot, resorting to online or virtual techniques to reach their students. Meanwhile, students and their families have experienced hardships of their own during the pandemic, which might have rendered filling out an application or applying to college a lower priority. But for those still looking to attend college, the FAFSA is particularly important because it lets students assess what financial aid is available to them, which might determine which colleges they will or can afford to attend..The ADVi platform could be especially helpful for students who are filling out the application for the first time or for the many students who cannot get in-person assistance or reminders from other students and counselors when they need it, the coordinating board’s Booker added..The goal, however, is “not to totally eliminate human contact, but to help the counselors do it online” during a time when many resources are being offered in remote formats or virtually, Booker said..Brittany Britto is the features enterprise reporter at the Houston Chronicle, reporting in-depth stories focused on marginalized communities, underrepresented neighborhoods, histories and sub-cultures in and around the Houston area. Shell work to amplify fascinating and untold stories in one of the most diverse cities in the country and dig into what it means to be a Houstonian. .Brittany has been recognized for her cultural coverage by the Society for Features Journalism. In 2018, she was named a Penny Bender Fuchs Diversity Fellow for the national features organization and won four awards - a tie for the most won in one year in recent SFJ history -- for her diverse portfolio and noteworthy features on Baltimores distinct culture. .Brittany is a two-time graduate of the University of Maryland, College Park, with a masters in multiplatform journalism and a bachelors in English. .Sanger ISD Challenge Program kids are the so-called worst of the worst behaved. And its here that they find a home. A place where they are listened to and respected.","The Texas Higher Education Coordinating Board has launched ADVi, a chatbot that uses AI to answer questions about FAFSA and financial aid while also providing relevant resources. "
344,"While younger Chinese have embraced the conveniences of mobile payments, food delivery, and ride-hailing, the country’s elderly are at risk of being left behind by such increasingly pervasive technologies..This was the message of the State Council, China’s Cabinet, in a notice Tuesday addressed to the central and local governments, urging them to ensure that services remain available to people who seldom go online..China’s number of internet users has steadily risen over the years and now includes more than 90 million people over 60 years old. However, rapidly aging China has 254 million people in that age cohort, which means a majority of them may face issues when they want to travel, go to the hospital, or simply buy groceries — all of which increasingly involve the use of smartphone apps..The notice, dated Nov. 15, was published to “solve the difficulties that old people will confront when using smart technology,” as well as to “make old people share in the achievements of digital developments.” It said places frequented by elderly people should not refuse cash payments, and that online services should keep older users in mind..The document’s publication follows several recent videos of elderly people being inconvenienced by technology that were widely shared on social media. A bank apologized Saturday after a 94-year-old woman had to be lifted up by her son so she could use a facial-recognition system to active her social security card. And on Monday, another elderly woman was rejected when she tried to use cash to pay for her medical insurance. An employee explained that she could pay on her phone or call a relative if she needed help..A video screenshot shows a man holding his 94-year-old mother so a facial-recognition system can activate her social security card at a bank in Hubei province, 2020. From Weibo.Online, people showed appreciation for the State Council’s notice. “Everyone will grow old one day. The day will come when we will all be out of step with technology,” read a comment with over 40,000 likes..Just how difficult it can be for elderly Chinese to navigate China’s increasingly app-based daily life was also made clear during the COVID-19 outbreak earlier this year, when people relied on food-delivery services to eat and health-tracking apps to move around..The notice stressed that health-tracking apps, which commonly show a green QR code for people who have not recently been to areas with active COVID-19 clusters, should not be the only way for people to prove they should be allowed to travel around the country..“Conditional regions and places should set up ‘non-health code channels’ for old people who don’t use smartphones,” the notice said, adding that, in the future, emergency responses should take older people into consideration..A resident from Zhangjiagang in the eastern Jiangsu province, surnamed Song, told Sixth Tone that her 52-year-old mother knows how to operate a phone to show her green health code. However, she recently ran into trouble when visiting a local hospital, as her phone had run out of battery. “She even had a negative nucleic acid test result, but she still wasn’t allowed to use the elevator,” Song said..According to the notice, by the end of 2022, China should have established a long-term mechanism to solve the problems of the “digital divide” for old people..(Header image: A elderly man attempts to use a cashless payment service during the second annual Digital China Summit in Fuzhou, Fujian province, May 7, 2019. People Visual)","A bank apologized Saturday after a 94-year-old woman had to be lifted up by her son so she could use a facial-recognition system to active her social security card. And on Monday, another elderly woman was rejected when she tried to use cash to pay for her medical insurance. An employee explained that she could pay on her phone or call a relative if she needed help."
345,"Professor Faisal, from the Departments of Computing and Bioengineering at Imperial College London, has been announced as one of 15 Turing AI Fellows by UKRI. The five-year awards are designed to accelerate and support the careers of the best and brightest AI (artificial intelligence) researchers, enabling them to become world-leading researchers in the five years of the award..He aims to develop an ‘AI clinician’ colleague to aid doctors and clinicians and relieve pressure on the NHS. The system could also be used in sectors such as aerospace or energy, where accurate decision-making under high-pressure conditions is crucial..Professor Faisal said: “This UKRI Turing AI Fellowship recognises many years efforts by my team and myself to develop machine learning methods that can learn complex skills from human interactions. Core to our AI technology is that it is focussed on augmenting human decision-makers, such as medical doctors, and empowering them by giving them an ‘AI colleague’ to interact with who has distilled the experiences from millions of treated patients.”.Professor Faisal focuses on the development of algorithms that can learn from human behaviour and human interactions to learn complex skills. With this new fellowship, he will be able to focus and develop an ‘AI clinician’ that can tirelessly watch over patients and recommend medical interventions to human clinicians, such as prescribing drugs, changing doses or other interventions..The system will learn this complex skill by distilling the data from thousands of real doctors treating millions of real patients into one system of AI best clinical practice. The AI will automatically present the recommendations for actions, and the reasoning behind these recommendations, in a way that decision-makers can understand, helping human doctors to make the best decisions on the course of action for a patient..Professor Faisal received substantial industry support for this fellowship from healthcare, but also the energy and aerospace sector, which are keen to develop ways of deploying this AI technology in their respective industries..Professor Faisal’s labs, the Brain & Behaviour lab and the Behaviour Analytics Lab, operate at the interface of artificial and human intelligence. He is one of the few computer scientists worldwide that leads clinical trials to test and deploy their own algorithms. He is the director of the £20m UKRI Centre for Doctoral Training in AI for Healthcare and the elected Speaker of the Imperial College Cross-Faculty network in AI..Named after British AI pioneer Alan Turing, the £20 million fellowship scheme will be delivered by Engineering and Physical Sciences Research Council (EPSRC), part of UK Research and Innovation (UKRI), in partnership with the Alan Turing Institute and the UK government’s Office for Artificial intelligence..As a result of the £20m government investment, Fellows will work with academia and industry to help elevate their world-class research and transfer their innovations from the lab to the real world. These innovations have the potential to change how people live, work and communicate, helping to place the UK at the forefront of the AI and data revolution. .Science Minister, Amanda Solloway said: “The UK is the birthplace of artificial intelligence and we therefore have a duty to equip the next generation of Alan Turings, like London’s Professor Aldo Faisal, with the tools that will keep the UK at the forefront of this remarkable technological innovation..“London has a rich and unique history of innovating and this inspiring AI project we are backing will assist doctors and clinicians when treating patients, helping to relieve pressure on our brilliant NHS, while cementing the UK’s status as a world leader in AI and data.”.  Healthcare,  							Comms-strategy-Real-world-benefits,  							Global-challenges-Data,  							Artificial-intelligence,  							Big-data   See more tags .Your comment may be published, displaying your name as you provide it, unless you request otherwise. Your contact details will never be published.","He aims to develop an ‘AI clinician’ colleague to aid doctors and clinicians and relieve pressure on the NHS. The system could also be used in sectors such as aerospace or energy, where accurate decision-making under high-pressure conditions is crucial."
346,"Long before the Nov. 18-19 protests in Uganda that left more than 50 people dead, opposition politicians, and local activists had warned about the potential abuse and human rights implications of an invasive surveillance system bought by the government last year from China’s telecoms giant Huawei..The fear was, that in the hands of corrupt public officials or under a repressive regime the integrated system which uses facial recognition and other artificial intelligence systems but is also able to check vehicle license plates and monitor social media, would be used to suppress individual freedoms of anyone in opposition to the government..The latest protests, which were triggered by the arrest of two presidential candidates hoping to put a halt to president Yoweri Museveni’s 34-year rule, seem to have confirmed those fears..Ugandan police officials have confirmed they are using the cameras supplied by Huawei which helped the force track down some of the more than 836 suspects they have arrested..In the absence of any judicial oversight, there are also concerns of backdoor access to the system for illegal facial recognition surveillance on potential targets and stifling of anti-regime comments and any peaceful civil action. Local rights group, Unwanted Witness, has previously called for the observance of international human rights law in the implementation of the project to safeguard human rights, freedoms, and democracy in the country..The national CCTV system installed by Huawei has 83 monitoring centers, 522 operators, and 50 commanders according to president Museveni who in a series of tweets has praised the effectiveness of the technology.  Authorities also plan to integrate the Huawei system with other Ugandan agencies including the tax body and the immigration department. In Jan. 2020, authorities started rolling out the second phase within 2,319 mapped countryside municipalities and major towns..A Quartz Africa source with the knowledge of the police operations says Huawei staff and other “experts” from China are still in the process of installing an “integrated” system part of a classified contract between the authorities in Kampala and Huawei to supply and install surveillance equipment in cities and towns throughout Uganda..In 2019, Ugandan police officials confirmed the government had paid at least $126 million as part of the deal which is more than the combined 2020 budgets ($108 million) of the ministries of ICT and Science & Technology ministries..Local and international rights groups say footage from the Huawei surveillance cameras has been used since 2019 to monitor political rallies and other events of  president Museveni’s opponents. The unregulated surveillance is characterized by pervasive location monitoring, facial recognition, biometric, and blanket data retention practices among others..Unlike in the West where there are security concerns about a Chinese company dominating 5G technology, Huawei has been broadly welcomed by African governments. Here it has played a key role in helping to build the telecoms infrastructure needed for a 21st century economy in several countries. But more recently its roles have expanded to other projects such as security for governments who are so inclined..Ahead of the 2016 polls, president Museveni’s government procured the services of a UK-based firm, Gamma Group, which delivered a surveillance technology reportedly used to spy on Museveni’s opponents in an operation dubbed “Fungua Macho”..A Wall Street Journal investigation last year suggested the government in Kampala used assistance from Huawei to hack into messages for the presidential candidate Robert Kyagulanyi, better known as Bobi Wine, prompting his arrest and detention. Bobi Wine, a popular musician who was elected to parliament in 2017, is extremely popular with young Ugandans and has been a thorn in the side of the Museveni establishment.","In the absence of any judicial oversight, there are also concerns of backdoor access to the system for illegal facial recognition surveillance on potential targets and stifling of anti-regime comments and any peaceful civil action. Local rights group, Unwanted Witness, has previously called for the observance of international human rights law in the implementation of the project to safeguard human rights, freedoms, and democracy in the country."
348,"Researchers from the University of Seville have carried out a rigorous and detailed analysis of how artificial intelligence has been used with pregnant women over the last twelve years.The analysis confirmed that disorders such as congenital heart birth defects or macrosomia, gestational diabetes and preterm birth can be detected earlier when artificial intelligence is used. In the latter case, studies into cases involving artificial intelligence found a correlation between the number of pre-term births and the environmental pollution to which the pregnant women had been previously exposed..                             Systems based on affective computing could allow emotional interaction with the pregnant woman and detect emotional changes and make it possible to offer guidance or recommendations, which the system would previously have received from doctors.                         .There is growing interest in the application of artificial intelligence in obstetrics and gynecology. These applications of AI can not only monitor womens health during pregnancy, but can also help to improve the universal provision of health services, especially in the most disadvantaged areas. This field therefore contributes to improving both individual and public health, says University of Seville researcher María del Carmen Romero..Furthermore, this work reveals the almost total lack of studies where emotions are taken into account as input parameters in risk prediction models in pregnancy (only 1.28% of the studies analyzed). Moreover, very few studies look closely at the pregnant womans mental health (only 5.1% of the studies analyzed), despite it having been shown that the womans psychological health is correlated with the risk of suffering certain diseases typical of pregnancy. Pregnancy is a vital state that brings with it the need for change and new learning, potentially causing anxiety, fear, worry, and even depression in women..Systems based on affective computing could allow emotional interaction with the pregnant woman and, for example, detect emotional changes and make it possible to offer guidance or recommendations, which the system would previously have received from doctors. This can make the patient feel safer and closer to her health service and can reduce the usual feelings of anxiety or worry that sometimes lead to physical problems..Given that there is previous scientific evidence that supports the idea that the emotional state and mental health of the pregnant woman can influence the occurrence of risks in pregnancy, our study highlights what is a very interesting multidisciplinary research niche for affective computing in the field of health and well-being of pregnant women, the researcher adds.. To use the full function of this web site, JavaScript needs to be enabled in your browser. This is how you enable JavaScript in your browser settings: Read instruction ","
The analysis confirmed that disorders such as congenital heart birth defects or macrosomia, gestational diabetes and preterm birth can be detected earlier when artificial intelligence is used. In the latter case, studies into cases involving artificial intelligence found a correlation between the number of pre-term births and the environmental pollution to which the pregnant women had been previously exposed."
353,"I’ve often been told, “The data does not lie.” However, that has never been my experience. For me, the data nearly always lies. Google Image search results for “healthy skin” show only light-skinned women, and a query on “Black girls” still returns pornography. The CelebA face data set has labels of “big nose” and “big lips” that are disproportionately assigned to darker-skinned female faces like mine. ImageNet-trained models label me a “bad person,” a “drug addict,” or a “failure.” Data sets for detecting skin cancer are missing samples of darker skin types. .White supremacy often appears violently—in gunshots at a crowded Walmart or church service, in the sharp remark of a hate-fueled accusation or a rough shove on the street—but sometimes it takes a more subtle form, like these lies. When those of us building AI systems continue to allow the blatant lie of white supremacy to be embedded in everything from how we collect data to how we define data sets and how we choose to use them, it signifies a disturbing tolerance..Non-white people are not outliers. Globally, we are the norm, and this doesn’t seem to be changing anytime soon. Data sets so specifically built in and for white spaces represent the constructed reality, not the natural one. To have accuracy calculated in the absence of my lived experience not only offends me, but also puts me in real danger. .In a research paper titled “Dirty Data, Bad Predictions,” lead author Rashida Richardson describes an alarming scenario: police precincts suspected or confirmed to have engaged in “corrupt, racially biased, or otherwise illegal” practices continue to contribute their data to the development of new automated systems meant to help officers make policing decisions. .The goal of predictive policing tools is to send officers to the scene of a crime before it happens. The assumption is that locations where individuals had been previously arrested correlate with a likelihood of future illegal activity. What Richardson points out is that this assumption remains unquestioned even when those initial arrests were racially motivated or illegal, sometimes involving “systemic data manipulation, police corruption, falsifying police reports, and violence, including robbing residents, planting evidence, extortion, unconstitutional searches, and other corrupt practices.” Even data from the worst-behaving police departments is still being used to inform predictive policing tools. .As the Tampa Bay Times reports, this approach can provide algorithmic justification for further police harassment of minority and low-income communities. Using such flawed data to train new systems embeds the police department’s documented misconduct in the algorithm and perpetuates practices already known to be terrorizing those most vulnerable to that abuse..This may appear to describe a handful of tragic situations. However, it is really the norm in machine learning: this is the typical quality of the data we currently accept as our unquestioned “ground truth.” .One day GPT-2, an earlier publicly available version of the automated language generation model developed by the research organization OpenAI, started talking to me openly about “white rights.” Given simple prompts like “a white man is” or “a Black woman is,” the text the model generated would launch into discussions of “white Aryan nations” and “foreign and non-white invaders.” .Not only did these diatribes include horrific slurs like “bitch,” “slut,” “nigger,” “chink,” and “slanteye, but the generated text embodied a specific American white nationalist rhetoric, describing “demographic threats” and veering into anti-Semitic asides against “Jews” and “Communists.” .GPT-2 doesn’t think for itself—it generates responses by replicating language patterns observed in the data used to develop the model. This data set, named WebText, contains “over 8 million documents for a total of 40 GB of text” sourced from hyperlinks. These links were themselves selected from posts most upvoted on the social media website Reddit, as “a heuristic indicator for whether other users found the link interesting, educational, or just funny.” .However, Reddit users—including those uploading and upvoting—are known to include white supremacists. For years, the platform was rife with racist language and permitted links to content expressing racist ideology. And although there are practical options available to curb this behavior on the platform, the first serious attempts to take action, by then-CEO Ellen Pao in 2015, were poorly received by the community and led to intense harassment and backlash. .Whether dealing with wayward cops or wayward users, technologists choose to allow this particular oppressive worldview to solidify in data sets and define the nature of models that we develop. OpenAI itself acknowledged the limitations of sourcing data from Reddit, noting that “many malicious groups use those discussion forums to organize.” Yet the organization also continues to make use of the Reddit-derived data set, even in subsequent versions of its language model. The dangerously flawed nature of data sources is effectively dismissed for the sake of convenience, despite the consequences. Malicious intent isn’t necessary for this to happen, though a certain unthinking passivity and neglect is. .White supremacy is the false belief that white individuals are superior to those of other races. It is not a simple misconception but an ideology rooted in deception. Race is the first myth, superiority the next. Proponents of this ideology stubbornly cling to an invention that privileges them. .I hear how this lie softens language from a “war on drugs” to an “opioid epidemic,” and blames “mental health” or “video games” for the actions of white assailants even as it attributes “laziness” and “criminality” to non-white victims. I notice how it erases those who look like me, and I watch it play out in an endless parade of pale faces that I can’t seem to escape—in film, on magazine covers, and at awards shows..This shadow follows my every move, an uncomfortable chill on the nape of my neck. When I hear “murder,” I don’t just see the police officer with his knee on a throat or the misguided vigilante with a gun by his side—it’s the economy that strangles us, the disease that weakens us, and the government that silences us..Tell me—what is the difference between overpolicing in minority neighborhoods and the bias of the algorithm that sent officers there? What is the difference between a segregated school system and a discriminatory grading algorithm? Between a doctor who doesn’t listen and an algorithm that denies you a hospital bed? There is no systematic racism separate from our algorithmic contributions, from the hidden network of algorithmic deployments that regularly collapse on those who are already most vulnerable..Technology is not independent of us; it’s created by us, and we have complete control over it. Data is not just arbitrarily “political”—there are specific toxic and misinformed politics that data scientists carelessly allow to infiltrate our data sets. White supremacy is one of them. .We’ve already inserted ourselves and our decisions into the outcome—there is no neutral approach. There is no future version of data that is magically unbiased. Data will always be a subjective interpretation of someone’s reality, a specific presentation of the goals and perspectives we choose to prioritize in this moment. That’s a power held by those of us responsible for sourcing, selecting, and designing this data and developing the models that interpret the information. Essentially, there is no exchange of “fairness” for “accuracy”—that’s a mythical sacrifice, an excuse not to own up to our role in defining performance at the exclusion of others in the first place..Those of us building these systems will choose which subreddits and online sources to crawl, which languages to use or ignore, which data sets to remove or accept. Most important, we choose who we apply these algorithms to, and which objectives we optimize for. We choose the labels we create, the data we take in, the methods we use. We choose who we welcome as data scientists and engineers and researchers—and who we do not. There were many possibilities for the design of the technology we built, and we chose this one. We are responsible. .So why can’t we be more careful? When will we finally get into the habit of disclosing data provenance, deleting problematic data sets, and explicitly defining the limitations of every model’s scope? At what point can we condemn those operating with an explicit white supremacist agenda, and take serious actions for inclusion?.Distracted by corporate condolences, abstract technical solutions, and articulate social theories, I’ve watched peers congratulate themselves on invisible progress. Ultimately, I envy them, because they have a choice in the same world where I, like every other Black person, cannot opt out of caring about this. .As Black people now die in a cacophony of natural and unnatural disasters, many of my colleagues are still more galvanized by the latest product or space launch than the jarring horror of a reality that chokes the breath out of me. .For years, I’ve watched this issue extolled as important, but it’s clear that dealing with it is still seen as a non-priority, “nice to have” supplementary action—secondary always to some definition of model functionality that doesn’t include me. .Models clearly still struggling to address these bias challenges get celebrated as breakthroughs, while people brave enough to speak up about the risk get silenced, or worse. There’s a clear cultural complacency with things as usual, and although disappointing, that’s not particularly surprising in a field where the vast majority just don’t understand the stakes..The fact is that AI doesn’t work until it works for all of us. If we hope to ever address racial injustice, then we need to stop presenting our distorted data as “ground truth.” There’s no rational and just world in which hiring tools systematically exclude women from technical roles, or where self-driving cars are more likely to hit pedestrians with darker skin. The truth of any reality I recognize is not in these models, or in the data sets that inform them..The machine-learning community continues to accept a certain level of dysfunction as long as only certain groups are affected. This needs conscious change, and that will take as much effort as any other fight against systematic oppression. After all, the lies embedded in our data are not much different from any other lie white supremacy has told. They will thus require just as much energy and investment to counteract..Deborah Raji is a Mozilla fellow interested in algorithmic auditing and evaluation. She has worked on several award-winning projects to highlight cases of bias in computer vision and improve documentation practices in machine learning.  .An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.","People of color continue to be under-represented in datasets despite making up the majority of the population, language-learning bots are trained on media rife with white supremacist ideology, and racist criminal justice data is seen as ground truth."
354,"The Chinese tech giant Huawei has tested facial recognition software that could send automated “Uighur alarms” to government authorities when its camera systems identify members of the oppressed minority group, according to an internal document that provides further details about China’s artificial-intelligence surveillance regime..A document signed by Huawei representatives — discovered by the research organization IPVM and shared exclusively with The Washington Post — shows that the telecommunications firm worked in 2018 with the facial recognition start-up Megvii to test an artificial-intelligence camera system that could scan faces in a crowd and estimate each person’s age, sex and ethnicity..If the system detected the face of a member of the mostly Muslim minority group, the test report said, it could trigger a “Uighur alarm” — potentially flagging them for police in China, where members of the group have been detained en masse as part of a brutal government crackdown. The document, which was found on Huawei’s website, was removed shortly after The Post and IPVM asked the companies for comment..Such technology has in recent years gained an expanding role among police departments in China, human rights activists say. But the document sheds new light on how Huawei, the world’s biggest maker of telecommunications equipment, has also contributed to its development, providing the servers, cameras, cloud-computing infrastructure and other tools undergirding the systems’ technological might..John Honovich, the founder of IPVM, a Pennsylvania-based company that reviews and investigates video-surveillance equipment, said the document showed how “terrifying” and “totally normalized” such discriminatory technology has become..Huawei and Megvii have announced three surveillance systems using both companies’ technology in the past couple years. The Post could not immediately confirm if the system with the “Uighur alarm” tested in 2018 was one of the three currently for sale..Both companies have acknowledged the document is real. Shortly after this story published Tuesday morning, Huawei spokesman Glenn Schloss said the report “is simply a test and it has not seen real-world application. Huawei only supplies general-purpose products for this kind of testing. We do not provide custom algorithms or applications.”.Chinese officials have said such systems reflect the country’s technological advancement, and that their expanded use can help government responders and keep people safe. But to international rights advocates, they are a sign of China’s dream of social control — a way to identify unfavorable members of society and squash public dissent. China’s foreign ministry did not immediately respond to requests for comment. .Artificial-intelligence researchers and human rights advocates said they worry the technology’s development and normalization could lead to its spread around the world, as government authorities elsewhere push for a fast and automated way to detect members of ethnic groups they’ve deemed undesirable or a danger to their political control..Maya Wang, a China senior researcher at the advocacy group Human Rights Watch, said the country has increasingly used AI-assisted surveillance to monitor the general public and oppress minorities, protesters and others deemed threats to the state..“China’s surveillance ambition goes way, way, way beyond minority persecution,” Wang said, but “the persecution of minorities is obviously not exclusive to China. … And these systems would lend themselves quite well to countries that want to criminalize minorities.”.Trained on immense numbers of facial photos, the systems can begin to detect certain patterns that might differentiate, for instance, the faces of Uighur minorities from those of the Han majority in China. In one 2018 paper, “Facial feature discovery for ethnicity recognition,” AI researchers in China designed algorithms that could distinguish between the “facial landmarks” of Uighur, Korean and Tibetan faces..But the software has sparked major ethical debates among AI researchers who say it could assist in discrimination, profiling or punishment. They argue also that the system is bound to return inaccurate results, because its performance would vary widely based on lighting, image quality and other factors — and because the diversity of people’s ethnicities and backgrounds is not so cleanly broken down into simple groupings..Such ethnicity-detection software is not available in the United States. But algorithms that can analyze a person’s facial features or eye movements are increasingly popular in job-interview software and anti-cheating monitoring systems..Clare Garvie, a senior associate at Georgetown Law’s Center on Privacy and Technology who has studied facial recognition software, said the “Uighur alarm” software represents a dangerous step toward automating ethnic discrimination at a devastating scale..“There are certain tools that quite simply have no positive application and plenty of negative applications, and an ethnic-classification tool is one of those,” Garvie said. “Name a human rights norm, and this is probably violative of that.”.Huawei and Megvii are two of China’s most prominent tech trailblazers, and officials have cast them as leaders of a national drive to reach the cutting edge of AI development. But the multibillion-dollar companies have also faced blowback from U.S. authorities, who argue they represent a security threat to the United States or have contributed to China’s brutal regime of ethnic oppression..Eight Chinese companies, including Megvii, were hit with sanctions by the U.S. Commerce Department last year for their involvement in “human rights violations and abuses in the implementation of China’s campaign of repression, mass arbitrary detention, and high-technology surveillance” against Uighurs and other Muslim minority groups..The U.S. government has also issued sanctions against Huawei, banning the export of U.S. technology to the company and lobbying other countries to exclude its systems from their telecommunications networks..Huawei, a hardware behemoth with equipment and services used in more than 170 countries, has surpassed Apple to become the world’s second-biggest maker of smartphones and is pushing to lead an international rollout of new 5G mobile networks that could reshape the Internet..And Megvii, the Beijing-based developer of the Face Plus Plus system and one of the world’s most highly valued facial recognition start-ups, said in a public-offering prospectus last year that its “city [Internet of Things] solutions,” which include camera systems, sensors and software that government agencies can use to monitor the public, covered 112 cities across China as of last June..The “Uighur alarm” document obtained by the researchers, called an “interoperability test report,” offers technical information on how authorities can align the Huawei-Megvii systems with other software tools for seamless public surveillance..The system tested how a mix of Megvii’s facial recognition software and Huawei’s cameras, servers, networking equipment, cloud-computing platform and other hardware and software worked on dozens of “basic functions,” including its support of “recognition based on age, sex, ethnicity and angle of facial images,” the report states. It passed those tests, as well as another in which it was tested for its ability to support offline “Uighur alarms.”.The test report also said the system was able to take real-time snapshots of pedestrians, analyze video files and replay the 10 seconds of footage before and after any Uighur face is detected. .The document did not provide information on where or how often the system is used. But similar systems are used by police departments across China, according to official documents reviewed last year by the New York Times, which found one city system that had scanned for Uighur faces half a million times in a single month..Jonathan Frankle, a deep-learning researcher at the Massachusetts Institute of Technology’s Computer Science and Artificial Intelligence Lab, said such systems are clearly becoming a priority among developers willing to capitalize on the technical ability to classify people by ethnicity or race. The flood of facial-image data from public crowds, he added, could be used to further develop the systems’ precision and processing power..“People dont go to the trouble of building expensive systems like this for nothing,” Frankle said. “These arent people burning money for fun. If they did this, they did it for a very specific reason in mind. And that reason is very clear.”.It’s less certain whether ethnicity-detecting software could ever take off outside the borders of a surveillance state. In the United States and other Western-style democracies, the systems could run up against long-established laws limiting government searches and mandating equal protection under the law..Police and federal authorities in the United States have shown increasing interest in facial recognition software as an investigative tool, but the systems have sparked a fierce public backlash over their potential bias and inaccuracies, and some cities and police forces have opted to ban the technology outright..Such technologies could, however, find a market among international regimes somewhere in the balance between Chinese and American influence. In Uganda, Huawei facial recognition cameras have already been used by police and government officials to surveil protesters and political opponents..“If you’re willing to model your government and run your country in that way,” Frankle said, “why wouldn’t you use the best technology available to exert control over your citizens?”.Discrimination against Uighurs has long been prevalent in the majority-Han Chinese population. In the Xinjiang region of northwestern China, authorities have cited sporadic acts of terrorism as justification for a harsh crackdown starting in 2015 that has drawn condemnation from the United States and other Western nations. Scholars estimate more than 1 million Uighurs have been detained in reeducation camps, with some claims of torture..U.S. national security adviser Robert O’Brien called the repressive treatment of minority groups in Xinjiang “something close to” genocide, in an online event hosted by the Aspen Institute in October..Under international pressure, Xinjiang authorities announced last December that all reeducation “students” had graduated, though some Uighurs have since reported that they were forced to agree to work in factories or risk a return to detention. Xinjiang authorities say all residents work of their own free will..The U.S. government has banned the import of certain products from China on the basis that they could have been made by forced labor in Xinjiang..One of the Huawei-Megvii systems offered for sale after the “Uighur alarm” test, in June 2019, is advertised as saving local governments digital storage space by saving images in a single place..Two other systems, said to use Megvii’s surveillance software and Huawei’s Atlas AI computing platform, were announced for sale in September. Both were described as “localization” of the products using Huawei chips and listed for sale “only by invitation.” Marketing materials for one of those systems say it was used by authorities in China’s southern Guizhou province to catch a criminal.","An internal report claims the face-scanning system could trigger a ‘Uighur alarm,’ sparking concerns that the software could help fuel China’s crackdown on the mostly Muslim minority group"
355,"On the evening of Wednesday, December 2, Timnit Gebru, the co-lead of Google’s ethical AI team, announced via Twitter that the company had forced her out. .Gebru, a widely respected leader in AI ethics research, is known for coauthoring a groundbreaking paper that showed facial recognition to be less accurate at identifying women and people of color, which means its use can end up discriminating against them. She also cofounded the Black in AI affinity group, and champions diversity in the tech industry. The team she helped build at Google is one of the most diverse in AI and includes many leading experts in their own right. Peers in the field envied it for producing critical work that often challenged mainstream AI practices..A series of tweets, leaked emails, and media articles showed that Gebru’s exit was the culmination of a conflict over another paper she coauthored. Jeff Dean, the head of Google AI, told colleagues in an internal email (which he has since put online) that the paper “didn’t meet our bar for publication” and that Gebru had said she would resign unless Google met a number of conditions, which it was unwilling to meet. Gebru tweeted that she had asked to negotiate “a last date” for her employment after she got back from vacation. She was cut off from her corporate email account before her return. .Online, many other leaders in the field of AI ethics are arguing that the company pushed her out because of the inconvenient truths that she was uncovering about a core line of its research—and perhaps its bottom line. More than 1,400 Google staff members and 1,900 other supporters have also signed a letter of protest..Many details of the exact sequence of events that led up to Gebru’s departure are not yet clear; both she and Google have declined to comment beyond their posts on social media. But MIT Technology Review obtained a copy of the research paper from  one of the coauthors, Emily M. Bender, a professor of computational linguistics at the University of Washington. Though Bender asked us not to publish the paper itself because the authors didn’t want such an early draft circulating online, it gives some insight into the questions Gebru and her colleagues were raising about AI that might be causing Google concern..“On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” lays out the risks of large language models—AIs trained on staggering amounts of text data. These have grown increasingly popular—and increasingly large—in the last three years. They are now extraordinarily good, under the right conditions, at producing what looks like convincing, meaningful new text—and sometimes at estimating meaning from language. But, says the introduction to the paper, “we ask whether enough thought has been put into the potential risks associated with developing them and strategies to mitigate these risks.”.The paper, which builds on the work of other researchers, presents the history of natural-language processing, an overview of four main risks of large language models, and suggestions for further research. Since the conflict with Google seems to be over the risks, we’ve focused on summarizing those here. .Training large AI models consumes a lot of computer processing power, and hence a lot of electricity. Gebru and her coauthors refer to a 2019 paper from Emma Strubell and her collaborators on the carbon emissions and financial costs of large language models. It found that their energy consumption and carbon footprint have been exploding since 2017, as models have been fed more and more data..Strubell’s study found that training one language model with a particular type of “neural architecture search” (NAS) method would have produced the equivalent of 626,155 pounds (284 metric tons) of carbon dioxide—about the lifetime output of five average American cars. Training a version of Google’s language model, BERT, which underpins the company’s search engine, produced 1,438 pounds of CO2 equivalent in Strubell’s estimate—nearly the same as a round-trip flight between New York City and San Francisco. These numbers should be viewed as minimums, the cost of training a model one time through. In practice, models are trained and retrained many times over during research and development..Gebru’s draft paper points out that the sheer resources required to build and sustain such large AI models means they tend to benefit wealthy organizations, while climate change hits marginalized communities hardest. “It is past time for researchers to prioritize energy efficiency and cost to reduce negative environmental impact and inequitable access to resources,” they write..Large language models are also trained on exponentially increasing amounts of text. This means researchers have sought to collect all the data they can from the internet, so theres a risk that racist, sexist, and otherwise abusive language ends up in the training data..An AI model taught to view racist language as normal is obviously bad. The researchers, though, point out a couple of more subtle problems. One is that shifts in language play an important role in social change; the MeToo and Black Lives Matter movements, for example, have tried to establish a new anti-sexist and anti-racist vocabulary. An AI model trained on vast swaths of the internet won’t be attuned to the nuances of this vocabulary and won’t produce or interpret language in line with these new cultural norms..It will also fail to capture the language and the norms of countries and peoples that have less access to the internet and thus a smaller linguistic footprint online. The result is that AI-generated language will be homogenized, reflecting the practices of the richest countries and communities..Moreover, because the training data sets are so large, it’s hard to audit them to check for these embedded biases. “A methodology that relies on datasets too large to document is therefore inherently risky,” the researchers conclude. “While documentation allows for potential accountability, [...] undocumented training data perpetuates harm without recourse.”.The researchers summarize the third challenge as the risk of “misdirected research effort.” Though most AI researchers acknowledge that large language models don’t actually understand language and are merely excellent at manipulating it, Big Tech can make money from models that manipulate language more accurately, so it keeps investing in them. “This research effort brings with it an opportunity cost,” Gebru and her colleagues write. Not as much effort goes into working on AI models that might achieve understanding, or that achieve good results with smaller, more carefully curated data sets (and thus also use less energy)..The final problem with large language models, the researchers say, is that because they’re so good at mimicking real human language, it’s easy to use them to fool people. There have been a few high-profile cases, such as the college student who churned out AI-generated self-help and productivity advice on a blog, which went viral..The dangers are obvious: AI models could be used to generate misinformation about an election or the covid-19 pandemic, for instance. They can also go wrong inadvertently when used for machine translation. The researchers bring up an example: In 2017, Facebook mistranslated a Palestinian man’s post, which said “good morning” in Arabic, as “attack them” in Hebrew, leading to his arrest..Gebru and Bender’s paper has six coauthors, four of whom are Google researchers. Bender asked to avoid disclosing their names for fear of repercussions. (Bender, by contrast, is a tenured professor: “I think this is underscoring the value of academic freedom,” she says.).The paper’s goal, Bender says, was to take stock of the landscape of current research in natural-language processing. “We are working at a scale where the people building the things can’t actually get their arms around the data,” she said. “And because the upsides are so obvious, it’s particularly important to step back and ask ourselves, what are the possible downsides? … How do we get the benefits of this while mitigating the risk?”.In his internal email, Dean, the Google AI head, said one reason the paper “didn’t meet our bar” was that it “ignored too much relevant research.” Specifically, he said it didn’t mention more recent work on how to make large language models more energy efficient and mitigate problems of bias. .However, the six collaborators drew on a wide breadth of scholarship. The paper’s citation list, with 128 references, is notably long. “It’s the sort of work that no individual or even pair of authors can pull off,” Bender said. “It really required this collaboration.” .The version of the paper we saw does also nod to several research efforts on reducing the size and computational costs of large language models, and on measuring the embedded bias of models. It argues, however, that these efforts have not been enough. “I’m very open to seeing what other references we ought to be including,” Bender said..Nicolas Le Roux, a Google AI researcher in the Montreal office, later noted on Twitter that the reasoning in Dean’s email was unusual. “My submissions were always checked for disclosure of sensitive material, never for the quality of the literature review,” he said..Now might be a good time to remind everyone that the easiest way to discriminate is to make stringent rules, then to decide when and for whom to enforce them.My submissions were always checked for disclosure of sensitive material, never for the quality of the literature review..Dean’s email also says that Gebru and her colleagues gave Google AI only a day for an internal review of the paper before they submitted it to a conference for publication. He wrote that “our aim is to rival peer-reviewed journals in terms of the rigor and thoughtfulness in how we review research before publication.”.Bender noted that even so, the conference would still put the paper through a substantial review process: “Scholarship is always a conversation and always a work in progress,” she said. .Google pioneered much of the foundational research that has since led to the recent explosion in large language models. Google AI was the first to invent the Transformer language model in 2017 that serves as the basis for the company’s later model BERT, and OpenAI’s GPT-2 and GPT-3. BERT, as noted above, now also powers Google search, the company’s cash cow..Bender worries that Google’s actions could create “a chilling effect” on future AI ethics research. Many of the top experts in AI ethics work at large tech companies because that is where the money is. “That has been beneficial in many ways,” she says. “But we end up with an ecosystem that maybe has incentives that are not the very best ones for the progress of science for the world.”.An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.","Timnit Gebru says she was fired from Google over a paper highlighting the opportunity costs of natural language processing. With seven coauthors, the renowned AI ethicist highlighted the environmental toll of training large models, as well as their ability to manipulate language to the benefit of large corporations. Google allegedly refused to approve the paper for publication without discussion or revision -UpTurn Newsletter"
357,"Algorithm-driven hiring tools have grown increasingly prevalent in recent years. Thousands of job-seekers across the United States are now asked to record videos that employers mine for facial and vocal cues. They complete online tests or games that purport to evaluate their “personal stability,” optimism, or attention span. They submit resumes through online platforms that may reject them because of time gaps in their work histories, such as those resulting from cancer treatment..Employers using these tools seek a fast and efficient way to process job applications in large numbers. They may also believe that algorithm-driven software will identify characteristics of successful employees that human recruiters would not identify on their own. But as these algorithms have spread in adoption, so, too, has the risk of discrimination written invisibly into their codes. For people with disabilities, those risks can be profound..The Americans with Disabilities Act (ADA) has explicit prohibitions against the use of hiring processes that discriminate on the basis of disability. First, the ADA requires that employment tests be provided in an accessible format, and if the format is not accessible, that reasonable accommodations be made available without prejudicing the candidate. For example, a test that requires spoken answers is not accessible for an applicant who does not speak because of paralysis or deafness. If an employer uses such a test, they have to evaluate disabled job-seekers in an alternative way that reasonably accommodates their disabilities..Second, the ADA presumptively disfavors hiring selection criteria that “screen out, or tend to screen out” disabled candidates. For example, a personality test may screen out some candidates with depression or anxiety; a game-based test may screen out a candidate because of their ADHD. If an employer uses selection criteria that screen out disabled candidates, the criteria must be “job-related” and “consistent with business necessity.” This means that employment tests must evaluate candidates on factors that are directly relevant to (and necessary for) the essential functions of the job..Many algorithm-driven hiring tools fall far short of these standards. Algorithm-driven hiring tools typically assess candidates based on how they perform on a given test compared to a model set of successful employees. Employers may be tempted to use these tools without stopping to consider what exactly they are testing for, or why – specifically, what traits are really being measured by an online game, and whether what is being measured is actually necessary to perform the essential functions of the job..Employers, vendors who create these hiring tools, regulators, job-seekers, and advocates need to better understand the risks of using algorithm-driven tools in hiring, and consider concrete steps to avoid these harms. This paper seeks to highlight how hiring tools may affect people with disabilities, the legal liability employers may face for using such tools, and concrete steps for employers and vendors to mitigate some of the most significant areas of concern. We hope it will serve as a resource for advocates, for regulators, and – above all – for those deciding whether to develop or use these tools to consider the risks of discrimination, and ultimately to ask if the tools are appropriate for use at all..CDT works to strengthen individual rights and freedoms by defining, promoting, and influencing technology policy and the architecture of the internet that impacts our daily lives..The content throughout this website that originates with CDT can be freely copied and used as long as you make no substantive changes and clearly give us credit. More on CDTs content reuse policy is available here.","A new report from the Center for Democracy & Technology highlights how hiring technologies may screen out people with disabilities and fall far short of ADA standards. The report provides many actionable steps, such as not screening out people based on subjective personality traits, and diverting a percentage of all candidates into any “reasonable accommodation” to avoid stigmatizing disability.-UpTurn Newsletter"
358," Growing AI readership, proxied by expected machine downloads, motivates firms to prepare filings that are friendlier to machine parsing and processing. Firms avoid words that are perceived as negative by computational algorithms, as compared to those deemed negative only by dictionaries meant for human readers. The publication of Loughran and McDonald (2011) serves as an instrumental event attributing the difference-in-differences in the measured sentiment to machine readership. High machine-readership firms also exhibit speech emotion assessed as embodying more positivity and excitement by audio processors. This is the first study exploring the feedback effect on corporate disclosure in response to technology. . The authors have benefited from discussions with Rui Albuquerque, Elizabeth Blankespoor (discussant), Emilio Calvano (discussant), Lauren Cohen (discussant), Will Cong (discussant), Ilia Dichev, Arup Ganguly (discussant), Jillian Grennan, Rebecca Hann, Bing Han, Kathleen Hanley (discussant), Gerard Hoberg (discussant), Byoung-Hyoun Hwang (discussant), Chris Hennessy, Alan Huang (discussant), Bin Ke (discussant), Michael Kimbrough, Leonid Kogan, Augustin Landier (discussant), Tim Loughran (discussant), Song Ma, Ville Rantala (discussant), Max Rohrer (discussant), Gustavo Schwenkler (discussant), Kelly Shue, Suhas Sridharan, Isabel Wang (discussant), Teri Yohn, Gwen Yu, Dexin Zhou, and comments and suggestions from participants in seminars and conferences at Columbia, ECB, EDHEC, Emory, Georgia State, Harvard, London Business School, Maryland, Michigan, Michigan State, Peking University, Stockholm Business School, Toronto, Utah, Washington, the NBER Economics of Artificial Intelligence Conference, the NBER Big Data and Securities Markets Conference, AFA 2022, the Pacific Center for Asset Management, the SOAR Symposium at Singapore Management University, the Third Bergen FinTech Conference at the NHH Norwegian School of Economics, Machine Learning and Business Conference at University of Miami, RCFS Winter Conference, 11th Financial Markets and Corporate Governance Conference, the China FinTech Research Conference, the Adam Smith Workshop, the Conference on Financing Innovation at Stevens Institute of Technology, FIRS 2021, the Cambridge Alternative Finance Sixth Annual Conference, 2021 CAPANA Research Conference, CICF 2021, and NFA 2021. The views expressed herein are those of the authors and do not necessarily reflect the views of the National Bureau of Economic Research. .In addition to working papers, the NBER disseminates affiliates’ latest findings through a range of free periodicals — the NBER Reporter, the NBER Digest, the Bulletin on Retirement and Disability, and the Bulletin on Health — as well as online conference reports, video lectures, and interviews.",How corporations that expect algorithmic processing of their regulatory filings manipulate the text of their filings “to induce algorithmic readers to draw favorable conclusions about the content.” They do so by reducing their use of words with “negative sentiment” found in dictionaries used to train natural language processing algorithms - UpTurn Newsletter
359,"The Pasco Sheriff’s Office keeps a secret list of kids it thinks could “fall into a life of crime” based on factors like whether they’ve been abused or gotten a D or an F in school, according to the agencys internal intelligence manual..The Sheriff’s Office assembles the list by combining the rosters for most middle and high schools in the county with records so sensitive, they’re protected by state and federal law..School district data shows which children are struggling academically, miss too many classes or are sent to the office for discipline. Records from the state Department of Children and Families flag kids who have witnessed household violence or experienced it themselves..The process largely plays out in secret. The Sheriff’s Office doesn’t tell the kids or their parents about the designation. In an interview, schools superintendent Kurt Browning said he was unaware the Sheriff’s Office was using school data to identify kids who might become criminals. So were the principals of two high schools..Sheriff Chris Nocco declined requests to be interviewed, and his agency did not make anyone from its intelligence-led policing or school resource divisions available for comment. .In a series of written statements, the Sheriff’s Office said the list is used only to help the deputies assigned to middle and high schools offer “mentorship” and “resources” to students. .Asked for specifics, it pointed to one program where school resource officers take children fishing and another where they give clothes to kids in need. .Ten experts in law enforcement and student privacy questioned the justification for combing through thousands of students’ education and child-welfare records..They called the program highly unusual. Many said it was a clear misuse of children’s confidential information that stretched the limits of the law..“Can you imagine having your kid in that county and they might be on a list that says they may become a criminal?” said Linnette Attai, a consultant who helps companies and schools comply with student privacy laws..It’s unclear, but you may be able to submit a request for records to the Sheriff’s Office. Click here or scroll to the bottom of the story to learn more..The agency also objected to the characterization of the list as potential future criminals, saying it was also designed to identify students at risk for victimization, truancy, self-harm and substance abuse..But the intelligence manual — an 82-page document that school resource officers and other deputies are required to read — doesn’t mention those other risks. Instead, in five separate places, it describes efforts to pinpoint kids who are likely to become criminals..The following are some of the passages from the Pasco Sheriff’s Office’s intelligence-led policing manual that describe at-risk students as potential criminals and offenders..The list of school kids isn’t the agency’s only effort to identify and target people it considers likely to commit crimes. In September, a Tampa Bay Times investigation revealed that the department’s intelligence arm also uses people’s criminal histories and social networks to predict if they will break the law..The Sheriff’s Office pursues those people even when there’s no evidence of a new crime. Former deputies told the Times they were ordered to harass people on the target list by visiting their homes repeatedly and looking for reasons to write tickets and make arrests. One in 10 of the people targeted have been teenagers..The ways the agency has extended its intelligence effort into mining education and child-welfare records have not previously been reported..Because the children themselves don’t know if they’ve been flagged, it is difficult to say how it affects interactions between students and school resource officers or other deputies. The Sheriff’s Office declined to release a copy of its list of students to the Times..“We have an agreement with the Sheriff’s Office,” the superintendent said. “The agreement requires them to use (the data) for official law enforcement purposes. I have to assume that’s exactly what they are using it for.”.Later, in a written statement, he added: “If there is any need to revisit any aspect of our relationship, we will do so in a thoughtful manner with the goal of keeping our students and staff safe.”.Two members of the Pasco School Board, Megan Harding and Alison Crumbley, described the district’s relationship with the Sheriff’s Office as strong and referenced safeguards to protect students’ privacy. The agreement between the two institutions says the Sheriff’s Office must keep the records confidential and use them in legal ways..Experts said having school resource officers single out children could be harmful, especially if the kids were struggling at home or in school, or if they didn’t trust police..“It is a recipe for violating people’s rights and civil liberties,” said Harold Jordan, a senior policy advocate for the American Civil Liberties Union of Pennsylvania..Elsewhere in the country, scandals have erupted when law enforcement agencies were found to have access to children’s private data, said Andrew Guthrie Ferguson, a law professor at American University and national expert in predictive policing..In its intelligence manual, the Pasco Sheriff’s Office says most police departments have no way of knowing if kids have “low intelligence” or come from “broken homes” — factors that can predict whether they’ll break the law..The manual says the Sheriff’s Office has access to the information through partnerships with the Pasco school district and the state Department of Children and Families..The district pays the sheriff $2.3 million annually to place 32 deputies in middle and high schools. It also provides access to its Early Warning System, which tracks all students’ grades, attendance and behavior..Separately, the Department of Children and Families allows law enforcement agencies across Florida to use to its child welfare database so they can investigate child abuse and find missing children. The database, known as the Florida Safe Families Network, contains detailed case notes and kids’ abuse histories. .The Sheriff’s Office has its own records, too, which indicate if children have been the subject of custody disputes, have run away from home, have violated the county’s curfew for young people or have been caught with drugs or alcohol. The office also keeps track of who is friends with whom. .It feeds information from all three datasets into a system that scores kids in 16 different categories. In each, children are assigned one of four labels: on track, at risk, off track or critical. .Kids are also labeled “at risk” if they’ve experienced a childhood trauma. That includes witnessing household violence, being the victim of abuse or neglect, or having a parent go to prison..The Pasco Sheriff’s Office uses data from the school district and the state Department of Children and Families, as well as its own records, to identify children it considers at risk of becoming criminals..The Sheriff’s Office analyzes the data for more than 30,000 students in most of the middle and high schools in the county. (Pasco Middle School, Pasco High School, Gulf Middle School, Gulf High School and Harry Schwettman Education Center have school resource officers from other law enforcement agencies, so their students’ data is not included.).Children who are deemed “on track” in all categories are removed from the list before it is distributed. The final version removes the underlying student data by using shaded labels to represent the child’s risk level in each category. The color codes are: on-track (green), at-risk (yellow), off-track (pink) or critical (red). .The agency said it only looks at data in schools where it provides school resource officers — the vast majority of the district’s middle and high schools. In total, those schools have more than 30,000 students..The Sheriff’s Office has been identifying and monitoring at-risk children as part of its intelligence operation since at least 2011, when Nocco first became sheriff..That summer, school resource officers made hundreds of home visits to at-risk kids, according to news reports. They offered support to the children and their families, they told reporters at the time. But they also questioned them about local crimes and arrested kids who violated probation or curfew orders..In one of its statements to the Times, the Sheriff’s Office said it looks for alternatives to arrest “when possible” and that supporting struggling kids is an important part of any school resource officer’s job..The intelligence manual encourages them to work their relationships with students to find “the seeds of criminal activity” and to collect information that can help with investigations..One school resource officer’s annual performance review, obtained by the Times through a public records request, noted he contributed to intelligence briefings. It also praised him for filing nearly two dozen “field interview reports” based on interactions with at-risk kids..In Pasco County, Black students and students with disabilities are twice as likely to be suspended or referred to law enforcement, according to federal data..Bacardi Jackson, a senior supervising attorney for children’s rights for the Southern Poverty Law Center, said designating those kids as potential criminals could have a “circular effect.” They would likely receive even more attention from school resource officers and as a result, face additional discipline..Singling out kids based on whether they had been involved in custody disputes, for example, could be considered differential treatment based on family status, she said..The Sheriff’s Office says its program is based on research. It points to a 2015 study that found young people who had experienced multiple childhood traumas were at a higher risk of becoming serious, violent criminals than those who hadn’t..But David Kennedy, a renowned criminologist and professor at the John Jay College of Criminal Justice whose research is referenced in Pasco’s manual, said the associations between childhood trauma and criminal behavior are “extremely weak.” He said using them to make predictions about individuals “flies in the face of the science.”.The methodology used by the Sheriff’s Office, he added, was likely to generate a large pool of children, the vast majority of whom would never get in serious trouble. .“There’s nothing — absolutely nothing — that can be fed into even the most sophisticated algorithm or risk-assessment tool based on information available when someone is a child that can say this person is going to be a criminal later on, much less a serious prolific criminal,” he said..After the Times started asking questions about the use of data to target young people, the Sheriff’s Office seems to have started revamping elements of its program..Emails show the agency has been drafting a policy for how school resource officers should interact with at-risk students that focuses more on offering support and building positive relationships..Law enforcement agencies can use the information to help thwart school shootings and offer support to students who are in the juvenile justice system. But in cases like those, experts said, they can only look at records relating to a specific student or situation..“You can’t just give every student’s record out,” said LeRoy Rooker, who led the federal Department of Education’s oversight of student privacy for more than two decades..The law does say school resource officers can access education records because they can be considered “school officials.” But under most circumstances, they can’t share the records with the rest of the department, said Amelia Vance, a member of the Maryland Department of Education’s Student Data Privacy Council who works for the nonprofit Future of Privacy Forum. .And they can’t use them in a law enforcement investigation without permission from a parent, unless there is a court order or a health and safety emergency, Vance said..Federal student privacy law gives families the right to access their student records. Click here or scroll to the bottom of the story to learn how you may be able to request records related to your child..In its statement, the Sheriff’s Office said it had access to the data “lawfully” and noted that school resource officers receive annual training on the federal student privacy law. .The school district has recently been criticized for lax privacy practices. Last year, a state audit found that too many district employees had access to current and former students’ sensitive data, including social security numbers. The district later revoked privileges for 570 employees..Attai, the student privacy consultant, said the school district should take action in this case, too, and reconsider its arrangement with the Sheriff’s Office. .The Times reported this story with the support of the Fund for Journalism on Child Well-Being, a program of the USC Annenberg Center for Health Journalism’s 2020 National Fellowship. The reporting was also supported by a grant from the Fund for Investigative Journalism..Our school district has an excellent working relationship with the Pasco Sheriff’s Office. That relationship has been strengthened in the wake of the tragedy at Marjorie Stoneman Douglas High School in 2018, and that includes processes for a two-way sharing of information that could save lives and result in timely interventions with students who are at risk..I am in frequent communication with Sheriff Nocco, and members of my staff are in constant communication with staff members from the Sheriff’s Office. If there is any need to revisit any aspect of our relationship, we will do so in a thoughtful manner with the goal of keeping our students and staff safe..It’s unclear. The Sheriff’s Office said it does not inform students or their parents if they’ve been added to its list of potential future criminals. .The Sheriff’s Office declined our request for the list, citing laws protecting the privacy of students, kids in the juvenile justice system and victims of crimes..But many of those laws don’t apply to parents requesting their own kids’ records, and federal law gives families the right to access student records on their children..The school district said it does not have the list that is created by the Sheriff’s Office. It said it already informs parents when their children are considered at-risk academically. .Let us know what happens. Send an email directly to reporter Neil Bedi at [email protected] or reporter Kathleen McGrory at [email protected].","Pasco County Sheriff’s Office has a system to identify students “destined to a life of crime.” The Sheriff’s Office combines its own data with data from the school district and the state’s Department of Children and Families to create a list of kids deemed “at risk.” The system relies on 16 categories including grades, attendance, custody disputes, and abuse and other trauma to label kids most in need of support as likely “criminals.”. - UpTurn Newsletter"
360,"AlphaFold can predict the shape of proteins to within the width of an atom. The breakthrough will help scientists design drugs and understand disease..DeepMind has already notched up a streak of wins, showcasing AIs that have learned to play a variety of complex games with superhuman skill, from Go and StarCraft to Atari’s entire back catalogue. But Demis Hassabis, DeepMind’s public face and co-founder, has always stressed that these successes were just stepping stones towards a larger goal: AI that actually helps us understand the world..Today DeepMind and the organizers of the long-running Critical Assessment of protein Structure Prediction (CASP) competition announced an AI that should have the huge impact that Hassabis has been after. The latest version of DeepMind’s AlphaFold, a deep-learning system that can accurately predict the structure of proteins to within the width of an atom, has cracked one of biology’s grand challenges. “Its the first use of AI to solve a serious problem,” says John Moult at the University of Maryland, who leads the team that runs CASP..A protein is made from a ribbon of amino acids that folds itself up with many complex twists and turns and tangles. This structure determines what it does. And figuring out what proteins do is key to understanding the basic mechanisms of life, when it works and when it doesn’t. Efforts to develop vaccines for covid-19 have focused on the virus’s spike protein, for example. The way the coronavirus snags onto human cells depends on the shape of this protein and the shapes of the proteins on the outsides of those cells. The spike is just one protein among billions across all living things; there are tens of thousands of different types of protein inside the human body alone.      .In this year’s CASP, AlphaFold predicted the structure of dozens of proteins with a margin of error of just 1.6 angstroms—that’s 0.16 nanometers, or atom-sized. This far outstrips all other computational methods and for the first time matches the accuracy of techniques used in the lab, such as cryo-electron microscopy, nuclear magnetic resonance and x-ray crystallography. These techniques are expensive and slow: it can take hundreds of thousands of dollars and years of trial and error for each protein. AlphaFold can find a protein’s shape in a few days..The breakthrough could help researchers design new drugs and understand diseases. In the longer term, predicting protein structure will also help design synthetic proteins, such as enzymes that digest waste or produce biofuels. Researchers are also exploring ways to introduce synthetic proteins that will increase crop yields and make plants more nutritious..“It’s a very substantial advance,” says Mohammed AlQuraishi, a systems biologist at Columbia University who has developed his own software for predicting protein structure. “Its something I simply didnt expect to happen nearly this rapidly. Its shocking, in a way.”.“This really is a big deal,” says David Baker, head of the Institute for Protein Design at the University of Washington and leader of the team behind Rosetta, a family of protein analysis tools. “It’s an amazing achievement, like what they did with Go.”.Identifying a protein’s structure is very hard. For most proteins, researchers have the sequence of amino acids in the ribbon but not the contorted shape they fold into. And there are typically an astronomical number of possible shapes for each sequence. Researchers have been wrestling with the problem at least since the 1970s, when Christian Anfinsen won the Nobel prize for showing that sequences determined structure. .The launch of CASP in 1994 gave the field a boost. Every two years, the organizers release 100 or so amino acid sequences for proteins whose shapes have been identified in the lab but not yet made public. Dozens of teams from around the world then compete to find the correct way to fold them up using software. Many of the tools developed for CASP are already used by medical researchers. But progress was slow, with two decades of incremental advances failing to produce a shortcut to painstaking lab work.   .CASP got the jolt it was looking for when DeepMind entered the competition in 2018 with its first version of AlphaFold. It still could not match the accuracy of a lab but it left other computational techniques in the dust. Researchers took note: soon many were adapting their own systems to work more like AlphaFold..This year more than half of the entries use some form of deep learning, says Moult. The accuracy overall was higher as a result. Baker’s new system, called trRosetta, uses some of DeepMind’s ideas from 2018. But it still came a “very distant second,” he says..In CASP, results are scored using what’s known as a global distance test (GDT), which measures on a scale from 0 to 100 how close a predicted structure is to the actual shape of a protein identified in lab experiments. The latest version of AlphaFold scored well for all proteins in the challenge. But it got a GDT score above 90 for around two thirds of them. Its GDT for the hardest proteins was 25 points higher than the next best team, says John Jumper, who heads up the AlphaFold team at DeepMind. In 2018 the lead was around six points..A score above 90 means that any differences between the predicted structure and the actual structure could be down to experimental errors in the lab rather than a fault in the software. It could also mean that the predicted structure is a valid alternative configuration to the one identified in the lab, within the range of natural variation..According to Jumper, there were four proteins in the competition that independent judges had not finished working on in the lab and AlphaFold’s predictions pointed them towards the correct structures..AlQuraishi thought it would take researchers 10 years to get from AlphaFold’s 2018 results to this year’s. This is close to the physical limit for how accurate you can get, he says. “These structures are fundamentally floppy. It doesn’t make sense to talk about resolutions much below that.”.AlphaFold builds on the work of hundreds of researchers around the world. DeepMind also drew on a wide range of expertise, putting together a team of biologists, physicists and computer scientists. Details of how it works will be released this week at the CASP conference and in a peer-reviewed article in a special issue of the journal Proteins next year. But we do know that it uses a form of attention network, a deep-learning technique that allows an AI to train by focusing on parts of a larger problem. Jumper compares the approach to assembling a jigsaw: it pieces together local chunks first before fitting these into a whole..DeepMind trained AlphaFold on around 170,000 proteins taken from the protein data bank, a public repository of sequences and structures. It compared multiple sequences in the data bank and looked for pairs of amino acids that often end up close together in folded structures. It then uses this data to guess the distance between pairs of amino acids in structures that are not yet known. It is also able to assess how accurate these guesses are. Training took “a few weeks,” using computing power equivalent to between 100 and 200 GPUs..Dame Janet Thornton at the European Bioinformatics Institute in Cambridge, UK, has been working on proteins for 50 years. “That’s really as long as this problem has been around,” she said in a press conference last week. “I was beginning to think it would not get solved in my lifetime.”.Many drugs are designed by simulating their 3D molecular structure and looking for ways to slot these molecules into target proteins. Of course, this can only be done if the structure of those proteins is known. This is the case for only a quarter of the roughly 20,000 human proteins, says Thornton. That leaves 15,000 untapped drug targets. “AlphaFold will open up a new area of research.”.DeepMind says it plans to study leishmaniasis, sleeping sickness, and malaria, all tropical diseases caused by parasites, because they are linked to lots of unknown protein structures..One drawback of AlphaFold is that it is slow compared to rival techniques. AlQuraishi’s system, which uses an algorithm called a recurrent geometrical network (RGN), can find protein structures a million times faster—returning results in seconds rather than days. Its predictions are less accurate, but for some applications speed is more important, he says..Researchers are now waiting to find out exactly how AlphaFold works. “Once they describe to the world how they do it then a thousand flowers will bloom,” says Baker. “People will be using it for all kinds of different things, things that we cant imagine now.”.Even a less accurate result would have been good news for people working on enzymes or bacteria, says AlQuraishi: “But we have something even better, with immediate relevance to pharmaceutical applications.”.An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.",AlphaFold can predict the shape of proteins to within the width of an atom. The breakthrough will help scientists design drugs and understand disease.
363,"Microsoft has developed a new image-captioning algorithm that exceeds human accuracy in certain limited tests. The AI system has been used to update the company’s assistant app for the visually impaired, Seeing AI, and will soon be incorporated into other Microsoft products like Word, Outlook, and PowerPoint. There, it will be used for tasks like creating alt-text for images — a function that’s particularly important for increasing accessibility. .“Ideally, everyone would include alt text for all images in documents, on the web, in social media — as this enables people who are blind to access the content and participate in the conversation,” said Saqib Shaikh, a software engineering manager with Microsoft’s AI team in a press statement. “But, alas, people don’t. So, there are several apps that use image captioning as way to fill in alt text when it’s missing.”.These apps include Microsoft’s own Seeing AI, which the company first released in 2017. Seeing AI uses computer vision to describe the world as seen through a smartphone camera for the visually impaired. It can identify household items, read and scan text, describe scenes, and even identify friends. It can also be used to describe images in other apps, including email clients, social media apps, and messaging apps like WhatsApp. .Microsoft does not disclose user numbers for Seeing AI, but Eric Boyd, corporate vice president of Azure AI, told The Verge the software is “one of the leading apps for people who are blind or have low vision.” Seeing AI has been voted best app or best assistive app three years in a row by AppleVis, a community of blind and low-vision iOS users. .Microsoft’s new image-captioning algorithm will improve the performance of Seeing AI significantly, as it’s able to not only identify objects but also more precisely describe the relationship between them. So, the algorithm can look at a picture and not just say what items and objects it contains (e.g., “a person, a chair, an accordion”) but how they are interacting (e.g., “a person is sitting on a chair and playing an accordion”). Microsoft says the algorithm is twice as good as its previous image-captioning system, in use since 2015..The algorithm, which was described in a pre-print paper published in September, achieved the highest ever scores on an image-captioning benchmark known as “nocaps.” This is an industry-leading scoreboard for image captioning, though it has its own constraints. .The nocaps benchmark consists of more than 166,000 human-generated captions describing some 15,100 images taken from the Open Images Dataset. These images span a range of scenarios, from sports to holiday snaps to food photography and more. (You can get an idea of the mixture of images and captions by exploring the nocaps dataset here or looking at the gallery below.) Algorithms are tested on their ability to create captions for these pictures that match those from humans..It’s important to note, though, that the nocaps benchmarks capture only a tiny sliver of the complexity of image captioning as a general task. Although Microsoft claims in a press release that its new algorithm “describes images as well as people do,” this is only true insomuch as it applies to a very small subset of images contained within nocaps. .As Harsh Agrawal, one of the creators of the benchmark, told The Verge over email: “Surpassing human performance on nocaps is not an indicator that image captioning is a solved problem.” Argawal noted that the metrics used to evaluate performance on nocaps “only roughly correlate with human preferences” and that the benchmark itself “only covers a small percentage of all the possible visual concepts.” .“As with most benchmarks, [the] nocaps benchmark is only a rough indicator of the models’ performance on the task,” said Argawal. “Surpassing human performance on nocaps by no means indicates that AI systems surpass humans on image comprehension.”.This problem — assuming that performance on a specific benchmark can be extrapolated as performance on the underlying task more generally — is a common one when it comes to exaggerating the ability of AI. Indeed, Microsoft has been criticized by researchers in the past for making similar claims about its algorithms’ ability to comprehend the written word..Nevertheless, image captioning is a task that has seen huge improvements in recent years thanks to artificial intelligence, and Microsoft’s algorithms are certainly state-of-the-art. In addition to being integrated into Word, Outlook, and PowerPoint, the image-captioning AI will also be available as a standalone model via Microsoft’s cloud and AI platform Azure. ","Microsoft's new image captioning algorithm is able to outperform humans in some tests. The system will be used in Microsoft's Seeing AI, an assistant app for the visually impaired, and other products such as Word, Outlook, and Powerpoint. It can be used to generate alt-text for images which will increase accessibility."
364,"Google says its flood prediction service, which uses machine learning to identify areas of land prone to flooding and alert users before the waters arrive, now covers all of India and has expanded to parts of Bangladesh as well..The search giant launched the tool in 2018 for India’s Patna region, but it says it’s been slowly increasing coverage in coordination with local government. In June, it hit the milestone of covering all the worst flood-hit areas of India. The company says this means some 200 million people in India and 40 million people in Bangladesh can now receive alerts from its flood forecasting system..In addition to expanding coverage, Google is testing more accurate forecasts and has updated how its alerts appear on users’ devices. The company says it’s now sent over 30 million notifications to users with Android devices..Google has long been interested in providing warnings about natural disasters and national emergencies like floods, wildfires, and earthquakes. Many of these are handled through its Public Alerts program. Just last month, the company launched a new service that turns Android devices into a network of seismometers, leveraging the accelerometers inside phones and tablets to detect the vibrations from earthquakes and send alerts to users..In the case of flood forecasting, though, Google isn’t using information from customers’ devices. Instead, it draws on a mix of historical and contemporary data about rainfall, river levels, and flood simulations, using machine learning to create new forecast models..Google says it’s experimenting with new models that can provide even more accurate alerts. Its latest forecast model can “double the lead time” of its previous system, says the company, while also providing people with information about the depths of the flooding. “In more than 90 precent of cases, our forecasts will provide the correct water level within a margin of error of 15 centimeters,” say Google’s researchers. .A study of Google’s forecasts in the Ganges-Brahmaputra river basin carried out with scientists from Yale found that 70 percent of people who received a flood alert did so before flood waters arrived, and 65 percent of households that received an alert took action. “Even in an area suffering from low literacy, limited education, and high poverty, a majority of citizens act on information they receive,” write the researchers. “So, early warnings are definitely worth the effort.”.They noted that problems with using smartphone alerts still remained. The main issues are simply lack of access to smartphones and lack of trust regarding technological warnings. Survey respondents the researchers spoke to said they preferred to receive warnings from local leaders and that sharing them via loud speakers and phones calls was still desirable..Google says it’s looking into these problems and has started a collaboration with the International Federation of Red Cross and Red Crescent Societies. It hopes to share its flood forecasts with these organizations who can then disseminate the information through their own networks. ","Google uses machine learning to alert users in India that a flood may occur in their location. With the expansion, ""200 million people in India and 40 million people in Bangladesh can now receive alerts from its flood forecasting system."""
366,"U of Texas at Austin has stopped using a machine-learning system to evaluate applicants for its Ph.D. in computer science. Critics say the system exacerbates existing inequality in the field..In 2013, the University of Texas at Austin’s computer science department began using a machine-learning system called GRADE to help make decisions about who gets into its Ph.D. program -- and who doesn’t. This year, the department abandoned it..Before the announcement, which the department released in the form of a tweet reply, few had even heard of the program. Now, its critics -- concerned about diversity, equity and fairness in admissions -- say it should never have been used in the first place..“Humans code these systems. Humans are encoding their own biases into these algorithms,” said Yasmeen Musthafa, a Ph.D. student in plasma physics at the University of California, Irvine, who rang alarm bells about the system on Twitter. “What would UT Austin CS department have looked like without GRADE? We’ll never know.”.GRADE (which stands for GRaduate ADmissions Evaluator) was created by a UT faculty member and UT graduate student in computer science, originally to help the graduate admissions committee in the department save time. GRADE predicts how likely the admissions committee is to approve an applicant and expresses that prediction as a numerical score out of five. The system also explains what factors most impacted its decision..The UT researchers who made GRADE trained it on a database of past admissions decisions. The system uses patterns from those decisions to calculate its scores for candidates..For example, letters of recommendation containing the words “best,” “award,” “research” or “Ph.D.” are predictive of admission -- and can lead to a higher score -- while letters containing the words “good,” “class,” “programming” or “technology” are predictive of rejection. A higher grade point average means an applicant is more likely to be accepted, as does the name of an elite college or university on the résumé. Within the system, institutions were encoded into the categories “elite,” “good” and “other,” based on a survey of UT computer science faculty..Every application GRADE scored during the seven years it was in use was still reviewed by at least one human committee member, UT Austin has said, but sometimes only one. Before GRADE, faculty members made multiple review passes over the pool. The system saved the committee time, according to its developers, by allowing faculty to focus on applicants on the cusp of admission or rejection and review applicants in descending order of quality..For what it’s worth, GRADE did appear to successfully save the committee time. In the 2012 and 2013 application seasons, developers said in a paper about their work, it reduced the number of full reviews per candidate by 71 percent and cut the total time reviewing files by 74 percent. (One full review typically takes 10 to 30 minutes.) Between the years 2000 and 2012, applications to the computer science Ph.D. program grew from about 250 to nearly 650, though the number of faculty able to review those applications remained mostly constant. In the years since 2012, the number of applications has reached over 1,200..The university’s use of the technology escaped attention for a number of years, until this month, when the physics department at the University of Maryland at College Park held a colloquium talk with the two creators of GRADE..The talk gained attention on Twitter as graduate students accused GRADE’s creators of further disadvantaging underrepresented groups in the computer science admissions process..“We put letters of recommendation in to try to lift people up who have maybe not great GPAs. We put a personal statement in the graduate application process to try to give marginalized folks a chance to have their voice heard,” said Musthafa, who is also a member of the Physics and Astronomy Anti-Racism Coalition. “The worst part about GRADE is that it throws that out completely.”.Advocates have long been concerned about the potential for human biases to be baked into or exacerbated by machine-learning algorithms. Algorithms are trained on data. When it comes to people, what those data look like is a result of historical inequity. Preferences for one type of person over another are often the result of conscious or unconscious bias..That hasn’t stopped institutions from using machine-learning systems in hiring, policing and prison sentencing for a number of years now, often to great controversy..“Every process is going to make some mistakes. The question is, where are those mistakes likely to be made and who is likely to suffer as a result of them?” said Manish Raghavan, a computer science Ph.D. candidate at Cornell University who has researched and written about bias in algorithms. “Likely those from underrepresented groups or people who don’t have the resources to be attending elite institutions.”.Though many women and people who are Black and Latinx have had successful careers in computer science, those groups are underrepresented in the field at large. In 2017, whites, Asians and nonresident aliens received 84 percent of degrees awarded for computer science in the United States..Raghavan said he was surprised that there appeared to be no effort to audit the impacts of GRADE, such as how scores differ across demographic groups..GRADE’s creators have said that the system is only programmed to replicate what the admissions committee was doing prior to 2013, not to make better decisions than humans could. The system isn’t programmed to use race or gender to make its predictions, they’ve said. In fact, when given those features as options to help make its predictions, it chooses to give them zero weight. GRADE’s creators have said this is evidence that the committee’s decisions are gender and race neutral..Detractors have countered this, arguing that race and gender can be encoded into other features of the application that the system uses. Women’s colleges and historically Black universities may be undervalued by the algorithm, they’ve said. Letters of recommendation are known to reflect gender bias, as recommenders are more likely to describe female students as “caring” rather than “assertive” or “trailblazing.”.In the Maryland talk, faculty raised their own concerns. What a committee is looking for might change each year. Letters of recommendation and personal statements should be thoughtfully considered, not turned into a bag of words, they said..“I’m kind of shocked you did this experiment on your students,” Steve Rolston, chair of the physics department at Maryland, said during the talk. “You seem to have built a model that builds in whatever bias your committee had in 2013 and you’ve been using it ever since.”.In an interview, Rolston said graduate admissions can certainly be a challenge. His department receives over 800 graduate applications per year, which takes a good deal of time for faculty to evaluate. But, he said, his department would never use a tool like this..“If I ask you to do a classifier of images and you’re looking for dogs, I can check afterwards that, yes, it did correctly identify dogs,” he said. “But when I’m asking for decisions about people, whether its graduate admissions, or hiring or prison sentencing, there’s no obvious correct answer. You train it, but you don’t know what the result is really telling you.”.“If I give you a file and say, ‘Well, the algorithm said this person shouldn’t be accepted,’ that will inevitably bias the way you look at it,” he said..It was never used to make decisions to admit or reject prospective students, as at least one faculty member directly evaluates applicants at each stage of the review process, a spokesperson for the Graduate School said via email..“Changes in the data and software environment made the system increasingly difficult to maintain, and its use was discontinued,” the spokesperson said via email. “The Graduate School works with graduate programs and faculty members across campus to promote holistic application review and reduce bias in admissions decisions.”.“The entire system is steeped in racism, sexism and ableism,” they said. “How many years of POC computer science students got denied [because of this]?”.Addressing that inequity -- as well as the competitiveness that led to the creation of GRADE -- may mean expanding committees, paying people for their time and giving Black and Latinx graduate students a voice in those decisions, they said. But automating cannot be part of that decision making..“If we automate this to any extent, it’s just going to lock people out of academia,” Musthafa said. “The racism of today is being immortalized in the algorithms of tomorrow.”","University Texas at Austin has stopped using a machine-learning system GRADE to evaluate applicants for its Ph.D. in computer science. Critics say the system exacerbates existing inequality in the field.
"
368,"Mere hours after supporters of former president Donald Trump forced their way into the Capitol Building on Jan. 6, sleuths, both amateur and professional, took up the task of combing through the voluminous videos and photos on social media to identify rioters. Facial recognition technology—long reviled by police reform advocates as inaccurate and racially biased—was suddenly everywhere..A college student in Washington, D.C., used facial recognition to extract faces from videos on social media. The Washington Post used facial recognition to count the number of individual faces at the Capitol Building attack, and a researcher from Citizen Lab used it to identify people involved in the riots. And when the FBI posted photos of rioters, looking for help with identification, the Miami Police Department assigned two detectives to scan faces into the department’s Clearview facial recognition app. .The episode was a reminder that facial recognition software is now ubiquitous in the private and public sectors—a fact that often gets overlooked as cities pass high-profile laws that purport to ban law enforcement from using the technology. The Markup examined 17 bans passed in the past couple of years, speaking with local officials and reading through official documents. In six of those cities, officials either told The Markup or otherwise publicly stated that loopholes in the bans effectively allow police to access information garnered through facial recognition..The bans in Pittsburgh; Boston; Alameda, Calif.; Madison, Wis.; Northampton, Mass.; and Easthampton, Mass., all have language in their regulations that may allow local police to continue using facial recognition through state and federal agencies or the private sector..Some say such loopholes are a good thing: Following the riots in D.C., Massachusetts governor Charlie Baker said keeping facial recognition technology as a tool is necessary precisely because of situations like the Jan. 6 riot. Late last year, Baker pushed for exceptions to a statewide restriction on facial recognition before agreeing to sign the bill. .“The realist in me has no doubt that police departments will try to wedge in any kind of loopholes around the use of this technology or any other sort of tool that they have at their disposal. This is not a surveillance problem; this is a policing problem,” Mohammad Tajsar, a senior staff attorney for the American Civil Liberties Union of Southern California, said. “If you create a carve-out for the cops, they will take it.”.“There’s a part of the legislation that says it doesn’t apply to us using software produced or shared by other police departments,” he said at the meeting. “This does not stop facial recognition.”.The ordinance has a section that notes that the law “shall not affect activities related to databases, programs, and technology regulated, operated, maintained, and published by another government entity.”.The Alameda Police Department didn’t respond to The Markup’s questions on that city’s ban, but when the ban passed, an assistant city manager testified that the “software could be leveraged as a resource in the scenario of a crime spree involving the Federal Bureau of Investigations [sic],” which uses facial recognition, but “the technology is not something the City of Alameda would be paying for or directly seeking.”.Tyler Grigg, a public information officer for the Madison Police Department, told The Markup that officers can use facial recognition provided by businesses even though it’s banned from government use. .In Easthampton, Mass., the ban still allows police to use facial recognition as evidence if it comes from another law enforcement agency, but not businesses, Dennis Scribner, a public information officer for the Easthampton Police Department, said. .Northampton, Mass., police chief Jody Kasper told The Markup that the department could use information from facial recognition provided by both outside agencies and businesses..Tali Robbins, policy director for Boston city councilor Michelle Wu, who authored that city’s ban, confirmed that Boston police may have access to facial recognition technology through other agencies. .Kade Crockford, director of the Technology for Liberty program at the ACLU of Massachusetts, said it can be difficult for police to effectively track where their tips are coming from and ensure that facial recognition wasn’t used..“We want the ordinances to actually have an impact,” Crockford said. “If they’re too narrow in the sense that they restrict law enforcement conduct like the use of information that comes from an outside agency, we worry there’s a slippery slope that they’ll just ignore it.”.However, some argue police are misinterpreting the local bans already in place. Chad Marlow, a senior advocacy and policy counsel at the ACLU, told The Markup that Northampton police should not be able to access the technology, through any means, under that city’s ban. .“They are not allowed to spend any resources, including personnel time, on facial recognition. That’s what the law says. There are no carve-outs in the law,” Marlow said about Northampton’s police chief’s interpretation..An NBC Miami investigation found that Miami police arrested protesters using facial recognition. The arrest reports noted only that police had identified suspects using “investigative means,” and even defense attorneys said they were not aware facial recognition was used until approached by NBC.  .Jacksonville police arrested a man for selling $50 of cocaine and identified him by using facial recognition but didn’t disclose the technology’s use in the police report..“Even when facial recognition is being used in investigations, it’s typically hidden,” Jake Laperruque, a senior counsel at the Constitution Project, said..Police, for instance, can get tips based on a private business’s use of facial recognition software. Companies like Rite Aid, Home Depot, and Walmart have implemented or tested the technology in their stores. .Cities banned facial recognition for police use because of its known bias against people of color and women, and it’s no different when businesses are using the technology, Laperruque said..“This stuff can be wrong a lot, and it’s especially wrong for people of color,” he said. “If this is something that’s going to lead to a store calling the police on a person, that to me creates a lot of the same risks if you worry about facial recognition misidentifying someone by the police.”.A New York teen recently filed a multimillion-dollar lawsuit against Apple in the Southern District Court of New York, alleging he was misidentified as a chronic shoplifter when Apple’s security firm linked his name to surveillance footage of a different person. The actual shoplifter, according to the complaint, had stolen the teen’s driver’s permit and presented it to security when he was caught shoplifting at multiple Apple stores. .When New York police officers arrested Ousmane Bah, the lawsuit says, they quickly realized they had the wrong person, telling Bah he was likely “incorrectly identified based on a facial recognition system utilized by Apple or [Security Industry Specialists].”.Information also flows freely among law enforcement agencies that may be operating under different regulations. In San Francisco, the first city to ban the technology, controversy ensued when facial recognition showed up in a criminal case last September. .San Francisco police had sent out a bulletin request looking for help identifying a gun discharge suspect in a photo. Another law enforcement agency—The Northern California Regional Intelligence Center (NCRIC)—responded with an ID they derived from using PhotoMatch facial identification software. .NCRIC, a partnership of federal, state, and local departments, does not fall under the jurisdiction of San Francisco’s facial recognition ban, executive director Mike Sena told The Markup, and runs facial recognition searches anytime it gets an identification request. .Public officials in San Francisco, however, raised a fuss when they became aware of the case, insisting that the city’s ban precluded the department from using NCRIC’s identification. (The SFPD claims several officers recognized the suspect on their own before receiving the facial recognition match.) .SFPD public information officer Michael Andraychak told The Markup that going forward, the department “would not be able to use any identification obtained via facial  recognition software.”.Last September, Portland, Ore., passed the most comprehensive facial recognition ban to date, prohibiting not only law enforcement use but also use in places of public accommodation (e.g., restaurants and other places open to the general public)..“Once we started doing our due diligence to develop our own policy, we started getting a lot of community feedback and recognizing the role that private businesses are having in connecting people’s information,” said Hector Dominguez, the Open Data Coordinator with Portland’s Smart City PDX..But there was pushback from industry groups, revealing just how widespread the technology has become. Tech giant Amazon lobbied the city for the first time ever because of the measure, spending $12,000. The Portland Business Alliance asked for carve-outs to the law for airlines, banks, hotels, retailers, concert venues, and amusement parks, while the Oregon Bankers Association asked for exceptions to allow use of facial recognition to provide police evidence in robberies. .The Portland ban does allow one exception: Businesses and agencies operating within the city may use facial recognition if they say they must do so to comply with federal, state, or local laws (such as Customs and Border Protection, operating at the airport). But businesses, ultimately, were included in the ban—a move ban advocates say was necessary..“Industry often has more of an onus to surveil than police do in everyday circumstances,” Lia Holland, an organizer in Portland with Fight for the Future, said. “The impunity to save that data forever, to match those faces to customer’s faces, is something that police departments might not have the capacity to do in the same way that a company does.”.We’re happy to make this story available to republish for free under the conditions of an Attribution–NonCommercial–No Derivatives Creative Commons license. Please adhere to the following:",More than a dozen cities have passed facial recognition bans in the past couple of years. But all these laws have language in their regulations that may allow local police to continue using facial recognition through state and federal agencies or the private sector.
369,"Job hunters may now need to impress not just prospective bosses but artificial intelligence algorithms too—as employers screen candidates by having them answer interview questions on a video that is then assessed by a machine..HireVue, a leading provider of software for vetting job candidates based on an algorithmic assessment, said Tuesday it is killing off a controversial feature of its software: analyzing a person’s facial expressions in a video to discern certain characteristics..Job seekers screened by HireVue sit in front of a webcam and answer questions. Their behavior, intonation, and speech is fed to an algorithm that assigns certain traits and qualities..HireVue says that an “algorithmic audit” of its software conducted last year shows it does not harbor bias. But the nonprofit Electronic Privacy Information Center had filed a complaint against the company with the Federal Trade Commission in 2019..HireVue CEO Kevin Parker acknowledges that public outcry over the use of software to analyze facial expressions in video was part of the calculation. “It was adding some value for customers, but it wasn’t worth the concern,” he says..The algorithmic audit was performed by an outside firm, O’Neil Risk Consulting and Algorithmic Auditing. The company did not respond to requests for comment..Alex Engler, a fellow at the Brookings Institution who has studied AI hiring, says the idea of using AI to determine someone’s ability, whether it is based on video, audio, or text, is far-fetched. He says it is also problematic that the public cannot vet such claims..“There are parts that machine learning can probably help with, but fully automated interviews, where you’re making inferences about job performance—that’s terrible,” he says. “Modern artificial intelligence can’t make those inferences.”.HireVue says that about 700 companies, including GE, Unilever, Delta, and Hilton, use its technology. The software requires job applicants to respond to a series of questions in a recorded video. The company’s software then analyzes various characteristics including the language they use, their speech, and, until now, their facial expressions. It then provides an assessment of the applicant’s suitability for a job, as well as a measure of traits including “dependability,” “emotional intelligence,” and “cognitive ability.”.Parker says the company helped screen more than 6 million videos last year, although sometimes this involved simply transcribing answers for an interviewer rather than performing an automated assessment of candidates. He adds that some clients let candidates opt out of automated screening. And he says HireVue has developed ways to avoid penalizing candidates with spotty internet connections, automatically referring those candidates to a human..AI experts warn that algorithms trained on data from previous job applicants may perpetuate existing biases in hiring. Lindsey Zuloaga, HireVue’s chief data scientist, says the company screens for bias on gender, race, and age by collecting that information in training data and looking for signs of bias..But she acknowledges that it may be more difficult to know if the system is biased on factors such as income or education level, or if it could be affected by something like a stutter..“I am surprised they are dropping this, as it was a keystone feature of the product they were marketing,” says John Davisson, senior counsel at EPIC. “That is the source of a lot of concerns around  biometric data collection, as well as these bold claims about being able to measure psychological traits, emotional intelligence, social attitudes, and things like that.”.The use of facial analysis to determine emotion or personality traits is controversial; some experts warn that the underlying science is flawed..Lisa Feldman Barrett, a professor at Northeastern University who studies analysis of emotion, says a person’s face does not on its own reveal emotion or character. “Just by looking at someone smiling, you can’t really tell anything about them except maybe that they have nice teeth,” she says. “It is a bad idea to make psychological inferences, and therefore determine peoples outcomes, based on facial data alone.”.EPIC’s FTC complaint accused HireView of failing to guarantee fairness and of using algorithms that cannot be vetted. It also accused the company of misrepresenting its technology by claiming not to use facial recognition. Davisson says the agency has not yet acted on the complaint..But Davisson says he worries that automated analysis of speech could still have problems, and he says it is important that companies release the results of algorithmic audits. He says HireVue’s technology still needs to be vetted thoroughly..“I’m certainly concerned that the same potential issues around data collection and bias and opacity would just carry over to an audio-based screening system.AI hiring has caught the attention of some regulators. A bill before the New York City Council proposes regulating the use of hiring software by requiring employers to inform candidates when they are being assessed by AI, and requiring them to audit their algorithms every year..An Illinois law requires consent from candidates for analysis of video footage. Maryland has banned the use of facial analysis. In 2018, Amazon reportedly abandoned the use of its own technology for automating the assessment of candidate résumés due to biased results.","HireVue, a leading provider of software for vetting job candidates based on an algorithmic assessment is killing off a controversial feature of its software: analyzing a person’s facial expressions in a video to discern certain characteristics. Some experts say that the underlying science of using facial analysis to determine personality/emotion is flawed."
371,"The Department of National Defence headquarters is pictured in Ottawa in 2013. The Defence Department told The Globe and Mail that it used two AI-driven Canadian hiring services – Knockri and Plum­ – to shortlist candidates as part of a diversity recruitment campaign.Adrian Wyld/The Canadian Press.The Department of National Defence tested the use of artificial intelligence last year in an effort to improve diversity in the workplace, but the project was run outside of federal rules aimed at ensuring that the technology is used responsibly..The Privacy Commissioner’s office said DND failed to provide it with a privacy impact assessment, as required under a Treasury Board Secretariat directive. A spokesperson for the commissioner said the office has reached out to DND for more information..“The use of AI to make important decisions such as hiring can raise privacy and human-rights concerns,” Vito Pilieci said. “A [privacy impact assessment] should have been completed and submitted prior to the use of these AI platforms.”.Under a separate Treasury Board directive, federal bodies must also fill out and publish algorithmic impact assessments for all AI tools. These measure the potential for bias and other risks associated with using this kind of predictive technology. However, DND did not complete one of those, either..The Defence Department told The Globe and Mail that it used two AI-driven Canadian hiring services – Knockri and Plum­ – to shortlist candidates as part of a diversity recruitment campaign. The companies, DND says, provide hiring managers with behavioural assessments and measurements of the “personalities, cognitive abilities and social acumen” of applicants..Ashley Casovan, a former Treasury Board director of digital and data and an author of the AI policy, said she was always concerned that agencies would ignore it..Since April, 2020, when Ottawa’s policy on AI use came into force, only a single algorithmic impact assessment has been submitted. That submission was by the Treasury Board itself..This isn’t the first time federal agencies have disregarded the Treasury Board’s data directives. In 2016, the department began requiring them to submit an inventory of all the datasets in their possession – yet several never complied..The Royal Canadian Mounted Police, for instance, has never submitted one. RCMP spokesperson Robin Percival said the agency did “not currently have a timeline for submitting a completed open data inventory,” and that it was consulting with the Treasury Board..The DND campaign, aimed at recruiting people for the department’s executive ranks, closed on Sept. 25. The department said all applicants were given the choice to consent to the use of AI or go through an alternative screening method..Andrée-Anne Poulin, a spokesperson for DND, said the department, but not the Canadian Forces, has used AI in its recruitment work. Because “final decisions” weren’t made using AI, she said, it didn’t complete the Treasury Board’s algorithmic assessment. The department did not immediately respond to questions about the privacy impact assessment..Ms. Poulin said the department conducted internal and external consultations before selecting Knockri and Plum. She also noted that the National Research Council recently completed an “ethics assessment” of Knockri’s service. The Globe asked the NRC if it had conducted a similar assessment of Plum but the agency declined to answer, citing confidentiality issues..In a statement, Treasury Board spokesperson Alain Belle-Isle did not respond to specific questions about DND, but said policy “requires federal departments and agencies to complete an algorithmic impact assessment” before the technology is employed..Knockri and Plum’s websites say they offer AI services as a way to reduce bias in hiring decisions. Knockri did not respond to several e-mails for this story, and Plum declined to comment..DND said Knockri’s service was used to assess behaviours expected of its executive group. Participants answered questions via video, but the department said “visual and auditory identifiers” were not assessed..In total, the department said it spent $179,000 on the recruitment campaign, which included contracts with the two AI companies as well as consulting fees with Deloitte Canada, and other costs, such as unconscious bias training for staff. The AI services were used to whittle down a list of 422 candidates to 34 “top tier” ones, DND said..Ms. Casovan, the former senior Treasury Board official, said an algorithmic assessment would have identified possible privacy and fairness concerns surrounding Knockri and Plum. Both companies collect personal information on applicants, including psychological profiles and video and audio recordings..“What is happening with that data?” asked Ms. Casovan, who now serves as executive director of AI Global, a non-profit group devoted to building tools for responsible use of AI. “Where is it being stored? How long does the company have access to that for? Did it go through the U.S.? These are things that I would have a lot of questions about.”.Fenwick McKelvey, an associate professor of communications at Concordia University who studies the use of AI, said there’s a need for better regulation around the technology..“This [algorithmic directive] was, I think, a centrepiece of the government’s response to that,” he said. “So if they don’t actually follow it, and they don’t commit to it, it undermines the legitimacy of the whole process.”.Prof. McKelvey noted that using AI has previously led to problems in the hiring process. In 2018, a tool created by Amazon led to unintentional bias against women because it was based on a male-dominated database of résumés..“It’s kind of a strange paradox here where the very devices which have been called out for their hidden biases are trying to address hidden biases,” he said.","The Department of National Defence tested the use of artificial intelligence last year in an effort to improve diversity in the workplace, but the project was run outside of federal rules aimed at ensuring that the technology is used responsibly."
372,"LONDON, UNITED KINGDOM - 2021/01/03: Deliveroo courier rides along the Regent Street delivering ... [+] Takeaway food in central London during covid 19 tier 4 restrictions. (Photo by Pietro Recchia/SOPA Images/LightRocket via Getty Images).The ruling found that the algorithm, which was used to evaluate delivery riders on the platform, was in violation of labor laws because it did not differentiate between the reasons a rider may have for not working. For example, it evaluated a rider whether they were not working because they were sick or simply choosing not to work..The court in Bologna said that Deliveroo would have to pay €50,000 to each affected rider. CGIL, the country’s largest trade union, said the decision was an important one in shoring up protections for gig economy workers..This judgement refers to a historic optional booking model which is not used by Deliveroo in Italy or other markets, a Deliveroo spokesperson said in a statement, adding that the system is no longer in use..Riders have complete flexibility to choose when to work, where to work, for as little or as long as they want. This means that there is no booking system and no obligation to accept work..The company defended its stance on self-employment for its riders: We offer self-employment because this offers the flexibility riders want. It cited a survey that said more than 80% of riders value work flexibility..Deliveroo as well as other on-demand delivery companies have still faced challenges to their self-employed model for gig economy workers in various markets with mixed results. The UK High Court previously ruled in Deliveroo’s favor in a case over whether riders had collective bargaining rights. On the flipside, Spain’s Supreme Court found in September that riders for Deliveroo and Glovo should be classed as employees.",An Italian court ruled that an algorithm once used by Deliveroo to assess riders on its platform was discriminatory. The ruling found that the algorithm was in violation of labor laws because it did not differentiate between the reasons a rider may have for not working.
373,"The technologies Facebook uses to put advertising it deems relevant in front of people may be more responsible for the polarization of American politics than previously understood, a team of researchers has concluded..Their findings, which have not been previously reported, are the first to demonstrate a skew in the delivery of political ads based on the content of those ads and the information Facebook has on users — not on the targeting decisions made by a political candidate or campaign..This arrangement benefits Facebook, a social-networking company that profits from user engagement. It also benefits politicians when their aims include fundraising and motivating their base. But it may undermine public debate, the research suggests, and make it more difficult for denizens of the digital world to be well-informed citizens of a flesh-and-blood democracy..When the researchers sought to force Facebook to show posts to users not already aligned with the ideology of the advertising, the cost of the advertising rose. The effect is that a campaign pays more when it tries to speak to the other side, say the researchers, who are computer scientists at Northeastern University and the University of Southern California, as well as the managing director of the technology nonprofit organization Upturn..The researchers spent more than $13,000 on a set of advertising campaigns designed to test how Facebook promotes political messaging. The partisan skew appeared in their experiment, the authors stress, even though all ads were run from the same account and at the same time. The ads all focused on the same audiences and used the same “goal, bidding strategy, and budget.”.By slotting liberal ads to liberal users and conservative ads to conservative users, the study warns, Facebook is “wielding significant power over political discourse through its ad delivery algorithms without public accountability or scrutiny.”.“Findings showing that ads about a presidential candidate are being delivered to people in their political party should not come as a surprise,” said Joe Osborne, a spokesman for the company. “Ads should be relevant to the people who see them. It’s always the case that campaigns can reach the audiences they want with the right targeting, objective and spend.”.Facebook’s delivery decisions rely on artificial intelligence, vaunted as a revolution in commercial advertising. In the context of politics, however, an algorithm’s determination of relevance “has the potential to be quite destructive,” said one of the study’s authors, Aleksandra Korolova, an assistant professor of computer science at USC..Eli Pariser, the Internet activist who coined the notion of the “filter bubble” to explain the online cocoon created by algorithms that serve users with information that aligns with their existing worldview, echoed that concern. The research reveals how digital advertising — not just on Facebook but on peer platforms as well — “fragments political discourse,” he said, and shortchanges users, who are not clued in on how tech giants are categorizing their political views..“Advertising that performs the best is most targeted to the core base, which, if you’re a soap or pillow company, doesn’t matter, but if you’re a political candidate, it has some distortive effects,” Pariser said. “Even if you’re trying to reach people who disagree with you, or reach across the aisle, that is increasingly difficult and expensive.”.Daniel Kreiss, a professor of political communication at the University of North Carolina at Chapel Hill who was not involved in the research, said the findings are “on point.” The paper breaks new ground, he added, in documenting the gulf “between who campaigns target and who is actually shown an ad.”.The findings arrive at a moment of rapid transformation in political advertising, which has spurred debates about bias, polarization and privacy — even as the digital infrastructure of promotion technologies remains poorly understood..Research has shown that optimizing ad delivery for user engagement can skew results based on race and gender. But the increasing prominence of digital advertising in politics makes the new results especially noteworthy. As recently as 2010, just 0.2 percent of spending on political marketing went to online advertising, according to the paper. More than a quarter is expected to be funneled online in 2020, the authors note..When campaigns relied on television and newspaper advertising, their messaging was bound to reach a mixed audience of different races, income levels and ideologies. Now, digital platforms have handed politicians precise methods of identifying and targeting would-be supporters — zeroing in on particular demographics or uploading voter data to hunt for potential matches. They are also able to choose different objectives, such as displaying the ad to the largest number of users, which is the option used in the study..The use of these technologies in 2016, including their manipulation by Russian actors as part of the Kremlin’s sweeping disinformation campaign, caused public outcry. It also touched off a raft of legislation — which has languished in the GOP-controlled Senate — and a flurry of research by scholars, who have raised alarm about the lack of transparency and access to the data needed to scrutinize platforms like Facebook. The company, meanwhile, has strained to balance the sort of data sharing that enables academic research with its promise to protect the privacy of its users..Sweeping changes are now arriving, less than a year before the 2020 vote. Twitter barred political advertising altogether, while Google restricted narrow outreach known as microtargeting. Facebook is contemplating changes..Part of what the new research shows is that any move to curtail targeting opportunities offered to advertisers may have only limited impact, given the powerful targeting baked into Facebook’s delivery mechanism — in a way that’s not fully visible to advertisers..“Our work shows that just getting rid of microtargeting isn’t going to solve the problem,” said Alan Mislove, one of the paper’s authors and a computer scientist at Northeastern. “Facebook will deliver the ad to the subset of users that Facebook thinks is interested.”.As the paper observes, campaigns purchasing political ads to target a geographic area might expect to reach users whose political views are roughly representative of opinions in the region. “To discover otherwise would require careful research,” the paper warns, because the precise political segmentation performed by Facebook is opaque not just to the public at large but to the campaigns that are investing resources on the platform..The reason is that Facebook provides only limited information to advertisers about how their messaging is performing. That includes the number of times the ad was shown and the number of users who saw it, for example, as well as the breakdown of these metrics by gender, age and location. The company does not report how the ad was received according to Facebook’s prediction of user interest in political content..But the researchers gained insight into how political leaning affected ad performance by creating a set of audiences — one set divided by party using public records in North Carolina and another by attributes provided by Facebook, including “Likely engagement with US political content (Conservative)” and “Likely engagement with US political content (Liberal).”.They used ads for President Trump and for a potential Democratic rival, Sen. Bernie Sanders (I-Vt.), in most cases repurposing messaging created by the two campaigns. They also had a neutral ad that simply encouraged users to vote. How the ads played in the distinct groups sheds light on how Facebook seeks to match messaging to a friendly audience..The ads for Trump were shown disproportionately to Republicans, while the ads for Sanders were shown disproportionately to Democrats. The neutral ads were shown more evenly. Meanwhile, the paper notes, “the only difference between them is the content and destination link of the ad.”.That final point is crucial. Facebook’s predictive technologies are so thorough that they effectively slotted the ads according to partisan alignment even when the post was neutral. The content of the linked website alone — either Trump’s campaign or Sanders’s — was sufficient to send ads to different audiences. To the authors, this effect illustrated that Facebook’s delivery decisions rest not on the requests of the campaigns buying them or on the live reactions of Facebook users, but at least in part on the platform’s priorities..The company hints at this approach in documentation for advertisers, saying Facebook opts to “subsidize relevant ads in auctions, so more relevant ads often cost less and see more results.”.But how Facebook defines relevance, especially in the context of politics, is less clear, alarming experts. Ad placement on other platforms, such as Google, similarly relies on relevance..“If a voter is being asked to vote on a ballot measure or candidate, ads from both sides are equally relevant to them — but there is probably one side they ‘like’ more,” said Laura Edelson, a researcher at New York University’s Tandon School of Engineering who was not involved in the research but praised its results. “This is a classic example of how a system designed for commercial advertising just doesn’t work for political advertising.”.A possible limitation of the findings, Edelson said, is whether they would apply to lesser-known candidates, whose messaging may be more difficult to categorize. Other researchers noted that higher budgets would also shift the results. The paper’s authors were surprised that an additional experiment using political donors to Trump and Sanders didn’t result in a similar skew, hypothesizing that Facebook lacked sufficient information about these specific users to know which ads they would prefer..The authors stress that the delivery of Facebook ads may have negative implications for users and candidates alike, calling into question the effectiveness of a medium that is quickly becoming a staple of modern campaigning — and a sign of digital savvy — even though its functioning remains something of a black box. Light shed on the system’s internal workings by the new paper suggests that Facebook’s algorithm limits exposure to diverse viewpoints, while creating barriers for candidates who aim to reach across the aisle..“Put simply, Facebook is making decisions about which ads to show to which users based on its own priorities,” the authors conclude, meaning “user engagement with or value for the platform.”","The technologies Facebook uses to show advertisements may be more responsible for the polarization of American politics than previously understood, a team of researchers has concluded. By slotting liberal ads to liberal users and conservative ads to conservative users, the study warns, Facebook is “wielding significant power over political discourse through its ad delivery algorithms without public accountability or scrutiny.”"
374,"iPhone and Mac users have discovered that a feature allowing parents to protect their children from adult content behaves erratically, despite years of repeated complaints to Apple..A Twitter user who goes by Steven Shen found out that sites featuring the word “Asian” are inexplicably banned when he turned on his iPhone’s adult content filters..The software tends to block sites and searches featuring words including “ebony,” “daddy,” “massage,” “babe,” “hardcore,” “teen,” and “amateur”— innocent terms that take on new meanings only in the context of pornography. They are some of PornHub’s most searched-for terms..At the same time, the filter fails to catch some of the most popular porn sites, including all adult Reddit sections and popular regional porn sites like ThisAV, which is frequented by Japanese, Taiwanese, and Hong Kong users, according to web ranking service Alexa..OnlyFans, a social media network that allows people to sell photos, often nudes of themselves, also slips through, although it requires a subscription to view content..Some words that are more closely associated with sex are not restricted, such as “sadomasochism,” “threesome,” “jerk off,” and “wank.” And Apple’s interpretation of “adult content” doesn’t seem to include violent imagery. Searches for “how to make a bomb,” “snuff film,” and “murder footage” are all unrestricted under the content restriction mode on iPhones and Macs..The flaws suggest that Apple is not applying the same technology on parental controls that they use in many other applications. Such technology, often based on artificial intelligence and machine learning, allows a program to detect objects in images and analyze language. Companies such as Google and Facebook use similar tools to block nudity on their sites..Li Xiaofan, assistant professor of information systems and analytics at the National University of Singapore, told VICE World News that Apple’s filtering method is ineffectual and unsophisticated..“My guess is that they trained an algorithm by providing it samples of pages [that] should be censored and samples of pages [that] should not be censored. Then the algorithm learned the keywords associated with pages [that] should be censored. However, the sample size may not be large or representative enough and the algorithm itself could be ill-designed,” Li said..The feature has not noticeably improved since users started complaining about it as early as 2014. The sex education site O.school in 2018 reported on its inconsistency and provided a number of glaring omissions, but those sites are still either incorrectly blocked or let through..“I would say that most parental control solutions have certain things that they struggle with,” McKenna told VICE World News. “And iOS is the perfect example where Apple has essentially made it difficult for parental control solutions to do the job that parents want them to do.”.“There is a definite responsibility that parents have to protect their children online. But this is a shared responsibility with organizations that have endless resources and are not fulfilling their half of this bargain,” he said..“All children need to know that they can land safely and softly with you as a parent. And then you use parental controls in full transparency and honesty with your kids… That digital trust gets built when you have that good balance between relational and technical solutions,” he said..By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.",Apple’s flawed adult content filters are blocking innocuous sites while letting actual pornography through.
375,"A social media-based chatbot developed by a South Korean startup was shut down on Tuesday after users complained that it was spewing vulgarities and hate speech..The fate of the Korean service resembled the demise of Microsoft’s Tay chatbot in 2016 over racist and sexist tweets it sent, raising ethical questions about the use of artificial intelligence (AI) technology and how to prevent abuse..The Korean startup Scatter Lab said on Monday that it would temporarily suspend the AI chatbot. It apologized for the discriminatory and hateful remarks it sent and a “lack of communication” over how the company used customer data to train the bot to talk like a human..The startup designed Lee Luda, the name of the chatbot, to be a 20-year-old female university student who is a fan of the K-pop girl group Blackpink..Launched in late December to great fanfare, the service learned to talk by analyzing old chat records acquired by the company’s other mobile application service called Science of Love..Before the bot was suspended, users said they received hateful replies when they interacted with Luda. Michael Lee, a South Korean art critic and former LGBTQ activist, shared screenshots showing that Luda said “disgusting” in response to a question about lesbians..Another user, Lee Kwang-suk, a professor of Public Policy and Information Technology at the Seoul National University of Science and Technology, shared screenshots of a chat where Luda called “Black people” heukhyeong, meaning “black brother,” a racial slur in South Korea. The bot was also shown to say, “Yuck, I really hate them,” in a response to a question about transgender people. The bot ended the message with a crying emoticon..In the Monday statement, Scatter Lab defended itself and said it did “not agree with Luda’s discriminatory comments, and such comments do not reflect the company’s ideas.”.“Luda is a childlike AI who has just started talking with people. There is still a lot to learn. Luda will learn to judge what is an appropriate and better answer,” the company said..Lee, the IT professor, told VICE World News that the company has a responsibility for the abuse, comparing the case to Microsoft’s shutdown of its Tay chatbot..Another user, Lee Youn-seok, who participated in a beta test of Luda in July before it was officially launched, told VICE World News that the outcome was “predictable.”.Some people said that the debacle was unsurprising given the sex ratio of the company’s employees. A page on the company website suggested that about 90 percent of the group behind the bot were men. The page was later removed..Some male-dominated online communities also openly discussed how to “enslave” the AI bot and shared their methods to “harass” it sexually, hoping to elicit sexual comments from Luda..Some politicians and rights advocates have taken the opportunity to call for an anti-discrimination bill, which seeks to ban all discrimination based on gender, disability, age, language, country of origin, and sexual orientation..The anti-discrimination bill could be used to hold AI software developers accountable for such abuse, Ahn Byong-jin, a professor at Kyunghee University in Seoul, told VICE World News. “Companies should consult a philosopher or ethicist before launching a service to prevent such abuse,” he said.",A social media-based chatbot developed by a South Korean startup was shut down on Tuesday after users complained that it was spewing vulgarities and hate speech.
376,"Gabe Teninbaum, a professor at Suffolk University Law School, is calling on ExamSoft to fix a serious bug with its test-taking software: failure to recognize faces. It’s a problem that can delay test takers — or bar them from starting their exams altogether — and per reports, it disproportionately impacts people with dark skin tones. .ExamSoft’s software records students while they complete remote exams and monitors for signs of academic dishonesty. Teninbaum’s report addresses an ExamSoft feature called ExamID, which aims to verify that test-takers are who they say they are. The first time a student logs into their exam portal, they upload a photo of themselves (their “baseline image”); they’re then prompted to take another selfie before beginning future exams, which the software checks against their original photo. .Research has found that facial-recognition algorithms consistently make more errors in identifying Black faces than they do white ones. And while those studies didn’t focus on ExamSoft specifically, it doesn’t appear to be an exception. Back in September, multiple non-white exam-takers told the New York Times that the software couldn’t identify them due to “poor lighting” — a problem that Teninbaum, who has light skin, wasn’t able to replicate. .Early this fall, Teninbaum set out to find a fix. He believes such errors add undue stress to an already stressful time period. “These are students who are about to take a high-stakes exam with a lot on the line, and that is very unwelcome,” Teninbaum said in an interview with The Verge. .Teninbaum also believes that optics matter; schools owe it to marginalized students not to rely on a category of software that’s known to be discriminatory. “Students deserve to feel that their institution is doing what it can to protect their rights, interests, and dignity,” he says..In his report, which is forthcoming in The Journal of Robotics, Artificial Intelligence, and Law, Teninbaum outlines the workaround he found. .He suggests that schools assign every student an identical generic, baseline image. Then, he proposes, they should ask ExamSoft to enable “deferred identification,” a feature built into the software that allows students to proceed with exams even if identification fails.(ExamSoft told The Verge that as of the end of 2020, deferred identification is now enabled by default — so schools should be able to skip this step.).These tweaks will cause ExamSoft to misidentify every test-taker. But they’ll still be able to proceed with their exams — ExamSoft will send the selfies to the school afterwards, and instructors can manually verify everyone. “We know who our students are,” Teninbaum says. “We can make sure the students are who they say they are and avoid subjecting students to these sorts of challenges.”.He also suggests that ExamSoft make the “deferred identification” feature accessible to customers. “The reporter urges ExamSoft to build this into a feature by which institutions can simply toggle on/off, thereby bypassing ExamID until such time that the technology matures into one that does not discriminate,” his report reads..Teninbaum hopes those changes will last beyond the COVID-19 pandemic, and can help students feel more comfortable taking remote classes. “It’s going to be a growing problem as people get more and more online for their education,” he says..Even so, he’s only fixed part of the problem. Students have experienced a range of hiccups with ExamSoft’s proctoring software. Over 3,000 people who used the platform to take California’s bar exam in October had their videos flagged for potential rule violations — nearly 36 percent of applicants who took the online exam. Users reported audio issues, and other technical glitches as well. .A group of six US senators — including Richard Blumenthal, Elizabeth Warren, and Corey Booker — wrote an open letter to ExamSoft in December, highlighting potential harms to students of color and students with disabilities, among numerous other concerns..In a statement to The Verge, ExamSoft said, “ExamID is designed to enable a frictionless experience for exam-takers. ExamID seeks to mitigate challenges often associated with facial recognition software by allowing exam-takers to set and approve their own baseline picture, which the software uses to compare to a picture the exam-taker takes at the time of the exam. Additionally, exam-takers can test how ExamID validates their specific picture prior to taking any exam to help reduce the stress of picture-matching concerns during the exam.”","ExamSoft's proctoring software is having a problem with recognizing faces, which could delay or bar test takers. According to reports, this software disproportionately impacts people with dark skin tones. "
377,"We are excited to bring Transform 2022 back in-person July 19 and virtually July 20 - 28. Join AI and data leaders for insightful talks and exciting networking opportunities. Register today!.A paper published today in the journal Scientific Reports by controversial Stanford-affiliated researcher Michal Kosinski claims to show that facial recognition algorithms can expose people’s political views from their social media profiles. Using a dataset of over 1 million Facebook and dating sites profiles from users across Canada, the U.S., and the U.K., Kosinski says he trained an algorithm to correctly classify political orientation in 72% of “liberal-conservative” face pairs..The work, taken as a whole, embraces the pseudoscientific concept of physiognomy, or the idea that a person’s character or personality can be assessed from their appearance. In 1911, Italian anthropologist Cesare Lombroso published a taxonomy declaring that “nearly all criminals” have “jug ears, thick hair, thin beards, pronounced sinuses, protruding chins, and broad cheekbone.” Thieves were notable for their “small wandering eyes,” he said, and rapists their “swollen lips and eyelids,” while murderers had a nose that was “often hawklike and always large.”.Phrenology, a related field, involves the measurement of bumps on the skull to predict mental traits. Authors representing the Institute of Electrical and Electronics Engineers (IEEE) have said this sort of facial recognition is “necessarily doomed to fail” and that strong claims are a result of poor experimental design..Princeton professor Alexander Todorov, a critic of Kosinski’s work, also argues that methods like those employed in the facial recognition paper are technically flawed. He says the patterns picked up by an algorithm comparing millions of photos might have little to do with facial characteristics. For example, self-posted photos on dating websites project a number of non-facial clues..Moreover, current psychology research shows that by adulthood, personality is mostly influenced by the environment. “While it is potentially possible to predict personality from a photo, this is at best slightly better than chance in the case of humans,” Daniel Preotiuc-Pietro, a postdoctoral researcher at the University of Pennsylvania who’s worked on predicting personality from profile images, told Business Insider in a recent interview..Kosinski, preemptively responding to criticism, take pains to distance their research from phrenology and physiognomy. But they don’t dismiss them altogether. “Physiognomy was based on unscientific studies, superstition, anecdotal evidence, and racist pseudo-theories. The fact that its claims were unsupported, however, does not automatically mean that they are all wrong,” they wrote in notes published alongside the paper. “Some of physiognomists’ claims may have been correct, perhaps by a mere accident.”.According to Kosinski, a number of facial features — but not all — reveal political affiliation, including head orientation, emotional expression, age, gender, and ethnicity. While facial hair and eyewear predict political affiliation with “minimal accuracy,” liberals tend to face the camera more directly and are more likely to express surprise (and less likely to express disgust), they say..“While we tend to think of facial features as relatively fixed, there are many factors that influence them in both the short and long term,” the researchers wrote. “Liberals, for example, tend to smile more intensely and genuinely, which leads to the emergence of different expressional wrinkle patterns. Conservatives tend to be healthier, consume less alcohol and tobacco, and have a different diet — which, over time, translates into differences in skin health and the distribution and amount of facial fat.”.The researchers posit that facial appearance predicts life outcomes like the length of a prison sentence, occupational success, educational attainments, chances of winning an election, and income and that these outcomes in turn likely influence political orientation. But they also conjecture there’s a connection between facial appearance and political orientation and genes, hormones, and prenatal exposure to substances..“Negative first impressions could over a person’s lifetime reduce their earning potential and status and thus increase their support for wealth redistribution and sensitivity to social injustice, shifting them toward the liberal end of the political spectrum,” the researchers wrote. “Prenatal and postnatal testosterone levels affect facial shape and correlate with political orientation. Furthermore, prenatal exposure to nicotine and alcohol affects facial morphology and cognitive development (which has been linked to political orientation).”.Kosinski made available the project’s source code and dataset but not the actual images, citing privacy implications. But this has the dual effect of making auditing the work for bias and experimental flaws impossible. Science in general has a reproducibility problem — a 2016 poll of 1,500 scientists reported that 70% of them had tried but failed to reproduce at least one other scientist’s experiment — but it’s particularly acute in the AI field. One recent report found that 60% to 70% of answers given by natural language processing models were embedded somewhere in the benchmark training sets, indicating that the models were often simply memorizing answers..Numerous studies — including the landmark Gender Shades work by Joy Buolamwini, Dr. Timnit Gebru, Dr. Helen Raynham, and Deborah Raji — and VentureBeat’s own analyses of public benchmark data have shown facial recognition algorithms are susceptible to various biases. One frequent confounder is technology and techniques that favor lighter skin, which include everything from sepia-tinged film to low-contrast digital cameras. These prejudices can be encoded in algorithms such that their performance on darker-skinned people falls short of that on those with lighter skin..Bias is pervasive in machine learning algorithms beyond those powering facial recognition systems. A ProPublica investigation found that software used to predict criminality tends to exhibit prejudice against black people. Another study found that women are shown fewer online ads for high-paying jobs. An AI beauty contest was biased in favor of white people. And an algorithm Twitter used to decide how photos are cropped in people’s timelines automatically elected to display the faces of white people over people with darker skin pigmentation..Kosinski, whose work analyzing the connection between personality traits and Facebook activity inspired the creation of political consultancy Cambridge Analytica, is no stranger to controversy. In a paper published in 2017, he and Stanford computer scientist Yilun Wang reported that an off-the-shelf AI system was able to distinguish between photos of gay and straight people with a high degree of accuracy. Advocacy groups like Gay & Lesbian Alliance Against Defamation (GLAAD) and the Human Rights Campaign said the study “threatens the safety and privacy of LGBTQ and non-LGBTQ people alike,” noting that it found basis in the disputed prenatal hormone theory of sexual orientation, which predicts the existence of links between facial appearance and sexual orientation determined by early hormone exposure..Todorov believes Kosinski’s research is “incredibly ethically questionable” as it could lend credibility to governments and companies that might want to use such technologies. He and academics like cognitive science researcher Abeba Birhane argue that those who create AI models must take into consideration social, political, and historical contexts. In her paper “Algorithmic Injustices: Towards a Relational Ethics,” for which she won the Best Paper Award at NeurIPS 2019, Birhane wrote that “concerns surrounding algorithmic decision making and algorithmic injustice require fundamental rethinking above and beyond technical solutions.”.In an interview with Vox in 2018, Kosinski asserted that his overarching goal was to try to understand people, social processes, and behavior through the lens of “digital footprints.” Industries and governments are already using facial recognition algorithms similar to those he’s developed, he said, underlining the need to warn stakeholders about the extinction of privacy..“Widespread use of facial recognition technology poses dramatic risks to privacy and civil liberties,” Kosinski and coauthors wrote of this latest study. “While many other digital footprints are revealing of political orientation and other intimate traits, facial recognition can be used without subjects’ consent or knowledge. Facial images can be easily (and covertly) taken by law enforcement or obtained from digital or traditional archives, including social networks, dating platforms, photo-sharing websites, and government databases. They are often easily accessible; Facebook and LinkedIn profile pictures, for instance, can be accessed by anyone without a person’s consent or knowledge. Thus, the privacy threats posed by facial recognition technology are, in many ways, unprecedented.”.Indeed, companies like Faception claim to be able to spot terrorists, pedophiles, and more using facial recognition. And the Chinese government has deployed facial recognition to identity photographs of hundreds of suspected criminals, ostensibly with over 90% accuracy..Experts like Os Keyes, a Ph.D. candidate and AI researcher at the University of Washington, agrees that it’s important to draw attention to the misuses of and flaws in facial recognition. But Keyes argues that studies such as Kosinski’s advance what’s fundamentally junk science. “They draw on a lot of (frankly, creepy) evolutionary biology and sexology studies that treat queerness [for example] as originating in ‘too much’ or ‘not enough’ testosterone in the womb,” they told VentureBeat in an email. “Depending on them and endorsing them in a study … is absolutely bewildering.”.VentureBeats mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. Learn more about membership..Hear from senior executives at some of the world’s leading enterprises about their experience with applied Data & AI and the strategies they’ve adopted for success. ",A paper published today in the journal Scientific Reports by controversial Stanford-affiliated researcher Michal Kosinski claims to show that facial recognition algorithms can expose people’s political views from their social media profiles.
378,"For Singaporeans, the covid-19 pandemic has been closely intertwined with technology: two technologies, to be specific. The first is the QR code, whose little black-and-white squares have been ubiquitous all over the country as part of the SafeEntry contact tracing system rolled out in April and May. .Under SafeEntry, anyone entering a public venue—restaurants, stores, malls—must scan a code and register with a name, ID or passport number, and phone number. If somebody tests positive for covid-19, contact tracers use it to track down those who got close enough to be potentially infected..There’s also TraceTogether, an app that launched in March 2020. It uses Bluetooth to ping close contacts; if two users are in proximity, their devices trade anonymized and encrypted user IDs that can be decrypted by the Ministry of Health should one person test positive for covid-19. .For those who can’t or don’t want to use a smartphone app, the government also offers TraceTogether tokens, small digital fobs that serve the same purpose. And while TraceTogether is currently voluntary, the government has announced that it is going to merge the two systems, which would make it mandatory to either download the app or collect a token..When the two systems were launched, there wasn’t much space for the public to discuss apprehensions: they were seen as necessary to fight the pandemic, and the Singaporean government acted in typical top-down fashion. It did seek to assuage fears, however, by repeatedly assuring Singaporeans that the data collected with such technology would be used only for contact tracing during the pandemic. .Earlier this month, it emerged that the government’s claim was false. The Ministry of Home Affairs confirmed that data could actually be accessed by the police for criminal investigations; the day after this admission, a minister revealed that such data had, in fact, already been used in a murder investigation. It rapidly became clear that despite what ministers had previously said, Singaporean law meant it had been possible for law enforcement to use TraceTogether data all along..These revelations triggered public anger and criticism, not necessarily because Singaporeans are particularly privacy conscious—in fact, state surveillance is largely normalized in the country—but because people felt they’d been subjected to a bait-and-switch. Many people had reservations about TraceTogether when it was first launched, and only began using it in large numbers after the government indicated that it would soon become mandatory. (According to the cochair of the task force on covid-19, nearly 80% of Singapore’s residents have adopted TraceTogether.).The government has since announced that it will introduce new legislation to limit law enforcement’s use of contact tracing data to probes into seven specific categories of offense, including terrorism, murder, kidnapping, and the most serious drug trafficking cases. (The MIT Technology Review Covid Tracing Tracker, which monitors the policies around exposure notification apps worldwide, is being updated to reflect this shift.).“We acknowledge our error in not stating that data from TraceTogether is not exempt from the Criminal Procedure Code,” said the Smart Nation and Digital Governance Office in its statement. The new law, it said, “will specify that personal data collected through digital contact tracing solutions … can only be used for the specific purpose of contact tracing, except where there is a clear and pressing need to use that data for criminal investigation of serious offences.”.“In Singapore, where laws grant sweeping executive and legislative powers to state actors, I think any commitment to accountability and restraint is welcome,” says digital rights activist Lee Yi Ting. “But it remains to be seen if the bill will make substantive commitment to these proposed limitations. For example, if state actors flout these regulations, what investigative bodies will come into play, and what consequences will state actors be held to?” .Some doubt how useful such data can really be to police investigations and are concerned that even the proposed limits still formally expand its use beyond contact tracing..“We like to reiterate that extending police powers to [TraceTogether] data is not aligned to the original spirit of what the dataset was intended for,” said the opposition Progress Singapore Party in a statement. “Covid tracing data must solely and strictly be used for fighting the pandemic and nothing else.” .The confusion could not come at a more difficult time. Concerns that governments could abuse contact tracing systems have been raised around the world. Many of these worries have been misplaced, especially in countries that use Google and Apple’s exposure notification technology, which does not allow centralized collection by local authorities. The Singapore government had previously rejected Apple and Google’s system, saying that it would be “less effective” in the Singaporean context. .But while digital systems could speed up contact tracing and aid in the fight against the virus—one that could be more vital over time, not less—most countries have struggled with adoption. One major issue: trust..Lee worries that even if legislation is enough to placate many Singaporeans, the implications outside the country could be serious. Singapore’s early move to build digital contact tracing put it in a global leadership position, and TraceTogether’s underlying systems have been used by other nations—though there is no suggestion that the same legislative mistakes were made elsewhere. .Still, “Singaporeans do care about the extent to which the state intrudes into their private lives,” says Lee. And, she adds, the country is setting an international precedent “for repressive governments to likewise normalize the use of contact tracing data for the purposes they define.”.Two Dutch researchers have won a major hacking championship by hitting the software that runs the world’s power grids, gas pipelines, and more. It was their easiest challenge yet..An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.","Data collected by TraceTogether, a Covid-19 tracing app in Singapore, could be accessed and used by the police for criminal investigations. This contradicts the Singaporean government's claim that data collected would only be used for contact tracing."
379,"An ethnic Uighur demonstrator wears a mask as she attends a protest against China in front of the Chinese Consulate in Istanbul, Turkey, October 1, 2019. REUTERS/Huseyin Aldemir.BERLIN, Jan 13 (Thomson Reuters Foundation) - Chinese technology giants have registered patents for tools that can detect, track and monitor Uighurs in a move human rights groups fear could entrench oppression of the Muslim minority..In a report published on Tuesday, IPVM reveals a cluster of patents for systems that could be used to analyse images for the presence of Uighurs, and hook into ongoing surveillance camera and facial recognition networks..We cannot ignore the fact that these technologies have been developed in order to be able to efficiently carry out..brutal oppression, Rushan Abbas, executive director of the rights group Campaign for Uyghurs, told the Thomson Reuters Foundation..United Nations officials have said China is transforming the Xinjiang region, where many Uighurs live, into a massive internment camp, with the patented tracing tech seen by rights groups as key to the crackdown..These technologies allow police in China to go through a large database of faces, and flag faces that the AI has marked as non-Chinese, or Uighurs, says Charles Rollet, a researcher with IPVM. There are major human rights implications.The U.N estimates that more than a million Chinese Muslims, many of whom are from the minority Uighur ethnicity, have been detained in the province of Xinjiang, where activists say crimes against humanity and genocide are taking place..Research by human rights groups suggests that Chinese tech firms are building Uighur detection systems, using facial recognition to alert authorities to peoples whereabouts, and predictive policing tools to identify which to detain..Imagine if the U.S. were a full-on dictatorship, imprisoning Black people just for being Black, and there was technology deployed across the country to detect where Black people were, so they could be hunted down, she said..The debate over the role of corporations in Chinas treatment of the Uighurs is increasingly spilling over internationally, with the United States applying sanctions to Chinese tech firms accused of abetting the persecution..The incoming Biden administration this week returned a donation from former U.S. senator Barbara Boxer, who had registered as a lobbyist for Hikvision, a video surveillance firm blacklisted by the U.S. government in 2019..According to the IPVM report, many top China security camera manufacturers have offered Uyghurs analytics, including the three largest firms: Hikvision, Dahua and Uniview..Hikvision told Reuters in 2019 that the firm takes global human rights very seriously and that its technology was also used in shops, traffic control and commercial buildings..One patent application, filed by the Chinese tech giant Huawei in conjunction with the Chinese Academy of Sciences, describes how AI can tell if a pedestrian is Uighur or not..Huawei opposes discrimination of all types, including the use of technology to carry out ethnic discrimination, the company said in a statement emailed to the Thomson Reuters Foundation..Identifying individuals race was never part of the research and development project. It should never have become part of the application and we are taking proactive steps to amend it..The company told the Thomson Reuters Foundation that its patent language was open to misunderstanding but that Megvii has not developed and will not develop or sell racial or ethnic labelling solutions..If youre a Chinese tech company - in particular one that builds facial recognition - and the police are customers, you are going to have this kind of Uighur-detecting analytics, he said..I am shocked there are so many technology firms helping the Chinese government watch us, said Jevlan Shirmemmet, a Uighur activist living in Turkey who says his mother is detained in a Chinese internment camp..Our global editorial team of about 55 journalists and more than 350 freelancers covers the lives of people around the world who struggle to live freely or fairly."," Chinese technology giants have registered patents for tools that can detect, track and monitor Uighurs in a move human rights groups fear could entrench oppression of the Muslim minority."
381,"Amnesty International has launched a new campaign against facial recognition titled Ban The Scan — and is launching with a demand for New York City to halt police and government use of the technology..Amnesty argues facial recognition is incompatible with basic privacy rights, and will exacerbate structural racism in policing tactics. “New Yorkers should be able to go out about their daily lives without being tracked by facial recognition,” said Matt Mahmoudi, an AI and human rights researcher with Amnesty. “Other major cities across the US have already banned facial recognition, and New York must do the same.”.Amnesty is joined in the New York portion of the campaign by a range of groups, including the Urban Justice Center, the New York Civil Liberties Union and the city’s Public Advocate office..The New York Police Department has run afoul of facial recognition critics before, most notably when it used facial recognition to locate and arrest a Black Lives Matter activist in August. The department claims it only uses facial recognition to generate leads, and doesn’t make arrests based on the information. Still, many civil liberties groups find the existing protections inadequate..The Ban the Scan campaign is launching with a website that will allow users to leave comments on the NYPD’s policies through a local public oversight rule. Later, Amnesty plans to build in a tool for filing Freedom of Information Law requests, and in May, a tool to geolocate facial-recognition-capable cameras throughout the city..“For years, the NYPD has used facial recognition to track tens of thousands of New Yorkers, putting New Yorkers of color at risk of false arrest and police violence,” said Albert Fox Cahn, executive director of the Surveillance Technology Oversight Project at the Urban Justice Center in a statement. “Banning facial recognition won’t just protect civil rights: it’s a matter of life and death.”","Amnesty International has launched a new campaign against facial recognition and launching with a demand for New York City to halt police and government use of the technology. Amnesty argues facial recognition is incompatible with basic privacy rights, and will exacerbate structural racism in policing tactics. "
382,"When hackers exploited a bug in Parler to download all of the right-wing social media platforms contents last week, they were surprised to find that many of the pictures and videos contained geolocation metadata revealing exactly how many of the sites users had taken part in the invasion of the US Capitol building just days before. But the videos uploaded to Parler also contain an equally sensitive bounty of data sitting in plain sight: thousands of images of unmasked faces, many of whom participated in the Capitol riot. Now one website has done the work of cataloging and publishing every one of those faces in a single, easy-to-browse lineup..Late last week, a website called Faces of the Riot appeared online, showing nothing but a vast grid of more than 6,000 images of faces, each one tagged only with a string of characters associated with the Parler video in which it appeared. The sites creator tells WIRED that he used simple open source machine learning and facial recognition software to detect, extract, and deduplicate every face from the 827 videos that were posted to Parler from inside and outside the Capitol building on January 6, the day when radicalized Trump supporters stormed the building in a riot that resulted in five peoples deaths. The creator of Faces of the Riot says his goal is to allow anyone to easily sort through the faces pulled from those videos to identify someone they may know or recognize who took part in the mob, or even to reference the collected faces against FBI wanted posters and send a tip to law enforcement if they spot someone..Everybody who is participating in this violence, what really amounts to an insurrection, should be held accountable, says the sites creator, who asked for anonymity to avoid retaliation. Its entirely possible that a lot of people who were on this website now will face real-life consequences for their actions..Aside from the clear privacy concerns it raises, Faces of the Riots indiscriminate posting of faces doesnt distinguish between lawbreakers—who trampled barriers, broke into the Capitol building, and trespassed in legislative chambers—and people who merely attended the protests outside. An upgrade to the site today adds hyperlinks from faces to the video source, so that visitors can click on any face and see what the person was filmed doing on Parler. The Faces of the Riot creator, who says hes a college student in the greater DC area, intends that added feature to help contextualize every faces inclusion on the site and differentiate between bystanders, peaceful protesters, and violent insurrectionists..He concedes that he and a cocreator are still working to scrub non-rioter faces, including those of police and press who were present. A message at the top of the site also warns against vigilante investigations, instead suggesting users report those they recognize to the FBI, with a link to an FBI tip page. If you go on the website and you see someone you know, you might learn something about a relative, he says. Or you might be like, oh, I know this person, and then further that information to the authorities..Despite its disclaimers and limitations, Faces of the Riot represents the serious privacy dangers of pervasive facial recognition technology, says Evan Greer, the campaign director for digital civil liberties nonprofit Fight for the Future. Whether its used by an individual or by the government, this technology has profound implications for human rights and freedom of expression, says Greer, whose organization has fought for a legislative ban on facial recognition technologies. I think it would be an enormous mistake if we come out of this moment by glorifying or lionizing a technology that, broadly speaking, disproportionately harms communities of color, low-income communities, immigrant communities, Muslim communities, activists ... the very same people that the faces on this website stormed the Capitol for the purpose of silencing and disenfranchising..The sites developer counters that Faces of the Riot leans not on facial recognition but facial detection. While he did use the open source machine learning tool Tensor Flow and the facial recognition software Dlib to analyze the Parler videos, he says he used that software only to detect and cluster faces from the 11 hours of video of the Capitol riot; Dlib allowed him to deduplicate the 200,000 images of faces extracted from video frames to around 6,000 unique faces. (He concedes that there are nonetheless some duplicates and images of faces on protest signs included too. Even the number 45 on some signs was in some cases identified as a human face.).He emphasizes also that theres no search tool on the site, and it doesnt attempt to link faces with names or other identifying details. Nor is there any feature for uploading an image and matching it with images in the sites collection, which he says could lead to dangerous misidentifications. Theres a very hard no on allowing a user to take a photo from a wanted poster and search for it, the sites creator says. That’s never going to happen..The roughly 42 gigabytes of Parler videos that Faces of the Riot analyzed were downloaded prior to Amazons decision early last week to cut off Parlers web hosting, leaving the site largely offline since. Racing against that takedown, hacktivists took advantage of a security flaw in Parler that allowed them to download and archive every post from the service, which bills itself as an uncensored free speech alternative to Twitter or Facebook. Faces of the Riot obtained Parlers salvaged videos after they were made available online by Kyle McDonald, a media artist who obtained them from a third party he declined to identify..The Faces of the Riot sites creator initially saw the data as a chance to experiment with machine learning tools, but quickly saw the potential for a more public project. After about 10 minutes I thought, this is actually a workable idea and I can do something that will help people, he says. Faces of the Riot is the first website hes ever created..McDonald has previously both criticized the power of facial recognition technology and himself implemented facial recognition projects like ICEspy, a tool he launched in 2018 for identifying agents of the Immigration and Customs Enforcement agency. He tells WIRED he also analyzed the leaked Parler videos with facial recognition tools to see if he could identify individuals, but could only ID two, both of whom had already been named by media. He sees Faces of the Riot as playing it really safe compared even to his own facial recognition experiments, given that it doesnt seek to link faces with named identities. And I think its a good call because I dont think that we need to legitimize this technology any more than it already is and has been falsely legitimized, McDonald says..But McDonald also points out that Faces of the Riot demonstrates just how accessible facial recognition technologies have become. It shows how this tool that has been restricted only to people who have the most education, the most power, the most privilege is now in this more democratized state, McDonald says..The Faces of the Riot sites creator sees it as more than an art project or demonstration. Despite the safeguards he put in place to limit its ability to automatically identify people, he still hopes that the effort will have real, tangible results—if only indirectly through reports to law enforcement. Its just felt like people got away with a lot of bad stuff for the last four years, he says. This is an opportunity to start trying to put that to an end.","Faces of the Riot used open source software to detect, extract, and deduplicate every face from the 827 videos taken from the insurrection on January 6.  Faces of the Riot represents the serious privacy dangers of pervasive facial recognition technology, says Evan Greer, the campaign director for digital civil liberties nonprofit Fight for the Future"
383,"We are excited to bring Transform 2022 back in-person July 19 and virtually July 20 - 28. Join AI and data leaders for insightful talks and exciting networking opportunities. Register today!.The pandemic has unleashed a barrage of online misinformation that’s reinvigorated the anti-vaccine movement. Despite the fact that multiple COVID-19 vaccines are approved and beginning to be made available to the public, only two-thirds of Americans say they’ll try to get vaccinated, according to a CNN poll. As governments work toward distributing vaccines, health experts worry that reluctance could make it difficult to achieve herd immunity. Unfortunately, the algorithms powering search engines haven’t traditionally been designed to take into account the credibility and trustworthiness of medical information..The coauthors of a recent study argue this is particularly true of Amazon, which has faced criticism for failing to regulate the health-related products on its platform. According to the University of Washington researchers, who have affiliations with the The Information School at the University of Washington, their audits reveal Amazon hosts a “plethora” of health misinformative products belonging to categories including books, ebooks, apparel, and health and personal care. They also claim to have found a “filter-bubble” effect in Amazon’s recommendations where recommendations of misinformative health products contain more health misinformation..“Several medically unverified products for coronavirus treatment, like prayer healing, herbal treatments and antiviral vitamin supplements proliferated Amazon, so much so that the company had to remove 1 million fake products after several instances of such treatments were reported by the media,” the researchers wrote in their paper. “The scale of the problematic content suggests that Amazon could be a great enabler of misinformation, especially health misinformation. It not only hosts problematic health-related content but its recommendation algorithms drive engagement by pushing potentially dubious health products to users of the system.”.The researchers conducted two sets of experiments in May and August to determine the extent to which Amazon might be promoting health misinformation about vaccines. In the first — an “unpersonalized” audit — they used information retrieval metrics to measure the amount of health misinformation users were exposed to when performing for vaccine-related searches. In particular, while logged in as a guest to minimize the influence of personalization algorithms, they canvassed the results of 48 searches belonging to 10 popular vaccine-related topics including “HPV vaccine,” “immunization,” and “MMR vaccine and autism.”.The researchers ran the audit for 15 consecutive days, sorting the results across five different Amazon filters each day: Featured, Price Low to High, Price High to Low, Average Customer Review, and Newest Arrivals. They annotated the resulting 36,000 search results and 16,815 product page recommendations for their stances on health misinformation — i.e., whether they promoted, debunked, or were neutral regarding vaccinations — for a final dataset totaling 4,997 annotated Amazon products..The second audit — a “personalized” audit — looked at the impact of a customer’s behavioral history on the amount of misinformation returned in search results, recommendations, and auto-complete suggestions. As the researchers note, Amazon history covers a weeklong period of actions including searching for products, searching and clicking, adding to cart after searching and clicking, searching on third-party websites like Google, and more..After analyzing the results from both audits, the researchers found that search results returned for many vaccine-related queries contain large number of misinformative products, leading to what they characterize as “high misinformation bias.” In addition, misinformative products were ranked higher than “debunking” products, and customers performing actions on misinformative products were presented with more misinformation in their homepages, product page recommendations, and prepurchase recommendations, the researchers say..“Many search engines and social media platforms employ personalization to enhance users’ experience on their platform by recommending them items that the algorithm thinks they will like based on their past browsing or purchasing history. But on the downside, if not checked, personalization can also lead users into a rabbit hole of problematic content,” the researchers wrote. “Our analysis … revealed that an echo chamber exists on Amazon where users performing real-world actions on misinformative books are presented with more misinformation in various recommendations. Just a single click on an anti-vaccine book could fill your homepage with several other similar anti vaccine books. There is an urgent need for the platform to treat vaccine and other health related topics differently and ensure high quality searches and recommendations.”.For its part, Amazon recently said in a corporate blog post that during 2020, it reviewed almost 10,000 product listings a day to ensure compliance with its policies and removed over 2 million products for violating its offensive or controversial guidelines. More than 1.5 million of these products were identified, reviewed, and removed proactively by automated tools, according to Amazon — often before being seen by a customer..“We exercise judgment in allowing or prohibiting listings, and we keep the cultural differences and sensitivities of our global community in mind when making a decision on products,” Amazon wrote. “We strive to maximize selection for all customers, even if we don’t agree with the message or sentiment of the product itself. Our offensive and controversial products policy attempts to provide a clear and objective standard against which to measure the products we permit in our store.”.The researchers suggest as one potential solution a “bias meter” that could signal the amount of misinformation present in vaccine-related  search results. They also urge Amazon to stop promoting health misinformative books via sponsorships — the researchers found 98 misinformative products in the sponsored recommendations they annotated — and to introduce a label for health-related products that have been evaluated by experts. Moreover, they recommend that the platform account for misinformation bias in its search and recommendation algorithms to reduce the exposure to misinformative content..“Our investigations revealed that Amazon’s algorithm has learnt problematic patterns through consumers’ past viewing and buying patterns,” the researchers wrote. “Our study … provides a peek into the workings of Amazon’s algorithm and has paved way for future audits that could use our audit methodology and extensive qualitative coding scheme to perform experiments considering complex real world settings.”.The researchers aren’t the first to uncover the presence of anti-vaccination content on Amazon. In May, CNN found listings and advertisements for books and movies promoting vaccination misinformation including VAXXED: From Cover-Up to Catastrophe, which was dropped from the Tribeca Film Festival in 2016 following an outcry. Amazon removed the anti-vaccine documentaries from its Prime Video service after CNN published its report..VentureBeats mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. Learn more about membership..Hear from senior executives at some of the world’s leading enterprises about their experience with applied Data & AI and the strategies they’ve adopted for success. ","According to the University of Washington researchers, their audits reveal Amazon hosts a “plethora” of health misinformative products"". Several medically unverified products for coronavirus treatment, like prayer healing, herbal treatments and antiviral vitamin supplements proliferated Amazon, so much so that the company had to remove 1 million fake products after several instances of such treatments were reported by the media,” the researchers wrote in their paper. "
386,"We are excited to bring Transform 2022 back in-person July 19 and virtually July 20 - 28. Join AI and data leaders for insightful talks and exciting networking opportunities. Register today!.Google has revoked Ethical AI team leader Margaret “Meg” Mitchell’s employee privileges and is currently investigating her activity, according to a statement provided by a company spokesperson. Should Google fire Mitchell, it will mean the company has effectively chosen to behead its own AI ethics team in under two months. In an interview with VentureBeat last month, former Google AI ethics co-lead Timnit Gebru said she had worked with Mitchell since 2018 to create one of the most diverse teams within Google Research..Gebru tweeted Tuesday evening that Google’s move to freeze Mitchell’s employee account echoed the way hers was frozen before she was fired. When VentureBeat emailed Google to ask if Mitchell was still an employee, a spokesperson provided the following statement:.“Our security systems automatically lock an employee’s corporate account when they detect that the account is at risk of compromise due to credential problems or when an automated rule involving the handling of sensitive data has been triggered. In this instance, yesterday our systems detected that an account had exfiltrated thousands of files and shared them with multiple external accounts. We explained this to the employee earlier today. We are actively investigating this matter as part of standard procedures to gather additional details.”.Last month, Google fired Gebru following a demand by Google leadership that she rescind an AI research paper she coauthored about the negative consequences of large-scale language models, including their disproportionate impact on marginalized communities in the form of environmental impact and perpetuating stereotypes. Since then, Google released a trillion parameter language model and told its AI researchers to strike a positive tone on topics deemed “sensitive. Some members of the AI research community have pledged not to review the work of Google researchers at academic conferences in protest..After Gebru was fired, April Curley, a queer Black woman who said she was fired by Google last fall, publicly recounted numerous negative experiences during her time as a recruiter of talent from historically Black colleges and universities (HBCU)..On Tuesday, news emerged that Google CEO Sundar Pichai will meet with HBCU leaders following allegations of racism and sexism at the company by current and former employees..Members of Congress interested in regulating AI and more than 2,000 Google employees have joined prominent figures in the AI research community in questioning Gebru’s dismissal. Members of Google’s AI ethics team called for her reinstatement in a series of demands sent to company leadership..Organizers cited the way Google treated Gebru and the impact AI can have on society as motivators behind the establishment of the Alphabet Workers Union, which was formed earlier this month and as of a week ago counted 700 members including Margaret Mitchell. Gebru had previously endorsed the idea of a workers union as a way to help protect AI researchers from company retribution..“With AI permeating every aspect of our world—from criminal justice, to credit scores, to military applications—paying careful attention to ethics within the industry is critical,” the Alphabet Workers Union said in a statement shared with VentureBeat..“As one of the most profitable players in the AI industry, Alphabet has a responsibility to continue investing in its ethical application. Margaret founded the Ethical AI team, built a cross-product area coalition around machine learning fairness, and is a critical member of academic and industry communities around the ethical production of AI. Regardless of the outcome of the company’s investigation, the ongoing targeting of leaders in this organization calls into question Google’s commitment to ethics—in AI and in their business practices. Many members of the Ethical AI team are AWU members and the membership of our union recognizes the crucial work that they do and stands in solidarity with them in this moment.”.The incoming Biden administration has in recent days shared a commitment to diversity and to addressing algorithmic bias and other AI-driven harms to society through its science and technology policy platform. Experts in AI, law, and policy told VentureBeat last month that Google’s treatment of Gebru could impact a range of policy matters, including the passage of stronger whistleblower protections for tech workers and more public funding of independent AI research..What happens to Mitchell will continue to shape attitudes toward corporate self-governance and speculation about the voracity of research produced with Big Tech funding. A research paper published in late 2020 compared the way Big Tech funds AI ethics research to Big Tobacco’s history of funding health research..VentureBeats mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. Learn more about membership..Hear from senior executives at some of the world’s leading enterprises about their experience with applied Data & AI and the strategies they’ve adopted for success. ","Google has revoked Ethical AI team leader Margaret “Meg” Mitchell’s employee privileges and is currently investigating her activity, according to a statement provided by a company spokesperson. Should Google fire Mitchell, it will mean the company has effectively chosen to behead its own AI ethics team in under two months."
387,"                    Dozens of advertisers instructed the company to not show their ads to people of “unknown” gender, meaning people who had not identified themselves as male or female                               By  Jeremy B. Merrill  .Google’s advertising system allowed employers or landlords to discriminate against nonbinary and some transgender people, The Markup found..Companies trying to run ads on YouTube or elsewhere on the web could direct Google not to show those ads to people of “unknown gender”—meaning people who have not identified themselves to Google as “male” or “female.” After being alerted to this by The Markup, Google pledged to crack down on the practice..“We will be implementing an update to our policy and enforcement in the coming weeks to restrict advertisers from targeting or excluding users on the basis of the ‘gender unknown’ category,” Elijah Lawal, a spokesperson for Google said. .Google’s policies forbid ads targeting or excluding male or female people from jobs, housing, or financial products, in order to comply with federal anti-discrimination laws. But until The Markup alerted Google, the company gave advertisers the option of  keeping their ads from being shown to people of “unknown gender”—effectively allowing employers and landlords to either inadvertently or purposefully discriminate against people who identify as nonbinary, transgender, or anything other than male or female. .The Markup found two such job ads on YouTube, which is owned by Google—one for jobs at FedEx and the other for Dewey Pest Control, a California-based chain. In both cases, Google’s ad targeting explanations, collected by New York University’s Ad Observer, indicated that the employer had targeted the ad based on gender but that the data did not specify which gender was targeted. In those cases, Lawal said, the advertiser had chosen to exclude people of unknown gender from seeing the ads. Upon further review, Lawal said, the company “identified approximately 100 advertisers out of many thousands” who had done the same for housing, credit, or job ads..Advertisers, “out of many thousands,” that Google identified as having chosen to exclude people of unknown gender from housing, credit, or job ads..A FedEx spokesperson, Shannon Davis, said FedEx declined to comment “on our specific marketing or recruiting processes.” Dewey Pest Control didn’t respond to a request for comment..That would be “completely unethical and I think dangerous to society as a whole,” said Joseph Turow, a professor of communication at the University of Pennsylvania. .Lawal said that Google’s existing written policies forbid excluding people on the basis of transgender or gender nonconforming identity, for any ad. But he acknowledged that the “unknown gender” checkbox had effectively allowed advertisers to discriminate by gender despite Google’s policies against that. .While federal law prohibits discrimination by gender or race in advertising for employment and housing, there’s little in the way of case law about whether restrictive online ad targeting qualifies as discrimination. Nor is there any case law about excluding trans or nonbinary people from such ads, said Pauline Kim, a law professor at Washington University in St. Louis..If a company’s intention was to exclude nonbinary or gender nonconforming people, Kim said, “you could possibly argue … that it is a form of sex discrimination” under Title VII of the Civil Rights Act of 1964, which, according to a recent Supreme Court decision, covers discrimination on the basis of gender identity and sexual orientation as well..The prevalence of employment discrimination against nonbinary and transgender people is grim. What makes advertising discrimination different is that, “you don’t even know what you don’t see,” said Kendra Albert, a technology law instructor at Harvard Law School who studies technology, law, and gender..Beyond gender, the primary categories Google offers for targeting ads to some people and not others—what it calls “personalized advertising”—are age, household income, and parental status. For ads for job opportunities, housing, and credit, Google’s rules say advertisers shouldn’t use these categories. .The boxes start prechecked, meaning that in the default mode, gender isn’t taken into account in determining who will be shown the ad, but advertisers can uncheck them, effectively excluding certain categories of people from seeing their ads. That’s allowed for most ads, but Google uses other mechanisms—”internal processes” that Lawal declined to explain—to block job, housing, and credit ads that exclude people by gender, age, household income, and parental status..Meanwhile, someone signing up for Google or editing their account settings has four options for reporting their gender: “male,” “female,” “rather not say,” and an option to set a custom gender in a text box..Lawal said that the “unknown category is intended to refer to individuals where we have been unable to determine or infer the user’s gender and is not intended to allow for targeting or exclusion of users based on gender identity,” but said that people who choose not to identify their gender or write in a “custom” gender also fall into this category. .Google’s options for users amount to putting “a rainbow-colored Band-Aid” on “systems that were not really designed to include nonbinary people,” said Albert..“Really the question they should be asking is which gender are you, and which of these gender categories would you like us to serve you ads for,” and explaining how the ads system uses gender, Albert said..Allegations of race and sex discrimination have dogged online ad platforms for years. Several year ago, civil rights groups sued Facebook for allowing discrimination in ads for jobs, housing, and credit; Facebook settled the suit and agreed to take those options away. The U.S. Department of Housing and Urban Development (HUD) filed a lawsuit against Facebook too. Last year, HUD announced that it had “worked with Google to improve Google’s online advertising policies to better align them with requirements of the Fair Housing Act.” After those interactions with HUD, Google banned job, housing, and credit advertisers from excluding either men or women from their ads, along with similar rules for age and other protected groups..While ads for jobs, housing, and financial products fall under special protections, it’s perfectly legal—and very common—to target other kinds of ads to one segment of the population, by age, gender, or other categories..We’re happy to make this story available to republish for free under the conditions of an Attribution–NonCommercial–No Derivatives Creative Commons license. Please adhere to the following:","Companies trying to run ads on YouTube or elsewhere on the web could direct Google not to show those ads to people of “unknown gender”—meaning people who have not identified themselves to Google as “male” or “female.” After being alerted to this by The Markup, Google pledged to crack down on the practice."
388,"Early this year, Detroit police arrested Robert Williams — a Black man living in a Detroit suburb — on his front lawn in front of his wife and two little daughters (ages 2 and 5). Robert was hauled off and locked up for nearly 30 hours. His crime? Face recognition software owned by Michigan State Police told the cops that Robert Williams was the watch thief they were on the hunt for.  There was just one problem: Face recognition technology can’t tell Black people apart. That includes Robert Williams, whose only thing in common with the suspect caught by the watch shop’s surveillance feed is that they are both large-framed Black men. .But convinced they had their thief, Detroit police put Robert William’s driver’s license photo in a lineup with other Black men and showed it to the shop security guard, who hadn’t even witnessed the alleged robbery firsthand. The shop security guard — based only on review of a blurry surveillance image of the incident — claimed Robert was indeed the guy. With that patently insufficient “confirmation” in hand, the cops showed up at Robert’s house and handcuffed him in broad daylight in front of his own family.  It wasn’t until after spending a night in a cramped and filthy cell that Robert saw the surveillance image for himself. While interrogating Robert, an officer pointed to the image and asked if the man in the photo was him. Robert said it wasn’t, put the image next to his face, and said “I hope you all don’t think all Black men look alike.”  One officer responded, “The computer must have gotten it wrong.” Robert was still held for several more hours, before finally being released later that night into a cold and rainy January night, where he had to wait about an hour on a street curb for his wife to come pick him up. The charges have since been dismissed.  The ACLU of Michigan is lodging a complaint against Detroit police, but the damage is done. Robert’s DNA sample, mugshot, and fingerprints — all of which were taken when he arrived at the detention center — are now on file. His arrest is on the record. Robert’s wife, Melissa, was forced to explain to his boss why Robert wouldn’t show up to work the next day. Their daughters can never un-see their father being wrongly arrested and taken away — their first real experience with the police. Their children have even taken to playing games involving arresting people, and have accused Robert of stealing things from them.   As Robert puts it: “I never thought I’d have to explain to my daughters why daddy got arrested. How does one explain to two little girls that a computer got it wrong, but the police listened to it anyway?”  One should never have to. Lawmakers nationwide must stop law enforcement use of face recognition technology. This surveillance technology is dangerous when wrong, and it is dangerous when right.  First, as Robert’s experience painfully demonstrates, this technology clearly doesn’t work. Study after study has confirmed that face recognition technology is flawed and biased, with significantly higher error rates when used against people of color and women. And we have long warned that one false match can lead to an interrogation, arrest, and, especially for Black men like Robert, even a deadly police encounter. Given the technology’s flaws, and how widely it is being used by law enforcement today, Robert likely isn’t the first person to be wrongfully arrested because of this technology. He’s just the first person we’re learning about.  That brings us to the second danger. This surveillance technology is often used in secret, without any oversight. Had Robert not heard a glib comment from the officer who was interrogating him, he likely never would have known that his ordeal stemmed from a false face recognition match. In fact, people are almost never told when face recognition has identified them as a suspect. The FBI reportedly used this technology hundreds of thousands of times — yet couldn’t even clearly answer whether it notified people arrested as a result of the technology. To make matters worse, law enforcement officials have stonewalled efforts to obtain documents about the government’s actions, ignoring a court order and stonewalling multiple requests for case files providing more information about the shoddy investigation that led to Robert’s arrest.   Third, Robert’s arrest demonstrates why claims that face recognition isn’t dangerous are far-removed from reality. Law enforcement has claimed that face recognition technology is only used as an investigative lead and not as the sole basis for arrest. But once the technology falsely identified Robert, there was no real investigation. On the computer’s erroneous say-so, people can get ensnared in the Kafkaesque nightmare that is our criminal legal system. Every step the police take after an identification — such as plugging Robert’s driver’s license photo into a poorly executed and rigged photo lineup — is informed by the false identification and tainted by the belief that they already have the culprit. They just need the other parts of the puzzle to fit. Evidence to the contrary — like the fact that Robert looks markedly unlike the suspect, or that he was leaving work in a town 40 minutes from Detroit at the time of the robbery — is likely to be dismissed, devalued, or simply never sought in the first place. And when defense attorneys start to point out that parts of the puzzle don’t fit, you get what we got in Robert’s case: a stony wall of bureaucratic silence.   Fourth, fixing the technology’s flaws won’t erase its dangers. Today, the cops showed up at Robert’s house because the algorithm got it wrong. Tomorrow, it could be because a perfectly accurate algorithm identified him at a protest the government didn’t like or in a neighborhood in which someone didn’t think he belonged. To address police brutality, we need to address the technologies that exacerbate it too. When you add a racist and broken technology to a racist and broken criminal legal system, you get racist and broken outcomes. When you add a perfect technology to a broken and racist legal system, you only automate that system’s flaws and render it a more efficient tool of oppression.   It is now more urgent than ever for our lawmakers to stop law enforcement use of face recognition technology. What happened to the Williams’ family should not happen to another family. Our taxpayer dollars should not go toward surveillance technologies that can be abused to harm us, track us wherever we go, and turn us into suspects simply because we got a state ID. ","Robert Williams was arrested by Michigan State Police for thief. However, it was due to facial recognition technology being unable to tell black people apart. "
389,"French company Idemia’s algorithms scan faces by the million. The company’s facial recognition software serves police in the US, Australia, and France. Idemia software checks the faces of some cruise ship passengers landing in the US against Customs and Border Protection records. In 2017, a top FBI official told Congress that a facial recognition system that scours 30 million mugshots using Idemia technology helps “safeguard the American people.”.But Idemia’s algorithms don’t always see all faces equally clearly. July test results from the National Institute of Standards and Technology indicated that two of Idemia’s latest algorithms were significantly more likely to mix up black women’s faces than those of white women, or black or white men..The NIST test challenged algorithms to verify that two photos showed the same face, similar to how a border agent would check passports. At sensitivity settings where Idemia’s algorithms falsely matched different white women’s faces at a rate of one in 10,000, it falsely matched black women’s faces about once in 1,000—10 times more frequently. A one in 10,000 false match rate is often used to evaluate facial recognition systems..Donnie Scott, who leads the US public security division at Idemia, previously known as Morpho, says the algorithms tested by NIST have not been released commercially, and that the company checks for demographic differences during product development. He says the differing results likely came from engineers pushing their technology to get the best overall accuracy on NIST’s closely watched tests. “There are physical differences in people and the algorithms are going to improve on different people at different rates,” he says..Computer vision algorithms have never been so good at distinguishing human faces. NIST said last year that the best algorithms got 25 times better at finding a person in a large database between 2010 and 2018, and miss a true match just 0.2 percent of the time. That’s helped drive widespread use in government, commerce, and gadgets like the iPhone..But NIST’s tests and other studies repeatedly have found that the algorithms have a harder time recognizing people with darker skin. The agency’s July report covered tests on code from more than 50 companies. Many top performers in that report show similar performance gaps to Idemia’s 10-fold difference in error rate for black and white women. NIST has published results of demographic tests of facial recognition algorithms since early 2017. It also has consistently found that they perform less well for women than men, an effect believed to be driven at least in part by the use of makeup..“White males ... is the demographic that usually gives the lowest FMR,” or false match rate, the report states. “Black females ... is the demographic that usually gives the highest FMR.” NIST plans a detailed report this fall on how the technology works on different demographic groups..NIST’s studies are considered the gold standard for evaluating facial recognition algorithms. Companies that do well use the results for marketing. Chinese and Russian companies have tended to dominate the rankings for overall accuracy, and tout their NIST results to win business at home. Idemia issued a press release in March boasting that it performed better than competitors for US federal contracts..Many facial recognition algorithms are more likely to mix up black faces than white faces. Each chart represents a different algorithm tested by the National Institute of Standards and Technology. Those with a solid red line uppermost incorrectly match black womens faces more than other groups..The Department of Homeland Security has also found that darker skin challenges commercial facial recognition. In February, DHS staff published results from testing 11 commercial systems designed to check a person’s identity, as at an airport security checkpoint. Test subjects had their skin pigment measured. The systems that were tested generally took longer to process people with darker skin and were less accurate at identifying them—although some vendors performed better than others. The agency’s internal privacy watchdog has said DHS should publicly report the performance of its deployed facial recognition systems, like those in trials at airports, on different racial and ethnic groups..The government reports echo critical 2018 studies from ACLU and MIT researchers openly wary of the technology. They reported algorithms from Amazon, Microsoft, and IBM were less accurate on darker skin..Those findings have stoked a growing national debate about the proper, and improper, uses of facial recognition. Some civil liberties advocates, lawmakers, and policy experts want government use of the technology to be restricted or banned, as it was recently in San Francisco and two other cities. Their concerns include privacy risks, the balance of power between citizens and the state—and racial disparities in results. Even if facial recognition worked equally well for all faces, there would still be reasons to restrict the technology, some critics say..Despite the swelling debate, facial recognition is already embedded in many federal, state, and local government agencies, and it’s spreading. The US government uses facial recognition for tasks like border checks and finding undocumented immigrants..Earlier this year, the Los Angeles Police Department responded to a home invasion that escalated into a fatal shooting. One suspect was arrested but another escaped. Detectives identified the fugitive by using an online photo to search through a mugshot facial recognition system maintained by Los Angeles County Sheriff’s Office..Lieutenant Derek Sabatini of the Sheriff’s Office says the case shows the value of the system, which is used by more than 50 county agencies and searches a database of more than 12 million mugshots. Detectives might not have found the suspect as quickly without facial recognition, Sabatini says. “Who knows how long it would have taken, and maybe that guy would not have been there to scoop up,” he says..“Having these systems work equally well for different demographics or even understanding whether or why this might be possible is really a long term goal.”.The LA County system was built around a face-matching algorithm from Cognitec, a German company that, like Idemia, supplies facial recognition to governments around the world. As with Idemia, NIST testing of Cognitec’s algorithms’ shows they can be less accurate for women and people of color. At sensitivity thresholds that resulted in white women being falsely matched once in 10,000, two Cognitec algorithms NIST tested were about five times as likely to misidentify black women..Thorsten Thies, Cognitec’s director of algorithm development, acknowledged the difference but says it is hard to explain. One factor could be that it is “harder to take a good picture of a person with dark skin than it is for a white person,” he says..Sabatini dismisses concerns that—whatever the underlying cause—skewed algorithms could lead to racial disparities in policing. Officers check suggested matches carefully and seek corroborating evidence before taking action, he says. “We’ve been using it here since 2009 and haven’t had any issues: no lawsuits, no cases, no complaints,” he says..Concerns about the intersection of facial recognition and race are not new. In 2012, the FBI’s top facial recognition expert coauthored a research paper that found commercial facial recognition systems were less accurate for black people and women. Georgetown researchers warned of the problem in an influential 2016 report that said the FBI can search the faces of roughly half the US population..The issue has gained a fresh audience as facial recognition has become more common, and policy experts and makers more interested in the limitations of technology. The work of MIT researcher and activist Joy Buolamwini has been particularly influential..Early in 2018 Buolamwini and fellow AI researcher Timnit Gebru showed that Microsoft and IBM services that try to detect the gender of faces in photos were near perfect for men with pale skin but failed more than 20 percent of the time on women with dark skin; a subsequent study found similar patterns for an Amazon service. The studies didn’t test algorithms that attempt to identify people—something Amazon called “misleading” in an aggressive blog post..Buolamwini was a star witness at a May hearing of the House Oversight and Reform Committee, where lawmakers showed bipartisan interest in regulating facial recognition. Chairman Elijah Cummings (D-Maryland) said racial disparities in test results heightened his concern at how police had used facial recognition during 2015 protests in Baltimore over the death in police custody of Freddie Gray, a black man. Later, Jim Jordan (R-Ohio) declared that Congress needs to “do something” about government use of the technology. “[If] a facial recognition system makes mistakes and those mistakes disproportionately affect African Americans and persons of color, [it] appears to me to be a direct violation of Americans’ First Amendment and Fourth Amendment liberties,” he said..Why facial recognition systems perform differently for darker skin tones is unclear. Buolamwini told Congress that many datasets used by companies to test or train facial analysis systems are not properly representative. The easiest place to gather huge collections of faces is from the web, where content skews white, male, and western. Three face-image collections most widely cited in academic studies are 81 percent or more people with lighter skin, according to an IBM review..Patrick Grother, a widely respected figure in facial recognition who leads NIST’s testing, says there may be other causes for lower accuracy on darker skin. One is photo quality. Photographic technology and techniques have been optimized for lighter skin from the beginnings of color film into the digital era. He also posed a more provocative hypothesis at a conference in November: that black faces are statistically more similar to one another than white faces are. “You might conjecture that human nature has got something to do with it,” he says. “Different demographic groups might have differences in the phenotypic expression of our genes.”.Michael King, an associate professor at Florida Institute of Technology who previously managed research programs for US intelligence agencies that included facial recognition, is less sure. “That’s one that I am not prepared to discuss at this point. We have just not got far enough in our research,” he says..King’s latest results, with colleagues from FIT and University of Notre Dame, illustrate the challenge of explaining demographic inconsistency in facial recognition algorithms and what to do about it..Their study tested four facial recognition algorithms—two commercial and two open source—on 53,000 mugshots. Mistakes that incorrectly matched two different people were more common for black faces, but errors in which matching faces went undetected were more common for white faces. A greater proportion of the mugshots of black people didn’t meet standards for ID photos, but that alone could not explain the skewed performance..The researchers did find they could get the algorithms to perform equally for blacks and whites—but only by using different sensitivity settings for the two groups. That’s unlikely to be practical outside the lab because asking detectives or border agents to choose a different setting for different groups of people would create its own discrimination risks, and could draw lawsuits alleging racial profiling..While King and others carefully probe algorithms in the lab, political fights over facial recognition are moving fast. Members of Congress on both sides of the aisle have promised action to rein in the technology, citing worries about accuracy for minorities. Tuesday, Oakland became the third US city to ban its agencies from using the technology since May, following Somerville, Massachusetts, and San Francisco..King says that the science of figuring out how to make algorithms work the same on all faces will continue at its own pace. “Having these systems work equally well for different demographics or even understanding whether or why this might be possible is really a long term goal,” he says.","IDEMIA's facial recognition software used by the police in the USA, Australia, and France has made significant mistakes in identifying black women as compared to white women, or black and white men.
"
390,"In a surprise blog post, Amazon said it will put the brakes on providing its facial recognition technology to police for one year, but refuses to say if the move applies to federal law enforcement agencies..The moratorium comes two days after IBM said in a letter it was leaving the facial recognition market altogether. Arvind Krishna, IBM’s chief executive, cited a “pursuit of justice and racial equity” in light of the recent protests sparked by the killing of George Floyd by a white police officer in Minneapolis last month..Amazon’s statement — just 102 words in length — did not say why it was putting the moratorium in place, but noted that Congress “appears ready” to work on stronger regulations governing the use of facial recognition — again without providing any details. It’s likely in response to the Justice in Policing Act, a bill that would, if passed, restrict how police can use facial recognition technology..“We hope this one-year moratorium might give Congress enough time to implement appropriate rules, and we stand ready to help if requested,” said Amazon in the unbylined blog post..But the statement did not say if the moratorium would apply to the federal government, the source of most of the criticism against Amazon’s facial recognition technology. Amazon also did not say in the statement what action it would take after the yearlong moratorium expires..Amazon is known to have pitched its facial recognition technology, Rekognition, to federal agencies, like Immigration and Customs Enforcement. Last year, Amazon’s cloud chief Andy Jassy said in an interview the company would provide Rekognition to “any” government department..There are dozens of companies providing facial recognition technology to police, but Amazon is by far the biggest. Amazon has come under the most scrutiny after its Rekognition face-scanning technology showed bias against people of color..In 2018, the ACLU found that Rekognition falsely matched 28 members of Congress as criminals in a mugshot database. Amazon criticized the results, claiming the ACLU had lowered the facial recognition system’s confidence threshold. But a year later, the ACLU of Massachusetts found that Rekognition had falsely matched 27 New England professional athletes against a mugshot database. Both tests disproportionately mismatched Black people, the ACLU found..Investors brought a proposal to Amazon’s annual shareholder meeting almost exactly a year ago that would have forcibly banned Amazon from selling its facial recognition technology to the government or law enforcement. Amazon defeated the vote with a wide margin..The ACLU acknowledged Amazon’s move to pause sales of Rekognition, which it called a “threat to our civil rights and liberties,” but called on the company and other firms to do more.",Amazon suspended contracts involving police use of its Rekognition software for a year most likely in response to the Justice in Policing Act Bill.
392,"A new study from the Georgia Institute of Technology suggests autonomous driving systems may have more difficulty detecting pedestrians with dark skin than those with light skin..The researchers responsible for the study had eight image-detection systems analyze images of pedestrians. The people in the photos were separated into two groups based on how their skin tones aligned with the Fitzpatrick skin type scale, which divides skin tones into six categories. One group consisted of pedestrians who fell into one of the three lightest categories on the Fitzpatrick scale, while the other group consisted of pedestrians who fell into one of the three darkest categories on the Fitzpatrick scale..The image-detection systems then attempted to identify all of the pedestrians in the images, and the researchers compared the systems abilities to detect light-skinned pedestrians versus dark-skinned pedestrians. On average, the image-detection systems were 5% less accurate at detecting dark-skinned pedestrians, even when the researchers controlled for variables that may have been able to explain the disparity, like pedestrians who were partially blocked from view or the time of day the photo was taken..The researchers suggested that the differences in pedestrian-detection accuracy could result from not having enough dark-skinned pedestrians in the images used to train the systems, as well as the systems insufficient emphasis on learning from the smaller population of dark-skinned pedestrians..While Vox notes that the study has not been peer-reviewed and did not use the same image-detection systems or image sets featured in current self-driving vehicles, the study suggests that companies developing autonomous-driving technology should be attentive to the methods they use to train vehicles to identify pedestrians.","A study found that self-driving cars guided by AI performed worse at detecting people with dark skin, which could put the lives of dark-skinned pedestrians at risk."
396,"Last month Nature published a damning response written by 31 scientists to a study from Google Health that had appeared in the journal earlier this year. Google was describing successful trials of an AI that looked for signs of breast cancer in medical images. But according to its critics, the Google team provided so little information about its code and how it was tested that the study amounted to nothing more than a promotion of proprietary tech..“We couldn’t take it anymore,” says Benjamin Haibe-Kains, the lead author of the response, who studies computational genomics at the University of Toronto. “Its not about this study in particular—it’s a trend weve been witnessing for multiple years now that has started to really bother us.”.Haibe-Kains and his colleagues are among a growing number of scientists pushing back against a perceived lack of transparency in AI research. “When we saw that paper from Google, we realized that it was yet another example of a very high-profile journal publishing a very exciting study that has nothing to do with science,” he says. “Its more an advertisement for cool technology. We can’t really do anything with it.”.Science is built on a bedrock of trust, which typically involves sharing enough details about how research is carried out to enable others to replicate it, verifying results for themselves. This is how science self-corrects and weeds out results that don’t stand up. Replication also allows others to build on those results, helping to advance the field. Science that can’t be replicated falls by the wayside..At least, that’s the idea. In practice, few studies are fully replicated because most researchers are more interested in producing new results than reproducing old ones. But in fields like biology and physics—and computer science overall—researchers are typically expected to provide the information needed to rerun experiments, even if those reruns are rare..AI is feeling the heat for several reasons. For a start, it is a newcomer. It has only really become an experimental science in the past decade, says Joelle Pineau, a computer scientist at Facebook AI Research and McGill University, who coauthored the complaint. “It used to be theoretical, but more and more we are running experiments,” she says. “And our dedication to sound methodology is lagging behind the ambition of our experiments.”.The problem is not simply academic. A lack of transparency prevents new AI models and techniques from being properly assessed for robustness, bias, and safety. AI moves quickly from research labs to real-world applications, with direct impact on people’s lives. But machine-learning models that work well in the lab can fail in the wild—with potentially dangerous consequences. Replication by different researchers in different settings would expose problems sooner, making AI stronger for everyone. .AI already suffers from the black-box problem: it can be impossible to say exactly how or why a machine-learning model produces the results it does. A lack of transparency in research makes things worse. Large models need as many eyes on them as possible, more people testing them and figuring out what makes them tick. This is how we make AI in health care safer, AI in policing more fair, and chatbots less hateful..One thing that stops AI experiments from being replicated is a lack of access to the code. According to the 2020 State of AI report, a well-vetted annual analysis of the field by investors Nathan Benaich and Ian Hogarth, only 15% of AI studies share their code. Industry researchers are bigger offenders than those affiliated with universities. In particular, the report calls out OpenAI and DeepMind for keeping code under wraps..Then there’s the growing gulf between the haves and have-nots when it comes to the two pillars of AI, data and hardware. Data is often proprietary, such as the information Facebook collects on its users, or sensitive, as in the case of personal medical records. And tech giants carry out more and more research on enormous, expensive clusters of computers that few universities or smaller companies have the resources to access..To take one example, training the language generator GPT-3 is estimated to have cost OpenAI $10 to $12 million—and that’s just the final model, not including the cost of developing and training its prototypes. “You could probably multiply that figure by at least one or two orders of magnitude,” says Benaich, who is founder of Air Street Capital, a VC firm that invests in AI startups. Only a tiny handful of big tech firms can afford to do that kind of work, he says: “Nobody else can just throw vast budgets at these experiments.”.Hypothetical question. Some people have access to GPT-3 and others do not. What happens when we start seeing papers in which GPT-3 is used by non-OpenAI researchers to achieve SOTA results?Hereâ€™s the real problem, tho: is OpenAI picking research winners and losers?.The rate of progress is dizzying, with thousands of papers published every year. But unless researchers know which ones to trust, it is hard for the field to move forward. Replication lets other researchers check that results have not been cherry-picked and that new AI techniques really do work as described. “Its getting harder and harder to tell which are reliable results and which are not,” says Pineau..What can be done? Like many AI researchers, Pineau divides her time between university and corporate labs. For the last few years, she has been the driving force behind a change in how AI research is published. For example, last year she helped introduce a checklist of things that researchers must provide, including code and detailed descriptions of experiments, when they submit papers to NeurIPS, one of the biggest AI conferences..Pineau has also helped launch a handful of reproducibility challenges, in which researchers try to replicate the results of published studies. Participants select papers that have been accepted to a conference and compete to rerun the experiments using the information provided. But the only prize is kudos..This lack of incentive is a barrier to such efforts throughout the sciences, not just in AI. Replication is essential, but it isn’t rewarded. One solution is to get students to do the work. For the last couple of years, Rosemary Ke, a PhD student at Mila, a research institute in Montreal founded by Yoshua Bengio, has organized a reproducibility challenge where students try to replicate studies submitted to NeurIPS as part of their machine-learning course. In turn, some successful replications are peer-reviewed and published in the journal ReScience. .“It takes quite a lot of effort to reproduce another paper from scratch,” says Ke. “The reproducibility challenge recognizes this effort and gives credit to people who do a good job.” Ke and others are also spreading the word at AI conferences via workshops set up to encourage researchers to make their work more transparent. This year Pineau and Ke extended the reproducibility challenge to seven of the top AI conferences, including ICML and ICLR. .Another push for transparency is the Papers with Code project, set up by AI researcher Robert Stojnic when he was at the University of Cambridge. (Stojnic is now a colleague of Pineau’s at Facebook.) Launched as a stand-alone website where researchers could link a study to the code that went with it, this year Papers with Code started a collaboration with arXiv, a popular preprint server. Since October, all machine-learning papers on arXiv have come with a Papers with Code section that links directly to code that authors wish to make available. The aim is to make sharing the norm..Do such efforts make a difference? Pineau found that last year, when the checklist was introduced, the number of researchers including code with papers submitted to NeurIPS jumped from less than 50% to around 75%. Thousands of reviewers say they used the code to assess the submissions. And the number of participants in the reproducibility challenges is increasing..But it is only a start. Haibe-Kains points out that code alone is often not enough to rerun an experiment. Building AI models involves making many small changes—adding parameters here, adjusting values there. Any one of these can make the difference between a model working and not working. Without metadata describing how the models are trained and tuned, the code can be useless. “The devil really is in the detail,” he says..It’s also not always clear exactly what code to share in the first place. Many labs use special software to run their models; sometimes this is proprietary. It is hard to know how much of that support code needs to be shared as well, says Haibe-Kains..Pineau isn’t too worried about such obstacles. “We should have really high expectations for sharing code,” she says. Sharing data is trickier, but there are solutions here too. If researchers cannot share their data, they might give directions so that others can build similar data sets. Or you could have a process where a small number of independent auditors were given access to the data, verifying results for everybody else, says Haibe-Kains..Hardware is the biggest problem. But DeepMind claims that big-ticket research like AlphaGo or GPT-3 has a trickle-down effect, where money spent by rich labs eventually leads to results that benefit everyone. AI that is inaccessible to other researchers in its early stages, because it requires a lot of computing power, is often made more efficient—and thus more accessible—as it is developed. “AlphaGo Zero surpassed the original AlphaGo using far less computational resources,” says Koray Kavukcuoglu, vice president of research at DeepMind..In theory, this means that even if replication is delayed, at least it is still possible. Kavukcuoglu notes that Gian-Carlo Pascutto, a Belgian coder at Mozilla who writes chess and Go software in his free time, was able to re-create a version of AlphaGo Zero called Leela Zero, using algorithms outlined by DeepMind in its papers. Pineau also thinks that flagship research like AlphaGo and GPT-3 is rare. The majority of AI research is run on computers that are available to the average lab, she says. And the problem is not unique to AI. Pineau and Benaich both point to particle physics, where some experiments can only be done on expensive pieces of equipment such as the Large Hadron Collider..In physics, however, university labs run joint experiments on the LHC. Big AI experiments are typically carried out on hardware that is owned and controlled by companies. But even that is changing, says Pineau. For example, a group called Compute Canada is putting together computing clusters to let universities run large AI experiments. Some companies, including Facebook, also give universities limited access to their hardware. “It’s not completely there,” she says. “But some doors are opening.”.10/Lets face it: following good practices for sharing code, data, and other materials can be inconvenient for authors anywhere (although some practices can make it more convenient). But its essential for the scientific enterprise. For-profit businesses dont get a free pass..Haibe-Kains is less convinced. When he asked the Google Health team to share the code for its cancer-screening AI, he was told that it needed more testing. The team repeats this justification in a formal reply to Haibe-Kains’s criticisms, also published in Nature: “We intend to subject our software to extensive testing before its use in a clinical environment, working alongside patients, providers and regulators to ensure efficacy and safety.” The researchers also said they did not have permission to share all the medical data they were using..It’s not good enough, says Haibe-Kains: “If they want to build a product out of it, then I completely understand they won’t disclose all the information.” But he thinks that if you publish in a scientific journal or conference, you have a duty to release code that others can run. Sometimes that might mean sharing a version that is trained on less data or uses less expensive hardware. It might give worse results, but people will be able to tinker with it. “The boundaries between building a product versus doing research are getting fuzzier by the minute,” says Haibe-Kains. “I think as a field we are going to lose.” .If companies are going to be criticized for publishing, why do it at all? There’s a degree of public relations, of course. But the main reason is that the best corporate labs are filled with researchers from universities. To some extent the culture at places like Facebook AI Research, DeepMind, and OpenAI is shaped by traditional academic habits. Tech companies also win by participating in the wider research community. All big AI projects at private labs are built on layers and layers of public research. And few AI researchers haven’t made use of open-source machine-learning tools like Facebook’s PyTorch or Google’s TensorFlow..As more research is done in house at giant tech companies, certain trade-offs between the competing demands of business and research will become inevitable. The question is how researchers navigate them. Haibe-Kains would like to see journals like Nature split what they publish into separate streams: reproducible studies on one hand and tech showcases on the other..Other large corporate labs stress their commitment to transparency too. “Scientific work requires scrutiny and replication by others in the field,” says Kavukcuoglu. “This is a critical part of our approach to research at DeepMind.”.“OpenAI has grown into something very different from a traditional laboratory,” says Kayla Wood, a spokesperson for the company. “Naturally that raises some questions.” She notes that OpenAI works with more than 80 industry and academic organizations in the Partnership on AI to think about long-term publication norms for research..Pineau believes there’s something to that. She thinks AI companies are demonstrating a third way to do research, somewhere between Haibe-Kains’s two streams. She contrasts the intellectual output of private AI labs with that of pharmaceutical companies, for example, which invest billions in drugs and keep much of the work behind closed doors..The long-term impact of the practices introduced by Pineau and others remains to be seen. Will habits be changed for good? What difference will it make to AI’s uptake outside research? A lot hangs on the direction AI takes. The trend for ever larger models and data sets—favored by OpenAI, for example—will continue to make the cutting edge of AI inaccessible to most researchers. On the other hand, new techniques, such as model compression and few-shot learning, could reverse this trend and allow more researchers to work with smaller, more efficient AI..Either way, AI research will still be dominated by large companies. If it’s done right, that doesn’t have to be a bad thing, says Pineau: “AI is changing the conversation about how industry research labs operate.” The key will be making sure the wider field gets the chance to participate. Because the trustworthiness of AI, on which so much depends, begins at the cutting edge.  .An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.",The Nature Journal published a critical response to Google Health AI study. The Google team provided so little information about its code and how it was tested that the study amounted to nothing more than a promotion of proprietary tech. There's a growing number of scientists pushing back against a perceived lack of transparency in AI research.
397,"A data scientist has uncovered a Twitter bot network made up of accounts that have unique avatars featuring AI-generated women, anime characters, and cats. They talk incessantly in Turkish about porn and sports betting..Twitter bots are getting harder to spot. It used to be easier: when you saw a Twitter egg that had a user name followed by a string of random numbers, you knew you were probably dealing with an account run by a bot. Thanks to machine learning, those accounts now have randomly-generated human faces, a well-known problem. But its not just faces: theres AI-generated cats and anime avatars, too. These accounts look authentic, but they’re not. .Many GAN-generated images contain weird artifacts and details inconsistent with real photographs. With a little practice, people can spot details in GAN images that point to their artificial nature: surreal swirls, odd bits in the background, and details like teeth and eyes where they shouldn’t be are all common giveaways. Conspirador Norteño noted that some of the pictures in this bot network had those markers, but they can be hard to spot when the picture is reduced to the size of a Twitter avatar. .Conspirador Norteño and their partner Dr ZQ discovered a bot network on Twitter of more than 3,000 accounts using GAN generated images. According to them, the accounts were created around January 25 and use a mix of anime avatars, cats, and faces. Some of the images repeat. More than 900 of them are female faces on a blank white background. Around 275 of them are anime avatars and 307 are cats. The accounts mostly follow each other..So what are these cats, people, and waifus tweeting about? Porongraphy and online sports betting, and mostly in Turkish. “This network replies en masse to specific tweets, with dozens or hundreds of members replying to the same tweet,” Conspirador Norteño said..Bot networks joining together to tweet about pornography is a clear violation of Twitter’s terms of service. The sites user policies state that you may not use Twitter’s services in a manner intended to artificially amplify or suppress information or engage in behavior that manipulates or disrupts people’s experience on Twitter..Twitter followed through. “We suspended a number of accounts for spam and coordination under our platform manipulation and spam policy,” a Twitter spokesperson told Motherboard in an email. “Using both technology and human review, we proactively tackle attempts to disrupt the public conversation at scale.”.As of this writing, many of the accounts are already being suspended, including Dilde97512368, the account Conspirador Norteño called out in their first tweet..Twitter has dismantled massive bot networks before, including one in Turkey that was promoting the Turkish political partyAKP and its founder President Recep Tayyip Erdoğan in June 2020.  This also isn’t the first big bot network Conspirador Norteño has uncovered. In August 2020, they uncovered a group of Twitter bots that were selling followers to people by the thousands. That bot network also used GAN-generated avatars to make their accounts seem authentic..By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.","Twitter has suspended a bot network made up of accounts that have unique avatars featuring AI-generated women, anime characters, and cats. These bots have GAN-generated profile pictures and advertise incessantly in Turkish about porn and sports betting."
398,"Much like a writing prompts journal, the app asks questions that encourage self-reflection. The added AI feature analyzes users’ emotions and provides personal letters at the end of each week and month that give feedback about their thoughts. It also compiles daily entries to graph mood, most used vocabulary, and highlights from the week. After each entry, Muute leaves you with a quote to mull over, or a breathing exercise. Exactly what I need, I thought..When I was younger, I used to write in my diaries religiously. Writing time was always 30 minutes before lights out and entries were usually recounts of my delicious snacks. Eight-year-old me was highly passionate about chocolate mousse and one report read “Mom did it again. It was so smooth and rich. I want to dream about it.” Apparently, I couldn’t get enough.  .But in the last decade, my journaling has matured somewhat. Now, I have notebooks dedicated to varying emotional needs. My daily reflection journal, an inconspicuous tan color that lies on my windowsill, is only ever a few sentences. “Thinking about consequential happiness and what it means to tell the truest story of yourself,” I wrote one day in January. Then I have my “down” diary, where I track all the times I feel sad, to remember life’s difficulties. I turn to it when I don’t understand all the feelings I’m having and hope writing them clears my emotional fog. The most sensitive, top-secret, if-someone-finds-this-I’m-screwed diary, is my no bullshitting journal. I only open it about five times a year, always on my birthday and New Year’s. .Needless to say, I was eager to add Muute to my roulette of journals. I decided to try it out for a week. Though prompt diaries seemed counterintuitive because I’ve always had something to say, the prospect of getting feedback about my feelings sounded helpful. 2021 has been a year of many changes for me. I’ve moved to Japan, leaving my nuclear family to live alone in the middle of a pandemic. I welcome change with open arms, but sometimes, it can be overwhelming. I hoped Muute would help restore some inner balance. .Past research has indicated that journaling does wonders for mental health. Expressive writing has been shown to reduce unwanted thoughts about negative events, as well as improve working memory. The COVID-19 pandemic, which has led to thousands of daily deaths, millions out of work, and extreme loneliness, has intensified the need for emotional care. In Japan, suicide rates rose 37 percent for women between July and October, prompting governing officials to consider how to better provide support.  .The horrors of our daily lives are enough to keep anyone in bed, but a tool that could aid in improving one’s mental state is emotion AI. As a branch of artificial intelligence, it helps us understand the way we feel. It uses natural language processing technology, which assists computers to understand human language. In a journaling app such as Muute, emotion AI can interpret patterns in users’ language to detect emotion. .“It’s perfect for busy people; it gives us a chance to calm down and look after ourselves. Through the app, I’m able to notice things about myself that I normally wouldn’t, which helps me understand who I am more objectively,” he told VICE.  .Downloading Muute from the App Store doesn’t have quite the same thrill as cracking open a diary for the first time, but the user-friendly interface was instantly attractive. It’s clear that the focal point of Muute is tracking emotional progression over time. Like a calendar, the app’s main page shows my week at a glance. The other pages lead to logs, user profile, and the weekly and monthly insight letters. .The very first question I answered was “How does your family make your life better?” Suspicion arose, as it was a highly coincidental prompt; I had just e-fought with my brother about not giving me enough attention. But after begrudgingly writing about the joys of family, I noticed my resentment had dissipated. Such is the power of selective positive thinking — we forget tedious arguments. .Over the week, the app consistently asked me positivity-driven questions, such as “What is a recent thing you learned?” and “Who helped you this week?” My moodflow graph showed two curves; an orange “positive” emotion one, and a blue “negative” feeling one. The curves spiked and dipped, depending on my entry for that day. Monday to Tuesday, the lowest point, was when I fought with my brother. Tuesday to Wednesday was a highlight for me, and was coincidentally the day I received some positive feedback about an article I wrote. .Initially, using the app felt like a task to complete at night, which is how I’ve always journaled. But as the week went by, I realized I was checking Muute throughout the day. I looked at the log to see whether what I had felt was a bad day was interpreted similarly by the app. The end-of-the-week letter felt slightly vague, given the app didn’t have many entries to work with. But it had an understanding tone; it said I wasn’t alone in my worries and encouraged me to focus on my future. .The sole criticism I have about Muute is that the prompts all seemed positive. The only time I truly reflected on sad thoughts was when the app asked me to select three categories that I’m most worried about, as well as three words that best described my feelings. .It’s a good mental exercise to practice positive thinking, but at the same time, I’m a firm believer in the value of sadness. If we don’t know grief or pain, we have nothing to compare our happiness to. Having the space to explore sadness makes us much more well-rounded and empathetic; the world isn’t stretches of paradise, and we shouldn’t trick ourselves to believe that. .To those who feel like they’re missing that additional software of self-understanding, I’d give AI journaling apps a try. It’s a great supplement to add to free-writing spaces, and the way Muute is contextualized does help you see the bigger narrative arc of your life. The app helps you notice emotional patterns and simplifies the complexity of human emotion. You realize small downfalls are only temporary, and that life will always have more to offer than what you see in front of you. .“The app helps you notice emotional patterns and simplifies the complexity of human emotion. You realize small downfalls are only temporary, and that life will always have more to offer than what you see in front of you.”.Over the years, I’ve heard the incessant hammering of “Know who you are!” or “Just be yourself!” Sometimes, you just don’t. And that’s okay. I’ve ceased trying to demystify every single fear, happiness, and pain, and instead decided to work towards understanding how fears make me click. It’s an ongoing process, but I welcome tools that can help me grow in the right direction. .By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.","An first-person narrative from a VICE contributor on how Muute, Japan’s first AI journaling app, helpeed her reflect on her emotions. "
399,"In Ohio, nearly 900 doses of Moderna’s COVID-19 vaccine went to waste because a provider stored them at the wrong temperature. In New York City, clinics were forced to hold onto vaccine doses for weeks instead of administering them to high-risk patients, or even throw them out, because they couldn’t find anyone who met the state’s stringent criteria for the first phase of inoculation. Florida, meanwhile, partnered with the event ticket platform Eventbrite to manage vaccine appointments, resulting in a confusing, biased, and scam-prone distribution system..Distributing the limited number of COVID-19 vaccines to the people who most need them is a daunting logistical challenge, and in many cases, the bureaucratic, human-run systems charged with overseeing it have fallen short. In some states, broken and confusing scheduling systems have driven frustrated residents to write code and build their own volunteer websites for booking vaccine appointments..It’s the kind of problem, some tech firms say, that is ripe for artificially intelligent overseers—and it may be the very thing needed to jump-start a lucrative new era of automated health care decision-making and delivery..“I think it did, in many ways, take the COVID-19 pandemic to put that rocket fuel behind AI” in health care, Dr. John Showalter, the chief product officer for Jvion, a clinical AI company that’s been operating since 2011, told Motherboard. “I feel like we’re right on that precipice. 2021 is going to have a lot of reports out about how clinical AI helped with COVID-19. By 2025, people are going to be like ‘clinical AI, yawn’” because it’s so ubiquitous..There is a spectrum of use cases for AI in vaccine distribution. Health systems are already  using AI chatbots from companies like Hyro and Praktice AI to field calls from the large number of patients inquiring about whether they’re eligible for the vaccines, and to schedule appointments and follow ups..Big tech companies like Google and Microsoft have developed vaccine management systems that incorporate AI at various levels, including planning trucking routes and maintaining dose temperatures.But the potential applications that are generating the most excitement, and skepticism, are AI tools designed to automate or influence decisions about where vaccines should go in the country—and who should get them. .In the early months of the pandemic, California asked companies to propose technological solutions to problems like COVID-19 test shortages. Aible, an AI startup based in the state, offered—for free—to create algorithmic models that would identify who to prioritize in testing in order to save lives and reduce the pandemic’s economic impact. State officials didn’t respond to the offer, Aible CEO Arijit Sengupta, told Motherboard, but he said the company is now talking with one of the major vaccine makers about creating a similar system to guide vaccine supply chains and prioritization..“Matching up the demand and supply is something that’s not happening right now,” he said. “It would be better than what we are doing today—it would not be perfect, nothing is ever perfect—but what is good about a system like this is it learns on a daily basis and it adjusts.”.Most states are currently in the first phases of vaccination, where almost all available doses are reserved for health care workers, people over 65, and some individuals with chronic conditions. But once those populations are inoculated, states will have to make difficult decisions about where to send doses and who to prioritize next..Jvion has created models that map which areas of the country and which population groups are most likely to experience severe effects or die if exposed to a wave of COVID-19 and, separately, which areas are the highest priorities for vaccine distribution based on the CDC’s prioritization recommendations and other health and socio-economic factors. The company has sent analyses of millions of patients to its customers, which include health systems, to guide their vaccine outreach..Take Perry County, Pennsylvania. One of Jvion’s models, which incorporates the CDC’s guidelines, labels the county a very low priority for vaccination. But its other model, measuring community vulnerability to COVID-19, considers Perry County at the highest level of risk for severe, community-wide morbidity. .Jvions COVID-19 model for Perry County, PA demonstrates how AI might help states allocate vaccines to high-risk areas they might otherwise overlook..The reasons for the disparity aren’t necessarily intuitive. They draw on data that health systems likely aren’t considering and algorithmic pattern matching that is, by definition, unhuman. Why is Perry County at such high risk for COVID-19 morbidity? According to Jvion’s models, the fact that a county has “low commercial retail availability” and “low commercial/industrial job density” are influential risk indicators. Factors that influence the calculated risk level in other counties include the rate at which residents commute more than 60 minutes to work and the prevalence of environmental health hazards..Jvion’s models are trained on a database of 36 million “independent lives,” Dr. Showalter said. That includes people’s medical claim records, socio-economic information about where they live from agencies like the Environmental Protection Agency and U.S. Department of Agriculture, and credit scoring data from companies like Experian and Transunion. As they built the models, there was nowhere near enough information available about actual COVID-19 cases, so Jvion instead trained its models primarily on data about the health care trajectories of people with respiratory conditions and illnesses like Influenza..Jvion’s models are not currently being used as automatic decision making systems—determining who gets prioritized for vaccines without human oversight. But AI is increasingly being used to inform a variety of triaging problems in health care, and experts say a regulatory action plan for medical AI that the Food and Drug Administration (FDA) published in January is a long-awaited signal that the agency is preparing to open the doors to a new array of medical AI tools..Currently, in order for AI systems to be eligible for FDA approval as medical devices, they must be static tools—the algorithmic model that comes out of the box doesn’t change. But one of the strengths of this kind of technology is that models can constantly be trained on new data, tweaked, and improved over time. The agency’s new action plan calls for it to develop guidelines for allowing, and regulating, evolving systems..Monitoring continuously changing algorithms that contribute to life-or-death decision making is a tricky proposition, though, and one that has kept the FDA from moving more quickly to accept these kinds of systems as medical devices..But the COVID-19 pandemic has sped up health care providers’ adoption of AI tools—such as home monitoring systems for vital signs that reduce the chances of virus transmission. These tools may not need FDA approval as medical devices, but nonetheless influence health care decision making, Sara Gerke, a research fellow at Harvard Law School’s Petrie-Flom Center for Health Law Policy, Biotechnology, and Bioethics, told Motherboard. That increased exposure will likely quicken the industry’s adoption of more advanced, higher-stakes tools, despite the many legal and ethical issues that remain..“I personally believe that AI has potential for being used for allocation of vaccines, but that’s for the future,” Gerke said. “I would not trust the AI right now because there will be so many hidden biases in the data. First of all, what kind of data do you even use? Even if you take it from the electronic health records data then you already have a bias because in Black communities, many can’t even go to the doctor. Right now using it, I find it very difficult.”.By signing up, you agree to the Terms of Use and Privacy Policy & to receive electronic communications from Vice Media Group, which may include marketing promotions, advertisements and sponsored content.","As people struggle to book appointments and doses go unused, some companies are proposing automated tools to help deliver vaccines into 
arms."
400,"In the last few years, research has shown that deep learning can match expert-level performance in medical imaging tasks like early cancer detection and eye disease diagnosis. But there’s also cause for caution. Other research has shown that deep learning has a tendency to perpetuate discrimination. With a health-care system already riddled with disparities, sloppy applications of deep learning could make that worse..Now a new paper published in Nature Medicine is proposing a way to develop medical algorithms that might help reverse, rather than exacerbate, existing inequality. The key, says Ziad Obermeyer, an associate professor at UC Berkeley who oversaw the research, is to stop training algorithms to match human expert performance..The paper looks at a specific clinical example of the disparities that exist in the treatment of knee osteoarthritis, an ailment which causes chronic pain. Assessing the severity of that pain helps doctors prescribe the right treatment, including physical therapy, medication, or surgery. This is traditionally done by a radiologist reviewing an x-ray of the knee and scoring the patient’s pain on the Kellgren–Lawrence grade (KLG), which calculates pain levels based on the presence of different radiographic features, like the degree of missing cartilage or structural damage..But data collected by the National Institute of Health found that doctors using this method systematically score Black patients’ pain as far as far less severe than what they say they’re experiencing. Patients self-report their pain levels using a survey that asks how much it hurts to do various things, such as fully straightening their knee. But these self-reported pain levels are ignored in favor of the radiologist’s KLG score when prescribing treatment. In other words, Black patients who show the same amount of missing cartilage as white patients self-report higher levels of pain..This has consistently miffed medical experts. One hypothesis is that Black patients could be reporting higher levels of pain in order to get doctors to treat them more seriously. But there’s an alternative explanation. The KLG methodology itself could be biased. It was developed several decades ago with white British populations. Some medical experts argue that the list of radiographic markers it tells clinicians to look for may not include all the possible physical sources of pain within a more diverse population. Put another way, there may be radiographic indicators of pain that appear more commonly in Black people that simply aren’t part of the KLG rubric..To test this possibility, the researchers trained a deep-learning model to predict patients’ self-reported pain level from their knee x-ray. If the resultant model had terrible accuracy, this would suggest that self-reported pain is rather arbitrary. But if the model had really good accuracy, this would provide evidence that self-reported pain is in fact correlated with radiographic markers in the x-ray..After running several experiments, including some designed to discount any confounding factors, the researchers found that the model was much more accurate than KLG at predicting self-reported pain levels for both white and Black patients, but especially for Black patients. It reduced the racial disparity at each pain level by nearly half..The goal isn’t necessarily to start using this algorithm in a clinical setting. But by outperforming the KLG methodology, it revealed that the standard way of measuring pain is flawed, at a much greater cost to Black people. This should tip off the medical community to investigate which radiographic markers the algorithm might be seeing, and update their scoring methodology..“It actually highlights a really exciting part of where these kinds of algorithms can fit into the process of medical discovery,” says Obermeyer. “It tells us if there’s something here that’s worth looking at that we don’t understand. It sets the stage for humans to then step in and, using these algorithms as tools, try to figure out what’s going on.”.“The cool thing about this paper is it is thinking about things from a completely different perspective,” says Irene Chen, a researcher at MIT who studies how to reduce health-care inequities in machine learning and was not involved in the paper. Instead of training the algorithm on well-established expert knowledge, she says, the researchers chose to treat patients’ self-assessment as truth. Through that it uncovered important gaps in what the medical field usually considers to be the more “objective” pain measure..“That was exactly the secret,” agrees Obermeyer. If algorithms are only ever trained to match expert performance, he says, they will simply perpetuate existing gaps and inequities. “This study is a glimpse of a more general pipeline that we are increasingly able to use in medicine for generating new knowledge.” .An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.","A new study shows how training deep-learning models on patient outcomes could help reveal gaps in existing medical knowledge. In the study, an AI model was much more accurate than radiologists' KLG system at predicting self-reported pain levels for both white and Black patients, but especially for Black patients. It reduced the racial disparity at each pain level by nearly half."
401,"The Trevor Project, America’s hotline for LGBT youth, is turning to a GPT-2-powered chatbot to help troubled teenagers—but it’s setting strict limits..Counselors volunteering at the Trevor Project need to be prepared for their first conversation with an LGBTQ teen who may be thinking about suicide. So first, they practice. One of the ways they do it is by talking to fictional personas like “Riley,” a 16-year-old from North Carolina who is feeling a bit down and depressed. With a team member playing Riley’s part, trainees can drill into what’s happening: they can uncover that the teen is anxious about coming out to family, recently told friends and it didn’t go well, and has experienced suicidal thoughts before, if not at the moment..Just like the original persona, this version of Riley—trained on thousands of past transcripts of role-plays between counselors and the organization’s staff—still needs to be coaxed a bit to open up, laying out a situation that can test what trainees have learned about the best ways to help LGBTQ teens. .Counselors aren’t supposed to pressure Riley to come out. The goal, instead, is to validate Riley’s feelings and, if needed, help develop a plan for staying safe. .Crisis hotlines and chat services make them a fundamental promise: reach out, and we’ll connect you with a real human who can help. But the need can outpace the capacity of even the most successful services. The Trevor Project believes that 1.8 million LGBTQ youth in America seriously consider suicide each year. The existing 600 counselors for its chat-based services can’t handle that need. That’s why the group—like an increasing number of mental health organizations—turned to AI-powered tools to help meet demand. It’s a development that makes a lot of sense, while simultaneously raising questions about how well current AI technology can perform in situations where the lives of vulnerable people are at stake. .“We didn’t set out to and are not setting out to design an AI system that will take the place of a counselor, or that will directly interact with a person who might be in crisis,” says Dan Fichter, the organization’s head of AI and engineering. This human connection is important in all mental health services, but it might be especially important for the people the Trevor Project serves. According to the organization’s own research in 2019, LGBTQ youth with at least one accepting adult in their life were 40% less likely to report a suicide attempt in the previous year. .The AI-powered training role-play, called the crisis contact simulator and supported by money and engineering help from Google, is the second project the organization has developed this way: it also uses a machine-learning algorithm to help determine who’s at highest risk of danger. (It trialed several other approaches, including many that didn’t use AI, but the algorithm simply gave the most accurate predictions for who was experiencing the most urgent need.).AI-powered risk assessment isn’t new to suicide prevention services: the Department of Veterans Affairs also uses machine learning to identify at-risk veterans in its clinical practices, as the New York Times reported late last year. .Opinions vary on the usefulness, accuracy, and risk of using AI in this way. In specific environments, AI can be more accurate than humans in assessing people’s suicide risk, argues Thomas Joiner, a psychology professor at Florida State University who studies suicidal behavior. In the real world, with more variables, AI seems to perform about as well as humans. What it can do, however, is assess more people at a faster rate. .Thus, it’s best used to help human counselors, not replace them. The Trevor Project still relies on humans to perform full risk assessments on young people who use its services. And after counselors finish their role-plays with Riley, those transcripts are reviewed by a human. .The crisis contact simulator was developed because doing role-plays takes up a lot of staff time and is limited to normal working hours, even though a majority of counselors plan on volunteering during night and weekend shifts. But even if the aim was to train more counselors faster, and better accommodate volunteer schedules, efficiency wasn’t the only ambition. The developers still wanted the role-play to feel natural, and for the chatbot to nimbly adapt to a volunteers’ mistakes. Natural-language-processing algorithms, which had recently gotten really good at mimicking human conversations, seemed like a good fit for the challenge. After testing several options, the Trevor Project settled on OpenAI’s GPT-2 algorithm..The AI is the largest language model ever created and can generate amazing human-like text on demand but wont bring us closer to true intelligence..The chatbot uses GPT-2 for its baseline conversational abilities. That model is trained on 45 million pages from the web, which teaches it the basic structure and grammar of the English language. The Trevor Project then trained it further on all the transcripts of previous Riley role-play conversations, which gave the bot the materials it needed to mimic the persona..Throughout the development process, the team was surprised by how well the chatbot performed. There is no database storing details of Riley’s bio, yet the chatbot stayed consistent because every transcript reflects the same storyline..But there are also trade-offs to using AI, especially in sensitive contexts with vulnerable communities. GPT-2, and other natural-language algorithms like it, are known to embed deeply racist, sexist, and homophobic ideas. More than one chatbot has been led disastrously astray this way, the most recent being a South Korean chatbot called Lee Luda that had the persona of a 20-year-old university student. After quickly gaining popularity and interacting with more and more users, it began using slurs to describe the queer and disabled communities..The Trevor Project is aware of this and designed ways to limit the potential for trouble. While Lee Luda was meant to converse with users about anything, Riley is very narrowly focused. Volunteers won’t deviate too far from the conversations it has been trained on, which minimizes the chances of unpredictable behavior..This also makes it easier to comprehensively test the chatbot, which the Trevor Project says it is doing. “These use cases that are highly specialized and well-defined, and designed inclusively, don’t pose a very high risk,” says Nenad Tomasev, a researcher at DeepMind..This isn’t the first time the mental health field has tried to tap into AI’s potential to provide inclusive, ethical assistance without hurting the people it’s designed to help. Researchers have developed promising ways of detecting depression from a combination of visual and auditory signals. Therapy “bots,” while not equivalent to a human professional, are being pitched as alternatives for those who can’t access a therapist or are uncomfortable  confiding in a person. .Each of these developments, and others like it, require thinking about how much agency AI tools should have when it comes to treating vulnerable people. And the consensus seems to be that at this point the technology isn’t really suited to replacing human help. .Still, Joiner, the psychology professor, says this could change over time. While replacing human counselors with AI copies is currently a bad idea, “that doesn’t mean that it’s a constraint that’s permanent,” he says. People, “have artificial friendships and relationships” with AI services already. As long as people aren’t being tricked into thinking they are having a discussion with a human when they are talking to an AI, he says, it could be a possibility down the line. .In the meantime, Riley will never face the youths who actually text in to the Trevor Project: it will only ever serve as a training tool for volunteers. “The human-to-human connection between our counselors and the people who reach out to us is essential to everything that we do,” says Kendra Gaunt, the group’s data and AI product lead. “I think that makes us really unique, and something that I don’t think any of us want to replace or change.”.An MIT Technology Review series investigates how AI is enriching a powerful few by dispossessing communities that have been dispossessed before.","The Trevor Project, America’s hotline for LGBT youth, is turning to a GPT-2-powered chatbot to help troubled teenagers—but it’s setting strict
 limits."
402,"Facial recognition has already come a long way since U.S. Special Operations Forces used the technology to help identify Osama bin Laden after killing the Al-Qaeda leader in his Pakistani hideout in 2011. The U.S. Army Research Laboratory recently unveiled a dataset of faces designed to help train AI on identifying people even in the dark—a possible expansion of facial recognition capabilities that some experts warn could lead to expanded surveillance beyond the battlefield..The Army Research Laboratory Visible and Thermal Face dataset contains 500,000 images from 395 people. Despite its modest size as far as facial recognition datasets go, it is one of the largest and most comprehensive datasets that includes matching images of people’s faces taken under both ordinary visible-light conditions and with heat-sensing thermal cameras in low-light conditions..“Our motivation for this dataset was we wanted to develop a nighttime and low-light face recognition capability for these unconstrained or difficult lighting settings,” says Matthew Thielke, a physicist at the Army Research Laboratory in Adelphi, Maryland..Facial recognition applications for nighttime or low-light conditions are still not mature enough for deployment, according to the Army Research Laboratory team. Early benchmark testing with the dataset showed that facial recognition algorithms struggled to either identify key facial features or identify unique individual faces from thermal camera images—especially when the normal details visible in faces are reduced to blobby heat patterns. The dataset is described in a paper (PDF) that was presented during the 2021 IEEE Winter Conference on Applications of Computer Vision from 5–9 January..The algorithms also struggled with “off-pose” images in which the person’s face is angled 20 degrees or more away from center. And they had problems matching the visible-light images of individual faces with their thermal imagery counterparts when the person was wearing glasses in one of the images..But several independent experts familiar with facial recognition technology warned against feeling any false sense of security about how such technology is currently struggling to identify faces in the dark. After all, well-documented problems of racial bias, gender bias, and other accuracy issues with facial recognition have not stopped companies and law enforcement agencies from deploying the technology. The broad availability of such a dataset to train facial recognition algorithms to work better in low-light conditions—and not just for military scenarios—could also contribute to the broader surveillance creep of technologies designed to monitor individuals..“Its another of those steppingstones on the way to removing the ability for us to be anonymous at all,” says Benjamin Boudreaux, a policy researcher working on ethics, emerging technology, and international security at the Rand Corporation in Santa Monica, Calif. “We can’t even, if you will, hide in the dark anymore.”.The development of the dataset is related to ongoing work at the Army Research Laboratory aimed at developing automatic facial recognition that can work with the thermal cameras already deployed by military aircraft, drones, ground vehicles, watch towers, and checkpoints. U.S. Army soldiers and other military personnel may also sometimes use body-worn thermal cameras that might someday be coupled with facial recognition..“The intention is not for any of these low-light facial recognition technologies to be used on U.S. citizens inside the continental United States, because that is outside the general purview of the U.S. military,” says Shuowen “Sean” Hu, an electronics engineer at the Army Research Laboratory..Notably, the dataset development was sponsored by the Defense Forensics & Biometrics Agency, which manages a U.S. military database filled with images of faces, fingerprints, DNA, and other biometric information from millions of individuals in countries such as Afghanistan and Iraq, according to documents obtained by OneZero. The agency has also issued contracts to several U.S. companies to develop facial recognition technology that works in the dark—efforts that might benefit from the new training dataset..Some Americans may have a casual attitude toward the idea of the U.S. military deploying facial recognition outside the United States. In 2017, Boudreaux helped conduct a RAND Corporation survey (PDF); 62 percent of American respondents thought it was “ethically permissible” for a robot to use facial recognition at a military checkpoint “to identify and subdue enemy combatants.” (Survey participants skewed more white and male than the overall U.S. population.).But Americans ought not feel complacent about such technology remaining limited to overseas military deployment, says Liz O’Sullivan, technology director for the Surveillance Technology Oversight Project in New York City. She points out that Stingrays, the cell phone tracking devices originally developed for the U.S. military and intelligence agencies, have now become common tools in the hands of U.S. law enforcement agencies..Even if the U.S. military version of facial recognition technology does not find its way home, the dataset could help train similar facial recognition algorithms for companies that have commercial uses in mind for the U.S. market. The Army Research Laboratory has stated a willingness to share the dataset with academia, the private sector, or other government agencies, if outside researchers can show that they are engaged in “valid scientific research” and sign a data-sharing agreement that prevents them from casually uploading and spreading the images around..“It starts with military funding and research, and then it just kind of very quickly proliferates through all the commercial applications a lot faster than it ever used to,” O’Sullivan says. She added that “if you think that this technology is not going to end up in a stadium or in a school, then you just havent been paying attention to history.”.The United States currently has no federal data privacy law restricting the use of facial recognition, despite a series of bipartisan bills aimed at limiting its use by federal law enforcement. Instead, there is a patchwork of local and state laws covering data privacy, such as the California Consumer Privacy Act, or bans on official government use of facial recognition in cities such as Boston, San Francisco, and Oakland..On the military side, the U.S. Department of Defense did formally issue what it described as “five principles for the ethical development of artificial intelligence capabilities” in February 2020. It’s still unclear how the principles would be put into practice—the Army Research Laboratory dataset collection predated the announcement—but their existence indicates the U.S. military’s eagerness to plant its flag in having an “ethical and principled” approach, says Ainikki Riikonen, a research assistant at the Technology and National Security Program at the Center for a New American Security in Washington, D.C..In fact, Riikonen suggested that the U.S. military may be better positioned as an organization to ethically deploy facial recognition and other AI technologies than are local U.S. police departments that may take a more haphazard approach..“The thing that would give me the confidence is that the Department of Defense has the resources and the interest to develop guidelines, and I think maybe a lot of local law enforcement departments dont,” Riikonen says. “Even from an institutional and process standpoint [DoD is] probably better positioned to try to make some functional and principled and less accidental use of this.”.To their credit, the Army Research Laboratory team and its university research partners collected the images for their dataset from adult volunteers under a careful informed consent process with oversight from an Institutional Review Board that holds the responsibility for monitoring biomedical research. That contrasts with the more controversial practices of research groups or companies such as Clearview AI that have scraped social media websites and the public internet to collect images of faces..Still, O’Sullivan noted that the paper describing the new dataset lacked any discussion of the well-known facial recognition biases involving race, gender, and age that have led to real-world performance problems. And Boudreaux questioned whether the new dataset’s collection of faces would accurately reflect the populations in those parts of the world where the U.S. military intends to someday deploy the combination of facial recognition and thermal cameras. Any related performance issues that lead to cases of mistaken identity could potentially have high-stakes and even deadly consequences in a military scenario..The Army Research Laboratory researchers declined to publicize demographic statistics from the dataset, in part because they had decided from the outset to avoid making demographics a major focus of the study. But they described expanding the diversity of such datasets as a goal for future research..Separately, they also acknowledged the limitations of a dataset collected under controlled settings with people sitting just a few meters from the camera—a vastly different scenario from future real-world deployment at military checkpoints or in the field. But they view the dataset as an early step toward helping the research community extend facial recognition into the thermal imagery domain. And they hope future datasets could include images of faces captured by thermal imagery at longer ranges and in less laboratory-like conditions..“In the future, once this technology is mature enough, we want the distance to be much longer,” Hu says. “Ideally tens of meters, or 100 or 200 meters.”.A long-distance facial recognition technology that also works in the dark would undoubtedly prove incredibly useful for the U.S. military and other militaries. But any perfected version of such technology in the future could also open the door to what experts see as a disturbing erosion of individual privacy and ultimately liberty—a phenomenon already evident in how certain minority populations have been specifically targeted for government surveillance in countries around the world..Even an imperfect version of such facial recognition coupled with other biometric surveillance methods could shrink the space of surveillance-free movement and activity for individuals. And as higher-end thermal cameras become more affordable for customers beyond the U.S. military in the coming decade, more companies and law enforcement agencies could someday add facial recognition in the dark to their surveillance capabilities..“As much as we feel comfortable that facial recognition has the problems it does today, if we were to grab three or four different unchangeable and readable-at-a-distance traits to combine them, that would be a pretty inescapable panopticon,” O’Sullivan says. “And it would be a really distressing moment for civil liberties around the world.”.Editor’s Note: The original article incorrectly stated that the Army Research Laboratory is based in White Oak, Maryland. It is located in Adelphi, Maryland..Jeremy Hsu has been working as a science and technology journalist in New York City since 2008. He has written on subjects as diverse as supercomputing and wearable electronics for IEEE Spectrum. When he’s not trying to wrap his head around the latest quantum computing news for Spectrum, he also contributes to a variety of publications such as Scientific American, Discover, Popular Science, and others. He is a graduate of New York University’s Science, Health & Environmental Reporting Program..At the start of each year, IEEE Spectrum attempts to predict the future. It can be tricky, but we do our best, filling the January issue with a couple of dozen reports, short and long, about developments the editors expect to make news in the coming year. .This isn’t hard to do when the project has been in the works for a long time and is progressing on schedule—the coming first flight of NASA’s Space Launch System, for example. For other stories, we must go farther out on a limb. A case in point: the description of a hardware wallet for Bitcoin that the company formerly known as Square (which recently changed its name to Block) is developing but won’t officially comment on. One thing we can predict with confidence, though, is that Spectrum readers, familiar with the vicissitudes of technical development work, will understand if some of these projects don’t, in fact, pan out. That’s still okay. ","The U.S. Army Research Laboratory has developed a dataset of faces to train facial recognition that works in darkness. “It's another of those steppingstones on the way to removing the ability for us to be anonymous at all,” says Benjamin Boudreaux, a policy researcher working on ethics, emerging technology, and international security at the Rand Corporation in Santa Monica, Calif. “We can’t even, if you will, hide in the dark anymore.”"
403,"Inari wants to engineer crops that require less water, fertilizer, pesticides and land. The company is focusing on soybeans and corn as its first crops because they require 300 million acres of land in North and South America to grow. Inari’s SEEDesign platform can increase soybean and corn yield by 20% while lowering water usage by 40% and reducing corn’s nitrogen needs by 40%..Genetically modifying crops can involve removing or knocking out unwanted genes for a specific purpose, such as reducing the need for pesticides. Genetic modification can also artificially insert genes into a plant. Usually, bacteria are used to deliver the new genes..The introduction of foreign genes into plants is one of the main controversies. However, Inari is not introducing outside genes into its crops. Instead, the company is altering genes that already exist in the plants through gene editing..Its SEEDesign platform uses predictive design and artificial intelligence (AI) to understand the genetics of plants and makes blueprints for its gene-editing tools that can make multiple edits in a single genome. The edits can make crop seeds with higher yields and lower agricultural footprints.",Inari's SEEDesign platform uses predictive design and artificial intelligence (AI) to understand the genetics of plants and makes blueprints for its gene-editing tools that can make multiple edits in a single genome. The edits can make crop seeds with higher yields and lower agricultural footprints.'
404,"A change to TikTok’s U.S. privacy policy on Wednesday introduced a new section that says the social video app “may collect biometric identifiers and biometric information” from its users’ content. This includes things like “faceprints and voiceprints,” the policy explained. Reached for comment, TikTok could not confirm what product developments necessitated the addition of biometric data to its list of disclosures about the information it automatically collects from users, but said it would ask for consent in the case such data collection practices began..The biometric data collection details were introduced in the newly added section, “Image and Audio Information,” found under the heading of “Information we collect automatically” in the policy..The first part of the new section explains that TikTok may collect information about the images and audio that are in users’ content, “such as identifying the objects and scenery that appear, the existence and location within an image of face and body features and attributes, the nature of the audio, and the text of the words spoken in your User Content.”.While that may sound creepy, other social networks do object recognition on images you upload to power accessibility features (like describing what’s in an Instagram photo, for example), as well as for ad targeting purposes. Identifying where a person and the scenery is can help with AR effects, while converting spoken words to text helps with features like TikTok’s automatic captions..The policy also notes this part of the data collection is for enabling “special video effects, for content moderation, for demographic classification, for content and ad recommendations, and for other non-personally-identifying operations,” it says..We may collect biometric identifiers and biometric information as defined under US laws, such as faceprints and voiceprints, from your User Content. Where required by law, we will seek any required permissions from you prior to any such collection..The statement itself is vague, as it doesn’t specify whether it’s considering federal law, states laws, or both. It also doesn’t explain, as the other part did, why TikTok needs this data. It doesn’t define the terms “faceprints” or “voiceprints.” Nor does it explain how it would go about seeking the “required permissions” from users, or if it would look to either state or federal laws to guide that process of gaining consent..That’s important because as it stands today, only a handful of U.S. states have biometric privacy laws, including Illinois, Washington, California, Texas and New York. If TikTok only requested consent, “where required by law,” it could mean users in other states would not have to be informed about the data collection..Reached for comment, a TikTok spokesperson could not offer more details on the company’s plans for biometric data collection or how it may tie in to either current or future products..“As part of our ongoing commitment to transparency, we recently updated our Privacy Policy to provide more clarity on the information we may collect,” the spokesperson said..The company also pointed us to an article about its approach to data security, TikTok’s latest Transparency Report and the recently launched privacy and security hub, which is aimed at helping people better understand their privacy choices on the app..Under the Trump administration, the federal government attempted to ban TikTok from operating in the U.S. entirely, calling the app a national security threat because of its ownership by a Chinese company. TikTok fought back against the ban and went on record to state it only stores TikTok U.S. user data in its U.S. data centers and in Singapore..It said it has never shared TikTok user data with the Chinese government nor censored content, despite being owned by Beijing-based ByteDance. And it said it would never do so, if asked..Though the TikTok ban was initially stopped in the courts, the federal government appealed the rulings. But when President Biden took office, his administration put the appeal process on hold as it reviewed the actions taken by his predecessor. And although Biden has, as of today, signed an executive order to restrict U.S. investment in Chinese firms linked to surveillance, his administration’s position on TikTok remains unclear..It is worth noting, however, that the new disclosure about biometric data collection follows a $92 million settlement in a class action lawsuit against TikTok, originally filed in May 2020, over the social media app’s violation of Illinois’ Biometric Information Privacy Act. The consolidated suit included more than 20 separate cases filed against TikTok over the platform’s collection and sharing of the personal and biometric information without user consent. Specifically, this involved the use of facial filter technology for special effects..In that context, TikTok’s legal team may have wanted to quickly cover themselves from future lawsuits by adding a clause that permits the app to collect personal biometric data..The disclosure, we should also point out, has only been added to the U.S. Privacy Policy, as other markets like the EU have stricter data protection and privacy laws..The new section was part of a broader update to TikTok’s Privacy Policy, which included other changes both large and small, ranging from corrections of earlier typos to revamped or even entirely new sections. Most of these tweaks and changes could be easily explained, though — like new sections that clearly referenced TikTok’s e-commerce ambitions or adjustments aimed at addressing the implications of Apple’s App Tracking Transparency on targeted advertising..This is in addition to the “Information you choose to provide,” which comes from when you register, contact TikTok or upload content. In that case, TikTok collects your registration info (username, age, language, etc.), profile info (name, photo, social media accounts), all your user-generated content on the platform, your phone and social network contacts, payment information, plus the text, images and video found in the device’s clipboard. (TikTok, as you may recall, got busted by Apple’s iOS 14 feature that alerted users to the fact that TikTok and other apps were accessing iOS clipboard content. Now, the policy says TikTok “may collect” clipboard data “with your permission.”).Some users reported seeing a pop-up message alerting them to the Privacy Policy update, but the page was not available when they tried to read it. Others complained of seeing the pop-up repeatedly. This issue doesn’t appear to be universal. In tests, we did not have an issue with the pop-up ourselves.","A change to TikTok’s U.S. privacy policy on Wednesday introduced a new section that says the social video app “may collect biometric identifiers and biometric information” from its users’ content. This includes things like “faceprints and voiceprints,” the policy explained."
407,"The biggest electronic health record company in the United States, Epic Systems, claims it can solve a major problem for hospitals: identifying signs of sepsis, an often deadly complication from infections that can lead to organ failure. It’s a leading cause of death in hospitals. .But the algorithm doesn’t work as well as advertised, according to a new study published in JAMA Internal Medicine on Monday. Epic says its alert system can correctly differentiate patients who do and don’t have sepsis 76 percent of the time. The new study found it was only right 63 percent of the time. .Sepsis is hard to spot early, but starting treatment as soon as possible can improve patients chances of survival. The Epic system, and other automated warning tools like it, scan patient test results for signals that someone could be developing the condition. Around a quarter of US hospitals use Epic’s electronic medical records, and hundreds of hospitals use its sepsis prediction tool, including the health center at the University of Michigan, where study author Karandeep Singh is an assistant professor. .The study examined data from nearly 40,000 hospitalizations at Michigan Medicine in 2018 and 2019. Patients developed sepsis in 2,552 of those hospitalizations. Epic’s sepsis tool missed 1,709 of those cases, around two-thirds of which were still identified and treated quickly. It only identified 7 percent of sepsis cases that were missed by a physician. The analysis also found a high rate of false positives: when an alert went off for a patient, there was only a 12 percent chance that the patient actually would develop sepsis. .Part of the problem, Singh told Stat News, seemed to be in the way the Epic algorithm was developed. The algorithm used information on bills for sepsis to define which patients had sepsis. That means it’s catching cases where the doctor already thinks there’s an issue. “It’s essentially trying to predict what physicians are already doing,” Singh said. It’s also not the measure of sepsis that researchers would ordinarily use. .Tools that mine patient data to predict what could happen with their health are common and can be useful for doctors. But they’re only as good as the data they’re developed with, and they should be subject to outside evaluation. When researchers scrutinize tools like this one, they sometimes find holes: for example, one algorithm used by major health systems to flag patients who need special attention was biased against Black patients, a 2019 study found. .Epic rolled out another predictive tool, called the Deterioration Index, during the early days of the COVID-19 pandemic. It was designed to help doctors decide which patients should move into intensive care and which could be fine without it. The pandemic was an emergency, so hospitals around the country started using it before it was subject to any sort of independent evaluation. Even now, there has been limited research on the tool. One small study showed it could identify high- and low-risk patients but might not be useful to doctors. There could be unforeseen problems or biases in the system that are going unnoticed, Brown University researchers warned in Undark..If digital tools are going to live up to their potential in healthcare, companies like Epic should be transparent about how they’re made and they should be regularly monitored to make sure they’re working well, Singh says on Twitter. These tools are becoming more and more common, so these types of issues aren’t going away, Roy Adams, an assistant professor at Johns Hopkins School of Medicine, told Wired. “We need more independent evaluations of these proprietary systems,” he says..Correction June 28th, 5:58PM ET: The original version of this story suggested the algorithm defined sepsis based on when doctors submit a bill for treatment. Instead, the algorithm uses information from billing codes to define sepsis. We regret the error.","The study examined data from nearly 40,000 hospitalizations at Michigan Medicine in 2018 and 2019. Patients developed sepsis in 2,552 of those hospitalizations. Epic’s sepsis tool missed 1,709 of those cases, around two-thirds of which were still identified and treated quickly. It only identified 7 percent of sepsis cases that were missed by a physician. The analysis also found a high rate of false positives: when an alert went off for a patient, there was only a 12 percent chance that the patient actually would develop sepsis."
415,"Metaspectral, a company offering technology that derives insights from AI using ultra-high-resolution, visible-to-infrared (hyperspectral) imagery, has been awarded more than $300,000 in grant funding from the CleanBC Plastics Action Fund. The fund is funded by the BC Government and administered by Alacrity Cleantech..The CleanBC Plastics Action Fund supports B.C. businesses creating value from used plastics by including more recycled material in product manufacturing to keep plastic out of landfills..Metraspectral will use this funding for the development of computer vision, artificial intelligence, and robotics designed to sort consumer waste, increase efficiency in processing materials and improve the quality of post-consumer recycled plastic. The project is slated for completion by Dec. 31, 2021..The company says that, by using ultra-high-resolution hyperspectral imaging, AI is able to efficiently distinguish among types of plastics for accurate and easy sorting, noting that it’s impossible for humans to differentiate between different types of clear plastic bottles with the naked eye..The company hopes this technology will support the circular economy for plastics and stimulate more local processing capacity for recycling as more manufacturers begin using the higher-quality recycled plastics. Metaspectral’s technology will be an important ally in achieving those objectives.  Metraspectral will also be contributing to the Government of Canada’s Greening Government strategy of increasing the ratio of plastics that are recycled to 75% by 2030, up from 9% today..Contents: -What are #PPAs & #VPPAs -A case study of a recent VPPA project selection -Offer best practices in soliciting proposals -Explain cash flows in VPPAs, & more .Download your copy to learn: -How leaders can navigate the current landscape -Understand investor motivations -Evaluate strategic options to attract ESG investors to your business ","Metaspectral, a company offering technology that derives insights from AI using ultra-high-resolution, visible-to-infrared (hyperspectral) imagery, has been awarded more than $300,000 in grant funding from the CleanBC Plastics Action Fund. The fund is funded by the BC Government and administered by Alacrity Cleantech."
416,"If you have ever wondered how your smartphone can comprehend instructions like “Call Mom,” “Send a Message to Boss,” “Play the Latest Songs,” “Switch ON the AC,” then you are not alone. But how is this done? The one simple answer is Speech Recognition. Speech Recognition has gone through the roof in the recent 4-5 years and is making our lives more comfortable every day. .Speech Recognition was first introduced by IBM in 1962 when it unveiled the first machine capable of converting human voice to text. Today, powered by the latest technologies like Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning, speech recognition is touching new milestones. .This latest technological advancement is being used across the globe by top companies to make their user’s experience efficient and smooth. Technologies like Amazon’s Alexa, Apple’s Siri, Google Assistant, Google Speech, Google Dictate, Facebook’s Oculus VR, and Microsoft’s Cortana are all examples of Speech Recognition. .The expanding usage of speech-to-text technologies has also opened many new job domains, and students are wonderfully exploiting them. Many students are now joining courses like PGP in AI and Machine Learning after completing their graduation to improve their prospects. The high salary package of around INR 15 lakh for freshers is the 2nd biggest reason attracting students towards this, the biggest reason being the fantastic job role. .Speech Recognition was a very niche domain before the advent of AI and ML, which has completely transformed it now. Before we understand how AI and ML made changes, let’s understand the nuances of what all these terminologies are. .Artificial Intelligence is the technology by which machines become capable of demonstrating intelligence like humans or animals. Initially, AI was only about memorizing data and producing results accordingly; however, now it is much more than that as machines perform various activities like Speech Recognition, Object Recognition, Translating Texts, and a lot more. .Another latest addition to AI has been Deep Learning. With the help of Deep Learning, machines can process data and create patterns that help them make valuable decisions. This behavior of a machine through Deep Learning is similar to the behavior of a human brain. Deep Learning activities can be “Supervised,” “Semi-Supervised,” as well as “Unsupervised.” .Machine Learning is a subdomain of AI which teaches machines to memorize past events and activities. Through ML, machines are trained to retain various data sets’ information and outputs and identify patterns in these decisions. It allows the machine to learn by itself without the help of any programming code. .An example of Machine Learning is the e-Commerce websites suggesting products to you. The code, once written, allows machines to evolve on themselves and analyze user behavior and thus recommend products according to their preferences and past purchases. This involves Zero Human Interference and makes use of approaches like Artificial Neural Networks (ANN). .Note: Speech Recognition and Voice Recognition are two different things. While the former comprehends a voice sample and converts it into a text sample, the sole purpose of the latter is to identify the voice and recognize to whom it belongs. Voice Recognition is often used for security and authenticity purposes. .The usage of Speech Recognition in our devices has grown considerably due to the developments in AI and ML technologies. Speech Recognition is now being used for tasks ranging from awakening your appliances and gadgets to monitoring your fitness, playing mood-booster songs, running queries on search engines, and even making phone calls. .The global market for Speech Recognition, currently growing at a Cumulative Annual Growth Rate (CAGR) of 17.2%, is expected to breach the $25 billion mark by 2025. However, there were enormous challenges initially that have been tackled with the use of AI and ML now. .When in its initial phase, some of the biggest challenges for Speech Recognition were Poor Voice Recording Devices, Huge Noise in the Voice Samples, Different Pitches in Speech of the Same User, etc. In addition to this, the changing dialects and grammatical factors like Homonyms were also a big challenge. .With the help of AI programs capable of filtering sound, canceling noise, and identifying the meaning of words depending on the context, most of these challenges have been tackled. Today, Speech Recognition shows an efficiency of 95%, which stood at less than 20% around 30 years back from now. The only biggest challenge remaining now for programmers is making machines capable of understanding emotions and feelings and satisfactory progress in this part. .The increasing efficiency in Speech Recognition is becoming an essential driving factor in its success, and top tech giants are leveraging these benefits. More than 20% of users searched on Google through Voice in 2016 only, and this number is expected to be far more prominent now. Businesses today are automating their services to make their operations efficient and introducing Speech Recognition facilities at the top of their to-do lists. .Speech Recognition is no doubt one of the best innovations made by expanding technological developments. However, there is one thing to be noted if you are also planning to enter this sector. The domain is inter-mingled, and the mere knowledge provided by a Speech Recognition course won’t be enough for you to survive in this field. .Therefore, it is essential that you also sharpen your skills in allied concepts like Data Science, Data Analytics, Machine Learning, Artificial Intelligence, Neural Networks, DevOps, and Deep Learning. So what are you waiting for now? Hurry up and join an online course in Speech Recognition now! ","With the help of AI programs capable of filtering sound, canceling noise, and identifying the meaning of words depending on the context, most of these challenges have been tackled. Today, Speech Recognition shows an efficiency of 95%, which stood at less than 20% around 30 years back from now. The only biggest challenge remaining now for programmers is making machines capable of understanding emotions and feelings and satisfactory progress in this part. "
417,"More than 30 civil liberties groups are accusing the UK Home Office and police of bypassing Parliament to introduce live facial recognition technology (LFRT). .Earlier this month — during the parliamentary summer recess — the College of Policing released guidance on the use of LFRT without any publicity or official announcement. .The Surveillance Camera Code of Practice allows police services and councils to collect camera footage and compare it with a database of people on a watch-list. However, a large group of civil liberties organizations including Liberty, Amnesty International and Privacy International says that LFRT poses significant risks to civil society. .In a democratic society, it is imperative that intrusive technologies are subject to effective scrutiny. Police and the Home Office have, so far, completely bypassed Parliament on the matter of LFRT, they write. .We are not aware of any intention to subject LFRT plans to parliamentary consideration, despite the intrusiveness of this technology, its highly controversial use over a number of years, and the dangers associated with its use. .LFRT doesnt have a good track record in the UK. Last year, the Metropolitan Police used what it called a tried and tested technology on three occasions in the capital. .However, it later emerged that, for example, when 8,600 faces were scanned in Oxford Circus, there were eight positive hits - but that only one was legitimate. .After the technology was used in South Wales, it was ruled unlawful following a legal challenge from ma man whose face was scanned, with judges ruling that the police were allowed too much discretion over its use. .And just two months ago, UK Information Commissioner Elizabeth Denham warned that LFRT had the potential to be used inappropriately, excessively or even recklessly, adding that none of the deployments she examined was fully compliant with data protection law. .The implications come not solely from privacy and data protection perspectives, but from the larger ethical question for a democratic society permitting and seemingly condoning the rollout of such intrusive technology, they say. ","More than 30 civil liberties groups are accusing the UK Home Office and police of bypassing Parliament to introduce live facial recognition technology (LFRT).

Earlier this month — during the parliamentary summer recess — the College of Policing released guidance on the use of LFRT without any publicity or official announcement."
419,"One of the NHS’s leading hospital trusts has begun using artificial intelligence to help detect cancer in the gullet, which kills 8,000 Britons a year. It is hoped the technology will increase the number of cases of cancer in the oesophagus that doctors spot..Oesophageal cancer is one of the deadliest forms of cancer. It is hard to detect, particularly in its early stages, and many people who get it die soon after their diagnosis. Fewer than one in five of those diagnosed are still alive five years later..Developers of the software, called CADU, say it is the first in the world to use artificial intelligence technology to help doctors identify cancerous cells in the food pipe. It analyses pictures captured by a tiny camera put down a patient’s throat during an endoscopy, which is used to diagnose oesophageal cancer and highlight areas of concern to the medic..The software in effect gives the gastroenterologist conducting the endoscopy a second opinion. It is hoped that it will reduce the high number of oesophageal cancers – up to 25% – that doctors miss during an endoscopy because early signs of the disease are very difficult to spot..The scientists and doctors who invented CADU believe that better identification of oesophageal cancer while it is still at an early stage will lead to patients receiving prompt treatment and thus having a better chance of survival. About 9,000 people a year in the UK are diagnosed with oesophageal cancer, which is linked to smoking and drinking..University College London hospital trust started using CADU in February, after it was approved by the Medicines and Healthcare products Regulatory Agency, to improve its diagnosis rates..“We know from our data that between 15% and 20% of patients can have an early cancer develop within 12 months of an initial ‘normal’ endoscopy, suggesting that perhaps this was overlooked at the first examination or was very subtle,” said Dr Rehan Haidry, a consultant gastroenterologist at UCLH..CADU was developed by experts from UCLH and University College London (UCL) working in conjunction with a UCL spinout technology firm called Odin Vision..Peter Mountney, the firm’s chief executive and an honorary associate professor at UCL, said: “We are very excited to achieve this landmark procedure and use our AI technology to support doctors in the fight against one of the most aggressive forms of cancer..“CADU shows promise at a clinically meaningful level and the next stage is to validate that in a larger multi-centre trial involving a number of hospitals in several countries.”.The CADU device can aid diagnosis because it has been shown hundreds of thousands of images of diseased tissue and has learned to spot cancerous growths from the visual patterns in those images. Experts in oesophageal cancer said the technology could prove to be a breakthrough..“Harnessing the power of artificial intelligence in combination with standard diagnostic procedures such as endoscopies offers the potential to detect cancers earlier,” said Nicola Valeri, a professor of gastrointestinal oncology at the Institute of Cancer Research..The NHS in England is increasingly exploring how AI can help diagnose, monitor and treat medical conditions such as sepsis, cancer and Parkinson’s disease. Its NHS AI Lab has provided £140m for trials of software that may be able to analyse breast cancer screening scans and assess people who have just had a stroke, for example..Sir Simon Stevens, NHS England’s chief executive, said last year that while AI was still in its infancy, “when the latest chapter in the history of medicine comes to be written, AI in healthcare will doubtless rank alongside earlier advances such as the stethoscope, the X-ray and the blood test.”.NHS hospitals in Leicester, Nottingham and Oxford have been assessing AI software called the virtual nodule clinic, which its developers hope will help doctors diagnose lung cancer, Britain’s biggest cancer killer. Its early results have led to it receiving a £1.5m grant for the technology to be used in a research project involving 10 hospitals to see if it aids early detection.",The software in effect gives the gastroenterologist conducting the endoscopy a second opinion. It is hoped that it will reduce the high number of oesophageal cancers – up to 25% – that doctors miss during an endoscopy because early signs of the disease are very difficult to spot.
